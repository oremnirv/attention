{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.gaussian_process as gp\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gp_prior(4, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "#         self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_hidden, encoder_out):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(decoder_hidden_with_time_axis) + self.W2(encoder_out)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * encoder_out\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, dec_inp, dec_hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(dec_hidden, enc_output)\n",
    "        \n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), tf.expand_dims(dec_inp, 1)], \n",
    "                      axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(32, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_object = tf.keras.losses.mean_squared_error\n",
    "\n",
    "def loss_function(real, pred):\n",
    "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.keras.losses.mean_squared_error(real, pred)\n",
    "\n",
    "#     # Casts a tensor to a new type.\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#     # equivalent to element-wise multiplication \n",
    "#     loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = './training_checkpoints'\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "#                                  encoder=encoder,\n",
    "#                                  decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# french_tr is the input to decoder\n",
    "# \n",
    "@tf.function\n",
    "def train_step(eng_tr, fren_tr, y_fren_tr, init_enc_hidden):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(eng_tr, init_enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # basically same behavior as reshape\n",
    "        dec_input = tf.expand_dims(fren_tr, 1)\n",
    "        \n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "#         for t in range(1, targ.shape[1]):\n",
    "          # passing enc_output to the decoder\n",
    "          # pay attention that the decoder returns a new dec_hidden (which is the state of the GRU)\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "        loss = loss_function(y_fren_tr, predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "#             dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "#     batch_loss = (loss / int(y_fren_tr.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    batch_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(eng_te, fren_te, y_fren_te, batch_s):\n",
    "    init_enc_hidden = encoder.initialize_hidden_state(batch_s)\n",
    "    enc_output, enc_hidden = encoder(eng_te, init_enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims(fren_te, 1)\n",
    "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "    loss = loss_function(y_fren_te, predictions)\n",
    "    test_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = tf.keras.metrics.Mean(name='batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.8388\n",
      "Epoch 1 Batch 0 test Loss 1.1163\n",
      "Epoch 1 Batch 50 Loss 0.7517\n",
      "Epoch 1 Batch 50 test Loss 1.1166\n",
      "Epoch 1 Batch 100 Loss 0.7596\n",
      "Epoch 1 Batch 100 test Loss 1.1169\n",
      "Epoch 1 Batch 150 Loss 0.7585\n",
      "Epoch 1 Batch 150 test Loss 1.1171\n",
      "Epoch 1 Batch 200 Loss 0.7584\n",
      "Epoch 1 Batch 200 test Loss 1.1174\n",
      "Epoch 1 Batch 250 Loss 0.7545\n",
      "Epoch 1 Batch 250 test Loss 1.1177\n",
      "Epoch 1 Batch 300 Loss 0.7520\n",
      "Epoch 1 Batch 300 test Loss 1.1180\n",
      "Epoch 1 Batch 350 Loss 0.7545\n",
      "Epoch 1 Batch 350 test Loss 1.1183\n",
      "Epoch 1 Batch 400 Loss 0.7552\n",
      "Epoch 1 Batch 400 test Loss 1.1186\n",
      "Epoch 1 Batch 450 Loss 0.7543\n",
      "Epoch 1 Batch 450 test Loss 1.1189\n",
      "Epoch 1 Batch 500 Loss 0.7538\n",
      "Epoch 1 Batch 500 test Loss 1.1191\n",
      "Epoch 1 Batch 550 Loss 0.7548\n",
      "Epoch 1 Batch 550 test Loss 1.1194\n",
      "Epoch 1 Batch 600 Loss 0.7561\n",
      "Epoch 1 Batch 600 test Loss 1.1197\n",
      "Epoch 1 Batch 650 Loss 0.7557\n",
      "Epoch 1 Batch 650 test Loss 1.1200\n",
      "Epoch 1 Batch 700 Loss 0.7556\n",
      "Epoch 1 Batch 700 test Loss 1.1203\n",
      "Epoch 1 Batch 750 Loss 0.7553\n",
      "Epoch 1 Batch 750 test Loss 1.1206\n",
      "Epoch 1 Batch 800 Loss 0.7559\n",
      "Epoch 1 Batch 800 test Loss 1.1208\n",
      "Epoch 1 Batch 850 Loss 0.7549\n",
      "Epoch 1 Batch 850 test Loss 1.1211\n",
      "Epoch 1 Batch 900 Loss 0.7547\n",
      "Epoch 1 Batch 900 test Loss 1.1214\n",
      "Epoch 1 Batch 950 Loss 0.7547\n",
      "Epoch 1 Batch 950 test Loss 1.1217\n",
      "Epoch 2 Batch 0 Loss 0.7958\n",
      "Epoch 2 Batch 0 test Loss 1.1220\n",
      "Epoch 2 Batch 50 Loss 0.7265\n",
      "Epoch 2 Batch 50 test Loss 1.1222\n",
      "Epoch 2 Batch 100 Loss 0.7440\n",
      "Epoch 2 Batch 100 test Loss 1.1225\n",
      "Epoch 2 Batch 150 Loss 0.7365\n",
      "Epoch 2 Batch 150 test Loss 1.1227\n",
      "Epoch 2 Batch 200 Loss 0.7425\n",
      "Epoch 2 Batch 200 test Loss 1.1230\n",
      "Epoch 2 Batch 250 Loss 0.7442\n",
      "Epoch 2 Batch 250 test Loss 1.1232\n",
      "Epoch 2 Batch 300 Loss 0.7459\n",
      "Epoch 2 Batch 300 test Loss 1.1235\n",
      "Epoch 2 Batch 350 Loss 0.7468\n",
      "Epoch 2 Batch 350 test Loss 1.1238\n",
      "Epoch 2 Batch 400 Loss 0.7451\n",
      "Epoch 2 Batch 400 test Loss 1.1241\n",
      "Epoch 2 Batch 450 Loss 0.7433\n",
      "Epoch 2 Batch 450 test Loss 1.1244\n",
      "Epoch 2 Batch 500 Loss 0.7417\n",
      "Epoch 2 Batch 500 test Loss 1.1246\n",
      "Epoch 2 Batch 550 Loss 0.7402\n",
      "Epoch 2 Batch 550 test Loss 1.1249\n",
      "Epoch 2 Batch 600 Loss 0.7398\n",
      "Epoch 2 Batch 600 test Loss 1.1252\n",
      "Epoch 2 Batch 650 Loss 0.7384\n",
      "Epoch 2 Batch 650 test Loss 1.1255\n",
      "Epoch 2 Batch 700 Loss 0.7392\n",
      "Epoch 2 Batch 700 test Loss 1.1257\n",
      "Epoch 2 Batch 750 Loss 0.7409\n",
      "Epoch 2 Batch 750 test Loss 1.1260\n",
      "Epoch 2 Batch 800 Loss 0.7409\n",
      "Epoch 2 Batch 800 test Loss 1.1263\n",
      "Epoch 2 Batch 850 Loss 0.7399\n",
      "Epoch 2 Batch 850 test Loss 1.1266\n",
      "Epoch 2 Batch 900 Loss 0.7388\n",
      "Epoch 2 Batch 900 test Loss 1.1268\n",
      "Epoch 2 Batch 950 Loss 0.7393\n",
      "Epoch 2 Batch 950 test Loss 1.1271\n",
      "Epoch 3 Batch 0 Loss 0.5882\n",
      "Epoch 3 Batch 0 test Loss 1.1274\n",
      "Epoch 3 Batch 50 Loss 0.7766\n",
      "Epoch 3 Batch 50 test Loss 1.1276\n",
      "Epoch 3 Batch 100 Loss 0.7603\n",
      "Epoch 3 Batch 100 test Loss 1.1279\n",
      "Epoch 3 Batch 150 Loss 0.7472\n",
      "Epoch 3 Batch 150 test Loss 1.1282\n",
      "Epoch 3 Batch 200 Loss 0.7495\n",
      "Epoch 3 Batch 200 test Loss 1.1284\n",
      "Epoch 3 Batch 250 Loss 0.7439\n",
      "Epoch 3 Batch 250 test Loss 1.1287\n",
      "Epoch 3 Batch 300 Loss 0.7382\n",
      "Epoch 3 Batch 300 test Loss 1.1290\n",
      "Epoch 3 Batch 350 Loss 0.7415\n",
      "Epoch 3 Batch 350 test Loss 1.1293\n",
      "Epoch 3 Batch 400 Loss 0.7427\n",
      "Epoch 3 Batch 400 test Loss 1.1295\n",
      "Epoch 3 Batch 450 Loss 0.7413\n",
      "Epoch 3 Batch 450 test Loss 1.1298\n",
      "Epoch 3 Batch 500 Loss 0.7415\n",
      "Epoch 3 Batch 500 test Loss 1.1301\n",
      "Epoch 3 Batch 550 Loss 0.7418\n",
      "Epoch 3 Batch 550 test Loss 1.1303\n",
      "Epoch 3 Batch 600 Loss 0.7407\n",
      "Epoch 3 Batch 600 test Loss 1.1306\n",
      "Epoch 3 Batch 650 Loss 0.7409\n",
      "Epoch 3 Batch 650 test Loss 1.1309\n",
      "Epoch 3 Batch 700 Loss 0.7409\n",
      "Epoch 3 Batch 700 test Loss 1.1311\n",
      "Epoch 3 Batch 750 Loss 0.7393\n",
      "Epoch 3 Batch 750 test Loss 1.1314\n",
      "Epoch 3 Batch 800 Loss 0.7390\n",
      "Epoch 3 Batch 800 test Loss 1.1316\n",
      "Epoch 3 Batch 850 Loss 0.7374\n",
      "Epoch 3 Batch 850 test Loss 1.1319\n",
      "Epoch 3 Batch 900 Loss 0.7387\n",
      "Epoch 3 Batch 900 test Loss 1.1321\n",
      "Epoch 3 Batch 950 Loss 0.7376\n",
      "Epoch 3 Batch 950 test Loss 1.1324\n",
      "Epoch 4 Batch 0 Loss 0.6947\n",
      "Epoch 4 Batch 0 test Loss 1.1327\n",
      "Epoch 4 Batch 50 Loss 0.7054\n",
      "Epoch 4 Batch 50 test Loss 1.1329\n",
      "Epoch 4 Batch 100 Loss 0.7251\n",
      "Epoch 4 Batch 100 test Loss 1.1332\n",
      "Epoch 4 Batch 150 Loss 0.7264\n",
      "Epoch 4 Batch 150 test Loss 1.1334\n",
      "Epoch 4 Batch 200 Loss 0.7303\n",
      "Epoch 4 Batch 200 test Loss 1.1337\n",
      "Epoch 4 Batch 250 Loss 0.7284\n",
      "Epoch 4 Batch 250 test Loss 1.1339\n",
      "Epoch 4 Batch 300 Loss 0.7265\n",
      "Epoch 4 Batch 300 test Loss 1.1342\n",
      "Epoch 4 Batch 350 Loss 0.7280\n",
      "Epoch 4 Batch 350 test Loss 1.1345\n",
      "Epoch 4 Batch 400 Loss 0.7289\n",
      "Epoch 4 Batch 400 test Loss 1.1347\n",
      "Epoch 4 Batch 450 Loss 0.7299\n",
      "Epoch 4 Batch 450 test Loss 1.1350\n",
      "Epoch 4 Batch 500 Loss 0.7289\n",
      "Epoch 4 Batch 500 test Loss 1.1352\n",
      "Epoch 4 Batch 550 Loss 0.7269\n",
      "Epoch 4 Batch 550 test Loss 1.1355\n",
      "Epoch 4 Batch 600 Loss 0.7275\n",
      "Epoch 4 Batch 600 test Loss 1.1358\n",
      "Epoch 4 Batch 650 Loss 0.7277\n",
      "Epoch 4 Batch 650 test Loss 1.1360\n",
      "Epoch 4 Batch 700 Loss 0.7278\n",
      "Epoch 4 Batch 700 test Loss 1.1363\n",
      "Epoch 4 Batch 750 Loss 0.7264\n",
      "Epoch 4 Batch 750 test Loss 1.1366\n",
      "Epoch 4 Batch 800 Loss 0.7273\n",
      "Epoch 4 Batch 800 test Loss 1.1368\n",
      "Epoch 4 Batch 850 Loss 0.7274\n",
      "Epoch 4 Batch 850 test Loss 1.1371\n",
      "Epoch 4 Batch 900 Loss 0.7278\n",
      "Epoch 4 Batch 900 test Loss 1.1374\n",
      "Epoch 4 Batch 950 Loss 0.7281\n",
      "Epoch 4 Batch 950 test Loss 1.1376\n",
      "Epoch 5 Batch 0 Loss 0.7436\n",
      "Epoch 5 Batch 0 test Loss 1.1378\n",
      "Epoch 5 Batch 50 Loss 0.7199\n",
      "Epoch 5 Batch 50 test Loss 1.1381\n",
      "Epoch 5 Batch 100 Loss 0.7282\n",
      "Epoch 5 Batch 100 test Loss 1.1383\n",
      "Epoch 5 Batch 150 Loss 0.7176\n",
      "Epoch 5 Batch 150 test Loss 1.1386\n",
      "Epoch 5 Batch 200 Loss 0.7172\n",
      "Epoch 5 Batch 200 test Loss 1.1388\n",
      "Epoch 5 Batch 250 Loss 0.7159\n",
      "Epoch 5 Batch 250 test Loss 1.1391\n",
      "Epoch 5 Batch 300 Loss 0.7144\n",
      "Epoch 5 Batch 300 test Loss 1.1394\n",
      "Epoch 5 Batch 350 Loss 0.7131\n",
      "Epoch 5 Batch 350 test Loss 1.1396\n",
      "Epoch 5 Batch 400 Loss 0.7164\n",
      "Epoch 5 Batch 400 test Loss 1.1399\n",
      "Epoch 5 Batch 450 Loss 0.7175\n",
      "Epoch 5 Batch 450 test Loss 1.1401\n",
      "Epoch 5 Batch 500 Loss 0.7185\n",
      "Epoch 5 Batch 500 test Loss 1.1404\n",
      "Epoch 5 Batch 550 Loss 0.7170\n",
      "Epoch 5 Batch 550 test Loss 1.1407\n",
      "Epoch 5 Batch 600 Loss 0.7183\n",
      "Epoch 5 Batch 600 test Loss 1.1409\n",
      "Epoch 5 Batch 650 Loss 0.7168\n",
      "Epoch 5 Batch 650 test Loss 1.1412\n",
      "Epoch 5 Batch 700 Loss 0.7154\n",
      "Epoch 5 Batch 700 test Loss 1.1415\n",
      "Epoch 5 Batch 750 Loss 0.7157\n",
      "Epoch 5 Batch 750 test Loss 1.1417\n",
      "Epoch 5 Batch 800 Loss 0.7171\n",
      "Epoch 5 Batch 800 test Loss 1.1419\n",
      "Epoch 5 Batch 850 Loss 0.7178\n",
      "Epoch 5 Batch 850 test Loss 1.1422\n",
      "Epoch 5 Batch 900 Loss 0.7190\n",
      "Epoch 5 Batch 900 test Loss 1.1425\n",
      "Epoch 5 Batch 950 Loss 0.7188\n",
      "Epoch 5 Batch 950 test Loss 1.1427\n",
      "Epoch 6 Batch 0 Loss 0.7517\n",
      "Epoch 6 Batch 0 test Loss 1.1430\n",
      "Epoch 6 Batch 50 Loss 0.7281\n",
      "Epoch 6 Batch 50 test Loss 1.1432\n",
      "Epoch 6 Batch 100 Loss 0.7209\n",
      "Epoch 6 Batch 100 test Loss 1.1435\n",
      "Epoch 6 Batch 150 Loss 0.7193\n",
      "Epoch 6 Batch 150 test Loss 1.1437\n",
      "Epoch 6 Batch 200 Loss 0.7163\n",
      "Epoch 6 Batch 200 test Loss 1.1439\n",
      "Epoch 6 Batch 250 Loss 0.7160\n",
      "Epoch 6 Batch 250 test Loss 1.1442\n",
      "Epoch 6 Batch 300 Loss 0.7198\n",
      "Epoch 6 Batch 300 test Loss 1.1444\n",
      "Epoch 6 Batch 350 Loss 0.7190\n",
      "Epoch 6 Batch 350 test Loss 1.1447\n",
      "Epoch 6 Batch 400 Loss 0.7206\n",
      "Epoch 6 Batch 400 test Loss 1.1449\n",
      "Epoch 6 Batch 450 Loss 0.7242\n",
      "Epoch 6 Batch 450 test Loss 1.1452\n",
      "Epoch 6 Batch 500 Loss 0.7214\n",
      "Epoch 6 Batch 500 test Loss 1.1455\n",
      "Epoch 6 Batch 550 Loss 0.7231\n",
      "Epoch 6 Batch 550 test Loss 1.1457\n",
      "Epoch 6 Batch 600 Loss 0.7215\n",
      "Epoch 6 Batch 600 test Loss 1.1460\n",
      "Epoch 6 Batch 650 Loss 0.7192\n",
      "Epoch 6 Batch 650 test Loss 1.1462\n",
      "Epoch 6 Batch 700 Loss 0.7188\n",
      "Epoch 6 Batch 700 test Loss 1.1464\n",
      "Epoch 6 Batch 750 Loss 0.7187\n",
      "Epoch 6 Batch 750 test Loss 1.1467\n",
      "Epoch 6 Batch 800 Loss 0.7198\n",
      "Epoch 6 Batch 800 test Loss 1.1469\n",
      "Epoch 6 Batch 850 Loss 0.7179\n",
      "Epoch 6 Batch 850 test Loss 1.1472\n",
      "Epoch 6 Batch 900 Loss 0.7176\n",
      "Epoch 6 Batch 900 test Loss 1.1474\n",
      "Epoch 6 Batch 950 Loss 0.7168\n",
      "Epoch 6 Batch 950 test Loss 1.1477\n",
      "Epoch 7 Batch 0 Loss 0.7285\n",
      "Epoch 7 Batch 0 test Loss 1.1480\n",
      "Epoch 7 Batch 50 Loss 0.7179\n",
      "Epoch 7 Batch 50 test Loss 1.1482\n",
      "Epoch 7 Batch 100 Loss 0.7119\n",
      "Epoch 7 Batch 100 test Loss 1.1484\n",
      "Epoch 7 Batch 150 Loss 0.7094\n",
      "Epoch 7 Batch 150 test Loss 1.1487\n",
      "Epoch 7 Batch 200 Loss 0.7154\n",
      "Epoch 7 Batch 200 test Loss 1.1489\n",
      "Epoch 7 Batch 250 Loss 0.7143\n",
      "Epoch 7 Batch 250 test Loss 1.1492\n",
      "Epoch 7 Batch 300 Loss 0.7102\n",
      "Epoch 7 Batch 300 test Loss 1.1494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 350 Loss 0.7097\n",
      "Epoch 7 Batch 350 test Loss 1.1497\n",
      "Epoch 7 Batch 400 Loss 0.7104\n",
      "Epoch 7 Batch 400 test Loss 1.1499\n",
      "Epoch 7 Batch 450 Loss 0.7116\n",
      "Epoch 7 Batch 450 test Loss 1.1502\n",
      "Epoch 7 Batch 500 Loss 0.7098\n",
      "Epoch 7 Batch 500 test Loss 1.1504\n",
      "Epoch 7 Batch 550 Loss 0.7099\n",
      "Epoch 7 Batch 550 test Loss 1.1507\n",
      "Epoch 7 Batch 600 Loss 0.7102\n",
      "Epoch 7 Batch 600 test Loss 1.1509\n",
      "Epoch 7 Batch 650 Loss 0.7108\n",
      "Epoch 7 Batch 650 test Loss 1.1511\n",
      "Epoch 7 Batch 700 Loss 0.7093\n",
      "Epoch 7 Batch 700 test Loss 1.1514\n",
      "Epoch 7 Batch 750 Loss 0.7083\n",
      "Epoch 7 Batch 750 test Loss 1.1516\n",
      "Epoch 7 Batch 800 Loss 0.7071\n",
      "Epoch 7 Batch 800 test Loss 1.1519\n",
      "Epoch 7 Batch 850 Loss 0.7064\n",
      "Epoch 7 Batch 850 test Loss 1.1522\n",
      "Epoch 7 Batch 900 Loss 0.7070\n",
      "Epoch 7 Batch 900 test Loss 1.1524\n",
      "Epoch 7 Batch 950 Loss 0.7068\n",
      "Epoch 7 Batch 950 test Loss 1.1526\n",
      "Epoch 8 Batch 0 Loss 0.7083\n",
      "Epoch 8 Batch 0 test Loss 1.1529\n",
      "Epoch 8 Batch 50 Loss 0.7006\n",
      "Epoch 8 Batch 50 test Loss 1.1531\n",
      "Epoch 8 Batch 100 Loss 0.6995\n",
      "Epoch 8 Batch 100 test Loss 1.1534\n",
      "Epoch 8 Batch 150 Loss 0.6960\n",
      "Epoch 8 Batch 150 test Loss 1.1536\n",
      "Epoch 8 Batch 200 Loss 0.7026\n",
      "Epoch 8 Batch 200 test Loss 1.1538\n",
      "Epoch 8 Batch 250 Loss 0.7034\n",
      "Epoch 8 Batch 250 test Loss 1.1540\n",
      "Epoch 8 Batch 300 Loss 0.7039\n",
      "Epoch 8 Batch 300 test Loss 1.1543\n",
      "Epoch 8 Batch 350 Loss 0.7061\n",
      "Epoch 8 Batch 350 test Loss 1.1545\n",
      "Epoch 8 Batch 400 Loss 0.7048\n",
      "Epoch 8 Batch 400 test Loss 1.1548\n",
      "Epoch 8 Batch 450 Loss 0.7068\n",
      "Epoch 8 Batch 450 test Loss 1.1551\n",
      "Epoch 8 Batch 500 Loss 0.7062\n",
      "Epoch 8 Batch 500 test Loss 1.1553\n",
      "Epoch 8 Batch 550 Loss 0.7051\n",
      "Epoch 8 Batch 550 test Loss 1.1556\n",
      "Epoch 8 Batch 600 Loss 0.7055\n",
      "Epoch 8 Batch 600 test Loss 1.1558\n",
      "Epoch 8 Batch 650 Loss 0.7048\n",
      "Epoch 8 Batch 650 test Loss 1.1560\n",
      "Epoch 8 Batch 700 Loss 0.7040\n",
      "Epoch 8 Batch 700 test Loss 1.1563\n",
      "Epoch 8 Batch 750 Loss 0.7031\n",
      "Epoch 8 Batch 750 test Loss 1.1565\n",
      "Epoch 8 Batch 800 Loss 0.7027\n",
      "Epoch 8 Batch 800 test Loss 1.1567\n",
      "Epoch 8 Batch 850 Loss 0.7029\n",
      "Epoch 8 Batch 850 test Loss 1.1570\n",
      "Epoch 8 Batch 900 Loss 0.7041\n",
      "Epoch 8 Batch 900 test Loss 1.1572\n",
      "Epoch 8 Batch 950 Loss 0.7036\n",
      "Epoch 8 Batch 950 test Loss 1.1574\n",
      "Epoch 9 Batch 0 Loss 0.5571\n",
      "Epoch 9 Batch 0 test Loss 1.1577\n",
      "Epoch 9 Batch 50 Loss 0.7078\n",
      "Epoch 9 Batch 50 test Loss 1.1579\n",
      "Epoch 9 Batch 100 Loss 0.7036\n",
      "Epoch 9 Batch 100 test Loss 1.1582\n",
      "Epoch 9 Batch 150 Loss 0.6986\n",
      "Epoch 9 Batch 150 test Loss 1.1584\n",
      "Epoch 9 Batch 200 Loss 0.6957\n",
      "Epoch 9 Batch 200 test Loss 1.1586\n",
      "Epoch 9 Batch 250 Loss 0.6958\n",
      "Epoch 9 Batch 250 test Loss 1.1589\n",
      "Epoch 9 Batch 300 Loss 0.6972\n",
      "Epoch 9 Batch 300 test Loss 1.1591\n",
      "Epoch 9 Batch 350 Loss 0.6951\n",
      "Epoch 9 Batch 350 test Loss 1.1594\n",
      "Epoch 9 Batch 400 Loss 0.6952\n",
      "Epoch 9 Batch 400 test Loss 1.1596\n",
      "Epoch 9 Batch 450 Loss 0.6964\n",
      "Epoch 9 Batch 450 test Loss 1.1599\n",
      "Epoch 9 Batch 500 Loss 0.6956\n",
      "Epoch 9 Batch 500 test Loss 1.1601\n",
      "Epoch 9 Batch 550 Loss 0.6952\n",
      "Epoch 9 Batch 550 test Loss 1.1604\n",
      "Epoch 9 Batch 600 Loss 0.6966\n",
      "Epoch 9 Batch 600 test Loss 1.1606\n",
      "Epoch 9 Batch 650 Loss 0.6949\n",
      "Epoch 9 Batch 650 test Loss 1.1608\n",
      "Epoch 9 Batch 700 Loss 0.6939\n",
      "Epoch 9 Batch 700 test Loss 1.1610\n",
      "Epoch 9 Batch 750 Loss 0.6943\n",
      "Epoch 9 Batch 750 test Loss 1.1612\n",
      "Epoch 9 Batch 800 Loss 0.6950\n",
      "Epoch 9 Batch 800 test Loss 1.1615\n",
      "Epoch 9 Batch 850 Loss 0.6961\n",
      "Epoch 9 Batch 850 test Loss 1.1617\n",
      "Epoch 9 Batch 900 Loss 0.6964\n",
      "Epoch 9 Batch 900 test Loss 1.1619\n",
      "Epoch 9 Batch 950 Loss 0.6964\n",
      "Epoch 9 Batch 950 test Loss 1.1622\n",
      "Epoch 10 Batch 0 Loss 0.6689\n",
      "Epoch 10 Batch 0 test Loss 1.1624\n",
      "Epoch 10 Batch 50 Loss 0.6939\n",
      "Epoch 10 Batch 50 test Loss 1.1626\n",
      "Epoch 10 Batch 100 Loss 0.6952\n",
      "Epoch 10 Batch 100 test Loss 1.1628\n",
      "Epoch 10 Batch 150 Loss 0.7033\n",
      "Epoch 10 Batch 150 test Loss 1.1631\n",
      "Epoch 10 Batch 200 Loss 0.7057\n",
      "Epoch 10 Batch 200 test Loss 1.1633\n",
      "Epoch 10 Batch 250 Loss 0.7040\n",
      "Epoch 10 Batch 250 test Loss 1.1635\n",
      "Epoch 10 Batch 300 Loss 0.7067\n",
      "Epoch 10 Batch 300 test Loss 1.1638\n",
      "Epoch 10 Batch 350 Loss 0.7015\n",
      "Epoch 10 Batch 350 test Loss 1.1640\n",
      "Epoch 10 Batch 400 Loss 0.6983\n",
      "Epoch 10 Batch 400 test Loss 1.1642\n",
      "Epoch 10 Batch 450 Loss 0.6980\n",
      "Epoch 10 Batch 450 test Loss 1.1645\n",
      "Epoch 10 Batch 500 Loss 0.6957\n",
      "Epoch 10 Batch 500 test Loss 1.1647\n",
      "Epoch 10 Batch 550 Loss 0.6946\n",
      "Epoch 10 Batch 550 test Loss 1.1650\n",
      "Epoch 10 Batch 600 Loss 0.6952\n",
      "Epoch 10 Batch 600 test Loss 1.1652\n",
      "Epoch 10 Batch 650 Loss 0.6938\n",
      "Epoch 10 Batch 650 test Loss 1.1655\n",
      "Epoch 10 Batch 700 Loss 0.6949\n",
      "Epoch 10 Batch 700 test Loss 1.1657\n",
      "Epoch 10 Batch 750 Loss 0.6939\n",
      "Epoch 10 Batch 750 test Loss 1.1659\n",
      "Epoch 10 Batch 800 Loss 0.6937\n",
      "Epoch 10 Batch 800 test Loss 1.1662\n",
      "Epoch 10 Batch 850 Loss 0.6943\n",
      "Epoch 10 Batch 850 test Loss 1.1664\n",
      "Epoch 10 Batch 900 Loss 0.6942\n",
      "Epoch 10 Batch 900 test Loss 1.1667\n",
      "Epoch 10 Batch 950 Loss 0.6944\n",
      "Epoch 10 Batch 950 test Loss 1.1669\n",
      "Epoch 11 Batch 0 Loss 0.7039\n",
      "Epoch 11 Batch 0 test Loss 1.1671\n",
      "Epoch 11 Batch 50 Loss 0.6917\n",
      "Epoch 11 Batch 50 test Loss 1.1674\n",
      "Epoch 11 Batch 100 Loss 0.7029\n",
      "Epoch 11 Batch 100 test Loss 1.1676\n",
      "Epoch 11 Batch 150 Loss 0.6966\n",
      "Epoch 11 Batch 150 test Loss 1.1678\n",
      "Epoch 11 Batch 200 Loss 0.6961\n",
      "Epoch 11 Batch 200 test Loss 1.1681\n",
      "Epoch 11 Batch 250 Loss 0.6951\n",
      "Epoch 11 Batch 250 test Loss 1.1683\n",
      "Epoch 11 Batch 300 Loss 0.6966\n",
      "Epoch 11 Batch 300 test Loss 1.1685\n",
      "Epoch 11 Batch 350 Loss 0.6949\n",
      "Epoch 11 Batch 350 test Loss 1.1687\n",
      "Epoch 11 Batch 400 Loss 0.6938\n",
      "Epoch 11 Batch 400 test Loss 1.1690\n",
      "Epoch 11 Batch 450 Loss 0.6914\n",
      "Epoch 11 Batch 450 test Loss 1.1692\n",
      "Epoch 11 Batch 500 Loss 0.6917\n",
      "Epoch 11 Batch 500 test Loss 1.1694\n",
      "Epoch 11 Batch 550 Loss 0.6923\n",
      "Epoch 11 Batch 550 test Loss 1.1696\n",
      "Epoch 11 Batch 600 Loss 0.6909\n",
      "Epoch 11 Batch 600 test Loss 1.1698\n",
      "Epoch 11 Batch 650 Loss 0.6914\n",
      "Epoch 11 Batch 650 test Loss 1.1700\n",
      "Epoch 11 Batch 700 Loss 0.6914\n",
      "Epoch 11 Batch 700 test Loss 1.1703\n",
      "Epoch 11 Batch 750 Loss 0.6906\n",
      "Epoch 11 Batch 750 test Loss 1.1705\n",
      "Epoch 11 Batch 800 Loss 0.6903\n",
      "Epoch 11 Batch 800 test Loss 1.1708\n",
      "Epoch 11 Batch 850 Loss 0.6910\n",
      "Epoch 11 Batch 850 test Loss 1.1710\n",
      "Epoch 11 Batch 900 Loss 0.6913\n",
      "Epoch 11 Batch 900 test Loss 1.1712\n",
      "Epoch 11 Batch 950 Loss 0.6916\n",
      "Epoch 11 Batch 950 test Loss 1.1714\n",
      "Epoch 12 Batch 0 Loss 0.7125\n",
      "Epoch 12 Batch 0 test Loss 1.1717\n",
      "Epoch 12 Batch 50 Loss 0.6661\n",
      "Epoch 12 Batch 50 test Loss 1.1719\n",
      "Epoch 12 Batch 100 Loss 0.6771\n",
      "Epoch 12 Batch 100 test Loss 1.1721\n",
      "Epoch 12 Batch 150 Loss 0.6814\n",
      "Epoch 12 Batch 150 test Loss 1.1724\n",
      "Epoch 12 Batch 200 Loss 0.6786\n",
      "Epoch 12 Batch 200 test Loss 1.1726\n",
      "Epoch 12 Batch 250 Loss 0.6765\n",
      "Epoch 12 Batch 250 test Loss 1.1728\n",
      "Epoch 12 Batch 300 Loss 0.6806\n",
      "Epoch 12 Batch 300 test Loss 1.1731\n",
      "Epoch 12 Batch 350 Loss 0.6818\n",
      "Epoch 12 Batch 350 test Loss 1.1733\n",
      "Epoch 12 Batch 400 Loss 0.6846\n",
      "Epoch 12 Batch 400 test Loss 1.1735\n",
      "Epoch 12 Batch 450 Loss 0.6829\n",
      "Epoch 12 Batch 450 test Loss 1.1737\n",
      "Epoch 12 Batch 500 Loss 0.6827\n",
      "Epoch 12 Batch 500 test Loss 1.1739\n",
      "Epoch 12 Batch 550 Loss 0.6831\n",
      "Epoch 12 Batch 550 test Loss 1.1742\n",
      "Epoch 12 Batch 600 Loss 0.6813\n",
      "Epoch 12 Batch 600 test Loss 1.1744\n",
      "Epoch 12 Batch 650 Loss 0.6810\n",
      "Epoch 12 Batch 650 test Loss 1.1746\n",
      "Epoch 12 Batch 700 Loss 0.6818\n",
      "Epoch 12 Batch 700 test Loss 1.1748\n",
      "Epoch 12 Batch 750 Loss 0.6813\n",
      "Epoch 12 Batch 750 test Loss 1.1750\n",
      "Epoch 12 Batch 800 Loss 0.6829\n",
      "Epoch 12 Batch 800 test Loss 1.1753\n",
      "Epoch 12 Batch 850 Loss 0.6829\n",
      "Epoch 12 Batch 850 test Loss 1.1755\n",
      "Epoch 12 Batch 900 Loss 0.6806\n",
      "Epoch 12 Batch 900 test Loss 1.1757\n",
      "Epoch 12 Batch 950 Loss 0.6797\n",
      "Epoch 12 Batch 950 test Loss 1.1759\n",
      "Epoch 13 Batch 0 Loss 0.5480\n",
      "Epoch 13 Batch 0 test Loss 1.1761\n",
      "Epoch 13 Batch 50 Loss 0.6754\n",
      "Epoch 13 Batch 50 test Loss 1.1763\n",
      "Epoch 13 Batch 100 Loss 0.6769\n",
      "Epoch 13 Batch 100 test Loss 1.1765\n",
      "Epoch 13 Batch 150 Loss 0.6838\n",
      "Epoch 13 Batch 150 test Loss 1.1768\n",
      "Epoch 13 Batch 200 Loss 0.6781\n",
      "Epoch 13 Batch 200 test Loss 1.1770\n",
      "Epoch 13 Batch 250 Loss 0.6736\n",
      "Epoch 13 Batch 250 test Loss 1.1772\n",
      "Epoch 13 Batch 300 Loss 0.6741\n",
      "Epoch 13 Batch 300 test Loss 1.1775\n",
      "Epoch 13 Batch 350 Loss 0.6769\n",
      "Epoch 13 Batch 350 test Loss 1.1777\n",
      "Epoch 13 Batch 400 Loss 0.6749\n",
      "Epoch 13 Batch 400 test Loss 1.1779\n",
      "Epoch 13 Batch 450 Loss 0.6737\n",
      "Epoch 13 Batch 450 test Loss 1.1781\n",
      "Epoch 13 Batch 500 Loss 0.6746\n",
      "Epoch 13 Batch 500 test Loss 1.1784\n",
      "Epoch 13 Batch 550 Loss 0.6758\n",
      "Epoch 13 Batch 550 test Loss 1.1786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 600 Loss 0.6745\n",
      "Epoch 13 Batch 600 test Loss 1.1788\n",
      "Epoch 13 Batch 650 Loss 0.6759\n",
      "Epoch 13 Batch 650 test Loss 1.1790\n",
      "Epoch 13 Batch 700 Loss 0.6756\n",
      "Epoch 13 Batch 700 test Loss 1.1793\n",
      "Epoch 13 Batch 750 Loss 0.6743\n",
      "Epoch 13 Batch 750 test Loss 1.1795\n",
      "Epoch 13 Batch 800 Loss 0.6746\n",
      "Epoch 13 Batch 800 test Loss 1.1797\n",
      "Epoch 13 Batch 850 Loss 0.6743\n",
      "Epoch 13 Batch 850 test Loss 1.1799\n",
      "Epoch 13 Batch 900 Loss 0.6735\n",
      "Epoch 13 Batch 900 test Loss 1.1801\n",
      "Epoch 13 Batch 950 Loss 0.6735\n",
      "Epoch 13 Batch 950 test Loss 1.1803\n",
      "Epoch 14 Batch 0 Loss 0.5337\n",
      "Epoch 14 Batch 0 test Loss 1.1805\n",
      "Epoch 14 Batch 50 Loss 0.6656\n",
      "Epoch 14 Batch 50 test Loss 1.1807\n",
      "Epoch 14 Batch 100 Loss 0.6614\n",
      "Epoch 14 Batch 100 test Loss 1.1810\n",
      "Epoch 14 Batch 150 Loss 0.6682\n",
      "Epoch 14 Batch 150 test Loss 1.1812\n",
      "Epoch 14 Batch 200 Loss 0.6685\n",
      "Epoch 14 Batch 200 test Loss 1.1814\n",
      "Epoch 14 Batch 250 Loss 0.6712\n",
      "Epoch 14 Batch 250 test Loss 1.1816\n",
      "Epoch 14 Batch 300 Loss 0.6695\n",
      "Epoch 14 Batch 300 test Loss 1.1818\n",
      "Epoch 14 Batch 350 Loss 0.6690\n",
      "Epoch 14 Batch 350 test Loss 1.1821\n",
      "Epoch 14 Batch 400 Loss 0.6685\n",
      "Epoch 14 Batch 400 test Loss 1.1823\n",
      "Epoch 14 Batch 450 Loss 0.6666\n",
      "Epoch 14 Batch 450 test Loss 1.1825\n",
      "Epoch 14 Batch 500 Loss 0.6689\n",
      "Epoch 14 Batch 500 test Loss 1.1827\n",
      "Epoch 14 Batch 550 Loss 0.6689\n",
      "Epoch 14 Batch 550 test Loss 1.1829\n",
      "Epoch 14 Batch 600 Loss 0.6684\n",
      "Epoch 14 Batch 600 test Loss 1.1831\n",
      "Epoch 14 Batch 650 Loss 0.6675\n",
      "Epoch 14 Batch 650 test Loss 1.1833\n",
      "Epoch 14 Batch 700 Loss 0.6669\n",
      "Epoch 14 Batch 700 test Loss 1.1835\n",
      "Epoch 14 Batch 750 Loss 0.6679\n",
      "Epoch 14 Batch 750 test Loss 1.1837\n",
      "Epoch 14 Batch 800 Loss 0.6680\n",
      "Epoch 14 Batch 800 test Loss 1.1840\n",
      "Epoch 14 Batch 850 Loss 0.6688\n",
      "Epoch 14 Batch 850 test Loss 1.1842\n",
      "Epoch 14 Batch 900 Loss 0.6691\n",
      "Epoch 14 Batch 900 test Loss 1.1844\n",
      "Epoch 14 Batch 950 Loss 0.6693\n",
      "Epoch 14 Batch 950 test Loss 1.1846\n",
      "Epoch 15 Batch 0 Loss 0.8558\n",
      "Epoch 15 Batch 0 test Loss 1.1848\n",
      "Epoch 15 Batch 50 Loss 0.6798\n",
      "Epoch 15 Batch 50 test Loss 1.1850\n",
      "Epoch 15 Batch 100 Loss 0.6693\n",
      "Epoch 15 Batch 100 test Loss 1.1853\n",
      "Epoch 15 Batch 150 Loss 0.6727\n",
      "Epoch 15 Batch 150 test Loss 1.1855\n",
      "Epoch 15 Batch 200 Loss 0.6780\n",
      "Epoch 15 Batch 200 test Loss 1.1857\n",
      "Epoch 15 Batch 250 Loss 0.6754\n",
      "Epoch 15 Batch 250 test Loss 1.1859\n",
      "Epoch 15 Batch 300 Loss 0.6769\n",
      "Epoch 15 Batch 300 test Loss 1.1861\n",
      "Epoch 15 Batch 350 Loss 0.6761\n",
      "Epoch 15 Batch 350 test Loss 1.1863\n",
      "Epoch 15 Batch 400 Loss 0.6769\n",
      "Epoch 15 Batch 400 test Loss 1.1866\n",
      "Epoch 15 Batch 450 Loss 0.6741\n",
      "Epoch 15 Batch 450 test Loss 1.1868\n",
      "Epoch 15 Batch 500 Loss 0.6730\n",
      "Epoch 15 Batch 500 test Loss 1.1870\n",
      "Epoch 15 Batch 550 Loss 0.6709\n",
      "Epoch 15 Batch 550 test Loss 1.1872\n",
      "Epoch 15 Batch 600 Loss 0.6708\n",
      "Epoch 15 Batch 600 test Loss 1.1874\n",
      "Epoch 15 Batch 650 Loss 0.6722\n",
      "Epoch 15 Batch 650 test Loss 1.1876\n",
      "Epoch 15 Batch 700 Loss 0.6723\n",
      "Epoch 15 Batch 700 test Loss 1.1878\n",
      "Epoch 15 Batch 750 Loss 0.6718\n",
      "Epoch 15 Batch 750 test Loss 1.1881\n",
      "Epoch 15 Batch 800 Loss 0.6714\n",
      "Epoch 15 Batch 800 test Loss 1.1883\n",
      "Epoch 15 Batch 850 Loss 0.6721\n",
      "Epoch 15 Batch 850 test Loss 1.1885\n",
      "Epoch 15 Batch 900 Loss 0.6716\n",
      "Epoch 15 Batch 900 test Loss 1.1887\n",
      "Epoch 15 Batch 950 Loss 0.6712\n",
      "Epoch 15 Batch 950 test Loss 1.1889\n",
      "Epoch 16 Batch 0 Loss 0.6303\n",
      "Epoch 16 Batch 0 test Loss 1.1891\n",
      "Epoch 16 Batch 50 Loss 0.6520\n",
      "Epoch 16 Batch 50 test Loss 1.1893\n",
      "Epoch 16 Batch 100 Loss 0.6544\n",
      "Epoch 16 Batch 100 test Loss 1.1895\n",
      "Epoch 16 Batch 150 Loss 0.6527\n",
      "Epoch 16 Batch 150 test Loss 1.1897\n",
      "Epoch 16 Batch 200 Loss 0.6573\n",
      "Epoch 16 Batch 200 test Loss 1.1899\n",
      "Epoch 16 Batch 250 Loss 0.6587\n",
      "Epoch 16 Batch 250 test Loss 1.1901\n",
      "Epoch 16 Batch 300 Loss 0.6565\n",
      "Epoch 16 Batch 300 test Loss 1.1903\n",
      "Epoch 16 Batch 350 Loss 0.6570\n",
      "Epoch 16 Batch 350 test Loss 1.1905\n",
      "Epoch 16 Batch 400 Loss 0.6588\n",
      "Epoch 16 Batch 400 test Loss 1.1907\n",
      "Epoch 16 Batch 450 Loss 0.6594\n",
      "Epoch 16 Batch 450 test Loss 1.1909\n",
      "Epoch 16 Batch 500 Loss 0.6613\n",
      "Epoch 16 Batch 500 test Loss 1.1912\n",
      "Epoch 16 Batch 550 Loss 0.6601\n",
      "Epoch 16 Batch 550 test Loss 1.1914\n",
      "Epoch 16 Batch 600 Loss 0.6624\n",
      "Epoch 16 Batch 600 test Loss 1.1916\n",
      "Epoch 16 Batch 650 Loss 0.6625\n",
      "Epoch 16 Batch 650 test Loss 1.1918\n",
      "Epoch 16 Batch 700 Loss 0.6631\n",
      "Epoch 16 Batch 700 test Loss 1.1920\n",
      "Epoch 16 Batch 750 Loss 0.6624\n",
      "Epoch 16 Batch 750 test Loss 1.1922\n",
      "Epoch 16 Batch 800 Loss 0.6630\n",
      "Epoch 16 Batch 800 test Loss 1.1924\n",
      "Epoch 16 Batch 850 Loss 0.6642\n",
      "Epoch 16 Batch 850 test Loss 1.1926\n",
      "Epoch 16 Batch 900 Loss 0.6626\n",
      "Epoch 16 Batch 900 test Loss 1.1928\n",
      "Epoch 16 Batch 950 Loss 0.6621\n",
      "Epoch 16 Batch 950 test Loss 1.1930\n",
      "Epoch 17 Batch 0 Loss 0.7673\n",
      "Epoch 17 Batch 0 test Loss 1.1932\n",
      "Epoch 17 Batch 50 Loss 0.6600\n",
      "Epoch 17 Batch 50 test Loss 1.1934\n",
      "Epoch 17 Batch 100 Loss 0.6641\n",
      "Epoch 17 Batch 100 test Loss 1.1936\n",
      "Epoch 17 Batch 150 Loss 0.6698\n",
      "Epoch 17 Batch 150 test Loss 1.1938\n",
      "Epoch 17 Batch 200 Loss 0.6651\n",
      "Epoch 17 Batch 200 test Loss 1.1940\n",
      "Epoch 17 Batch 250 Loss 0.6643\n",
      "Epoch 17 Batch 250 test Loss 1.1942\n",
      "Epoch 17 Batch 300 Loss 0.6650\n",
      "Epoch 17 Batch 300 test Loss 1.1944\n",
      "Epoch 17 Batch 350 Loss 0.6641\n",
      "Epoch 17 Batch 350 test Loss 1.1946\n",
      "Epoch 17 Batch 400 Loss 0.6634\n",
      "Epoch 17 Batch 400 test Loss 1.1949\n",
      "Epoch 17 Batch 450 Loss 0.6651\n",
      "Epoch 17 Batch 450 test Loss 1.1951\n",
      "Epoch 17 Batch 500 Loss 0.6645\n",
      "Epoch 17 Batch 500 test Loss 1.1953\n",
      "Epoch 17 Batch 550 Loss 0.6635\n",
      "Epoch 17 Batch 550 test Loss 1.1955\n",
      "Epoch 17 Batch 600 Loss 0.6632\n",
      "Epoch 17 Batch 600 test Loss 1.1957\n",
      "Epoch 17 Batch 650 Loss 0.6619\n",
      "Epoch 17 Batch 650 test Loss 1.1959\n",
      "Epoch 17 Batch 700 Loss 0.6596\n",
      "Epoch 17 Batch 700 test Loss 1.1961\n",
      "Epoch 17 Batch 750 Loss 0.6587\n",
      "Epoch 17 Batch 750 test Loss 1.1963\n",
      "Epoch 17 Batch 800 Loss 0.6589\n",
      "Epoch 17 Batch 800 test Loss 1.1965\n",
      "Epoch 17 Batch 850 Loss 0.6585\n",
      "Epoch 17 Batch 850 test Loss 1.1967\n",
      "Epoch 17 Batch 900 Loss 0.6580\n",
      "Epoch 17 Batch 900 test Loss 1.1969\n",
      "Epoch 17 Batch 950 Loss 0.6581\n",
      "Epoch 17 Batch 950 test Loss 1.1971\n",
      "Epoch 18 Batch 0 Loss 0.7283\n",
      "Epoch 18 Batch 0 test Loss 1.1973\n",
      "Epoch 18 Batch 50 Loss 0.6435\n",
      "Epoch 18 Batch 50 test Loss 1.1975\n",
      "Epoch 18 Batch 100 Loss 0.6453\n",
      "Epoch 18 Batch 100 test Loss 1.1978\n",
      "Epoch 18 Batch 150 Loss 0.6468\n",
      "Epoch 18 Batch 150 test Loss 1.1979\n",
      "Epoch 18 Batch 200 Loss 0.6498\n",
      "Epoch 18 Batch 200 test Loss 1.1981\n",
      "Epoch 18 Batch 250 Loss 0.6511\n",
      "Epoch 18 Batch 250 test Loss 1.1983\n",
      "Epoch 18 Batch 300 Loss 0.6525\n",
      "Epoch 18 Batch 300 test Loss 1.1985\n",
      "Epoch 18 Batch 350 Loss 0.6520\n",
      "Epoch 18 Batch 350 test Loss 1.1987\n",
      "Epoch 18 Batch 400 Loss 0.6544\n",
      "Epoch 18 Batch 400 test Loss 1.1989\n",
      "Epoch 18 Batch 450 Loss 0.6554\n",
      "Epoch 18 Batch 450 test Loss 1.1991\n",
      "Epoch 18 Batch 500 Loss 0.6563\n",
      "Epoch 18 Batch 500 test Loss 1.1993\n",
      "Epoch 18 Batch 550 Loss 0.6545\n",
      "Epoch 18 Batch 550 test Loss 1.1995\n",
      "Epoch 18 Batch 600 Loss 0.6551\n",
      "Epoch 18 Batch 600 test Loss 1.1997\n",
      "Epoch 18 Batch 650 Loss 0.6555\n",
      "Epoch 18 Batch 650 test Loss 1.1999\n",
      "Epoch 18 Batch 700 Loss 0.6560\n",
      "Epoch 18 Batch 700 test Loss 1.2001\n",
      "Epoch 18 Batch 750 Loss 0.6569\n",
      "Epoch 18 Batch 750 test Loss 1.2003\n",
      "Epoch 18 Batch 800 Loss 0.6564\n",
      "Epoch 18 Batch 800 test Loss 1.2005\n",
      "Epoch 18 Batch 850 Loss 0.6557\n",
      "Epoch 18 Batch 850 test Loss 1.2008\n",
      "Epoch 18 Batch 900 Loss 0.6572\n",
      "Epoch 18 Batch 900 test Loss 1.2009\n",
      "Epoch 18 Batch 950 Loss 0.6567\n",
      "Epoch 18 Batch 950 test Loss 1.2011\n",
      "Epoch 19 Batch 0 Loss 0.5613\n",
      "Epoch 19 Batch 0 test Loss 1.2014\n",
      "Epoch 19 Batch 50 Loss 0.6364\n",
      "Epoch 19 Batch 50 test Loss 1.2016\n",
      "Epoch 19 Batch 100 Loss 0.6401\n",
      "Epoch 19 Batch 100 test Loss 1.2018\n",
      "Epoch 19 Batch 150 Loss 0.6493\n",
      "Epoch 19 Batch 150 test Loss 1.2019\n",
      "Epoch 19 Batch 200 Loss 0.6475\n",
      "Epoch 19 Batch 200 test Loss 1.2021\n",
      "Epoch 19 Batch 250 Loss 0.6498\n",
      "Epoch 19 Batch 250 test Loss 1.2023\n",
      "Epoch 19 Batch 300 Loss 0.6477\n",
      "Epoch 19 Batch 300 test Loss 1.2025\n",
      "Epoch 19 Batch 350 Loss 0.6463\n",
      "Epoch 19 Batch 350 test Loss 1.2027\n",
      "Epoch 19 Batch 400 Loss 0.6462\n",
      "Epoch 19 Batch 400 test Loss 1.2029\n",
      "Epoch 19 Batch 450 Loss 0.6469\n",
      "Epoch 19 Batch 450 test Loss 1.2031\n",
      "Epoch 19 Batch 500 Loss 0.6478\n",
      "Epoch 19 Batch 500 test Loss 1.2033\n",
      "Epoch 19 Batch 550 Loss 0.6484\n",
      "Epoch 19 Batch 550 test Loss 1.2036\n",
      "Epoch 19 Batch 600 Loss 0.6468\n",
      "Epoch 19 Batch 600 test Loss 1.2038\n",
      "Epoch 19 Batch 650 Loss 0.6484\n",
      "Epoch 19 Batch 650 test Loss 1.2040\n",
      "Epoch 19 Batch 700 Loss 0.6492\n",
      "Epoch 19 Batch 700 test Loss 1.2042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 750 Loss 0.6502\n",
      "Epoch 19 Batch 750 test Loss 1.2044\n",
      "Epoch 19 Batch 800 Loss 0.6511\n",
      "Epoch 19 Batch 800 test Loss 1.2046\n",
      "Epoch 19 Batch 850 Loss 0.6505\n",
      "Epoch 19 Batch 850 test Loss 1.2048\n",
      "Epoch 19 Batch 900 Loss 0.6496\n",
      "Epoch 19 Batch 900 test Loss 1.2050\n",
      "Epoch 19 Batch 950 Loss 0.6492\n",
      "Epoch 19 Batch 950 test Loss 1.2052\n",
      "Epoch 20 Batch 0 Loss 0.6509\n",
      "Epoch 20 Batch 0 test Loss 1.2054\n",
      "Epoch 20 Batch 50 Loss 0.6192\n",
      "Epoch 20 Batch 50 test Loss 1.2056\n",
      "Epoch 20 Batch 100 Loss 0.6287\n",
      "Epoch 20 Batch 100 test Loss 1.2058\n",
      "Epoch 20 Batch 150 Loss 0.6291\n",
      "Epoch 20 Batch 150 test Loss 1.2060\n",
      "Epoch 20 Batch 200 Loss 0.6300\n",
      "Epoch 20 Batch 200 test Loss 1.2062\n",
      "Epoch 20 Batch 250 Loss 0.6355\n",
      "Epoch 20 Batch 250 test Loss 1.2064\n",
      "Epoch 20 Batch 300 Loss 0.6359\n",
      "Epoch 20 Batch 300 test Loss 1.2066\n",
      "Epoch 20 Batch 350 Loss 0.6366\n",
      "Epoch 20 Batch 350 test Loss 1.2068\n",
      "Epoch 20 Batch 400 Loss 0.6361\n",
      "Epoch 20 Batch 400 test Loss 1.2070\n",
      "Epoch 20 Batch 450 Loss 0.6388\n",
      "Epoch 20 Batch 450 test Loss 1.2072\n",
      "Epoch 20 Batch 500 Loss 0.6399\n",
      "Epoch 20 Batch 500 test Loss 1.2074\n",
      "Epoch 20 Batch 550 Loss 0.6401\n",
      "Epoch 20 Batch 550 test Loss 1.2076\n",
      "Epoch 20 Batch 600 Loss 0.6397\n",
      "Epoch 20 Batch 600 test Loss 1.2078\n",
      "Epoch 20 Batch 650 Loss 0.6389\n",
      "Epoch 20 Batch 650 test Loss 1.2080\n",
      "Epoch 20 Batch 700 Loss 0.6384\n",
      "Epoch 20 Batch 700 test Loss 1.2082\n",
      "Epoch 20 Batch 750 Loss 0.6395\n",
      "Epoch 20 Batch 750 test Loss 1.2084\n",
      "Epoch 20 Batch 800 Loss 0.6413\n",
      "Epoch 20 Batch 800 test Loss 1.2086\n",
      "Epoch 20 Batch 850 Loss 0.6423\n",
      "Epoch 20 Batch 850 test Loss 1.2088\n",
      "Epoch 20 Batch 900 Loss 0.6421\n",
      "Epoch 20 Batch 900 test Loss 1.2089\n",
      "Epoch 20 Batch 950 Loss 0.6419\n",
      "Epoch 20 Batch 950 test Loss 1.2091\n",
      "Epoch 21 Batch 0 Loss 0.6260\n",
      "Epoch 21 Batch 0 test Loss 1.2093\n",
      "Epoch 21 Batch 50 Loss 0.6467\n",
      "Epoch 21 Batch 50 test Loss 1.2095\n",
      "Epoch 21 Batch 100 Loss 0.6462\n",
      "Epoch 21 Batch 100 test Loss 1.2097\n",
      "Epoch 21 Batch 150 Loss 0.6414\n",
      "Epoch 21 Batch 150 test Loss 1.2099\n",
      "Epoch 21 Batch 200 Loss 0.6356\n",
      "Epoch 21 Batch 200 test Loss 1.2101\n",
      "Epoch 21 Batch 250 Loss 0.6352\n",
      "Epoch 21 Batch 250 test Loss 1.2103\n",
      "Epoch 21 Batch 300 Loss 0.6350\n",
      "Epoch 21 Batch 300 test Loss 1.2105\n",
      "Epoch 21 Batch 350 Loss 0.6381\n",
      "Epoch 21 Batch 350 test Loss 1.2107\n",
      "Epoch 21 Batch 400 Loss 0.6365\n",
      "Epoch 21 Batch 400 test Loss 1.2109\n",
      "Epoch 21 Batch 450 Loss 0.6367\n",
      "Epoch 21 Batch 450 test Loss 1.2111\n",
      "Epoch 21 Batch 500 Loss 0.6370\n",
      "Epoch 21 Batch 500 test Loss 1.2113\n",
      "Epoch 21 Batch 550 Loss 0.6379\n",
      "Epoch 21 Batch 550 test Loss 1.2115\n",
      "Epoch 21 Batch 600 Loss 0.6361\n",
      "Epoch 21 Batch 600 test Loss 1.2116\n",
      "Epoch 21 Batch 650 Loss 0.6349\n",
      "Epoch 21 Batch 650 test Loss 1.2118\n",
      "Epoch 21 Batch 700 Loss 0.6356\n",
      "Epoch 21 Batch 700 test Loss 1.2120\n",
      "Epoch 21 Batch 750 Loss 0.6359\n",
      "Epoch 21 Batch 750 test Loss 1.2122\n",
      "Epoch 21 Batch 800 Loss 0.6364\n",
      "Epoch 21 Batch 800 test Loss 1.2124\n",
      "Epoch 21 Batch 850 Loss 0.6370\n",
      "Epoch 21 Batch 850 test Loss 1.2126\n",
      "Epoch 21 Batch 900 Loss 0.6367\n",
      "Epoch 21 Batch 900 test Loss 1.2128\n",
      "Epoch 21 Batch 950 Loss 0.6368\n",
      "Epoch 21 Batch 950 test Loss 1.2130\n",
      "Epoch 22 Batch 0 Loss 0.6027\n",
      "Epoch 22 Batch 0 test Loss 1.2132\n",
      "Epoch 22 Batch 50 Loss 0.6300\n",
      "Epoch 22 Batch 50 test Loss 1.2134\n",
      "Epoch 22 Batch 100 Loss 0.6381\n",
      "Epoch 22 Batch 100 test Loss 1.2136\n",
      "Epoch 22 Batch 150 Loss 0.6377\n",
      "Epoch 22 Batch 150 test Loss 1.2137\n",
      "Epoch 22 Batch 200 Loss 0.6352\n",
      "Epoch 22 Batch 200 test Loss 1.2139\n",
      "Epoch 22 Batch 250 Loss 0.6377\n",
      "Epoch 22 Batch 250 test Loss 1.2141\n",
      "Epoch 22 Batch 300 Loss 0.6387\n",
      "Epoch 22 Batch 300 test Loss 1.2143\n",
      "Epoch 22 Batch 350 Loss 0.6382\n",
      "Epoch 22 Batch 350 test Loss 1.2145\n",
      "Epoch 22 Batch 400 Loss 0.6364\n",
      "Epoch 22 Batch 400 test Loss 1.2146\n",
      "Epoch 22 Batch 450 Loss 0.6348\n",
      "Epoch 22 Batch 450 test Loss 1.2148\n",
      "Epoch 22 Batch 500 Loss 0.6344\n",
      "Epoch 22 Batch 500 test Loss 1.2151\n",
      "Epoch 22 Batch 550 Loss 0.6346\n",
      "Epoch 22 Batch 550 test Loss 1.2153\n",
      "Epoch 22 Batch 600 Loss 0.6360\n",
      "Epoch 22 Batch 600 test Loss 1.2154\n",
      "Epoch 22 Batch 650 Loss 0.6360\n",
      "Epoch 22 Batch 650 test Loss 1.2156\n",
      "Epoch 22 Batch 700 Loss 0.6350\n",
      "Epoch 22 Batch 700 test Loss 1.2158\n",
      "Epoch 22 Batch 750 Loss 0.6349\n",
      "Epoch 22 Batch 750 test Loss 1.2160\n",
      "Epoch 22 Batch 800 Loss 0.6344\n",
      "Epoch 22 Batch 800 test Loss 1.2162\n",
      "Epoch 22 Batch 850 Loss 0.6349\n",
      "Epoch 22 Batch 850 test Loss 1.2164\n",
      "Epoch 22 Batch 900 Loss 0.6349\n",
      "Epoch 22 Batch 900 test Loss 1.2166\n",
      "Epoch 22 Batch 950 Loss 0.6355\n",
      "Epoch 22 Batch 950 test Loss 1.2168\n",
      "Epoch 23 Batch 0 Loss 0.7370\n",
      "Epoch 23 Batch 0 test Loss 1.2170\n",
      "Epoch 23 Batch 50 Loss 0.6339\n",
      "Epoch 23 Batch 50 test Loss 1.2171\n",
      "Epoch 23 Batch 100 Loss 0.6261\n",
      "Epoch 23 Batch 100 test Loss 1.2173\n",
      "Epoch 23 Batch 150 Loss 0.6306\n",
      "Epoch 23 Batch 150 test Loss 1.2175\n",
      "Epoch 23 Batch 200 Loss 0.6298\n",
      "Epoch 23 Batch 200 test Loss 1.2177\n",
      "Epoch 23 Batch 250 Loss 0.6309\n",
      "Epoch 23 Batch 250 test Loss 1.2178\n",
      "Epoch 23 Batch 300 Loss 0.6300\n",
      "Epoch 23 Batch 300 test Loss 1.2180\n",
      "Epoch 23 Batch 350 Loss 0.6315\n",
      "Epoch 23 Batch 350 test Loss 1.2182\n",
      "Epoch 23 Batch 400 Loss 0.6302\n",
      "Epoch 23 Batch 400 test Loss 1.2184\n",
      "Epoch 23 Batch 450 Loss 0.6307\n",
      "Epoch 23 Batch 450 test Loss 1.2186\n",
      "Epoch 23 Batch 500 Loss 0.6298\n",
      "Epoch 23 Batch 500 test Loss 1.2187\n",
      "Epoch 23 Batch 550 Loss 0.6303\n",
      "Epoch 23 Batch 550 test Loss 1.2189\n",
      "Epoch 23 Batch 600 Loss 0.6311\n",
      "Epoch 23 Batch 600 test Loss 1.2191\n",
      "Epoch 23 Batch 650 Loss 0.6314\n",
      "Epoch 23 Batch 650 test Loss 1.2193\n",
      "Epoch 23 Batch 700 Loss 0.6294\n",
      "Epoch 23 Batch 700 test Loss 1.2195\n",
      "Epoch 23 Batch 750 Loss 0.6291\n",
      "Epoch 23 Batch 750 test Loss 1.2197\n",
      "Epoch 23 Batch 800 Loss 0.6286\n",
      "Epoch 23 Batch 800 test Loss 1.2199\n",
      "Epoch 23 Batch 850 Loss 0.6287\n",
      "Epoch 23 Batch 850 test Loss 1.2201\n",
      "Epoch 23 Batch 900 Loss 0.6282\n",
      "Epoch 23 Batch 900 test Loss 1.2202\n",
      "Epoch 23 Batch 950 Loss 0.6279\n",
      "Epoch 23 Batch 950 test Loss 1.2205\n",
      "Epoch 24 Batch 0 Loss 0.7294\n",
      "Epoch 24 Batch 0 test Loss 1.2206\n",
      "Epoch 24 Batch 50 Loss 0.6420\n",
      "Epoch 24 Batch 50 test Loss 1.2208\n",
      "Epoch 24 Batch 100 Loss 0.6271\n",
      "Epoch 24 Batch 100 test Loss 1.2210\n",
      "Epoch 24 Batch 150 Loss 0.6299\n",
      "Epoch 24 Batch 150 test Loss 1.2212\n",
      "Epoch 24 Batch 200 Loss 0.6266\n",
      "Epoch 24 Batch 200 test Loss 1.2214\n",
      "Epoch 24 Batch 250 Loss 0.6302\n",
      "Epoch 24 Batch 250 test Loss 1.2216\n",
      "Epoch 24 Batch 300 Loss 0.6287\n",
      "Epoch 24 Batch 300 test Loss 1.2217\n",
      "Epoch 24 Batch 350 Loss 0.6275\n",
      "Epoch 24 Batch 350 test Loss 1.2220\n",
      "Epoch 24 Batch 400 Loss 0.6273\n",
      "Epoch 24 Batch 400 test Loss 1.2222\n",
      "Epoch 24 Batch 450 Loss 0.6257\n",
      "Epoch 24 Batch 450 test Loss 1.2223\n",
      "Epoch 24 Batch 500 Loss 0.6263\n",
      "Epoch 24 Batch 500 test Loss 1.2225\n",
      "Epoch 24 Batch 550 Loss 0.6273\n",
      "Epoch 24 Batch 550 test Loss 1.2227\n",
      "Epoch 24 Batch 600 Loss 0.6291\n",
      "Epoch 24 Batch 600 test Loss 1.2229\n",
      "Epoch 24 Batch 650 Loss 0.6287\n",
      "Epoch 24 Batch 650 test Loss 1.2231\n",
      "Epoch 24 Batch 700 Loss 0.6284\n",
      "Epoch 24 Batch 700 test Loss 1.2233\n",
      "Epoch 24 Batch 750 Loss 0.6278\n",
      "Epoch 24 Batch 750 test Loss 1.2235\n",
      "Epoch 24 Batch 800 Loss 0.6282\n",
      "Epoch 24 Batch 800 test Loss 1.2237\n",
      "Epoch 24 Batch 850 Loss 0.6298\n",
      "Epoch 24 Batch 850 test Loss 1.2238\n",
      "Epoch 24 Batch 900 Loss 0.6297\n",
      "Epoch 24 Batch 900 test Loss 1.2240\n",
      "Epoch 24 Batch 950 Loss 0.6292\n",
      "Epoch 24 Batch 950 test Loss 1.2242\n",
      "Epoch 25 Batch 0 Loss 0.7359\n",
      "Epoch 25 Batch 0 test Loss 1.2244\n",
      "Epoch 25 Batch 50 Loss 0.6150\n",
      "Epoch 25 Batch 50 test Loss 1.2246\n",
      "Epoch 25 Batch 100 Loss 0.6334\n",
      "Epoch 25 Batch 100 test Loss 1.2248\n",
      "Epoch 25 Batch 150 Loss 0.6389\n",
      "Epoch 25 Batch 150 test Loss 1.2249\n",
      "Epoch 25 Batch 200 Loss 0.6380\n",
      "Epoch 25 Batch 200 test Loss 1.2251\n",
      "Epoch 25 Batch 250 Loss 0.6381\n",
      "Epoch 25 Batch 250 test Loss 1.2253\n",
      "Epoch 25 Batch 300 Loss 0.6346\n",
      "Epoch 25 Batch 300 test Loss 1.2255\n",
      "Epoch 25 Batch 350 Loss 0.6308\n",
      "Epoch 25 Batch 350 test Loss 1.2257\n",
      "Epoch 25 Batch 400 Loss 0.6296\n",
      "Epoch 25 Batch 400 test Loss 1.2258\n",
      "Epoch 25 Batch 450 Loss 0.6297\n",
      "Epoch 25 Batch 450 test Loss 1.2260\n",
      "Epoch 25 Batch 500 Loss 0.6296\n",
      "Epoch 25 Batch 500 test Loss 1.2262\n",
      "Epoch 25 Batch 550 Loss 0.6297\n",
      "Epoch 25 Batch 550 test Loss 1.2264\n",
      "Epoch 25 Batch 600 Loss 0.6293\n",
      "Epoch 25 Batch 600 test Loss 1.2265\n",
      "Epoch 25 Batch 650 Loss 0.6305\n",
      "Epoch 25 Batch 650 test Loss 1.2267\n",
      "Epoch 25 Batch 700 Loss 0.6299\n",
      "Epoch 25 Batch 700 test Loss 1.2269\n",
      "Epoch 25 Batch 750 Loss 0.6290\n",
      "Epoch 25 Batch 750 test Loss 1.2271\n",
      "Epoch 25 Batch 800 Loss 0.6282\n",
      "Epoch 25 Batch 800 test Loss 1.2273\n",
      "Epoch 25 Batch 850 Loss 0.6280\n",
      "Epoch 25 Batch 850 test Loss 1.2275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 900 Loss 0.6284\n",
      "Epoch 25 Batch 900 test Loss 1.2277\n",
      "Epoch 25 Batch 950 Loss 0.6290\n",
      "Epoch 25 Batch 950 test Loss 1.2278\n",
      "Epoch 26 Batch 0 Loss 0.5537\n",
      "Epoch 26 Batch 0 test Loss 1.2280\n",
      "Epoch 26 Batch 50 Loss 0.6109\n",
      "Epoch 26 Batch 50 test Loss 1.2282\n",
      "Epoch 26 Batch 100 Loss 0.6183\n",
      "Epoch 26 Batch 100 test Loss 1.2284\n",
      "Epoch 26 Batch 150 Loss 0.6235\n",
      "Epoch 26 Batch 150 test Loss 1.2286\n",
      "Epoch 26 Batch 200 Loss 0.6296\n",
      "Epoch 26 Batch 200 test Loss 1.2287\n",
      "Epoch 26 Batch 250 Loss 0.6255\n",
      "Epoch 26 Batch 250 test Loss 1.2289\n",
      "Epoch 26 Batch 300 Loss 0.6278\n",
      "Epoch 26 Batch 300 test Loss 1.2291\n",
      "Epoch 26 Batch 350 Loss 0.6246\n",
      "Epoch 26 Batch 350 test Loss 1.2293\n",
      "Epoch 26 Batch 400 Loss 0.6234\n",
      "Epoch 26 Batch 400 test Loss 1.2294\n",
      "Epoch 26 Batch 450 Loss 0.6232\n",
      "Epoch 26 Batch 450 test Loss 1.2296\n",
      "Epoch 26 Batch 500 Loss 0.6225\n",
      "Epoch 26 Batch 500 test Loss 1.2298\n",
      "Epoch 26 Batch 550 Loss 0.6252\n",
      "Epoch 26 Batch 550 test Loss 1.2300\n",
      "Epoch 26 Batch 600 Loss 0.6252\n",
      "Epoch 26 Batch 600 test Loss 1.2302\n",
      "Epoch 26 Batch 650 Loss 0.6255\n",
      "Epoch 26 Batch 650 test Loss 1.2304\n",
      "Epoch 26 Batch 700 Loss 0.6261\n",
      "Epoch 26 Batch 700 test Loss 1.2305\n",
      "Epoch 26 Batch 750 Loss 0.6256\n",
      "Epoch 26 Batch 750 test Loss 1.2307\n",
      "Epoch 26 Batch 800 Loss 0.6256\n",
      "Epoch 26 Batch 800 test Loss 1.2309\n",
      "Epoch 26 Batch 850 Loss 0.6257\n",
      "Epoch 26 Batch 850 test Loss 1.2310\n",
      "Epoch 26 Batch 900 Loss 0.6254\n",
      "Epoch 26 Batch 900 test Loss 1.2312\n",
      "Epoch 26 Batch 950 Loss 0.6250\n",
      "Epoch 26 Batch 950 test Loss 1.2314\n",
      "Epoch 27 Batch 0 Loss 0.6437\n",
      "Epoch 27 Batch 0 test Loss 1.2315\n",
      "Epoch 27 Batch 50 Loss 0.6304\n",
      "Epoch 27 Batch 50 test Loss 1.2317\n",
      "Epoch 27 Batch 100 Loss 0.6227\n",
      "Epoch 27 Batch 100 test Loss 1.2319\n",
      "Epoch 27 Batch 150 Loss 0.6207\n",
      "Epoch 27 Batch 150 test Loss 1.2321\n",
      "Epoch 27 Batch 200 Loss 0.6188\n",
      "Epoch 27 Batch 200 test Loss 1.2322\n",
      "Epoch 27 Batch 250 Loss 0.6170\n",
      "Epoch 27 Batch 250 test Loss 1.2324\n",
      "Epoch 27 Batch 300 Loss 0.6133\n",
      "Epoch 27 Batch 300 test Loss 1.2326\n",
      "Epoch 27 Batch 350 Loss 0.6142\n",
      "Epoch 27 Batch 350 test Loss 1.2328\n",
      "Epoch 27 Batch 400 Loss 0.6141\n",
      "Epoch 27 Batch 400 test Loss 1.2330\n",
      "Epoch 27 Batch 450 Loss 0.6140\n",
      "Epoch 27 Batch 450 test Loss 1.2332\n",
      "Epoch 27 Batch 500 Loss 0.6150\n",
      "Epoch 27 Batch 500 test Loss 1.2333\n",
      "Epoch 27 Batch 550 Loss 0.6177\n",
      "Epoch 27 Batch 550 test Loss 1.2335\n",
      "Epoch 27 Batch 600 Loss 0.6168\n",
      "Epoch 27 Batch 600 test Loss 1.2337\n",
      "Epoch 27 Batch 650 Loss 0.6167\n",
      "Epoch 27 Batch 650 test Loss 1.2339\n",
      "Epoch 27 Batch 700 Loss 0.6168\n",
      "Epoch 27 Batch 700 test Loss 1.2340\n",
      "Epoch 27 Batch 750 Loss 0.6168\n",
      "Epoch 27 Batch 750 test Loss 1.2342\n",
      "Epoch 27 Batch 800 Loss 0.6163\n",
      "Epoch 27 Batch 800 test Loss 1.2344\n",
      "Epoch 27 Batch 850 Loss 0.6177\n",
      "Epoch 27 Batch 850 test Loss 1.2346\n",
      "Epoch 27 Batch 900 Loss 0.6169\n",
      "Epoch 27 Batch 900 test Loss 1.2347\n",
      "Epoch 27 Batch 950 Loss 0.6169\n",
      "Epoch 27 Batch 950 test Loss 1.2349\n",
      "Epoch 28 Batch 0 Loss 0.5274\n",
      "Epoch 28 Batch 0 test Loss 1.2351\n",
      "Epoch 28 Batch 50 Loss 0.6240\n",
      "Epoch 28 Batch 50 test Loss 1.2353\n",
      "Epoch 28 Batch 100 Loss 0.6225\n",
      "Epoch 28 Batch 100 test Loss 1.2354\n",
      "Epoch 28 Batch 150 Loss 0.6224\n",
      "Epoch 28 Batch 150 test Loss 1.2356\n",
      "Epoch 28 Batch 200 Loss 0.6201\n",
      "Epoch 28 Batch 200 test Loss 1.2358\n",
      "Epoch 28 Batch 250 Loss 0.6179\n",
      "Epoch 28 Batch 250 test Loss 1.2359\n",
      "Epoch 28 Batch 300 Loss 0.6148\n",
      "Epoch 28 Batch 300 test Loss 1.2361\n",
      "Epoch 28 Batch 350 Loss 0.6150\n",
      "Epoch 28 Batch 350 test Loss 1.2363\n",
      "Epoch 28 Batch 400 Loss 0.6174\n",
      "Epoch 28 Batch 400 test Loss 1.2365\n",
      "Epoch 28 Batch 450 Loss 0.6179\n",
      "Epoch 28 Batch 450 test Loss 1.2366\n",
      "Epoch 28 Batch 500 Loss 0.6164\n",
      "Epoch 28 Batch 500 test Loss 1.2368\n",
      "Epoch 28 Batch 550 Loss 0.6157\n",
      "Epoch 28 Batch 550 test Loss 1.2370\n",
      "Epoch 28 Batch 600 Loss 0.6152\n",
      "Epoch 28 Batch 600 test Loss 1.2372\n",
      "Epoch 28 Batch 650 Loss 0.6169\n",
      "Epoch 28 Batch 650 test Loss 1.2373\n",
      "Epoch 28 Batch 700 Loss 0.6167\n",
      "Epoch 28 Batch 700 test Loss 1.2375\n",
      "Epoch 28 Batch 750 Loss 0.6173\n",
      "Epoch 28 Batch 750 test Loss 1.2377\n",
      "Epoch 28 Batch 800 Loss 0.6176\n",
      "Epoch 28 Batch 800 test Loss 1.2379\n",
      "Epoch 28 Batch 850 Loss 0.6170\n",
      "Epoch 28 Batch 850 test Loss 1.2380\n",
      "Epoch 28 Batch 900 Loss 0.6175\n",
      "Epoch 28 Batch 900 test Loss 1.2382\n",
      "Epoch 28 Batch 950 Loss 0.6173\n",
      "Epoch 28 Batch 950 test Loss 1.2384\n",
      "Epoch 29 Batch 0 Loss 0.5495\n",
      "Epoch 29 Batch 0 test Loss 1.2385\n",
      "Epoch 29 Batch 50 Loss 0.5859\n",
      "Epoch 29 Batch 50 test Loss 1.2387\n",
      "Epoch 29 Batch 100 Loss 0.6074\n",
      "Epoch 29 Batch 100 test Loss 1.2389\n",
      "Epoch 29 Batch 150 Loss 0.6138\n",
      "Epoch 29 Batch 150 test Loss 1.2390\n",
      "Epoch 29 Batch 200 Loss 0.6149\n",
      "Epoch 29 Batch 200 test Loss 1.2392\n",
      "Epoch 29 Batch 250 Loss 0.6141\n",
      "Epoch 29 Batch 250 test Loss 1.2394\n",
      "Epoch 29 Batch 300 Loss 0.6163\n",
      "Epoch 29 Batch 300 test Loss 1.2395\n",
      "Epoch 29 Batch 350 Loss 0.6173\n",
      "Epoch 29 Batch 350 test Loss 1.2397\n",
      "Epoch 29 Batch 400 Loss 0.6176\n",
      "Epoch 29 Batch 400 test Loss 1.2399\n",
      "Epoch 29 Batch 450 Loss 0.6179\n",
      "Epoch 29 Batch 450 test Loss 1.2400\n",
      "Epoch 29 Batch 500 Loss 0.6173\n",
      "Epoch 29 Batch 500 test Loss 1.2402\n",
      "Epoch 29 Batch 550 Loss 0.6162\n",
      "Epoch 29 Batch 550 test Loss 1.2404\n",
      "Epoch 29 Batch 600 Loss 0.6175\n",
      "Epoch 29 Batch 600 test Loss 1.2405\n",
      "Epoch 29 Batch 650 Loss 0.6182\n",
      "Epoch 29 Batch 650 test Loss 1.2407\n",
      "Epoch 29 Batch 700 Loss 0.6179\n",
      "Epoch 29 Batch 700 test Loss 1.2408\n",
      "Epoch 29 Batch 750 Loss 0.6158\n",
      "Epoch 29 Batch 750 test Loss 1.2410\n",
      "Epoch 29 Batch 800 Loss 0.6153\n",
      "Epoch 29 Batch 800 test Loss 1.2412\n",
      "Epoch 29 Batch 850 Loss 0.6148\n",
      "Epoch 29 Batch 850 test Loss 1.2414\n",
      "Epoch 29 Batch 900 Loss 0.6146\n",
      "Epoch 29 Batch 900 test Loss 1.2415\n",
      "Epoch 29 Batch 950 Loss 0.6157\n",
      "Epoch 29 Batch 950 test Loss 1.2417\n",
      "Epoch 30 Batch 0 Loss 0.6741\n",
      "Epoch 30 Batch 0 test Loss 1.2419\n",
      "Epoch 30 Batch 50 Loss 0.6138\n",
      "Epoch 30 Batch 50 test Loss 1.2420\n",
      "Epoch 30 Batch 100 Loss 0.6128\n",
      "Epoch 30 Batch 100 test Loss 1.2422\n",
      "Epoch 30 Batch 150 Loss 0.6185\n",
      "Epoch 30 Batch 150 test Loss 1.2424\n",
      "Epoch 30 Batch 200 Loss 0.6129\n",
      "Epoch 30 Batch 200 test Loss 1.2425\n",
      "Epoch 30 Batch 250 Loss 0.6149\n",
      "Epoch 30 Batch 250 test Loss 1.2427\n",
      "Epoch 30 Batch 300 Loss 0.6138\n",
      "Epoch 30 Batch 300 test Loss 1.2429\n",
      "Epoch 30 Batch 350 Loss 0.6123\n",
      "Epoch 30 Batch 350 test Loss 1.2430\n",
      "Epoch 30 Batch 400 Loss 0.6133\n",
      "Epoch 30 Batch 400 test Loss 1.2432\n",
      "Epoch 30 Batch 450 Loss 0.6146\n",
      "Epoch 30 Batch 450 test Loss 1.2434\n",
      "Epoch 30 Batch 500 Loss 0.6152\n",
      "Epoch 30 Batch 500 test Loss 1.2435\n",
      "Epoch 30 Batch 550 Loss 0.6135\n",
      "Epoch 30 Batch 550 test Loss 1.2437\n",
      "Epoch 30 Batch 600 Loss 0.6139\n",
      "Epoch 30 Batch 600 test Loss 1.2438\n",
      "Epoch 30 Batch 650 Loss 0.6150\n",
      "Epoch 30 Batch 650 test Loss 1.2440\n",
      "Epoch 30 Batch 700 Loss 0.6165\n",
      "Epoch 30 Batch 700 test Loss 1.2441\n",
      "Epoch 30 Batch 750 Loss 0.6159\n",
      "Epoch 30 Batch 750 test Loss 1.2443\n",
      "Epoch 30 Batch 800 Loss 0.6160\n",
      "Epoch 30 Batch 800 test Loss 1.2445\n",
      "Epoch 30 Batch 850 Loss 0.6158\n",
      "Epoch 30 Batch 850 test Loss 1.2446\n",
      "Epoch 30 Batch 900 Loss 0.6159\n",
      "Epoch 30 Batch 900 test Loss 1.2448\n",
      "Epoch 30 Batch 950 Loss 0.6158\n",
      "Epoch 30 Batch 950 test Loss 1.2450\n",
      "Epoch 31 Batch 0 Loss 0.5350\n",
      "Epoch 31 Batch 0 test Loss 1.2451\n",
      "Epoch 31 Batch 50 Loss 0.6021\n",
      "Epoch 31 Batch 50 test Loss 1.2453\n",
      "Epoch 31 Batch 100 Loss 0.6028\n",
      "Epoch 31 Batch 100 test Loss 1.2454\n",
      "Epoch 31 Batch 150 Loss 0.6088\n",
      "Epoch 31 Batch 150 test Loss 1.2456\n",
      "Epoch 31 Batch 200 Loss 0.6104\n",
      "Epoch 31 Batch 200 test Loss 1.2458\n",
      "Epoch 31 Batch 250 Loss 0.6089\n",
      "Epoch 31 Batch 250 test Loss 1.2459\n",
      "Epoch 31 Batch 300 Loss 0.6064\n",
      "Epoch 31 Batch 300 test Loss 1.2461\n",
      "Epoch 31 Batch 350 Loss 0.6076\n",
      "Epoch 31 Batch 350 test Loss 1.2462\n",
      "Epoch 31 Batch 400 Loss 0.6097\n",
      "Epoch 31 Batch 400 test Loss 1.2464\n",
      "Epoch 31 Batch 450 Loss 0.6073\n",
      "Epoch 31 Batch 450 test Loss 1.2466\n",
      "Epoch 31 Batch 500 Loss 0.6083\n",
      "Epoch 31 Batch 500 test Loss 1.2467\n",
      "Epoch 31 Batch 550 Loss 0.6081\n",
      "Epoch 31 Batch 550 test Loss 1.2469\n",
      "Epoch 31 Batch 600 Loss 0.6085\n",
      "Epoch 31 Batch 600 test Loss 1.2471\n",
      "Epoch 31 Batch 650 Loss 0.6086\n",
      "Epoch 31 Batch 650 test Loss 1.2472\n",
      "Epoch 31 Batch 700 Loss 0.6083\n",
      "Epoch 31 Batch 700 test Loss 1.2474\n",
      "Epoch 31 Batch 750 Loss 0.6085\n",
      "Epoch 31 Batch 750 test Loss 1.2475\n",
      "Epoch 31 Batch 800 Loss 0.6090\n",
      "Epoch 31 Batch 800 test Loss 1.2477\n",
      "Epoch 31 Batch 850 Loss 0.6075\n",
      "Epoch 31 Batch 850 test Loss 1.2479\n",
      "Epoch 31 Batch 900 Loss 0.6079\n",
      "Epoch 31 Batch 900 test Loss 1.2480\n",
      "Epoch 31 Batch 950 Loss 0.6079\n",
      "Epoch 31 Batch 950 test Loss 1.2482\n",
      "Epoch 32 Batch 0 Loss 0.4236\n",
      "Epoch 32 Batch 0 test Loss 1.2484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 50 Loss 0.6026\n",
      "Epoch 32 Batch 50 test Loss 1.2485\n",
      "Epoch 32 Batch 100 Loss 0.6060\n",
      "Epoch 32 Batch 100 test Loss 1.2487\n",
      "Epoch 32 Batch 150 Loss 0.6071\n",
      "Epoch 32 Batch 150 test Loss 1.2488\n",
      "Epoch 32 Batch 200 Loss 0.6069\n",
      "Epoch 32 Batch 200 test Loss 1.2490\n",
      "Epoch 32 Batch 250 Loss 0.6112\n",
      "Epoch 32 Batch 250 test Loss 1.2492\n",
      "Epoch 32 Batch 300 Loss 0.6090\n",
      "Epoch 32 Batch 300 test Loss 1.2493\n",
      "Epoch 32 Batch 350 Loss 0.6090\n",
      "Epoch 32 Batch 350 test Loss 1.2494\n",
      "Epoch 32 Batch 400 Loss 0.6084\n",
      "Epoch 32 Batch 400 test Loss 1.2496\n",
      "Epoch 32 Batch 450 Loss 0.6073\n",
      "Epoch 32 Batch 450 test Loss 1.2498\n",
      "Epoch 32 Batch 500 Loss 0.6075\n",
      "Epoch 32 Batch 500 test Loss 1.2499\n",
      "Epoch 32 Batch 550 Loss 0.6073\n",
      "Epoch 32 Batch 550 test Loss 1.2501\n",
      "Epoch 32 Batch 600 Loss 0.6084\n",
      "Epoch 32 Batch 600 test Loss 1.2502\n",
      "Epoch 32 Batch 650 Loss 0.6082\n",
      "Epoch 32 Batch 650 test Loss 1.2504\n",
      "Epoch 32 Batch 700 Loss 0.6093\n",
      "Epoch 32 Batch 700 test Loss 1.2506\n",
      "Epoch 32 Batch 750 Loss 0.6079\n",
      "Epoch 32 Batch 750 test Loss 1.2507\n",
      "Epoch 32 Batch 800 Loss 0.6073\n",
      "Epoch 32 Batch 800 test Loss 1.2509\n",
      "Epoch 32 Batch 850 Loss 0.6065\n",
      "Epoch 32 Batch 850 test Loss 1.2510\n",
      "Epoch 32 Batch 900 Loss 0.6079\n",
      "Epoch 32 Batch 900 test Loss 1.2512\n",
      "Epoch 32 Batch 950 Loss 0.6077\n",
      "Epoch 32 Batch 950 test Loss 1.2514\n",
      "Epoch 33 Batch 0 Loss 0.5703\n",
      "Epoch 33 Batch 0 test Loss 1.2515\n",
      "Epoch 33 Batch 50 Loss 0.6061\n",
      "Epoch 33 Batch 50 test Loss 1.2517\n",
      "Epoch 33 Batch 100 Loss 0.6058\n",
      "Epoch 33 Batch 100 test Loss 1.2518\n",
      "Epoch 33 Batch 150 Loss 0.6072\n",
      "Epoch 33 Batch 150 test Loss 1.2520\n",
      "Epoch 33 Batch 200 Loss 0.6071\n",
      "Epoch 33 Batch 200 test Loss 1.2521\n",
      "Epoch 33 Batch 250 Loss 0.6076\n",
      "Epoch 33 Batch 250 test Loss 1.2522\n",
      "Epoch 33 Batch 300 Loss 0.6074\n",
      "Epoch 33 Batch 300 test Loss 1.2524\n",
      "Epoch 33 Batch 350 Loss 0.6094\n",
      "Epoch 33 Batch 350 test Loss 1.2526\n",
      "Epoch 33 Batch 400 Loss 0.6060\n",
      "Epoch 33 Batch 400 test Loss 1.2527\n",
      "Epoch 33 Batch 450 Loss 0.6045\n",
      "Epoch 33 Batch 450 test Loss 1.2529\n",
      "Epoch 33 Batch 500 Loss 0.6032\n",
      "Epoch 33 Batch 500 test Loss 1.2530\n",
      "Epoch 33 Batch 550 Loss 0.6042\n",
      "Epoch 33 Batch 550 test Loss 1.2532\n",
      "Epoch 33 Batch 600 Loss 0.6047\n",
      "Epoch 33 Batch 600 test Loss 1.2533\n",
      "Epoch 33 Batch 650 Loss 0.6042\n",
      "Epoch 33 Batch 650 test Loss 1.2535\n",
      "Epoch 33 Batch 700 Loss 0.6068\n",
      "Epoch 33 Batch 700 test Loss 1.2536\n",
      "Epoch 33 Batch 750 Loss 0.6064\n",
      "Epoch 33 Batch 750 test Loss 1.2538\n",
      "Epoch 33 Batch 800 Loss 0.6057\n",
      "Epoch 33 Batch 800 test Loss 1.2539\n",
      "Epoch 33 Batch 850 Loss 0.6054\n",
      "Epoch 33 Batch 850 test Loss 1.2541\n",
      "Epoch 33 Batch 900 Loss 0.6060\n",
      "Epoch 33 Batch 900 test Loss 1.2543\n",
      "Epoch 33 Batch 950 Loss 0.6065\n",
      "Epoch 33 Batch 950 test Loss 1.2544\n",
      "Epoch 34 Batch 0 Loss 0.5700\n",
      "Epoch 34 Batch 0 test Loss 1.2546\n",
      "Epoch 34 Batch 50 Loss 0.6222\n",
      "Epoch 34 Batch 50 test Loss 1.2547\n",
      "Epoch 34 Batch 100 Loss 0.6116\n",
      "Epoch 34 Batch 100 test Loss 1.2549\n",
      "Epoch 34 Batch 150 Loss 0.6063\n",
      "Epoch 34 Batch 150 test Loss 1.2550\n",
      "Epoch 34 Batch 200 Loss 0.6032\n",
      "Epoch 34 Batch 200 test Loss 1.2552\n",
      "Epoch 34 Batch 250 Loss 0.6018\n",
      "Epoch 34 Batch 250 test Loss 1.2554\n",
      "Epoch 34 Batch 300 Loss 0.6025\n",
      "Epoch 34 Batch 300 test Loss 1.2555\n",
      "Epoch 34 Batch 350 Loss 0.6025\n",
      "Epoch 34 Batch 350 test Loss 1.2557\n",
      "Epoch 34 Batch 400 Loss 0.6032\n",
      "Epoch 34 Batch 400 test Loss 1.2558\n",
      "Epoch 34 Batch 450 Loss 0.6019\n",
      "Epoch 34 Batch 450 test Loss 1.2560\n",
      "Epoch 34 Batch 500 Loss 0.6027\n",
      "Epoch 34 Batch 500 test Loss 1.2561\n",
      "Epoch 34 Batch 550 Loss 0.6032\n",
      "Epoch 34 Batch 550 test Loss 1.2563\n",
      "Epoch 34 Batch 600 Loss 0.6038\n",
      "Epoch 34 Batch 600 test Loss 1.2564\n",
      "Epoch 34 Batch 650 Loss 0.6042\n",
      "Epoch 34 Batch 650 test Loss 1.2565\n",
      "Epoch 34 Batch 700 Loss 0.6033\n",
      "Epoch 34 Batch 700 test Loss 1.2567\n",
      "Epoch 34 Batch 750 Loss 0.6036\n",
      "Epoch 34 Batch 750 test Loss 1.2568\n",
      "Epoch 34 Batch 800 Loss 0.6033\n",
      "Epoch 34 Batch 800 test Loss 1.2570\n",
      "Epoch 34 Batch 850 Loss 0.6035\n",
      "Epoch 34 Batch 850 test Loss 1.2572\n",
      "Epoch 34 Batch 900 Loss 0.6044\n",
      "Epoch 34 Batch 900 test Loss 1.2573\n",
      "Epoch 34 Batch 950 Loss 0.6055\n",
      "Epoch 34 Batch 950 test Loss 1.2575\n",
      "Epoch 35 Batch 0 Loss 0.5756\n",
      "Epoch 35 Batch 0 test Loss 1.2576\n",
      "Epoch 35 Batch 50 Loss 0.5977\n",
      "Epoch 35 Batch 50 test Loss 1.2578\n",
      "Epoch 35 Batch 100 Loss 0.5994\n",
      "Epoch 35 Batch 100 test Loss 1.2579\n",
      "Epoch 35 Batch 150 Loss 0.6034\n",
      "Epoch 35 Batch 150 test Loss 1.2581\n",
      "Epoch 35 Batch 200 Loss 0.6025\n",
      "Epoch 35 Batch 200 test Loss 1.2582\n",
      "Epoch 35 Batch 250 Loss 0.6059\n",
      "Epoch 35 Batch 250 test Loss 1.2584\n",
      "Epoch 35 Batch 300 Loss 0.6075\n",
      "Epoch 35 Batch 300 test Loss 1.2585\n",
      "Epoch 35 Batch 350 Loss 0.6061\n",
      "Epoch 35 Batch 350 test Loss 1.2587\n",
      "Epoch 35 Batch 400 Loss 0.6067\n",
      "Epoch 35 Batch 400 test Loss 1.2588\n",
      "Epoch 35 Batch 450 Loss 0.6050\n",
      "Epoch 35 Batch 450 test Loss 1.2590\n",
      "Epoch 35 Batch 500 Loss 0.6022\n",
      "Epoch 35 Batch 500 test Loss 1.2591\n",
      "Epoch 35 Batch 550 Loss 0.6021\n",
      "Epoch 35 Batch 550 test Loss 1.2593\n",
      "Epoch 35 Batch 600 Loss 0.6023\n",
      "Epoch 35 Batch 600 test Loss 1.2594\n",
      "Epoch 35 Batch 650 Loss 0.6022\n",
      "Epoch 35 Batch 650 test Loss 1.2596\n",
      "Epoch 35 Batch 700 Loss 0.6018\n",
      "Epoch 35 Batch 700 test Loss 1.2597\n",
      "Epoch 35 Batch 750 Loss 0.6009\n",
      "Epoch 35 Batch 750 test Loss 1.2599\n",
      "Epoch 35 Batch 800 Loss 0.6011\n",
      "Epoch 35 Batch 800 test Loss 1.2600\n",
      "Epoch 35 Batch 850 Loss 0.6012\n",
      "Epoch 35 Batch 850 test Loss 1.2602\n",
      "Epoch 35 Batch 900 Loss 0.6007\n",
      "Epoch 35 Batch 900 test Loss 1.2603\n",
      "Epoch 35 Batch 950 Loss 0.6010\n",
      "Epoch 35 Batch 950 test Loss 1.2605\n",
      "Epoch 36 Batch 0 Loss 0.7412\n",
      "Epoch 36 Batch 0 test Loss 1.2606\n",
      "Epoch 36 Batch 50 Loss 0.6102\n",
      "Epoch 36 Batch 50 test Loss 1.2608\n",
      "Epoch 36 Batch 100 Loss 0.6097\n",
      "Epoch 36 Batch 100 test Loss 1.2610\n",
      "Epoch 36 Batch 150 Loss 0.6108\n",
      "Epoch 36 Batch 150 test Loss 1.2611\n",
      "Epoch 36 Batch 200 Loss 0.6056\n",
      "Epoch 36 Batch 200 test Loss 1.2613\n",
      "Epoch 36 Batch 250 Loss 0.6040\n",
      "Epoch 36 Batch 250 test Loss 1.2614\n",
      "Epoch 36 Batch 300 Loss 0.6051\n",
      "Epoch 36 Batch 300 test Loss 1.2616\n",
      "Epoch 36 Batch 350 Loss 0.6051\n",
      "Epoch 36 Batch 350 test Loss 1.2618\n",
      "Epoch 36 Batch 400 Loss 0.6051\n",
      "Epoch 36 Batch 400 test Loss 1.2619\n",
      "Epoch 36 Batch 450 Loss 0.6054\n",
      "Epoch 36 Batch 450 test Loss 1.2621\n",
      "Epoch 36 Batch 500 Loss 0.6043\n",
      "Epoch 36 Batch 500 test Loss 1.2622\n",
      "Epoch 36 Batch 550 Loss 0.6047\n",
      "Epoch 36 Batch 550 test Loss 1.2624\n",
      "Epoch 36 Batch 600 Loss 0.6052\n",
      "Epoch 36 Batch 600 test Loss 1.2625\n",
      "Epoch 36 Batch 650 Loss 0.6058\n",
      "Epoch 36 Batch 650 test Loss 1.2627\n",
      "Epoch 36 Batch 700 Loss 0.6051\n",
      "Epoch 36 Batch 700 test Loss 1.2628\n",
      "Epoch 36 Batch 750 Loss 0.6054\n",
      "Epoch 36 Batch 750 test Loss 1.2630\n",
      "Epoch 36 Batch 800 Loss 0.6047\n",
      "Epoch 36 Batch 800 test Loss 1.2631\n",
      "Epoch 36 Batch 850 Loss 0.6053\n",
      "Epoch 36 Batch 850 test Loss 1.2633\n",
      "Epoch 36 Batch 900 Loss 0.6045\n",
      "Epoch 36 Batch 900 test Loss 1.2634\n",
      "Epoch 36 Batch 950 Loss 0.6039\n",
      "Epoch 36 Batch 950 test Loss 1.2636\n",
      "Epoch 37 Batch 0 Loss 0.6499\n",
      "Epoch 37 Batch 0 test Loss 1.2637\n",
      "Epoch 37 Batch 50 Loss 0.5856\n",
      "Epoch 37 Batch 50 test Loss 1.2638\n",
      "Epoch 37 Batch 100 Loss 0.5909\n",
      "Epoch 37 Batch 100 test Loss 1.2640\n",
      "Epoch 37 Batch 150 Loss 0.5945\n",
      "Epoch 37 Batch 150 test Loss 1.2641\n",
      "Epoch 37 Batch 200 Loss 0.5945\n",
      "Epoch 37 Batch 200 test Loss 1.2642\n",
      "Epoch 37 Batch 250 Loss 0.5950\n",
      "Epoch 37 Batch 250 test Loss 1.2644\n",
      "Epoch 37 Batch 300 Loss 0.5951\n",
      "Epoch 37 Batch 300 test Loss 1.2645\n",
      "Epoch 37 Batch 350 Loss 0.5971\n",
      "Epoch 37 Batch 350 test Loss 1.2646\n",
      "Epoch 37 Batch 400 Loss 0.5989\n",
      "Epoch 37 Batch 400 test Loss 1.2648\n",
      "Epoch 37 Batch 450 Loss 0.5981\n",
      "Epoch 37 Batch 450 test Loss 1.2649\n",
      "Epoch 37 Batch 500 Loss 0.6003\n",
      "Epoch 37 Batch 500 test Loss 1.2651\n",
      "Epoch 37 Batch 550 Loss 0.6004\n",
      "Epoch 37 Batch 550 test Loss 1.2652\n",
      "Epoch 37 Batch 600 Loss 0.6013\n",
      "Epoch 37 Batch 600 test Loss 1.2653\n",
      "Epoch 37 Batch 650 Loss 0.6014\n",
      "Epoch 37 Batch 650 test Loss 1.2655\n",
      "Epoch 37 Batch 700 Loss 0.6000\n",
      "Epoch 37 Batch 700 test Loss 1.2656\n",
      "Epoch 37 Batch 750 Loss 0.5992\n",
      "Epoch 37 Batch 750 test Loss 1.2658\n",
      "Epoch 37 Batch 800 Loss 0.5991\n",
      "Epoch 37 Batch 800 test Loss 1.2659\n",
      "Epoch 37 Batch 850 Loss 0.5990\n",
      "Epoch 37 Batch 850 test Loss 1.2661\n",
      "Epoch 37 Batch 900 Loss 0.5992\n",
      "Epoch 37 Batch 900 test Loss 1.2662\n",
      "Epoch 37 Batch 950 Loss 0.5986\n",
      "Epoch 37 Batch 950 test Loss 1.2664\n",
      "Epoch 38 Batch 0 Loss 0.6715\n",
      "Epoch 38 Batch 0 test Loss 1.2665\n",
      "Epoch 38 Batch 50 Loss 0.5978\n",
      "Epoch 38 Batch 50 test Loss 1.2666\n",
      "Epoch 38 Batch 100 Loss 0.5950\n",
      "Epoch 38 Batch 100 test Loss 1.2668\n",
      "Epoch 38 Batch 150 Loss 0.5992\n",
      "Epoch 38 Batch 150 test Loss 1.2669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 200 Loss 0.6033\n",
      "Epoch 38 Batch 200 test Loss 1.2671\n",
      "Epoch 38 Batch 250 Loss 0.5991\n",
      "Epoch 38 Batch 250 test Loss 1.2672\n",
      "Epoch 38 Batch 300 Loss 0.5974\n",
      "Epoch 38 Batch 300 test Loss 1.2673\n",
      "Epoch 38 Batch 350 Loss 0.5968\n",
      "Epoch 38 Batch 350 test Loss 1.2675\n",
      "Epoch 38 Batch 400 Loss 0.5961\n",
      "Epoch 38 Batch 400 test Loss 1.2676\n",
      "Epoch 38 Batch 450 Loss 0.5962\n",
      "Epoch 38 Batch 450 test Loss 1.2677\n",
      "Epoch 38 Batch 500 Loss 0.5964\n",
      "Epoch 38 Batch 500 test Loss 1.2679\n",
      "Epoch 38 Batch 550 Loss 0.5957\n",
      "Epoch 38 Batch 550 test Loss 1.2680\n",
      "Epoch 38 Batch 600 Loss 0.5958\n",
      "Epoch 38 Batch 600 test Loss 1.2682\n",
      "Epoch 38 Batch 650 Loss 0.5973\n",
      "Epoch 38 Batch 650 test Loss 1.2683\n",
      "Epoch 38 Batch 700 Loss 0.5971\n",
      "Epoch 38 Batch 700 test Loss 1.2684\n",
      "Epoch 38 Batch 750 Loss 0.5963\n",
      "Epoch 38 Batch 750 test Loss 1.2686\n",
      "Epoch 38 Batch 800 Loss 0.5962\n",
      "Epoch 38 Batch 800 test Loss 1.2687\n",
      "Epoch 38 Batch 850 Loss 0.5956\n",
      "Epoch 38 Batch 850 test Loss 1.2688\n",
      "Epoch 38 Batch 900 Loss 0.5954\n",
      "Epoch 38 Batch 900 test Loss 1.2690\n",
      "Epoch 38 Batch 950 Loss 0.5959\n",
      "Epoch 38 Batch 950 test Loss 1.2691\n",
      "Epoch 39 Batch 0 Loss 0.7140\n",
      "Epoch 39 Batch 0 test Loss 1.2693\n",
      "Epoch 39 Batch 50 Loss 0.5989\n",
      "Epoch 39 Batch 50 test Loss 1.2694\n",
      "Epoch 39 Batch 100 Loss 0.5941\n",
      "Epoch 39 Batch 100 test Loss 1.2696\n",
      "Epoch 39 Batch 150 Loss 0.5992\n",
      "Epoch 39 Batch 150 test Loss 1.2697\n",
      "Epoch 39 Batch 200 Loss 0.5986\n",
      "Epoch 39 Batch 200 test Loss 1.2698\n",
      "Epoch 39 Batch 250 Loss 0.5990\n",
      "Epoch 39 Batch 250 test Loss 1.2700\n",
      "Epoch 39 Batch 300 Loss 0.5947\n",
      "Epoch 39 Batch 300 test Loss 1.2701\n",
      "Epoch 39 Batch 350 Loss 0.5979\n",
      "Epoch 39 Batch 350 test Loss 1.2703\n",
      "Epoch 39 Batch 400 Loss 0.5978\n",
      "Epoch 39 Batch 400 test Loss 1.2704\n",
      "Epoch 39 Batch 450 Loss 0.5976\n",
      "Epoch 39 Batch 450 test Loss 1.2705\n",
      "Epoch 39 Batch 500 Loss 0.5980\n",
      "Epoch 39 Batch 500 test Loss 1.2707\n",
      "Epoch 39 Batch 550 Loss 0.5978\n",
      "Epoch 39 Batch 550 test Loss 1.2708\n",
      "Epoch 39 Batch 600 Loss 0.5981\n",
      "Epoch 39 Batch 600 test Loss 1.2710\n",
      "Epoch 39 Batch 650 Loss 0.5991\n",
      "Epoch 39 Batch 650 test Loss 1.2711\n",
      "Epoch 39 Batch 700 Loss 0.5975\n",
      "Epoch 39 Batch 700 test Loss 1.2712\n",
      "Epoch 39 Batch 750 Loss 0.5984\n",
      "Epoch 39 Batch 750 test Loss 1.2714\n",
      "Epoch 39 Batch 800 Loss 0.5977\n",
      "Epoch 39 Batch 800 test Loss 1.2715\n",
      "Epoch 39 Batch 850 Loss 0.5978\n",
      "Epoch 39 Batch 850 test Loss 1.2717\n",
      "Epoch 39 Batch 900 Loss 0.5981\n",
      "Epoch 39 Batch 900 test Loss 1.2718\n",
      "Epoch 39 Batch 950 Loss 0.5972\n",
      "Epoch 39 Batch 950 test Loss 1.2720\n",
      "Epoch 40 Batch 0 Loss 0.6054\n",
      "Epoch 40 Batch 0 test Loss 1.2721\n",
      "Epoch 40 Batch 50 Loss 0.6029\n",
      "Epoch 40 Batch 50 test Loss 1.2722\n",
      "Epoch 40 Batch 100 Loss 0.6094\n",
      "Epoch 40 Batch 100 test Loss 1.2724\n",
      "Epoch 40 Batch 150 Loss 0.5989\n",
      "Epoch 40 Batch 150 test Loss 1.2725\n",
      "Epoch 40 Batch 200 Loss 0.5991\n",
      "Epoch 40 Batch 200 test Loss 1.2726\n",
      "Epoch 40 Batch 250 Loss 0.5946\n",
      "Epoch 40 Batch 250 test Loss 1.2728\n",
      "Epoch 40 Batch 300 Loss 0.5946\n",
      "Epoch 40 Batch 300 test Loss 1.2729\n",
      "Epoch 40 Batch 350 Loss 0.5951\n",
      "Epoch 40 Batch 350 test Loss 1.2731\n",
      "Epoch 40 Batch 400 Loss 0.5957\n",
      "Epoch 40 Batch 400 test Loss 1.2732\n",
      "Epoch 40 Batch 450 Loss 0.5957\n",
      "Epoch 40 Batch 450 test Loss 1.2733\n",
      "Epoch 40 Batch 500 Loss 0.5949\n",
      "Epoch 40 Batch 500 test Loss 1.2735\n",
      "Epoch 40 Batch 550 Loss 0.5951\n",
      "Epoch 40 Batch 550 test Loss 1.2736\n",
      "Epoch 40 Batch 600 Loss 0.5946\n",
      "Epoch 40 Batch 600 test Loss 1.2737\n",
      "Epoch 40 Batch 650 Loss 0.5945\n",
      "Epoch 40 Batch 650 test Loss 1.2739\n",
      "Epoch 40 Batch 700 Loss 0.5948\n",
      "Epoch 40 Batch 700 test Loss 1.2740\n",
      "Epoch 40 Batch 750 Loss 0.5938\n",
      "Epoch 40 Batch 750 test Loss 1.2742\n",
      "Epoch 40 Batch 800 Loss 0.5936\n",
      "Epoch 40 Batch 800 test Loss 1.2743\n",
      "Epoch 40 Batch 850 Loss 0.5939\n",
      "Epoch 40 Batch 850 test Loss 1.2744\n",
      "Epoch 40 Batch 900 Loss 0.5934\n",
      "Epoch 40 Batch 900 test Loss 1.2746\n",
      "Epoch 40 Batch 950 Loss 0.5944\n",
      "Epoch 40 Batch 950 test Loss 1.2747\n",
      "Epoch 41 Batch 0 Loss 0.6264\n",
      "Epoch 41 Batch 0 test Loss 1.2749\n",
      "Epoch 41 Batch 50 Loss 0.5867\n",
      "Epoch 41 Batch 50 test Loss 1.2750\n",
      "Epoch 41 Batch 100 Loss 0.5943\n",
      "Epoch 41 Batch 100 test Loss 1.2752\n",
      "Epoch 41 Batch 150 Loss 0.5933\n",
      "Epoch 41 Batch 150 test Loss 1.2753\n",
      "Epoch 41 Batch 200 Loss 0.5955\n",
      "Epoch 41 Batch 200 test Loss 1.2754\n",
      "Epoch 41 Batch 250 Loss 0.5908\n",
      "Epoch 41 Batch 250 test Loss 1.2756\n",
      "Epoch 41 Batch 300 Loss 0.5893\n",
      "Epoch 41 Batch 300 test Loss 1.2757\n",
      "Epoch 41 Batch 350 Loss 0.5878\n",
      "Epoch 41 Batch 350 test Loss 1.2758\n",
      "Epoch 41 Batch 400 Loss 0.5882\n",
      "Epoch 41 Batch 400 test Loss 1.2760\n",
      "Epoch 41 Batch 450 Loss 0.5871\n",
      "Epoch 41 Batch 450 test Loss 1.2761\n",
      "Epoch 41 Batch 500 Loss 0.5863\n",
      "Epoch 41 Batch 500 test Loss 1.2762\n",
      "Epoch 41 Batch 550 Loss 0.5871\n",
      "Epoch 41 Batch 550 test Loss 1.2764\n",
      "Epoch 41 Batch 600 Loss 0.5853\n",
      "Epoch 41 Batch 600 test Loss 1.2765\n",
      "Epoch 41 Batch 650 Loss 0.5850\n",
      "Epoch 41 Batch 650 test Loss 1.2766\n",
      "Epoch 41 Batch 700 Loss 0.5834\n",
      "Epoch 41 Batch 700 test Loss 1.2768\n",
      "Epoch 41 Batch 750 Loss 0.5835\n",
      "Epoch 41 Batch 750 test Loss 1.2769\n",
      "Epoch 41 Batch 800 Loss 0.5830\n",
      "Epoch 41 Batch 800 test Loss 1.2770\n",
      "Epoch 41 Batch 850 Loss 0.5835\n",
      "Epoch 41 Batch 850 test Loss 1.2772\n",
      "Epoch 41 Batch 900 Loss 0.5838\n",
      "Epoch 41 Batch 900 test Loss 1.2773\n",
      "Epoch 41 Batch 950 Loss 0.5832\n",
      "Epoch 41 Batch 950 test Loss 1.2774\n",
      "Epoch 42 Batch 0 Loss 0.6042\n",
      "Epoch 42 Batch 0 test Loss 1.2776\n",
      "Epoch 42 Batch 50 Loss 0.5816\n",
      "Epoch 42 Batch 50 test Loss 1.2777\n",
      "Epoch 42 Batch 100 Loss 0.5911\n",
      "Epoch 42 Batch 100 test Loss 1.2778\n",
      "Epoch 42 Batch 150 Loss 0.5937\n",
      "Epoch 42 Batch 150 test Loss 1.2779\n",
      "Epoch 42 Batch 200 Loss 0.5891\n",
      "Epoch 42 Batch 200 test Loss 1.2781\n",
      "Epoch 42 Batch 250 Loss 0.5910\n",
      "Epoch 42 Batch 250 test Loss 1.2782\n",
      "Epoch 42 Batch 300 Loss 0.5911\n",
      "Epoch 42 Batch 300 test Loss 1.2783\n",
      "Epoch 42 Batch 350 Loss 0.5933\n",
      "Epoch 42 Batch 350 test Loss 1.2785\n",
      "Epoch 42 Batch 400 Loss 0.5903\n",
      "Epoch 42 Batch 400 test Loss 1.2786\n",
      "Epoch 42 Batch 450 Loss 0.5913\n",
      "Epoch 42 Batch 450 test Loss 1.2787\n",
      "Epoch 42 Batch 500 Loss 0.5905\n",
      "Epoch 42 Batch 500 test Loss 1.2789\n",
      "Epoch 42 Batch 550 Loss 0.5899\n",
      "Epoch 42 Batch 550 test Loss 1.2790\n",
      "Epoch 42 Batch 600 Loss 0.5895\n",
      "Epoch 42 Batch 600 test Loss 1.2791\n",
      "Epoch 42 Batch 650 Loss 0.5885\n",
      "Epoch 42 Batch 650 test Loss 1.2793\n",
      "Epoch 42 Batch 700 Loss 0.5888\n",
      "Epoch 42 Batch 700 test Loss 1.2794\n",
      "Epoch 42 Batch 750 Loss 0.5876\n",
      "Epoch 42 Batch 750 test Loss 1.2795\n",
      "Epoch 42 Batch 800 Loss 0.5878\n",
      "Epoch 42 Batch 800 test Loss 1.2797\n",
      "Epoch 42 Batch 850 Loss 0.5878\n",
      "Epoch 42 Batch 850 test Loss 1.2798\n",
      "Epoch 42 Batch 900 Loss 0.5865\n",
      "Epoch 42 Batch 900 test Loss 1.2799\n",
      "Epoch 42 Batch 950 Loss 0.5860\n",
      "Epoch 42 Batch 950 test Loss 1.2801\n",
      "Epoch 43 Batch 0 Loss 0.6396\n",
      "Epoch 43 Batch 0 test Loss 1.2802\n",
      "Epoch 43 Batch 50 Loss 0.5689\n",
      "Epoch 43 Batch 50 test Loss 1.2803\n",
      "Epoch 43 Batch 100 Loss 0.5802\n",
      "Epoch 43 Batch 100 test Loss 1.2805\n",
      "Epoch 43 Batch 150 Loss 0.5838\n",
      "Epoch 43 Batch 150 test Loss 1.2806\n",
      "Epoch 43 Batch 200 Loss 0.5846\n",
      "Epoch 43 Batch 200 test Loss 1.2807\n",
      "Epoch 43 Batch 250 Loss 0.5834\n",
      "Epoch 43 Batch 250 test Loss 1.2809\n",
      "Epoch 43 Batch 300 Loss 0.5855\n",
      "Epoch 43 Batch 300 test Loss 1.2810\n",
      "Epoch 43 Batch 350 Loss 0.5852\n",
      "Epoch 43 Batch 350 test Loss 1.2811\n",
      "Epoch 43 Batch 400 Loss 0.5855\n",
      "Epoch 43 Batch 400 test Loss 1.2813\n",
      "Epoch 43 Batch 450 Loss 0.5842\n",
      "Epoch 43 Batch 450 test Loss 1.2814\n",
      "Epoch 43 Batch 500 Loss 0.5848\n",
      "Epoch 43 Batch 500 test Loss 1.2815\n",
      "Epoch 43 Batch 550 Loss 0.5849\n",
      "Epoch 43 Batch 550 test Loss 1.2816\n",
      "Epoch 43 Batch 600 Loss 0.5847\n",
      "Epoch 43 Batch 600 test Loss 1.2818\n",
      "Epoch 43 Batch 650 Loss 0.5841\n",
      "Epoch 43 Batch 650 test Loss 1.2819\n",
      "Epoch 43 Batch 700 Loss 0.5844\n",
      "Epoch 43 Batch 700 test Loss 1.2820\n",
      "Epoch 43 Batch 750 Loss 0.5855\n",
      "Epoch 43 Batch 750 test Loss 1.2822\n",
      "Epoch 43 Batch 800 Loss 0.5862\n",
      "Epoch 43 Batch 800 test Loss 1.2823\n",
      "Epoch 43 Batch 850 Loss 0.5864\n",
      "Epoch 43 Batch 850 test Loss 1.2824\n",
      "Epoch 43 Batch 900 Loss 0.5865\n",
      "Epoch 43 Batch 900 test Loss 1.2826\n",
      "Epoch 43 Batch 950 Loss 0.5862\n",
      "Epoch 43 Batch 950 test Loss 1.2827\n",
      "Epoch 44 Batch 0 Loss 0.4964\n",
      "Epoch 44 Batch 0 test Loss 1.2828\n",
      "Epoch 44 Batch 50 Loss 0.5959\n",
      "Epoch 44 Batch 50 test Loss 1.2829\n",
      "Epoch 44 Batch 100 Loss 0.5942\n",
      "Epoch 44 Batch 100 test Loss 1.2831\n",
      "Epoch 44 Batch 150 Loss 0.5902\n",
      "Epoch 44 Batch 150 test Loss 1.2832\n",
      "Epoch 44 Batch 200 Loss 0.5917\n",
      "Epoch 44 Batch 200 test Loss 1.2833\n",
      "Epoch 44 Batch 250 Loss 0.5900\n",
      "Epoch 44 Batch 250 test Loss 1.2835\n",
      "Epoch 44 Batch 300 Loss 0.5923\n",
      "Epoch 44 Batch 300 test Loss 1.2836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 350 Loss 0.5941\n",
      "Epoch 44 Batch 350 test Loss 1.2837\n",
      "Epoch 44 Batch 400 Loss 0.5932\n",
      "Epoch 44 Batch 400 test Loss 1.2838\n",
      "Epoch 44 Batch 450 Loss 0.5920\n",
      "Epoch 44 Batch 450 test Loss 1.2840\n",
      "Epoch 44 Batch 500 Loss 0.5919\n",
      "Epoch 44 Batch 500 test Loss 1.2841\n",
      "Epoch 44 Batch 550 Loss 0.5899\n",
      "Epoch 44 Batch 550 test Loss 1.2842\n",
      "Epoch 44 Batch 600 Loss 0.5905\n",
      "Epoch 44 Batch 600 test Loss 1.2843\n",
      "Epoch 44 Batch 650 Loss 0.5903\n",
      "Epoch 44 Batch 650 test Loss 1.2844\n",
      "Epoch 44 Batch 700 Loss 0.5896\n",
      "Epoch 44 Batch 700 test Loss 1.2846\n",
      "Epoch 44 Batch 750 Loss 0.5886\n",
      "Epoch 44 Batch 750 test Loss 1.2847\n",
      "Epoch 44 Batch 800 Loss 0.5873\n",
      "Epoch 44 Batch 800 test Loss 1.2848\n",
      "Epoch 44 Batch 850 Loss 0.5875\n",
      "Epoch 44 Batch 850 test Loss 1.2849\n",
      "Epoch 44 Batch 900 Loss 0.5879\n",
      "Epoch 44 Batch 900 test Loss 1.2850\n",
      "Epoch 44 Batch 950 Loss 0.5888\n",
      "Epoch 44 Batch 950 test Loss 1.2852\n",
      "Epoch 45 Batch 0 Loss 0.6379\n",
      "Epoch 45 Batch 0 test Loss 1.2853\n",
      "Epoch 45 Batch 50 Loss 0.5793\n",
      "Epoch 45 Batch 50 test Loss 1.2854\n",
      "Epoch 45 Batch 100 Loss 0.5822\n",
      "Epoch 45 Batch 100 test Loss 1.2856\n",
      "Epoch 45 Batch 150 Loss 0.5863\n",
      "Epoch 45 Batch 150 test Loss 1.2857\n",
      "Epoch 45 Batch 200 Loss 0.5853\n",
      "Epoch 45 Batch 200 test Loss 1.2858\n",
      "Epoch 45 Batch 250 Loss 0.5876\n",
      "Epoch 45 Batch 250 test Loss 1.2860\n",
      "Epoch 45 Batch 300 Loss 0.5860\n",
      "Epoch 45 Batch 300 test Loss 1.2861\n",
      "Epoch 45 Batch 350 Loss 0.5829\n",
      "Epoch 45 Batch 350 test Loss 1.2862\n",
      "Epoch 45 Batch 400 Loss 0.5816\n",
      "Epoch 45 Batch 400 test Loss 1.2863\n",
      "Epoch 45 Batch 450 Loss 0.5826\n",
      "Epoch 45 Batch 450 test Loss 1.2864\n",
      "Epoch 45 Batch 500 Loss 0.5826\n",
      "Epoch 45 Batch 500 test Loss 1.2866\n",
      "Epoch 45 Batch 550 Loss 0.5834\n",
      "Epoch 45 Batch 550 test Loss 1.2867\n",
      "Epoch 45 Batch 600 Loss 0.5833\n",
      "Epoch 45 Batch 600 test Loss 1.2868\n",
      "Epoch 45 Batch 650 Loss 0.5832\n",
      "Epoch 45 Batch 650 test Loss 1.2870\n",
      "Epoch 45 Batch 700 Loss 0.5842\n",
      "Epoch 45 Batch 700 test Loss 1.2871\n",
      "Epoch 45 Batch 750 Loss 0.5828\n",
      "Epoch 45 Batch 750 test Loss 1.2872\n",
      "Epoch 45 Batch 800 Loss 0.5838\n",
      "Epoch 45 Batch 800 test Loss 1.2873\n",
      "Epoch 45 Batch 850 Loss 0.5838\n",
      "Epoch 45 Batch 850 test Loss 1.2875\n",
      "Epoch 45 Batch 900 Loss 0.5842\n",
      "Epoch 45 Batch 900 test Loss 1.2876\n",
      "Epoch 45 Batch 950 Loss 0.5849\n",
      "Epoch 45 Batch 950 test Loss 1.2877\n",
      "Epoch 46 Batch 0 Loss 0.5285\n",
      "Epoch 46 Batch 0 test Loss 1.2878\n",
      "Epoch 46 Batch 50 Loss 0.5816\n",
      "Epoch 46 Batch 50 test Loss 1.2879\n",
      "Epoch 46 Batch 100 Loss 0.5879\n",
      "Epoch 46 Batch 100 test Loss 1.2881\n",
      "Epoch 46 Batch 150 Loss 0.5832\n",
      "Epoch 46 Batch 150 test Loss 1.2882\n",
      "Epoch 46 Batch 200 Loss 0.5857\n",
      "Epoch 46 Batch 200 test Loss 1.2883\n",
      "Epoch 46 Batch 250 Loss 0.5893\n",
      "Epoch 46 Batch 250 test Loss 1.2884\n",
      "Epoch 46 Batch 300 Loss 0.5890\n",
      "Epoch 46 Batch 300 test Loss 1.2885\n",
      "Epoch 46 Batch 350 Loss 0.5900\n",
      "Epoch 46 Batch 350 test Loss 1.2886\n",
      "Epoch 46 Batch 400 Loss 0.5883\n",
      "Epoch 46 Batch 400 test Loss 1.2888\n",
      "Epoch 46 Batch 450 Loss 0.5865\n",
      "Epoch 46 Batch 450 test Loss 1.2889\n",
      "Epoch 46 Batch 500 Loss 0.5867\n",
      "Epoch 46 Batch 500 test Loss 1.2890\n",
      "Epoch 46 Batch 550 Loss 0.5897\n",
      "Epoch 46 Batch 550 test Loss 1.2891\n",
      "Epoch 46 Batch 600 Loss 0.5900\n",
      "Epoch 46 Batch 600 test Loss 1.2892\n",
      "Epoch 46 Batch 650 Loss 0.5905\n",
      "Epoch 46 Batch 650 test Loss 1.2893\n",
      "Epoch 46 Batch 700 Loss 0.5894\n",
      "Epoch 46 Batch 700 test Loss 1.2895\n",
      "Epoch 46 Batch 750 Loss 0.5891\n",
      "Epoch 46 Batch 750 test Loss 1.2896\n",
      "Epoch 46 Batch 800 Loss 0.5874\n",
      "Epoch 46 Batch 800 test Loss 1.2897\n",
      "Epoch 46 Batch 850 Loss 0.5864\n",
      "Epoch 46 Batch 850 test Loss 1.2898\n",
      "Epoch 46 Batch 900 Loss 0.5866\n",
      "Epoch 46 Batch 900 test Loss 1.2899\n",
      "Epoch 46 Batch 950 Loss 0.5867\n",
      "Epoch 46 Batch 950 test Loss 1.2900\n",
      "Epoch 47 Batch 0 Loss 0.5502\n",
      "Epoch 47 Batch 0 test Loss 1.2902\n",
      "Epoch 47 Batch 50 Loss 0.5898\n",
      "Epoch 47 Batch 50 test Loss 1.2903\n",
      "Epoch 47 Batch 100 Loss 0.5951\n",
      "Epoch 47 Batch 100 test Loss 1.2904\n",
      "Epoch 47 Batch 150 Loss 0.5896\n",
      "Epoch 47 Batch 150 test Loss 1.2905\n",
      "Epoch 47 Batch 200 Loss 0.5824\n",
      "Epoch 47 Batch 200 test Loss 1.2906\n",
      "Epoch 47 Batch 250 Loss 0.5831\n",
      "Epoch 47 Batch 250 test Loss 1.2907\n",
      "Epoch 47 Batch 300 Loss 0.5842\n",
      "Epoch 47 Batch 300 test Loss 1.2908\n",
      "Epoch 47 Batch 350 Loss 0.5821\n",
      "Epoch 47 Batch 350 test Loss 1.2909\n",
      "Epoch 47 Batch 400 Loss 0.5802\n",
      "Epoch 47 Batch 400 test Loss 1.2911\n",
      "Epoch 47 Batch 450 Loss 0.5807\n",
      "Epoch 47 Batch 450 test Loss 1.2912\n",
      "Epoch 47 Batch 500 Loss 0.5819\n",
      "Epoch 47 Batch 500 test Loss 1.2913\n",
      "Epoch 47 Batch 550 Loss 0.5824\n",
      "Epoch 47 Batch 550 test Loss 1.2914\n",
      "Epoch 47 Batch 600 Loss 0.5823\n",
      "Epoch 47 Batch 600 test Loss 1.2915\n",
      "Epoch 47 Batch 650 Loss 0.5821\n",
      "Epoch 47 Batch 650 test Loss 1.2916\n",
      "Epoch 47 Batch 700 Loss 0.5823\n",
      "Epoch 47 Batch 700 test Loss 1.2918\n",
      "Epoch 47 Batch 750 Loss 0.5818\n",
      "Epoch 47 Batch 750 test Loss 1.2919\n",
      "Epoch 47 Batch 800 Loss 0.5819\n",
      "Epoch 47 Batch 800 test Loss 1.2920\n",
      "Epoch 47 Batch 850 Loss 0.5808\n",
      "Epoch 47 Batch 850 test Loss 1.2921\n",
      "Epoch 47 Batch 900 Loss 0.5807\n",
      "Epoch 47 Batch 900 test Loss 1.2922\n",
      "Epoch 47 Batch 950 Loss 0.5810\n",
      "Epoch 47 Batch 950 test Loss 1.2923\n",
      "Epoch 48 Batch 0 Loss 0.4727\n",
      "Epoch 48 Batch 0 test Loss 1.2925\n",
      "Epoch 48 Batch 50 Loss 0.5805\n",
      "Epoch 48 Batch 50 test Loss 1.2926\n",
      "Epoch 48 Batch 100 Loss 0.5806\n",
      "Epoch 48 Batch 100 test Loss 1.2927\n",
      "Epoch 48 Batch 150 Loss 0.5913\n",
      "Epoch 48 Batch 150 test Loss 1.2928\n",
      "Epoch 48 Batch 200 Loss 0.5901\n",
      "Epoch 48 Batch 200 test Loss 1.2929\n",
      "Epoch 48 Batch 250 Loss 0.5869\n",
      "Epoch 48 Batch 250 test Loss 1.2930\n",
      "Epoch 48 Batch 300 Loss 0.5872\n",
      "Epoch 48 Batch 300 test Loss 1.2931\n",
      "Epoch 48 Batch 350 Loss 0.5881\n",
      "Epoch 48 Batch 350 test Loss 1.2932\n",
      "Epoch 48 Batch 400 Loss 0.5850\n",
      "Epoch 48 Batch 400 test Loss 1.2933\n",
      "Epoch 48 Batch 450 Loss 0.5840\n",
      "Epoch 48 Batch 450 test Loss 1.2934\n",
      "Epoch 48 Batch 500 Loss 0.5843\n",
      "Epoch 48 Batch 500 test Loss 1.2936\n",
      "Epoch 48 Batch 550 Loss 0.5825\n",
      "Epoch 48 Batch 550 test Loss 1.2937\n",
      "Epoch 48 Batch 600 Loss 0.5821\n",
      "Epoch 48 Batch 600 test Loss 1.2938\n",
      "Epoch 48 Batch 650 Loss 0.5832\n",
      "Epoch 48 Batch 650 test Loss 1.2939\n",
      "Epoch 48 Batch 700 Loss 0.5830\n",
      "Epoch 48 Batch 700 test Loss 1.2940\n",
      "Epoch 48 Batch 750 Loss 0.5844\n",
      "Epoch 48 Batch 750 test Loss 1.2941\n",
      "Epoch 48 Batch 800 Loss 0.5835\n",
      "Epoch 48 Batch 800 test Loss 1.2942\n",
      "Epoch 48 Batch 850 Loss 0.5834\n",
      "Epoch 48 Batch 850 test Loss 1.2943\n",
      "Epoch 48 Batch 900 Loss 0.5836\n",
      "Epoch 48 Batch 900 test Loss 1.2944\n",
      "Epoch 48 Batch 950 Loss 0.5843\n",
      "Epoch 48 Batch 950 test Loss 1.2945\n",
      "Epoch 49 Batch 0 Loss 0.5272\n",
      "Epoch 49 Batch 0 test Loss 1.2947\n",
      "Epoch 49 Batch 50 Loss 0.5712\n",
      "Epoch 49 Batch 50 test Loss 1.2948\n",
      "Epoch 49 Batch 100 Loss 0.5741\n",
      "Epoch 49 Batch 100 test Loss 1.2949\n",
      "Epoch 49 Batch 150 Loss 0.5771\n",
      "Epoch 49 Batch 150 test Loss 1.2950\n",
      "Epoch 49 Batch 200 Loss 0.5751\n",
      "Epoch 49 Batch 200 test Loss 1.2951\n",
      "Epoch 49 Batch 250 Loss 0.5732\n",
      "Epoch 49 Batch 250 test Loss 1.2952\n",
      "Epoch 49 Batch 300 Loss 0.5756\n",
      "Epoch 49 Batch 300 test Loss 1.2953\n",
      "Epoch 49 Batch 350 Loss 0.5746\n",
      "Epoch 49 Batch 350 test Loss 1.2954\n",
      "Epoch 49 Batch 400 Loss 0.5759\n",
      "Epoch 49 Batch 400 test Loss 1.2955\n",
      "Epoch 49 Batch 450 Loss 0.5765\n",
      "Epoch 49 Batch 450 test Loss 1.2956\n",
      "Epoch 49 Batch 500 Loss 0.5776\n",
      "Epoch 49 Batch 500 test Loss 1.2958\n",
      "Epoch 49 Batch 550 Loss 0.5762\n",
      "Epoch 49 Batch 550 test Loss 1.2959\n",
      "Epoch 49 Batch 600 Loss 0.5751\n",
      "Epoch 49 Batch 600 test Loss 1.2960\n",
      "Epoch 49 Batch 650 Loss 0.5759\n",
      "Epoch 49 Batch 650 test Loss 1.2961\n",
      "Epoch 49 Batch 700 Loss 0.5748\n",
      "Epoch 49 Batch 700 test Loss 1.2962\n",
      "Epoch 49 Batch 750 Loss 0.5755\n",
      "Epoch 49 Batch 750 test Loss 1.2963\n",
      "Epoch 49 Batch 800 Loss 0.5752\n",
      "Epoch 49 Batch 800 test Loss 1.2964\n",
      "Epoch 49 Batch 850 Loss 0.5744\n",
      "Epoch 49 Batch 850 test Loss 1.2965\n",
      "Epoch 49 Batch 900 Loss 0.5756\n",
      "Epoch 49 Batch 900 test Loss 1.2966\n",
      "Epoch 49 Batch 950 Loss 0.5754\n",
      "Epoch 49 Batch 950 test Loss 1.2967\n",
      "Epoch 50 Batch 0 Loss 0.5767\n",
      "Epoch 50 Batch 0 test Loss 1.2968\n",
      "Epoch 50 Batch 50 Loss 0.5747\n",
      "Epoch 50 Batch 50 test Loss 1.2970\n",
      "Epoch 50 Batch 100 Loss 0.5687\n",
      "Epoch 50 Batch 100 test Loss 1.2971\n",
      "Epoch 50 Batch 150 Loss 0.5683\n",
      "Epoch 50 Batch 150 test Loss 1.2972\n",
      "Epoch 50 Batch 200 Loss 0.5688\n",
      "Epoch 50 Batch 200 test Loss 1.2973\n",
      "Epoch 50 Batch 250 Loss 0.5672\n",
      "Epoch 50 Batch 250 test Loss 1.2974\n",
      "Epoch 50 Batch 300 Loss 0.5704\n",
      "Epoch 50 Batch 300 test Loss 1.2975\n",
      "Epoch 50 Batch 350 Loss 0.5676\n",
      "Epoch 50 Batch 350 test Loss 1.2976\n",
      "Epoch 50 Batch 400 Loss 0.5716\n",
      "Epoch 50 Batch 400 test Loss 1.2978\n",
      "Epoch 50 Batch 450 Loss 0.5735\n",
      "Epoch 50 Batch 450 test Loss 1.2979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 500 Loss 0.5760\n",
      "Epoch 50 Batch 500 test Loss 1.2980\n",
      "Epoch 50 Batch 550 Loss 0.5751\n",
      "Epoch 50 Batch 550 test Loss 1.2981\n",
      "Epoch 50 Batch 600 Loss 0.5747\n",
      "Epoch 50 Batch 600 test Loss 1.2982\n",
      "Epoch 50 Batch 650 Loss 0.5757\n",
      "Epoch 50 Batch 650 test Loss 1.2983\n",
      "Epoch 50 Batch 700 Loss 0.5748\n",
      "Epoch 50 Batch 700 test Loss 1.2984\n",
      "Epoch 50 Batch 750 Loss 0.5745\n",
      "Epoch 50 Batch 750 test Loss 1.2985\n",
      "Epoch 50 Batch 800 Loss 0.5749\n",
      "Epoch 50 Batch 800 test Loss 1.2987\n",
      "Epoch 50 Batch 850 Loss 0.5752\n",
      "Epoch 50 Batch 850 test Loss 1.2988\n",
      "Epoch 50 Batch 900 Loss 0.5756\n",
      "Epoch 50 Batch 900 test Loss 1.2989\n",
      "Epoch 50 Batch 950 Loss 0.5757\n",
      "Epoch 50 Batch 950 test Loss 1.2990\n",
      "Epoch 51 Batch 0 Loss 0.6689\n",
      "Epoch 51 Batch 0 test Loss 1.2991\n",
      "Epoch 51 Batch 50 Loss 0.5702\n",
      "Epoch 51 Batch 50 test Loss 1.2992\n",
      "Epoch 51 Batch 100 Loss 0.5740\n",
      "Epoch 51 Batch 100 test Loss 1.2994\n",
      "Epoch 51 Batch 150 Loss 0.5737\n",
      "Epoch 51 Batch 150 test Loss 1.2995\n",
      "Epoch 51 Batch 200 Loss 0.5735\n",
      "Epoch 51 Batch 200 test Loss 1.2996\n",
      "Epoch 51 Batch 250 Loss 0.5733\n",
      "Epoch 51 Batch 250 test Loss 1.2997\n",
      "Epoch 51 Batch 300 Loss 0.5721\n",
      "Epoch 51 Batch 300 test Loss 1.2998\n",
      "Epoch 51 Batch 350 Loss 0.5709\n",
      "Epoch 51 Batch 350 test Loss 1.2999\n",
      "Epoch 51 Batch 400 Loss 0.5736\n",
      "Epoch 51 Batch 400 test Loss 1.3000\n",
      "Epoch 51 Batch 450 Loss 0.5711\n",
      "Epoch 51 Batch 450 test Loss 1.3001\n",
      "Epoch 51 Batch 500 Loss 0.5715\n",
      "Epoch 51 Batch 500 test Loss 1.3002\n",
      "Epoch 51 Batch 550 Loss 0.5706\n",
      "Epoch 51 Batch 550 test Loss 1.3003\n",
      "Epoch 51 Batch 600 Loss 0.5720\n",
      "Epoch 51 Batch 600 test Loss 1.3004\n",
      "Epoch 51 Batch 650 Loss 0.5718\n",
      "Epoch 51 Batch 650 test Loss 1.3005\n",
      "Epoch 51 Batch 700 Loss 0.5727\n",
      "Epoch 51 Batch 700 test Loss 1.3006\n",
      "Epoch 51 Batch 750 Loss 0.5741\n",
      "Epoch 51 Batch 750 test Loss 1.3007\n",
      "Epoch 51 Batch 800 Loss 0.5737\n",
      "Epoch 51 Batch 800 test Loss 1.3008\n",
      "Epoch 51 Batch 850 Loss 0.5732\n",
      "Epoch 51 Batch 850 test Loss 1.3010\n",
      "Epoch 51 Batch 900 Loss 0.5732\n",
      "Epoch 51 Batch 900 test Loss 1.3011\n",
      "Epoch 51 Batch 950 Loss 0.5733\n",
      "Epoch 51 Batch 950 test Loss 1.3012\n",
      "Epoch 52 Batch 0 Loss 0.5898\n",
      "Epoch 52 Batch 0 test Loss 1.3013\n",
      "Epoch 52 Batch 50 Loss 0.5903\n",
      "Epoch 52 Batch 50 test Loss 1.3014\n",
      "Epoch 52 Batch 100 Loss 0.5908\n",
      "Epoch 52 Batch 100 test Loss 1.3015\n",
      "Epoch 52 Batch 150 Loss 0.5820\n",
      "Epoch 52 Batch 150 test Loss 1.3016\n",
      "Epoch 52 Batch 200 Loss 0.5860\n",
      "Epoch 52 Batch 200 test Loss 1.3017\n",
      "Epoch 52 Batch 250 Loss 0.5843\n",
      "Epoch 52 Batch 250 test Loss 1.3018\n",
      "Epoch 52 Batch 300 Loss 0.5816\n",
      "Epoch 52 Batch 300 test Loss 1.3019\n",
      "Epoch 52 Batch 350 Loss 0.5830\n",
      "Epoch 52 Batch 350 test Loss 1.3020\n",
      "Epoch 52 Batch 400 Loss 0.5819\n",
      "Epoch 52 Batch 400 test Loss 1.3022\n",
      "Epoch 52 Batch 450 Loss 0.5814\n",
      "Epoch 52 Batch 450 test Loss 1.3022\n",
      "Epoch 52 Batch 500 Loss 0.5802\n",
      "Epoch 52 Batch 500 test Loss 1.3023\n",
      "Epoch 52 Batch 550 Loss 0.5799\n",
      "Epoch 52 Batch 550 test Loss 1.3025\n",
      "Epoch 52 Batch 600 Loss 0.5797\n",
      "Epoch 52 Batch 600 test Loss 1.3026\n",
      "Epoch 52 Batch 650 Loss 0.5802\n",
      "Epoch 52 Batch 650 test Loss 1.3027\n",
      "Epoch 52 Batch 700 Loss 0.5803\n",
      "Epoch 52 Batch 700 test Loss 1.3028\n",
      "Epoch 52 Batch 750 Loss 0.5793\n",
      "Epoch 52 Batch 750 test Loss 1.3029\n",
      "Epoch 52 Batch 800 Loss 0.5791\n",
      "Epoch 52 Batch 800 test Loss 1.3030\n",
      "Epoch 52 Batch 850 Loss 0.5794\n",
      "Epoch 52 Batch 850 test Loss 1.3031\n",
      "Epoch 52 Batch 900 Loss 0.5794\n",
      "Epoch 52 Batch 900 test Loss 1.3032\n",
      "Epoch 52 Batch 950 Loss 0.5790\n",
      "Epoch 52 Batch 950 test Loss 1.3034\n",
      "Epoch 53 Batch 0 Loss 0.6078\n",
      "Epoch 53 Batch 0 test Loss 1.3035\n",
      "Epoch 53 Batch 50 Loss 0.5604\n",
      "Epoch 53 Batch 50 test Loss 1.3036\n",
      "Epoch 53 Batch 100 Loss 0.5625\n",
      "Epoch 53 Batch 100 test Loss 1.3037\n",
      "Epoch 53 Batch 150 Loss 0.5628\n",
      "Epoch 53 Batch 150 test Loss 1.3038\n",
      "Epoch 53 Batch 200 Loss 0.5661\n",
      "Epoch 53 Batch 200 test Loss 1.3039\n",
      "Epoch 53 Batch 250 Loss 0.5645\n",
      "Epoch 53 Batch 250 test Loss 1.3040\n",
      "Epoch 53 Batch 300 Loss 0.5693\n",
      "Epoch 53 Batch 300 test Loss 1.3041\n",
      "Epoch 53 Batch 350 Loss 0.5708\n",
      "Epoch 53 Batch 350 test Loss 1.3042\n",
      "Epoch 53 Batch 400 Loss 0.5717\n",
      "Epoch 53 Batch 400 test Loss 1.3043\n",
      "Epoch 53 Batch 450 Loss 0.5737\n",
      "Epoch 53 Batch 450 test Loss 1.3044\n",
      "Epoch 53 Batch 500 Loss 0.5752\n",
      "Epoch 53 Batch 500 test Loss 1.3045\n",
      "Epoch 53 Batch 550 Loss 0.5747\n",
      "Epoch 53 Batch 550 test Loss 1.3046\n",
      "Epoch 53 Batch 600 Loss 0.5744\n",
      "Epoch 53 Batch 600 test Loss 1.3047\n",
      "Epoch 53 Batch 650 Loss 0.5733\n",
      "Epoch 53 Batch 650 test Loss 1.3048\n",
      "Epoch 53 Batch 700 Loss 0.5720\n",
      "Epoch 53 Batch 700 test Loss 1.3049\n",
      "Epoch 53 Batch 750 Loss 0.5718\n",
      "Epoch 53 Batch 750 test Loss 1.3050\n",
      "Epoch 53 Batch 800 Loss 0.5727\n",
      "Epoch 53 Batch 800 test Loss 1.3051\n",
      "Epoch 53 Batch 850 Loss 0.5737\n",
      "Epoch 53 Batch 850 test Loss 1.3052\n",
      "Epoch 53 Batch 900 Loss 0.5744\n",
      "Epoch 53 Batch 900 test Loss 1.3053\n",
      "Epoch 53 Batch 950 Loss 0.5740\n",
      "Epoch 53 Batch 950 test Loss 1.3054\n",
      "Epoch 54 Batch 0 Loss 0.5745\n",
      "Epoch 54 Batch 0 test Loss 1.3055\n",
      "Epoch 54 Batch 50 Loss 0.5622\n",
      "Epoch 54 Batch 50 test Loss 1.3056\n",
      "Epoch 54 Batch 100 Loss 0.5729\n",
      "Epoch 54 Batch 100 test Loss 1.3057\n",
      "Epoch 54 Batch 150 Loss 0.5703\n",
      "Epoch 54 Batch 150 test Loss 1.3058\n",
      "Epoch 54 Batch 200 Loss 0.5706\n",
      "Epoch 54 Batch 200 test Loss 1.3059\n",
      "Epoch 54 Batch 250 Loss 0.5701\n",
      "Epoch 54 Batch 250 test Loss 1.3060\n",
      "Epoch 54 Batch 300 Loss 0.5694\n",
      "Epoch 54 Batch 300 test Loss 1.3061\n",
      "Epoch 54 Batch 350 Loss 0.5695\n",
      "Epoch 54 Batch 350 test Loss 1.3063\n",
      "Epoch 54 Batch 400 Loss 0.5696\n",
      "Epoch 54 Batch 400 test Loss 1.3064\n",
      "Epoch 54 Batch 450 Loss 0.5712\n",
      "Epoch 54 Batch 450 test Loss 1.3065\n",
      "Epoch 54 Batch 500 Loss 0.5705\n",
      "Epoch 54 Batch 500 test Loss 1.3066\n",
      "Epoch 54 Batch 550 Loss 0.5696\n",
      "Epoch 54 Batch 550 test Loss 1.3067\n",
      "Epoch 54 Batch 600 Loss 0.5695\n",
      "Epoch 54 Batch 600 test Loss 1.3068\n",
      "Epoch 54 Batch 650 Loss 0.5688\n",
      "Epoch 54 Batch 650 test Loss 1.3069\n",
      "Epoch 54 Batch 700 Loss 0.5697\n",
      "Epoch 54 Batch 700 test Loss 1.3070\n",
      "Epoch 54 Batch 750 Loss 0.5692\n",
      "Epoch 54 Batch 750 test Loss 1.3071\n",
      "Epoch 54 Batch 800 Loss 0.5688\n",
      "Epoch 54 Batch 800 test Loss 1.3072\n",
      "Epoch 54 Batch 850 Loss 0.5694\n",
      "Epoch 54 Batch 850 test Loss 1.3073\n",
      "Epoch 54 Batch 900 Loss 0.5705\n",
      "Epoch 54 Batch 900 test Loss 1.3074\n",
      "Epoch 54 Batch 950 Loss 0.5695\n",
      "Epoch 54 Batch 950 test Loss 1.3075\n",
      "Epoch 55 Batch 0 Loss 0.6022\n",
      "Epoch 55 Batch 0 test Loss 1.3076\n",
      "Epoch 55 Batch 50 Loss 0.5747\n",
      "Epoch 55 Batch 50 test Loss 1.3077\n",
      "Epoch 55 Batch 100 Loss 0.5883\n",
      "Epoch 55 Batch 100 test Loss 1.3078\n",
      "Epoch 55 Batch 150 Loss 0.5892\n",
      "Epoch 55 Batch 150 test Loss 1.3079\n",
      "Epoch 55 Batch 200 Loss 0.5827\n",
      "Epoch 55 Batch 200 test Loss 1.3080\n",
      "Epoch 55 Batch 250 Loss 0.5802\n",
      "Epoch 55 Batch 250 test Loss 1.3081\n",
      "Epoch 55 Batch 300 Loss 0.5794\n",
      "Epoch 55 Batch 300 test Loss 1.3082\n",
      "Epoch 55 Batch 350 Loss 0.5800\n",
      "Epoch 55 Batch 350 test Loss 1.3083\n",
      "Epoch 55 Batch 400 Loss 0.5761\n",
      "Epoch 55 Batch 400 test Loss 1.3084\n",
      "Epoch 55 Batch 450 Loss 0.5741\n",
      "Epoch 55 Batch 450 test Loss 1.3085\n",
      "Epoch 55 Batch 500 Loss 0.5733\n",
      "Epoch 55 Batch 500 test Loss 1.3086\n",
      "Epoch 55 Batch 550 Loss 0.5714\n",
      "Epoch 55 Batch 550 test Loss 1.3087\n",
      "Epoch 55 Batch 600 Loss 0.5706\n",
      "Epoch 55 Batch 600 test Loss 1.3088\n",
      "Epoch 55 Batch 650 Loss 0.5697\n",
      "Epoch 55 Batch 650 test Loss 1.3089\n",
      "Epoch 55 Batch 700 Loss 0.5690\n",
      "Epoch 55 Batch 700 test Loss 1.3090\n",
      "Epoch 55 Batch 750 Loss 0.5686\n",
      "Epoch 55 Batch 750 test Loss 1.3091\n",
      "Epoch 55 Batch 800 Loss 0.5690\n",
      "Epoch 55 Batch 800 test Loss 1.3092\n",
      "Epoch 55 Batch 850 Loss 0.5689\n",
      "Epoch 55 Batch 850 test Loss 1.3093\n",
      "Epoch 55 Batch 900 Loss 0.5688\n",
      "Epoch 55 Batch 900 test Loss 1.3094\n",
      "Epoch 55 Batch 950 Loss 0.5688\n",
      "Epoch 55 Batch 950 test Loss 1.3095\n",
      "Epoch 56 Batch 0 Loss 0.5330\n",
      "Epoch 56 Batch 0 test Loss 1.3096\n",
      "Epoch 56 Batch 50 Loss 0.5653\n",
      "Epoch 56 Batch 50 test Loss 1.3097\n",
      "Epoch 56 Batch 100 Loss 0.5749\n",
      "Epoch 56 Batch 100 test Loss 1.3098\n",
      "Epoch 56 Batch 150 Loss 0.5745\n",
      "Epoch 56 Batch 150 test Loss 1.3099\n",
      "Epoch 56 Batch 200 Loss 0.5707\n",
      "Epoch 56 Batch 200 test Loss 1.3100\n",
      "Epoch 56 Batch 250 Loss 0.5697\n",
      "Epoch 56 Batch 250 test Loss 1.3101\n",
      "Epoch 56 Batch 300 Loss 0.5707\n",
      "Epoch 56 Batch 300 test Loss 1.3102\n",
      "Epoch 56 Batch 350 Loss 0.5719\n",
      "Epoch 56 Batch 350 test Loss 1.3103\n",
      "Epoch 56 Batch 400 Loss 0.5722\n",
      "Epoch 56 Batch 400 test Loss 1.3104\n",
      "Epoch 56 Batch 450 Loss 0.5746\n",
      "Epoch 56 Batch 450 test Loss 1.3105\n",
      "Epoch 56 Batch 500 Loss 0.5756\n",
      "Epoch 56 Batch 500 test Loss 1.3106\n",
      "Epoch 56 Batch 550 Loss 0.5757\n",
      "Epoch 56 Batch 550 test Loss 1.3108\n",
      "Epoch 56 Batch 600 Loss 0.5756\n",
      "Epoch 56 Batch 600 test Loss 1.3109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Batch 650 Loss 0.5760\n",
      "Epoch 56 Batch 650 test Loss 1.3110\n",
      "Epoch 56 Batch 700 Loss 0.5756\n",
      "Epoch 56 Batch 700 test Loss 1.3111\n",
      "Epoch 56 Batch 750 Loss 0.5760\n",
      "Epoch 56 Batch 750 test Loss 1.3112\n",
      "Epoch 56 Batch 800 Loss 0.5756\n",
      "Epoch 56 Batch 800 test Loss 1.3113\n",
      "Epoch 56 Batch 850 Loss 0.5752\n",
      "Epoch 56 Batch 850 test Loss 1.3114\n",
      "Epoch 56 Batch 900 Loss 0.5744\n",
      "Epoch 56 Batch 900 test Loss 1.3115\n",
      "Epoch 56 Batch 950 Loss 0.5742\n",
      "Epoch 56 Batch 950 test Loss 1.3116\n",
      "Epoch 57 Batch 0 Loss 0.6112\n",
      "Epoch 57 Batch 0 test Loss 1.3117\n",
      "Epoch 57 Batch 50 Loss 0.5735\n",
      "Epoch 57 Batch 50 test Loss 1.3118\n",
      "Epoch 57 Batch 100 Loss 0.5618\n",
      "Epoch 57 Batch 100 test Loss 1.3119\n",
      "Epoch 57 Batch 150 Loss 0.5668\n",
      "Epoch 57 Batch 150 test Loss 1.3120\n",
      "Epoch 57 Batch 200 Loss 0.5675\n",
      "Epoch 57 Batch 200 test Loss 1.3121\n",
      "Epoch 57 Batch 250 Loss 0.5676\n",
      "Epoch 57 Batch 250 test Loss 1.3122\n",
      "Epoch 57 Batch 300 Loss 0.5673\n",
      "Epoch 57 Batch 300 test Loss 1.3123\n",
      "Epoch 57 Batch 350 Loss 0.5643\n",
      "Epoch 57 Batch 350 test Loss 1.3124\n",
      "Epoch 57 Batch 400 Loss 0.5659\n",
      "Epoch 57 Batch 400 test Loss 1.3125\n",
      "Epoch 57 Batch 450 Loss 0.5666\n",
      "Epoch 57 Batch 450 test Loss 1.3126\n",
      "Epoch 57 Batch 500 Loss 0.5658\n",
      "Epoch 57 Batch 500 test Loss 1.3128\n",
      "Epoch 57 Batch 550 Loss 0.5640\n",
      "Epoch 57 Batch 550 test Loss 1.3129\n",
      "Epoch 57 Batch 600 Loss 0.5639\n",
      "Epoch 57 Batch 600 test Loss 1.3130\n",
      "Epoch 57 Batch 650 Loss 0.5651\n",
      "Epoch 57 Batch 650 test Loss 1.3131\n",
      "Epoch 57 Batch 700 Loss 0.5653\n",
      "Epoch 57 Batch 700 test Loss 1.3132\n",
      "Epoch 57 Batch 750 Loss 0.5651\n",
      "Epoch 57 Batch 750 test Loss 1.3133\n",
      "Epoch 57 Batch 800 Loss 0.5653\n",
      "Epoch 57 Batch 800 test Loss 1.3134\n",
      "Epoch 57 Batch 850 Loss 0.5655\n",
      "Epoch 57 Batch 850 test Loss 1.3135\n",
      "Epoch 57 Batch 900 Loss 0.5664\n",
      "Epoch 57 Batch 900 test Loss 1.3136\n",
      "Epoch 57 Batch 950 Loss 0.5665\n",
      "Epoch 57 Batch 950 test Loss 1.3137\n",
      "Epoch 58 Batch 0 Loss 0.4824\n",
      "Epoch 58 Batch 0 test Loss 1.3138\n",
      "Epoch 58 Batch 50 Loss 0.5782\n",
      "Epoch 58 Batch 50 test Loss 1.3139\n",
      "Epoch 58 Batch 100 Loss 0.5686\n",
      "Epoch 58 Batch 100 test Loss 1.3140\n",
      "Epoch 58 Batch 150 Loss 0.5664\n",
      "Epoch 58 Batch 150 test Loss 1.3141\n",
      "Epoch 58 Batch 200 Loss 0.5681\n",
      "Epoch 58 Batch 200 test Loss 1.3142\n",
      "Epoch 58 Batch 250 Loss 0.5691\n",
      "Epoch 58 Batch 250 test Loss 1.3143\n",
      "Epoch 58 Batch 300 Loss 0.5705\n",
      "Epoch 58 Batch 300 test Loss 1.3144\n",
      "Epoch 58 Batch 350 Loss 0.5734\n",
      "Epoch 58 Batch 350 test Loss 1.3145\n",
      "Epoch 58 Batch 400 Loss 0.5719\n",
      "Epoch 58 Batch 400 test Loss 1.3146\n",
      "Epoch 58 Batch 450 Loss 0.5689\n",
      "Epoch 58 Batch 450 test Loss 1.3147\n",
      "Epoch 58 Batch 500 Loss 0.5675\n",
      "Epoch 58 Batch 500 test Loss 1.3148\n",
      "Epoch 58 Batch 550 Loss 0.5678\n",
      "Epoch 58 Batch 550 test Loss 1.3149\n",
      "Epoch 58 Batch 600 Loss 0.5680\n",
      "Epoch 58 Batch 600 test Loss 1.3150\n",
      "Epoch 58 Batch 650 Loss 0.5685\n",
      "Epoch 58 Batch 650 test Loss 1.3151\n",
      "Epoch 58 Batch 700 Loss 0.5685\n",
      "Epoch 58 Batch 700 test Loss 1.3151\n",
      "Epoch 58 Batch 750 Loss 0.5675\n",
      "Epoch 58 Batch 750 test Loss 1.3152\n",
      "Epoch 58 Batch 800 Loss 0.5682\n",
      "Epoch 58 Batch 800 test Loss 1.3153\n",
      "Epoch 58 Batch 850 Loss 0.5679\n",
      "Epoch 58 Batch 850 test Loss 1.3155\n",
      "Epoch 58 Batch 900 Loss 0.5673\n",
      "Epoch 58 Batch 900 test Loss 1.3156\n",
      "Epoch 58 Batch 950 Loss 0.5667\n",
      "Epoch 58 Batch 950 test Loss 1.3157\n",
      "Epoch 59 Batch 0 Loss 0.5256\n",
      "Epoch 59 Batch 0 test Loss 1.3158\n",
      "Epoch 59 Batch 50 Loss 0.5530\n",
      "Epoch 59 Batch 50 test Loss 1.3159\n",
      "Epoch 59 Batch 100 Loss 0.5566\n",
      "Epoch 59 Batch 100 test Loss 1.3160\n",
      "Epoch 59 Batch 150 Loss 0.5552\n",
      "Epoch 59 Batch 150 test Loss 1.3161\n",
      "Epoch 59 Batch 200 Loss 0.5527\n",
      "Epoch 59 Batch 200 test Loss 1.3162\n",
      "Epoch 59 Batch 250 Loss 0.5524\n",
      "Epoch 59 Batch 250 test Loss 1.3163\n",
      "Epoch 59 Batch 300 Loss 0.5545\n",
      "Epoch 59 Batch 300 test Loss 1.3164\n",
      "Epoch 59 Batch 350 Loss 0.5589\n",
      "Epoch 59 Batch 350 test Loss 1.3165\n",
      "Epoch 59 Batch 400 Loss 0.5595\n",
      "Epoch 59 Batch 400 test Loss 1.3166\n",
      "Epoch 59 Batch 450 Loss 0.5597\n",
      "Epoch 59 Batch 450 test Loss 1.3167\n",
      "Epoch 59 Batch 500 Loss 0.5615\n",
      "Epoch 59 Batch 500 test Loss 1.3168\n",
      "Epoch 59 Batch 550 Loss 0.5612\n",
      "Epoch 59 Batch 550 test Loss 1.3169\n",
      "Epoch 59 Batch 600 Loss 0.5611\n",
      "Epoch 59 Batch 600 test Loss 1.3170\n",
      "Epoch 59 Batch 650 Loss 0.5620\n",
      "Epoch 59 Batch 650 test Loss 1.3171\n",
      "Epoch 59 Batch 700 Loss 0.5627\n",
      "Epoch 59 Batch 700 test Loss 1.3172\n",
      "Epoch 59 Batch 750 Loss 0.5617\n",
      "Epoch 59 Batch 750 test Loss 1.3173\n",
      "Epoch 59 Batch 800 Loss 0.5622\n",
      "Epoch 59 Batch 800 test Loss 1.3174\n",
      "Epoch 59 Batch 850 Loss 0.5617\n",
      "Epoch 59 Batch 850 test Loss 1.3175\n",
      "Epoch 59 Batch 900 Loss 0.5620\n",
      "Epoch 59 Batch 900 test Loss 1.3176\n",
      "Epoch 59 Batch 950 Loss 0.5629\n",
      "Epoch 59 Batch 950 test Loss 1.3176\n",
      "Epoch 60 Batch 0 Loss 0.4971\n",
      "Epoch 60 Batch 0 test Loss 1.3177\n",
      "Epoch 60 Batch 50 Loss 0.5655\n",
      "Epoch 60 Batch 50 test Loss 1.3178\n",
      "Epoch 60 Batch 100 Loss 0.5555\n",
      "Epoch 60 Batch 100 test Loss 1.3179\n",
      "Epoch 60 Batch 150 Loss 0.5577\n",
      "Epoch 60 Batch 150 test Loss 1.3180\n",
      "Epoch 60 Batch 200 Loss 0.5559\n",
      "Epoch 60 Batch 200 test Loss 1.3181\n",
      "Epoch 60 Batch 250 Loss 0.5578\n",
      "Epoch 60 Batch 250 test Loss 1.3182\n",
      "Epoch 60 Batch 300 Loss 0.5595\n",
      "Epoch 60 Batch 300 test Loss 1.3183\n",
      "Epoch 60 Batch 350 Loss 0.5603\n",
      "Epoch 60 Batch 350 test Loss 1.3184\n",
      "Epoch 60 Batch 400 Loss 0.5597\n",
      "Epoch 60 Batch 400 test Loss 1.3185\n",
      "Epoch 60 Batch 450 Loss 0.5614\n",
      "Epoch 60 Batch 450 test Loss 1.3186\n",
      "Epoch 60 Batch 500 Loss 0.5635\n",
      "Epoch 60 Batch 500 test Loss 1.3187\n",
      "Epoch 60 Batch 550 Loss 0.5645\n",
      "Epoch 60 Batch 550 test Loss 1.3188\n",
      "Epoch 60 Batch 600 Loss 0.5649\n",
      "Epoch 60 Batch 600 test Loss 1.3189\n",
      "Epoch 60 Batch 650 Loss 0.5634\n",
      "Epoch 60 Batch 650 test Loss 1.3190\n",
      "Epoch 60 Batch 700 Loss 0.5632\n",
      "Epoch 60 Batch 700 test Loss 1.3190\n",
      "Epoch 60 Batch 750 Loss 0.5632\n",
      "Epoch 60 Batch 750 test Loss 1.3191\n",
      "Epoch 60 Batch 800 Loss 0.5618\n",
      "Epoch 60 Batch 800 test Loss 1.3192\n",
      "Epoch 60 Batch 850 Loss 0.5613\n",
      "Epoch 60 Batch 850 test Loss 1.3193\n",
      "Epoch 60 Batch 900 Loss 0.5620\n",
      "Epoch 60 Batch 900 test Loss 1.3194\n",
      "Epoch 60 Batch 950 Loss 0.5620\n",
      "Epoch 60 Batch 950 test Loss 1.3195\n",
      "Epoch 61 Batch 0 Loss 0.5481\n",
      "Epoch 61 Batch 0 test Loss 1.3196\n",
      "Epoch 61 Batch 50 Loss 0.5669\n",
      "Epoch 61 Batch 50 test Loss 1.3197\n",
      "Epoch 61 Batch 100 Loss 0.5669\n",
      "Epoch 61 Batch 100 test Loss 1.3198\n",
      "Epoch 61 Batch 150 Loss 0.5662\n",
      "Epoch 61 Batch 150 test Loss 1.3199\n",
      "Epoch 61 Batch 200 Loss 0.5628\n",
      "Epoch 61 Batch 200 test Loss 1.3200\n",
      "Epoch 61 Batch 250 Loss 0.5662\n",
      "Epoch 61 Batch 250 test Loss 1.3200\n",
      "Epoch 61 Batch 300 Loss 0.5691\n",
      "Epoch 61 Batch 300 test Loss 1.3201\n",
      "Epoch 61 Batch 350 Loss 0.5675\n",
      "Epoch 61 Batch 350 test Loss 1.3202\n",
      "Epoch 61 Batch 400 Loss 0.5664\n",
      "Epoch 61 Batch 400 test Loss 1.3203\n",
      "Epoch 61 Batch 450 Loss 0.5652\n",
      "Epoch 61 Batch 450 test Loss 1.3204\n",
      "Epoch 61 Batch 500 Loss 0.5654\n",
      "Epoch 61 Batch 500 test Loss 1.3205\n",
      "Epoch 61 Batch 550 Loss 0.5654\n",
      "Epoch 61 Batch 550 test Loss 1.3206\n",
      "Epoch 61 Batch 600 Loss 0.5640\n",
      "Epoch 61 Batch 600 test Loss 1.3207\n",
      "Epoch 61 Batch 650 Loss 0.5630\n",
      "Epoch 61 Batch 650 test Loss 1.3208\n",
      "Epoch 61 Batch 700 Loss 0.5629\n",
      "Epoch 61 Batch 700 test Loss 1.3209\n",
      "Epoch 61 Batch 750 Loss 0.5634\n",
      "Epoch 61 Batch 750 test Loss 1.3210\n",
      "Epoch 61 Batch 800 Loss 0.5644\n",
      "Epoch 61 Batch 800 test Loss 1.3211\n",
      "Epoch 61 Batch 850 Loss 0.5651\n",
      "Epoch 61 Batch 850 test Loss 1.3212\n",
      "Epoch 61 Batch 900 Loss 0.5655\n",
      "Epoch 61 Batch 900 test Loss 1.3212\n",
      "Epoch 61 Batch 950 Loss 0.5662\n",
      "Epoch 61 Batch 950 test Loss 1.3213\n",
      "Epoch 62 Batch 0 Loss 0.4743\n",
      "Epoch 62 Batch 0 test Loss 1.3214\n",
      "Epoch 62 Batch 50 Loss 0.5537\n",
      "Epoch 62 Batch 50 test Loss 1.3215\n",
      "Epoch 62 Batch 100 Loss 0.5580\n",
      "Epoch 62 Batch 100 test Loss 1.3216\n",
      "Epoch 62 Batch 150 Loss 0.5619\n",
      "Epoch 62 Batch 150 test Loss 1.3217\n",
      "Epoch 62 Batch 200 Loss 0.5623\n",
      "Epoch 62 Batch 200 test Loss 1.3218\n",
      "Epoch 62 Batch 250 Loss 0.5605\n",
      "Epoch 62 Batch 250 test Loss 1.3219\n",
      "Epoch 62 Batch 300 Loss 0.5626\n",
      "Epoch 62 Batch 300 test Loss 1.3220\n",
      "Epoch 62 Batch 350 Loss 0.5629\n",
      "Epoch 62 Batch 350 test Loss 1.3221\n",
      "Epoch 62 Batch 400 Loss 0.5625\n",
      "Epoch 62 Batch 400 test Loss 1.3222\n",
      "Epoch 62 Batch 450 Loss 0.5615\n",
      "Epoch 62 Batch 450 test Loss 1.3223\n",
      "Epoch 62 Batch 500 Loss 0.5635\n",
      "Epoch 62 Batch 500 test Loss 1.3224\n",
      "Epoch 62 Batch 550 Loss 0.5647\n",
      "Epoch 62 Batch 550 test Loss 1.3225\n",
      "Epoch 62 Batch 600 Loss 0.5646\n",
      "Epoch 62 Batch 600 test Loss 1.3226\n",
      "Epoch 62 Batch 650 Loss 0.5637\n",
      "Epoch 62 Batch 650 test Loss 1.3227\n",
      "Epoch 62 Batch 700 Loss 0.5641\n",
      "Epoch 62 Batch 700 test Loss 1.3228\n",
      "Epoch 62 Batch 750 Loss 0.5641\n",
      "Epoch 62 Batch 750 test Loss 1.3229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Batch 800 Loss 0.5636\n",
      "Epoch 62 Batch 800 test Loss 1.3229\n",
      "Epoch 62 Batch 850 Loss 0.5635\n",
      "Epoch 62 Batch 850 test Loss 1.3230\n",
      "Epoch 62 Batch 900 Loss 0.5629\n",
      "Epoch 62 Batch 900 test Loss 1.3231\n",
      "Epoch 62 Batch 950 Loss 0.5622\n",
      "Epoch 62 Batch 950 test Loss 1.3232\n",
      "Epoch 63 Batch 0 Loss 0.6009\n",
      "Epoch 63 Batch 0 test Loss 1.3233\n",
      "Epoch 63 Batch 50 Loss 0.5702\n",
      "Epoch 63 Batch 50 test Loss 1.3234\n",
      "Epoch 63 Batch 100 Loss 0.5713\n",
      "Epoch 63 Batch 100 test Loss 1.3235\n",
      "Epoch 63 Batch 150 Loss 0.5720\n",
      "Epoch 63 Batch 150 test Loss 1.3236\n",
      "Epoch 63 Batch 200 Loss 0.5689\n",
      "Epoch 63 Batch 200 test Loss 1.3237\n",
      "Epoch 63 Batch 250 Loss 0.5710\n",
      "Epoch 63 Batch 250 test Loss 1.3238\n",
      "Epoch 63 Batch 300 Loss 0.5705\n",
      "Epoch 63 Batch 300 test Loss 1.3239\n",
      "Epoch 63 Batch 350 Loss 0.5676\n",
      "Epoch 63 Batch 350 test Loss 1.3239\n",
      "Epoch 63 Batch 400 Loss 0.5692\n",
      "Epoch 63 Batch 400 test Loss 1.3240\n",
      "Epoch 63 Batch 450 Loss 0.5676\n",
      "Epoch 63 Batch 450 test Loss 1.3241\n",
      "Epoch 63 Batch 500 Loss 0.5690\n",
      "Epoch 63 Batch 500 test Loss 1.3242\n",
      "Epoch 63 Batch 550 Loss 0.5683\n",
      "Epoch 63 Batch 550 test Loss 1.3243\n",
      "Epoch 63 Batch 600 Loss 0.5673\n",
      "Epoch 63 Batch 600 test Loss 1.3244\n",
      "Epoch 63 Batch 650 Loss 0.5662\n",
      "Epoch 63 Batch 650 test Loss 1.3245\n",
      "Epoch 63 Batch 700 Loss 0.5662\n",
      "Epoch 63 Batch 700 test Loss 1.3246\n",
      "Epoch 63 Batch 750 Loss 0.5655\n",
      "Epoch 63 Batch 750 test Loss 1.3247\n",
      "Epoch 63 Batch 800 Loss 0.5654\n",
      "Epoch 63 Batch 800 test Loss 1.3248\n",
      "Epoch 63 Batch 850 Loss 0.5654\n",
      "Epoch 63 Batch 850 test Loss 1.3248\n",
      "Epoch 63 Batch 900 Loss 0.5656\n",
      "Epoch 63 Batch 900 test Loss 1.3249\n",
      "Epoch 63 Batch 950 Loss 0.5655\n",
      "Epoch 63 Batch 950 test Loss 1.3250\n",
      "Epoch 64 Batch 0 Loss 0.4668\n",
      "Epoch 64 Batch 0 test Loss 1.3251\n",
      "Epoch 64 Batch 50 Loss 0.5465\n",
      "Epoch 64 Batch 50 test Loss 1.3252\n",
      "Epoch 64 Batch 100 Loss 0.5486\n",
      "Epoch 64 Batch 100 test Loss 1.3253\n",
      "Epoch 64 Batch 150 Loss 0.5503\n",
      "Epoch 64 Batch 150 test Loss 1.3254\n",
      "Epoch 64 Batch 200 Loss 0.5569\n",
      "Epoch 64 Batch 200 test Loss 1.3255\n",
      "Epoch 64 Batch 250 Loss 0.5607\n",
      "Epoch 64 Batch 250 test Loss 1.3256\n",
      "Epoch 64 Batch 300 Loss 0.5631\n",
      "Epoch 64 Batch 300 test Loss 1.3257\n",
      "Epoch 64 Batch 350 Loss 0.5635\n",
      "Epoch 64 Batch 350 test Loss 1.3258\n",
      "Epoch 64 Batch 400 Loss 0.5628\n",
      "Epoch 64 Batch 400 test Loss 1.3259\n",
      "Epoch 64 Batch 450 Loss 0.5637\n",
      "Epoch 64 Batch 450 test Loss 1.3260\n",
      "Epoch 64 Batch 500 Loss 0.5627\n",
      "Epoch 64 Batch 500 test Loss 1.3260\n",
      "Epoch 64 Batch 550 Loss 0.5643\n",
      "Epoch 64 Batch 550 test Loss 1.3261\n",
      "Epoch 64 Batch 600 Loss 0.5633\n",
      "Epoch 64 Batch 600 test Loss 1.3262\n",
      "Epoch 64 Batch 650 Loss 0.5635\n",
      "Epoch 64 Batch 650 test Loss 1.3263\n",
      "Epoch 64 Batch 700 Loss 0.5641\n",
      "Epoch 64 Batch 700 test Loss 1.3264\n",
      "Epoch 64 Batch 750 Loss 0.5634\n",
      "Epoch 64 Batch 750 test Loss 1.3265\n",
      "Epoch 64 Batch 800 Loss 0.5641\n",
      "Epoch 64 Batch 800 test Loss 1.3266\n",
      "Epoch 64 Batch 850 Loss 0.5642\n",
      "Epoch 64 Batch 850 test Loss 1.3267\n",
      "Epoch 64 Batch 900 Loss 0.5642\n",
      "Epoch 64 Batch 900 test Loss 1.3267\n",
      "Epoch 64 Batch 950 Loss 0.5646\n",
      "Epoch 64 Batch 950 test Loss 1.3268\n",
      "Epoch 65 Batch 0 Loss 0.5905\n",
      "Epoch 65 Batch 0 test Loss 1.3269\n",
      "Epoch 65 Batch 50 Loss 0.5652\n",
      "Epoch 65 Batch 50 test Loss 1.3270\n",
      "Epoch 65 Batch 100 Loss 0.5614\n",
      "Epoch 65 Batch 100 test Loss 1.3271\n",
      "Epoch 65 Batch 150 Loss 0.5686\n",
      "Epoch 65 Batch 150 test Loss 1.3272\n",
      "Epoch 65 Batch 200 Loss 0.5695\n",
      "Epoch 65 Batch 200 test Loss 1.3272\n",
      "Epoch 65 Batch 250 Loss 0.5684\n",
      "Epoch 65 Batch 250 test Loss 1.3273\n",
      "Epoch 65 Batch 300 Loss 0.5669\n",
      "Epoch 65 Batch 300 test Loss 1.3274\n",
      "Epoch 65 Batch 350 Loss 0.5631\n",
      "Epoch 65 Batch 350 test Loss 1.3275\n",
      "Epoch 65 Batch 400 Loss 0.5625\n",
      "Epoch 65 Batch 400 test Loss 1.3276\n",
      "Epoch 65 Batch 450 Loss 0.5633\n",
      "Epoch 65 Batch 450 test Loss 1.3277\n",
      "Epoch 65 Batch 500 Loss 0.5625\n",
      "Epoch 65 Batch 500 test Loss 1.3278\n",
      "Epoch 65 Batch 550 Loss 0.5614\n",
      "Epoch 65 Batch 550 test Loss 1.3279\n",
      "Epoch 65 Batch 600 Loss 0.5612\n",
      "Epoch 65 Batch 600 test Loss 1.3279\n",
      "Epoch 65 Batch 650 Loss 0.5619\n",
      "Epoch 65 Batch 650 test Loss 1.3280\n",
      "Epoch 65 Batch 700 Loss 0.5604\n",
      "Epoch 65 Batch 700 test Loss 1.3281\n",
      "Epoch 65 Batch 750 Loss 0.5596\n",
      "Epoch 65 Batch 750 test Loss 1.3282\n",
      "Epoch 65 Batch 800 Loss 0.5588\n",
      "Epoch 65 Batch 800 test Loss 1.3283\n",
      "Epoch 65 Batch 850 Loss 0.5595\n",
      "Epoch 65 Batch 850 test Loss 1.3284\n",
      "Epoch 65 Batch 900 Loss 0.5593\n",
      "Epoch 65 Batch 900 test Loss 1.3284\n",
      "Epoch 65 Batch 950 Loss 0.5584\n",
      "Epoch 65 Batch 950 test Loss 1.3285\n",
      "Epoch 66 Batch 0 Loss 0.6137\n",
      "Epoch 66 Batch 0 test Loss 1.3286\n",
      "Epoch 66 Batch 50 Loss 0.5735\n",
      "Epoch 66 Batch 50 test Loss 1.3287\n",
      "Epoch 66 Batch 100 Loss 0.5705\n",
      "Epoch 66 Batch 100 test Loss 1.3288\n",
      "Epoch 66 Batch 150 Loss 0.5605\n",
      "Epoch 66 Batch 150 test Loss 1.3289\n",
      "Epoch 66 Batch 200 Loss 0.5593\n",
      "Epoch 66 Batch 200 test Loss 1.3290\n",
      "Epoch 66 Batch 250 Loss 0.5592\n",
      "Epoch 66 Batch 250 test Loss 1.3290\n",
      "Epoch 66 Batch 300 Loss 0.5613\n",
      "Epoch 66 Batch 300 test Loss 1.3291\n",
      "Epoch 66 Batch 350 Loss 0.5604\n",
      "Epoch 66 Batch 350 test Loss 1.3292\n",
      "Epoch 66 Batch 400 Loss 0.5596\n",
      "Epoch 66 Batch 400 test Loss 1.3293\n",
      "Epoch 66 Batch 450 Loss 0.5612\n",
      "Epoch 66 Batch 450 test Loss 1.3294\n",
      "Epoch 66 Batch 500 Loss 0.5615\n",
      "Epoch 66 Batch 500 test Loss 1.3295\n",
      "Epoch 66 Batch 550 Loss 0.5632\n",
      "Epoch 66 Batch 550 test Loss 1.3296\n",
      "Epoch 66 Batch 600 Loss 0.5637\n",
      "Epoch 66 Batch 600 test Loss 1.3297\n",
      "Epoch 66 Batch 650 Loss 0.5632\n",
      "Epoch 66 Batch 650 test Loss 1.3297\n",
      "Epoch 66 Batch 700 Loss 0.5627\n",
      "Epoch 66 Batch 700 test Loss 1.3298\n",
      "Epoch 66 Batch 750 Loss 0.5633\n",
      "Epoch 66 Batch 750 test Loss 1.3299\n",
      "Epoch 66 Batch 800 Loss 0.5622\n",
      "Epoch 66 Batch 800 test Loss 1.3300\n",
      "Epoch 66 Batch 850 Loss 0.5617\n",
      "Epoch 66 Batch 850 test Loss 1.3301\n",
      "Epoch 66 Batch 900 Loss 0.5610\n",
      "Epoch 66 Batch 900 test Loss 1.3302\n",
      "Epoch 66 Batch 950 Loss 0.5613\n",
      "Epoch 66 Batch 950 test Loss 1.3302\n",
      "Epoch 67 Batch 0 Loss 0.5697\n",
      "Epoch 67 Batch 0 test Loss 1.3303\n",
      "Epoch 67 Batch 50 Loss 0.5588\n",
      "Epoch 67 Batch 50 test Loss 1.3304\n",
      "Epoch 67 Batch 100 Loss 0.5683\n",
      "Epoch 67 Batch 100 test Loss 1.3305\n",
      "Epoch 67 Batch 150 Loss 0.5658\n",
      "Epoch 67 Batch 150 test Loss 1.3306\n",
      "Epoch 67 Batch 200 Loss 0.5604\n",
      "Epoch 67 Batch 200 test Loss 1.3307\n",
      "Epoch 67 Batch 250 Loss 0.5620\n",
      "Epoch 67 Batch 250 test Loss 1.3307\n",
      "Epoch 67 Batch 300 Loss 0.5612\n",
      "Epoch 67 Batch 300 test Loss 1.3308\n",
      "Epoch 67 Batch 350 Loss 0.5593\n",
      "Epoch 67 Batch 350 test Loss 1.3309\n",
      "Epoch 67 Batch 400 Loss 0.5587\n",
      "Epoch 67 Batch 400 test Loss 1.3310\n",
      "Epoch 67 Batch 450 Loss 0.5589\n",
      "Epoch 67 Batch 450 test Loss 1.3311\n",
      "Epoch 67 Batch 500 Loss 0.5609\n",
      "Epoch 67 Batch 500 test Loss 1.3312\n",
      "Epoch 67 Batch 550 Loss 0.5603\n",
      "Epoch 67 Batch 550 test Loss 1.3312\n",
      "Epoch 67 Batch 600 Loss 0.5608\n",
      "Epoch 67 Batch 600 test Loss 1.3313\n",
      "Epoch 67 Batch 650 Loss 0.5600\n",
      "Epoch 67 Batch 650 test Loss 1.3314\n",
      "Epoch 67 Batch 700 Loss 0.5596\n",
      "Epoch 67 Batch 700 test Loss 1.3315\n",
      "Epoch 67 Batch 750 Loss 0.5583\n",
      "Epoch 67 Batch 750 test Loss 1.3316\n",
      "Epoch 67 Batch 800 Loss 0.5588\n",
      "Epoch 67 Batch 800 test Loss 1.3317\n",
      "Epoch 67 Batch 850 Loss 0.5577\n",
      "Epoch 67 Batch 850 test Loss 1.3318\n",
      "Epoch 67 Batch 900 Loss 0.5576\n",
      "Epoch 67 Batch 900 test Loss 1.3319\n",
      "Epoch 67 Batch 950 Loss 0.5582\n",
      "Epoch 67 Batch 950 test Loss 1.3320\n",
      "Epoch 68 Batch 0 Loss 0.6545\n",
      "Epoch 68 Batch 0 test Loss 1.3321\n",
      "Epoch 68 Batch 50 Loss 0.5565\n",
      "Epoch 68 Batch 50 test Loss 1.3321\n",
      "Epoch 68 Batch 100 Loss 0.5583\n",
      "Epoch 68 Batch 100 test Loss 1.3322\n",
      "Epoch 68 Batch 150 Loss 0.5610\n",
      "Epoch 68 Batch 150 test Loss 1.3323\n",
      "Epoch 68 Batch 200 Loss 0.5605\n",
      "Epoch 68 Batch 200 test Loss 1.3324\n",
      "Epoch 68 Batch 250 Loss 0.5600\n",
      "Epoch 68 Batch 250 test Loss 1.3325\n",
      "Epoch 68 Batch 300 Loss 0.5592\n",
      "Epoch 68 Batch 300 test Loss 1.3326\n",
      "Epoch 68 Batch 350 Loss 0.5593\n",
      "Epoch 68 Batch 350 test Loss 1.3327\n",
      "Epoch 68 Batch 400 Loss 0.5585\n",
      "Epoch 68 Batch 400 test Loss 1.3327\n",
      "Epoch 68 Batch 450 Loss 0.5599\n",
      "Epoch 68 Batch 450 test Loss 1.3328\n",
      "Epoch 68 Batch 500 Loss 0.5619\n",
      "Epoch 68 Batch 500 test Loss 1.3329\n",
      "Epoch 68 Batch 550 Loss 0.5634\n",
      "Epoch 68 Batch 550 test Loss 1.3330\n",
      "Epoch 68 Batch 600 Loss 0.5640\n",
      "Epoch 68 Batch 600 test Loss 1.3331\n",
      "Epoch 68 Batch 650 Loss 0.5648\n",
      "Epoch 68 Batch 650 test Loss 1.3332\n",
      "Epoch 68 Batch 700 Loss 0.5648\n",
      "Epoch 68 Batch 700 test Loss 1.3332\n",
      "Epoch 68 Batch 750 Loss 0.5638\n",
      "Epoch 68 Batch 750 test Loss 1.3333\n",
      "Epoch 68 Batch 800 Loss 0.5629\n",
      "Epoch 68 Batch 800 test Loss 1.3334\n",
      "Epoch 68 Batch 850 Loss 0.5634\n",
      "Epoch 68 Batch 850 test Loss 1.3335\n",
      "Epoch 68 Batch 900 Loss 0.5646\n",
      "Epoch 68 Batch 900 test Loss 1.3336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 950 Loss 0.5638\n",
      "Epoch 68 Batch 950 test Loss 1.3337\n",
      "Epoch 69 Batch 0 Loss 0.5389\n",
      "Epoch 69 Batch 0 test Loss 1.3337\n",
      "Epoch 69 Batch 50 Loss 0.5433\n",
      "Epoch 69 Batch 50 test Loss 1.3338\n",
      "Epoch 69 Batch 100 Loss 0.5562\n",
      "Epoch 69 Batch 100 test Loss 1.3339\n",
      "Epoch 69 Batch 150 Loss 0.5605\n",
      "Epoch 69 Batch 150 test Loss 1.3340\n",
      "Epoch 69 Batch 200 Loss 0.5626\n",
      "Epoch 69 Batch 200 test Loss 1.3341\n",
      "Epoch 69 Batch 250 Loss 0.5631\n",
      "Epoch 69 Batch 250 test Loss 1.3342\n",
      "Epoch 69 Batch 300 Loss 0.5600\n",
      "Epoch 69 Batch 300 test Loss 1.3342\n",
      "Epoch 69 Batch 350 Loss 0.5618\n",
      "Epoch 69 Batch 350 test Loss 1.3343\n",
      "Epoch 69 Batch 400 Loss 0.5596\n",
      "Epoch 69 Batch 400 test Loss 1.3344\n",
      "Epoch 69 Batch 450 Loss 0.5583\n",
      "Epoch 69 Batch 450 test Loss 1.3345\n",
      "Epoch 69 Batch 500 Loss 0.5579\n",
      "Epoch 69 Batch 500 test Loss 1.3346\n",
      "Epoch 69 Batch 550 Loss 0.5564\n",
      "Epoch 69 Batch 550 test Loss 1.3347\n",
      "Epoch 69 Batch 600 Loss 0.5566\n",
      "Epoch 69 Batch 600 test Loss 1.3347\n",
      "Epoch 69 Batch 650 Loss 0.5579\n",
      "Epoch 69 Batch 650 test Loss 1.3348\n",
      "Epoch 69 Batch 700 Loss 0.5561\n",
      "Epoch 69 Batch 700 test Loss 1.3349\n",
      "Epoch 69 Batch 750 Loss 0.5559\n",
      "Epoch 69 Batch 750 test Loss 1.3350\n",
      "Epoch 69 Batch 800 Loss 0.5571\n",
      "Epoch 69 Batch 800 test Loss 1.3351\n",
      "Epoch 69 Batch 850 Loss 0.5578\n",
      "Epoch 69 Batch 850 test Loss 1.3352\n",
      "Epoch 69 Batch 900 Loss 0.5577\n",
      "Epoch 69 Batch 900 test Loss 1.3352\n",
      "Epoch 69 Batch 950 Loss 0.5584\n",
      "Epoch 69 Batch 950 test Loss 1.3353\n",
      "Epoch 70 Batch 0 Loss 0.5660\n",
      "Epoch 70 Batch 0 test Loss 1.3354\n",
      "Epoch 70 Batch 50 Loss 0.5532\n",
      "Epoch 70 Batch 50 test Loss 1.3355\n",
      "Epoch 70 Batch 100 Loss 0.5574\n",
      "Epoch 70 Batch 100 test Loss 1.3356\n",
      "Epoch 70 Batch 150 Loss 0.5580\n",
      "Epoch 70 Batch 150 test Loss 1.3357\n",
      "Epoch 70 Batch 200 Loss 0.5549\n",
      "Epoch 70 Batch 200 test Loss 1.3357\n",
      "Epoch 70 Batch 250 Loss 0.5539\n",
      "Epoch 70 Batch 250 test Loss 1.3358\n",
      "Epoch 70 Batch 300 Loss 0.5555\n",
      "Epoch 70 Batch 300 test Loss 1.3359\n",
      "Epoch 70 Batch 350 Loss 0.5544\n",
      "Epoch 70 Batch 350 test Loss 1.3360\n",
      "Epoch 70 Batch 400 Loss 0.5565\n",
      "Epoch 70 Batch 400 test Loss 1.3361\n",
      "Epoch 70 Batch 450 Loss 0.5559\n",
      "Epoch 70 Batch 450 test Loss 1.3361\n",
      "Epoch 70 Batch 500 Loss 0.5570\n",
      "Epoch 70 Batch 500 test Loss 1.3362\n",
      "Epoch 70 Batch 550 Loss 0.5576\n",
      "Epoch 70 Batch 550 test Loss 1.3363\n",
      "Epoch 70 Batch 600 Loss 0.5569\n",
      "Epoch 70 Batch 600 test Loss 1.3364\n",
      "Epoch 70 Batch 650 Loss 0.5570\n",
      "Epoch 70 Batch 650 test Loss 1.3365\n",
      "Epoch 70 Batch 700 Loss 0.5574\n",
      "Epoch 70 Batch 700 test Loss 1.3366\n",
      "Epoch 70 Batch 750 Loss 0.5587\n",
      "Epoch 70 Batch 750 test Loss 1.3366\n",
      "Epoch 70 Batch 800 Loss 0.5584\n",
      "Epoch 70 Batch 800 test Loss 1.3367\n",
      "Epoch 70 Batch 850 Loss 0.5579\n",
      "Epoch 70 Batch 850 test Loss 1.3368\n",
      "Epoch 70 Batch 900 Loss 0.5575\n",
      "Epoch 70 Batch 900 test Loss 1.3369\n",
      "Epoch 70 Batch 950 Loss 0.5574\n",
      "Epoch 70 Batch 950 test Loss 1.3370\n",
      "Epoch 71 Batch 0 Loss 0.4381\n",
      "Epoch 71 Batch 0 test Loss 1.3370\n",
      "Epoch 71 Batch 50 Loss 0.5558\n",
      "Epoch 71 Batch 50 test Loss 1.3371\n",
      "Epoch 71 Batch 100 Loss 0.5521\n",
      "Epoch 71 Batch 100 test Loss 1.3372\n",
      "Epoch 71 Batch 150 Loss 0.5518\n",
      "Epoch 71 Batch 150 test Loss 1.3373\n",
      "Epoch 71 Batch 200 Loss 0.5520\n",
      "Epoch 71 Batch 200 test Loss 1.3373\n",
      "Epoch 71 Batch 250 Loss 0.5557\n",
      "Epoch 71 Batch 250 test Loss 1.3374\n",
      "Epoch 71 Batch 300 Loss 0.5567\n",
      "Epoch 71 Batch 300 test Loss 1.3375\n",
      "Epoch 71 Batch 350 Loss 0.5565\n",
      "Epoch 71 Batch 350 test Loss 1.3376\n",
      "Epoch 71 Batch 400 Loss 0.5552\n",
      "Epoch 71 Batch 400 test Loss 1.3377\n",
      "Epoch 71 Batch 450 Loss 0.5560\n",
      "Epoch 71 Batch 450 test Loss 1.3378\n",
      "Epoch 71 Batch 500 Loss 0.5559\n",
      "Epoch 71 Batch 500 test Loss 1.3379\n",
      "Epoch 71 Batch 550 Loss 0.5560\n",
      "Epoch 71 Batch 550 test Loss 1.3379\n",
      "Epoch 71 Batch 600 Loss 0.5567\n",
      "Epoch 71 Batch 600 test Loss 1.3380\n",
      "Epoch 71 Batch 650 Loss 0.5566\n",
      "Epoch 71 Batch 650 test Loss 1.3381\n",
      "Epoch 71 Batch 700 Loss 0.5560\n",
      "Epoch 71 Batch 700 test Loss 1.3382\n",
      "Epoch 71 Batch 750 Loss 0.5564\n",
      "Epoch 71 Batch 750 test Loss 1.3383\n",
      "Epoch 71 Batch 800 Loss 0.5565\n",
      "Epoch 71 Batch 800 test Loss 1.3383\n",
      "Epoch 71 Batch 850 Loss 0.5570\n",
      "Epoch 71 Batch 850 test Loss 1.3384\n",
      "Epoch 71 Batch 900 Loss 0.5570\n",
      "Epoch 71 Batch 900 test Loss 1.3385\n",
      "Epoch 71 Batch 950 Loss 0.5571\n",
      "Epoch 71 Batch 950 test Loss 1.3386\n",
      "Epoch 72 Batch 0 Loss 0.5525\n",
      "Epoch 72 Batch 0 test Loss 1.3386\n",
      "Epoch 72 Batch 50 Loss 0.5609\n",
      "Epoch 72 Batch 50 test Loss 1.3387\n",
      "Epoch 72 Batch 100 Loss 0.5546\n",
      "Epoch 72 Batch 100 test Loss 1.3388\n",
      "Epoch 72 Batch 150 Loss 0.5489\n",
      "Epoch 72 Batch 150 test Loss 1.3389\n",
      "Epoch 72 Batch 200 Loss 0.5545\n",
      "Epoch 72 Batch 200 test Loss 1.3390\n",
      "Epoch 72 Batch 250 Loss 0.5585\n",
      "Epoch 72 Batch 250 test Loss 1.3391\n",
      "Epoch 72 Batch 300 Loss 0.5576\n",
      "Epoch 72 Batch 300 test Loss 1.3391\n",
      "Epoch 72 Batch 350 Loss 0.5579\n",
      "Epoch 72 Batch 350 test Loss 1.3392\n",
      "Epoch 72 Batch 400 Loss 0.5596\n",
      "Epoch 72 Batch 400 test Loss 1.3393\n",
      "Epoch 72 Batch 450 Loss 0.5578\n",
      "Epoch 72 Batch 450 test Loss 1.3394\n",
      "Epoch 72 Batch 500 Loss 0.5573\n",
      "Epoch 72 Batch 500 test Loss 1.3395\n",
      "Epoch 72 Batch 550 Loss 0.5560\n",
      "Epoch 72 Batch 550 test Loss 1.3395\n",
      "Epoch 72 Batch 600 Loss 0.5557\n",
      "Epoch 72 Batch 600 test Loss 1.3396\n",
      "Epoch 72 Batch 650 Loss 0.5557\n",
      "Epoch 72 Batch 650 test Loss 1.3397\n",
      "Epoch 72 Batch 700 Loss 0.5540\n",
      "Epoch 72 Batch 700 test Loss 1.3398\n",
      "Epoch 72 Batch 750 Loss 0.5536\n",
      "Epoch 72 Batch 750 test Loss 1.3399\n",
      "Epoch 72 Batch 800 Loss 0.5522\n",
      "Epoch 72 Batch 800 test Loss 1.3399\n",
      "Epoch 72 Batch 850 Loss 0.5525\n",
      "Epoch 72 Batch 850 test Loss 1.3400\n",
      "Epoch 72 Batch 900 Loss 0.5527\n",
      "Epoch 72 Batch 900 test Loss 1.3401\n",
      "Epoch 72 Batch 950 Loss 0.5524\n",
      "Epoch 72 Batch 950 test Loss 1.3402\n",
      "Epoch 73 Batch 0 Loss 0.6145\n",
      "Epoch 73 Batch 0 test Loss 1.3403\n",
      "Epoch 73 Batch 50 Loss 0.5414\n",
      "Epoch 73 Batch 50 test Loss 1.3403\n",
      "Epoch 73 Batch 100 Loss 0.5639\n",
      "Epoch 73 Batch 100 test Loss 1.3404\n",
      "Epoch 73 Batch 150 Loss 0.5586\n",
      "Epoch 73 Batch 150 test Loss 1.3405\n",
      "Epoch 73 Batch 200 Loss 0.5584\n",
      "Epoch 73 Batch 200 test Loss 1.3406\n",
      "Epoch 73 Batch 250 Loss 0.5617\n",
      "Epoch 73 Batch 250 test Loss 1.3407\n",
      "Epoch 73 Batch 300 Loss 0.5566\n",
      "Epoch 73 Batch 300 test Loss 1.3407\n",
      "Epoch 73 Batch 350 Loss 0.5575\n",
      "Epoch 73 Batch 350 test Loss 1.3408\n",
      "Epoch 73 Batch 400 Loss 0.5581\n",
      "Epoch 73 Batch 400 test Loss 1.3409\n",
      "Epoch 73 Batch 450 Loss 0.5562\n",
      "Epoch 73 Batch 450 test Loss 1.3410\n",
      "Epoch 73 Batch 500 Loss 0.5554\n",
      "Epoch 73 Batch 500 test Loss 1.3410\n",
      "Epoch 73 Batch 550 Loss 0.5551\n",
      "Epoch 73 Batch 550 test Loss 1.3411\n",
      "Epoch 73 Batch 600 Loss 0.5566\n",
      "Epoch 73 Batch 600 test Loss 1.3412\n",
      "Epoch 73 Batch 650 Loss 0.5554\n",
      "Epoch 73 Batch 650 test Loss 1.3413\n",
      "Epoch 73 Batch 700 Loss 0.5545\n",
      "Epoch 73 Batch 700 test Loss 1.3413\n",
      "Epoch 73 Batch 750 Loss 0.5536\n",
      "Epoch 73 Batch 750 test Loss 1.3414\n",
      "Epoch 73 Batch 800 Loss 0.5530\n",
      "Epoch 73 Batch 800 test Loss 1.3415\n",
      "Epoch 73 Batch 850 Loss 0.5527\n",
      "Epoch 73 Batch 850 test Loss 1.3416\n",
      "Epoch 73 Batch 900 Loss 0.5523\n",
      "Epoch 73 Batch 900 test Loss 1.3417\n",
      "Epoch 73 Batch 950 Loss 0.5522\n",
      "Epoch 73 Batch 950 test Loss 1.3417\n",
      "Epoch 74 Batch 0 Loss 0.5529\n",
      "Epoch 74 Batch 0 test Loss 1.3418\n",
      "Epoch 74 Batch 50 Loss 0.5597\n",
      "Epoch 74 Batch 50 test Loss 1.3419\n",
      "Epoch 74 Batch 100 Loss 0.5557\n",
      "Epoch 74 Batch 100 test Loss 1.3420\n",
      "Epoch 74 Batch 150 Loss 0.5569\n",
      "Epoch 74 Batch 150 test Loss 1.3421\n",
      "Epoch 74 Batch 200 Loss 0.5559\n",
      "Epoch 74 Batch 200 test Loss 1.3422\n",
      "Epoch 74 Batch 250 Loss 0.5540\n",
      "Epoch 74 Batch 250 test Loss 1.3422\n",
      "Epoch 74 Batch 300 Loss 0.5555\n",
      "Epoch 74 Batch 300 test Loss 1.3423\n",
      "Epoch 74 Batch 350 Loss 0.5573\n",
      "Epoch 74 Batch 350 test Loss 1.3424\n",
      "Epoch 74 Batch 400 Loss 0.5589\n",
      "Epoch 74 Batch 400 test Loss 1.3425\n",
      "Epoch 74 Batch 450 Loss 0.5576\n",
      "Epoch 74 Batch 450 test Loss 1.3426\n",
      "Epoch 74 Batch 500 Loss 0.5554\n",
      "Epoch 74 Batch 500 test Loss 1.3426\n",
      "Epoch 74 Batch 550 Loss 0.5541\n",
      "Epoch 74 Batch 550 test Loss 1.3427\n",
      "Epoch 74 Batch 600 Loss 0.5546\n",
      "Epoch 74 Batch 600 test Loss 1.3428\n",
      "Epoch 74 Batch 650 Loss 0.5542\n",
      "Epoch 74 Batch 650 test Loss 1.3429\n",
      "Epoch 74 Batch 700 Loss 0.5533\n",
      "Epoch 74 Batch 700 test Loss 1.3429\n",
      "Epoch 74 Batch 750 Loss 0.5527\n",
      "Epoch 74 Batch 750 test Loss 1.3430\n",
      "Epoch 74 Batch 800 Loss 0.5526\n",
      "Epoch 74 Batch 800 test Loss 1.3431\n",
      "Epoch 74 Batch 850 Loss 0.5520\n",
      "Epoch 74 Batch 850 test Loss 1.3432\n",
      "Epoch 74 Batch 900 Loss 0.5515\n",
      "Epoch 74 Batch 900 test Loss 1.3432\n",
      "Epoch 74 Batch 950 Loss 0.5510\n",
      "Epoch 74 Batch 950 test Loss 1.3433\n",
      "Epoch 75 Batch 0 Loss 0.4347\n",
      "Epoch 75 Batch 0 test Loss 1.3434\n",
      "Epoch 75 Batch 50 Loss 0.5612\n",
      "Epoch 75 Batch 50 test Loss 1.3435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Batch 100 Loss 0.5506\n",
      "Epoch 75 Batch 100 test Loss 1.3436\n",
      "Epoch 75 Batch 150 Loss 0.5530\n",
      "Epoch 75 Batch 150 test Loss 1.3437\n",
      "Epoch 75 Batch 200 Loss 0.5504\n",
      "Epoch 75 Batch 200 test Loss 1.3437\n",
      "Epoch 75 Batch 250 Loss 0.5521\n",
      "Epoch 75 Batch 250 test Loss 1.3438\n",
      "Epoch 75 Batch 300 Loss 0.5533\n",
      "Epoch 75 Batch 300 test Loss 1.3439\n",
      "Epoch 75 Batch 350 Loss 0.5519\n",
      "Epoch 75 Batch 350 test Loss 1.3440\n",
      "Epoch 75 Batch 400 Loss 0.5513\n",
      "Epoch 75 Batch 400 test Loss 1.3441\n",
      "Epoch 75 Batch 450 Loss 0.5495\n",
      "Epoch 75 Batch 450 test Loss 1.3441\n",
      "Epoch 75 Batch 500 Loss 0.5492\n",
      "Epoch 75 Batch 500 test Loss 1.3442\n",
      "Epoch 75 Batch 550 Loss 0.5496\n",
      "Epoch 75 Batch 550 test Loss 1.3443\n",
      "Epoch 75 Batch 600 Loss 0.5506\n",
      "Epoch 75 Batch 600 test Loss 1.3444\n",
      "Epoch 75 Batch 650 Loss 0.5506\n",
      "Epoch 75 Batch 650 test Loss 1.3445\n",
      "Epoch 75 Batch 700 Loss 0.5498\n",
      "Epoch 75 Batch 700 test Loss 1.3446\n",
      "Epoch 75 Batch 750 Loss 0.5503\n",
      "Epoch 75 Batch 750 test Loss 1.3446\n",
      "Epoch 75 Batch 800 Loss 0.5497\n",
      "Epoch 75 Batch 800 test Loss 1.3447\n",
      "Epoch 75 Batch 850 Loss 0.5497\n",
      "Epoch 75 Batch 850 test Loss 1.3448\n",
      "Epoch 75 Batch 900 Loss 0.5491\n",
      "Epoch 75 Batch 900 test Loss 1.3449\n",
      "Epoch 75 Batch 950 Loss 0.5489\n",
      "Epoch 75 Batch 950 test Loss 1.3450\n",
      "Epoch 76 Batch 0 Loss 0.6501\n",
      "Epoch 76 Batch 0 test Loss 1.3451\n",
      "Epoch 76 Batch 50 Loss 0.5574\n",
      "Epoch 76 Batch 50 test Loss 1.3451\n",
      "Epoch 76 Batch 100 Loss 0.5645\n",
      "Epoch 76 Batch 100 test Loss 1.3452\n",
      "Epoch 76 Batch 150 Loss 0.5593\n",
      "Epoch 76 Batch 150 test Loss 1.3453\n",
      "Epoch 76 Batch 200 Loss 0.5586\n",
      "Epoch 76 Batch 200 test Loss 1.3454\n",
      "Epoch 76 Batch 250 Loss 0.5566\n",
      "Epoch 76 Batch 250 test Loss 1.3455\n",
      "Epoch 76 Batch 300 Loss 0.5560\n",
      "Epoch 76 Batch 300 test Loss 1.3455\n",
      "Epoch 76 Batch 350 Loss 0.5567\n",
      "Epoch 76 Batch 350 test Loss 1.3456\n",
      "Epoch 76 Batch 400 Loss 0.5560\n",
      "Epoch 76 Batch 400 test Loss 1.3457\n",
      "Epoch 76 Batch 450 Loss 0.5557\n",
      "Epoch 76 Batch 450 test Loss 1.3458\n",
      "Epoch 76 Batch 500 Loss 0.5565\n",
      "Epoch 76 Batch 500 test Loss 1.3458\n",
      "Epoch 76 Batch 550 Loss 0.5555\n",
      "Epoch 76 Batch 550 test Loss 1.3459\n",
      "Epoch 76 Batch 600 Loss 0.5546\n",
      "Epoch 76 Batch 600 test Loss 1.3460\n",
      "Epoch 76 Batch 650 Loss 0.5539\n",
      "Epoch 76 Batch 650 test Loss 1.3461\n",
      "Epoch 76 Batch 700 Loss 0.5546\n",
      "Epoch 76 Batch 700 test Loss 1.3462\n",
      "Epoch 76 Batch 750 Loss 0.5544\n",
      "Epoch 76 Batch 750 test Loss 1.3462\n",
      "Epoch 76 Batch 800 Loss 0.5537\n",
      "Epoch 76 Batch 800 test Loss 1.3463\n",
      "Epoch 76 Batch 850 Loss 0.5538\n",
      "Epoch 76 Batch 850 test Loss 1.3464\n",
      "Epoch 76 Batch 900 Loss 0.5536\n",
      "Epoch 76 Batch 900 test Loss 1.3465\n",
      "Epoch 76 Batch 950 Loss 0.5527\n",
      "Epoch 76 Batch 950 test Loss 1.3465\n",
      "Epoch 77 Batch 0 Loss 0.7193\n",
      "Epoch 77 Batch 0 test Loss 1.3466\n",
      "Epoch 77 Batch 50 Loss 0.5567\n",
      "Epoch 77 Batch 50 test Loss 1.3467\n",
      "Epoch 77 Batch 100 Loss 0.5510\n",
      "Epoch 77 Batch 100 test Loss 1.3468\n",
      "Epoch 77 Batch 150 Loss 0.5473\n",
      "Epoch 77 Batch 150 test Loss 1.3469\n",
      "Epoch 77 Batch 200 Loss 0.5493\n",
      "Epoch 77 Batch 200 test Loss 1.3469\n",
      "Epoch 77 Batch 250 Loss 0.5476\n",
      "Epoch 77 Batch 250 test Loss 1.3470\n",
      "Epoch 77 Batch 300 Loss 0.5482\n",
      "Epoch 77 Batch 300 test Loss 1.3471\n",
      "Epoch 77 Batch 350 Loss 0.5490\n",
      "Epoch 77 Batch 350 test Loss 1.3472\n",
      "Epoch 77 Batch 400 Loss 0.5511\n",
      "Epoch 77 Batch 400 test Loss 1.3472\n",
      "Epoch 77 Batch 450 Loss 0.5502\n",
      "Epoch 77 Batch 450 test Loss 1.3473\n",
      "Epoch 77 Batch 500 Loss 0.5524\n",
      "Epoch 77 Batch 500 test Loss 1.3474\n",
      "Epoch 77 Batch 550 Loss 0.5520\n",
      "Epoch 77 Batch 550 test Loss 1.3475\n",
      "Epoch 77 Batch 600 Loss 0.5506\n",
      "Epoch 77 Batch 600 test Loss 1.3475\n",
      "Epoch 77 Batch 650 Loss 0.5496\n",
      "Epoch 77 Batch 650 test Loss 1.3476\n",
      "Epoch 77 Batch 700 Loss 0.5508\n",
      "Epoch 77 Batch 700 test Loss 1.3477\n",
      "Epoch 77 Batch 750 Loss 0.5511\n",
      "Epoch 77 Batch 750 test Loss 1.3478\n",
      "Epoch 77 Batch 800 Loss 0.5492\n",
      "Epoch 77 Batch 800 test Loss 1.3479\n",
      "Epoch 77 Batch 850 Loss 0.5499\n",
      "Epoch 77 Batch 850 test Loss 1.3479\n",
      "Epoch 77 Batch 900 Loss 0.5497\n",
      "Epoch 77 Batch 900 test Loss 1.3480\n",
      "Epoch 77 Batch 950 Loss 0.5489\n",
      "Epoch 77 Batch 950 test Loss 1.3481\n",
      "Epoch 78 Batch 0 Loss 0.5596\n",
      "Epoch 78 Batch 0 test Loss 1.3482\n",
      "Epoch 78 Batch 50 Loss 0.5326\n",
      "Epoch 78 Batch 50 test Loss 1.3483\n",
      "Epoch 78 Batch 100 Loss 0.5352\n",
      "Epoch 78 Batch 100 test Loss 1.3483\n",
      "Epoch 78 Batch 150 Loss 0.5427\n",
      "Epoch 78 Batch 150 test Loss 1.3484\n",
      "Epoch 78 Batch 200 Loss 0.5395\n",
      "Epoch 78 Batch 200 test Loss 1.3485\n",
      "Epoch 78 Batch 250 Loss 0.5425\n",
      "Epoch 78 Batch 250 test Loss 1.3486\n",
      "Epoch 78 Batch 300 Loss 0.5450\n",
      "Epoch 78 Batch 300 test Loss 1.3486\n",
      "Epoch 78 Batch 350 Loss 0.5469\n",
      "Epoch 78 Batch 350 test Loss 1.3487\n",
      "Epoch 78 Batch 400 Loss 0.5453\n",
      "Epoch 78 Batch 400 test Loss 1.3488\n",
      "Epoch 78 Batch 450 Loss 0.5457\n",
      "Epoch 78 Batch 450 test Loss 1.3489\n",
      "Epoch 78 Batch 500 Loss 0.5441\n",
      "Epoch 78 Batch 500 test Loss 1.3489\n",
      "Epoch 78 Batch 550 Loss 0.5435\n",
      "Epoch 78 Batch 550 test Loss 1.3490\n",
      "Epoch 78 Batch 600 Loss 0.5436\n",
      "Epoch 78 Batch 600 test Loss 1.3491\n",
      "Epoch 78 Batch 650 Loss 0.5436\n",
      "Epoch 78 Batch 650 test Loss 1.3492\n",
      "Epoch 78 Batch 700 Loss 0.5436\n",
      "Epoch 78 Batch 700 test Loss 1.3493\n",
      "Epoch 78 Batch 750 Loss 0.5441\n",
      "Epoch 78 Batch 750 test Loss 1.3493\n",
      "Epoch 78 Batch 800 Loss 0.5440\n",
      "Epoch 78 Batch 800 test Loss 1.3494\n",
      "Epoch 78 Batch 850 Loss 0.5448\n",
      "Epoch 78 Batch 850 test Loss 1.3495\n",
      "Epoch 78 Batch 900 Loss 0.5452\n",
      "Epoch 78 Batch 900 test Loss 1.3495\n",
      "Epoch 78 Batch 950 Loss 0.5450\n",
      "Epoch 78 Batch 950 test Loss 1.3496\n",
      "Epoch 79 Batch 0 Loss 0.4994\n",
      "Epoch 79 Batch 0 test Loss 1.3497\n",
      "Epoch 79 Batch 50 Loss 0.5456\n",
      "Epoch 79 Batch 50 test Loss 1.3498\n",
      "Epoch 79 Batch 100 Loss 0.5545\n",
      "Epoch 79 Batch 100 test Loss 1.3499\n",
      "Epoch 79 Batch 150 Loss 0.5522\n",
      "Epoch 79 Batch 150 test Loss 1.3499\n",
      "Epoch 79 Batch 200 Loss 0.5549\n",
      "Epoch 79 Batch 200 test Loss 1.3500\n",
      "Epoch 79 Batch 250 Loss 0.5581\n",
      "Epoch 79 Batch 250 test Loss 1.3501\n",
      "Epoch 79 Batch 300 Loss 0.5580\n",
      "Epoch 79 Batch 300 test Loss 1.3502\n",
      "Epoch 79 Batch 350 Loss 0.5590\n",
      "Epoch 79 Batch 350 test Loss 1.3502\n",
      "Epoch 79 Batch 400 Loss 0.5581\n",
      "Epoch 79 Batch 400 test Loss 1.3503\n",
      "Epoch 79 Batch 450 Loss 0.5563\n",
      "Epoch 79 Batch 450 test Loss 1.3504\n",
      "Epoch 79 Batch 500 Loss 0.5548\n",
      "Epoch 79 Batch 500 test Loss 1.3504\n",
      "Epoch 79 Batch 550 Loss 0.5537\n",
      "Epoch 79 Batch 550 test Loss 1.3505\n",
      "Epoch 79 Batch 600 Loss 0.5531\n",
      "Epoch 79 Batch 600 test Loss 1.3506\n",
      "Epoch 79 Batch 650 Loss 0.5520\n",
      "Epoch 79 Batch 650 test Loss 1.3507\n",
      "Epoch 79 Batch 700 Loss 0.5532\n",
      "Epoch 79 Batch 700 test Loss 1.3507\n",
      "Epoch 79 Batch 750 Loss 0.5542\n",
      "Epoch 79 Batch 750 test Loss 1.3508\n",
      "Epoch 79 Batch 800 Loss 0.5532\n",
      "Epoch 79 Batch 800 test Loss 1.3509\n",
      "Epoch 79 Batch 850 Loss 0.5527\n",
      "Epoch 79 Batch 850 test Loss 1.3510\n",
      "Epoch 79 Batch 900 Loss 0.5512\n",
      "Epoch 79 Batch 900 test Loss 1.3510\n",
      "Epoch 79 Batch 950 Loss 0.5507\n",
      "Epoch 79 Batch 950 test Loss 1.3511\n",
      "Epoch 80 Batch 0 Loss 0.6142\n",
      "Epoch 80 Batch 0 test Loss 1.3512\n",
      "Epoch 80 Batch 50 Loss 0.5511\n",
      "Epoch 80 Batch 50 test Loss 1.3513\n",
      "Epoch 80 Batch 100 Loss 0.5528\n",
      "Epoch 80 Batch 100 test Loss 1.3513\n",
      "Epoch 80 Batch 150 Loss 0.5517\n",
      "Epoch 80 Batch 150 test Loss 1.3514\n",
      "Epoch 80 Batch 200 Loss 0.5523\n",
      "Epoch 80 Batch 200 test Loss 1.3515\n",
      "Epoch 80 Batch 250 Loss 0.5545\n",
      "Epoch 80 Batch 250 test Loss 1.3516\n",
      "Epoch 80 Batch 300 Loss 0.5574\n",
      "Epoch 80 Batch 300 test Loss 1.3516\n",
      "Epoch 80 Batch 350 Loss 0.5578\n",
      "Epoch 80 Batch 350 test Loss 1.3517\n",
      "Epoch 80 Batch 400 Loss 0.5559\n",
      "Epoch 80 Batch 400 test Loss 1.3518\n",
      "Epoch 80 Batch 450 Loss 0.5549\n",
      "Epoch 80 Batch 450 test Loss 1.3519\n",
      "Epoch 80 Batch 500 Loss 0.5547\n",
      "Epoch 80 Batch 500 test Loss 1.3519\n",
      "Epoch 80 Batch 550 Loss 0.5554\n",
      "Epoch 80 Batch 550 test Loss 1.3520\n",
      "Epoch 80 Batch 600 Loss 0.5548\n",
      "Epoch 80 Batch 600 test Loss 1.3521\n",
      "Epoch 80 Batch 650 Loss 0.5545\n",
      "Epoch 80 Batch 650 test Loss 1.3522\n",
      "Epoch 80 Batch 700 Loss 0.5531\n",
      "Epoch 80 Batch 700 test Loss 1.3522\n",
      "Epoch 80 Batch 750 Loss 0.5528\n",
      "Epoch 80 Batch 750 test Loss 1.3523\n",
      "Epoch 80 Batch 800 Loss 0.5526\n",
      "Epoch 80 Batch 800 test Loss 1.3524\n",
      "Epoch 80 Batch 850 Loss 0.5527\n",
      "Epoch 80 Batch 850 test Loss 1.3525\n",
      "Epoch 80 Batch 900 Loss 0.5516\n",
      "Epoch 80 Batch 900 test Loss 1.3525\n",
      "Epoch 80 Batch 950 Loss 0.5512\n",
      "Epoch 80 Batch 950 test Loss 1.3526\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 80\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    batch_loss.reset_states()\n",
    "\n",
    "\n",
    "    init_enc_hidden = encoder.initialize_hidden_state(128)\n",
    "\n",
    "    for i in range(1000):\n",
    "        b_eng_tr, b_fren_tr, b_y_fren_tr, _ = create_batch_gp_mim(eng_tr, fren_tr, y_fren_tr)\n",
    "        train_step(b_eng_tr, b_fren_tr, b_y_fren_tr, init_enc_hidden)\n",
    "        \n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   i,\n",
    "                                                   batch_loss.result()))\n",
    "            test_step(eng_te, fren_te, y_fren_te, eng_te.shape[0])\n",
    "        \n",
    "            print('Epoch {} Batch {} test Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   i,\n",
    "                                                   test_loss.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "    init_enc_hidden = encoder.initialize_hidden_state(1000)\n",
    "    enc_output, enc_hidden = encoder(eng_te, init_enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims(fren_te, 1)\n",
    "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.2934513], dtype=float32)>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(( predictions - y_fren_te) ** 2) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'encoder_5/gru_12/gru_cell_12/kernel:0' shape=(2, 96) dtype=float32, numpy=\n",
       " array([[-0.24294144, -0.12160824, -0.04108305, -0.13510454, -0.4508549 ,\n",
       "          0.08995712,  0.46263385,  0.02382828, -0.1736701 , -0.08664411,\n",
       "         -0.0645362 ,  0.10717343,  0.39027807, -0.2536707 ,  0.26978347,\n",
       "          0.21451135,  0.17279914, -0.22395223,  0.38327673,  0.5131383 ,\n",
       "         -0.6774076 , -0.00840344, -0.01552058,  0.20810582, -0.06036491,\n",
       "         -0.01459866, -0.7317905 ,  0.41818264,  0.12667684, -0.08040617,\n",
       "          0.02328785, -0.849636  ,  0.3476957 , -0.09011477, -0.33210495,\n",
       "          0.18907492,  0.79412216, -0.46108374, -0.7936108 , -0.47735268,\n",
       "         -0.2614186 ,  0.17455386, -0.41601086,  0.1933774 , -0.2497674 ,\n",
       "          0.22317033, -0.08189869, -0.03433538, -0.23077747,  0.57423216,\n",
       "          0.43720472, -0.4934506 ,  0.859332  ,  0.3871569 ,  0.452166  ,\n",
       "          0.41956085,  0.42980555,  0.23904507,  0.23489945,  0.16605651,\n",
       "         -0.22875607, -0.44421604, -0.3486138 ,  0.25133145,  0.31181687,\n",
       "          0.06967046,  0.33070794,  0.1384436 , -0.36493957, -0.38665524,\n",
       "          0.16796085,  0.37440327,  0.2596985 , -0.30571455,  0.03567734,\n",
       "         -0.36190486, -0.22914508, -0.42053622,  0.25091705, -0.28036898,\n",
       "          0.61241084, -0.06267966,  0.4340943 , -0.13136393,  0.1265146 ,\n",
       "          0.04470358, -0.16493382,  0.08504023, -0.4348578 , -0.03108337,\n",
       "         -0.45310506, -0.3753903 ,  0.02472206,  0.21448673, -0.6460101 ,\n",
       "         -0.0549471 ],\n",
       "        [ 0.42093408, -0.04999859,  0.42630452,  0.15585363, -0.8919522 ,\n",
       "          0.05789125, -0.21274544,  0.73676527,  0.07628429, -0.6823269 ,\n",
       "         -0.53593224, -0.21747479,  0.1045029 ,  0.5667784 ,  0.31570184,\n",
       "         -0.6059957 , -0.19918522, -0.3786668 , -0.6850181 ,  0.3614619 ,\n",
       "         -0.18633692,  0.40691844, -0.129794  , -0.77349603, -0.03127688,\n",
       "         -0.17464113, -0.5454444 ,  0.3257035 ,  0.19735463, -0.3999998 ,\n",
       "         -0.30482537,  0.23704185, -0.23920357,  0.01554445, -0.30388322,\n",
       "          0.16089699,  0.43867067,  0.07254178,  0.07933496, -1.2534698 ,\n",
       "         -0.51505023, -0.4720859 ,  0.11754227, -0.03918319, -0.31088713,\n",
       "         -0.9377891 ,  0.33074266, -0.06138564, -0.61474586, -0.4876547 ,\n",
       "         -0.02266668,  0.15520923, -0.08743083,  0.45330822, -0.47840288,\n",
       "         -0.405769  , -0.47830918, -0.07609998,  0.47619385, -0.18895155,\n",
       "          1.056526  , -0.15973583,  0.13684365, -0.35247892,  0.69148624,\n",
       "         -0.46738687,  0.1523529 ,  0.7223645 , -0.67252785,  0.4095394 ,\n",
       "          0.8225792 ,  0.23163351,  0.4208125 ,  0.9448928 ,  0.06652103,\n",
       "         -0.6620116 ,  0.02664185, -0.63341564,  1.0354784 ,  0.49095926,\n",
       "         -0.11525481,  0.6796167 , -0.2686682 , -0.2603277 , -0.6959265 ,\n",
       "          0.7949959 , -0.0755331 , -0.5985421 , -0.15384582,  0.19318789,\n",
       "          0.42578986,  0.13684708,  0.70627874,  0.3375078 , -0.15207246,\n",
       "          0.10894605]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_5/gru_12/gru_cell_12/recurrent_kernel:0' shape=(32, 96) dtype=float32, numpy=\n",
       " array([[ 0.37668204, -0.45532295, -0.08941664, ..., -0.56213427,\n",
       "          0.31084377, -0.22262879],\n",
       "        [-0.41535383,  0.0440349 , -0.37544927, ...,  0.19660722,\n",
       "         -0.06502235, -0.04394658],\n",
       "        [ 0.07614719,  0.3392273 ,  0.46867284, ..., -0.3722256 ,\n",
       "         -0.06536653,  0.27893278],\n",
       "        ...,\n",
       "        [-0.31489083,  0.28144938, -0.0147178 , ...,  0.69760066,\n",
       "         -0.39441216, -0.08550712],\n",
       "        [-0.01468506,  0.49216166, -0.35813233, ..., -0.12504835,\n",
       "          0.36200386, -0.1194619 ],\n",
       "        [-0.6302966 ,  0.3520548 , -0.11873076, ...,  0.34860054,\n",
       "          0.5201868 ,  0.0213672 ]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_5/gru_12/gru_cell_12/bias:0' shape=(2, 96) dtype=float32, numpy=\n",
       " array([[-0.25670388, -0.5344397 , -0.08771432, -0.18145607, -0.11626081,\n",
       "         -0.2327212 , -0.25218734, -0.04364258,  0.08487444, -0.45147526,\n",
       "         -0.25751895, -0.24353702, -0.10420953, -0.34409457, -0.14099921,\n",
       "         -0.39603966, -0.32612908, -0.1900859 , -0.06402004, -0.1490735 ,\n",
       "         -0.34898534, -0.40348884, -0.2764429 , -0.13031594, -0.4608968 ,\n",
       "          0.09454478, -0.12320648, -0.14078723, -0.50851965, -0.06684607,\n",
       "         -0.07885461, -0.09246565,  0.5226969 ,  0.37759784,  0.18832804,\n",
       "          0.18208395, -0.20287226,  0.20437823,  0.22667634,  0.18665692,\n",
       "          0.2708183 ,  0.2804361 ,  0.34533143, -0.11907806,  0.09647851,\n",
       "          0.43727618,  0.2601664 ,  0.22633888,  0.4874975 ,  0.2596139 ,\n",
       "          0.13572945,  0.23944975,  0.15515062,  0.04127954,  0.14906216,\n",
       "          0.05631853, -0.00303849,  0.36197385,  0.01787709,  0.09384653,\n",
       "          0.6354218 ,  0.64578915,  0.10771915,  0.10140957, -0.16358966,\n",
       "         -0.09819508, -0.1367657 , -0.0255533 ,  0.33734852, -0.05697073,\n",
       "          0.02914814,  0.00275355, -0.19402277,  0.14811209,  0.13710803,\n",
       "          0.08318792, -0.07689993,  0.18291844,  0.22177008, -0.08086027,\n",
       "         -0.04474465, -0.1395294 ,  0.05485016, -0.16973019, -0.01230819,\n",
       "          0.09119442,  0.01884032,  0.13592365,  0.22274673, -0.13006394,\n",
       "          0.30608964, -0.03555011, -0.16877629,  0.06050098,  0.05385394,\n",
       "          0.0840948 ],\n",
       "        [-0.25670388, -0.5344397 , -0.08771432, -0.18145607, -0.11626081,\n",
       "         -0.2327212 , -0.25218734, -0.04364258,  0.08487444, -0.45147526,\n",
       "         -0.25751895, -0.24353702, -0.10420953, -0.34409457, -0.14099921,\n",
       "         -0.39603966, -0.32612908, -0.1900859 , -0.06402004, -0.1490735 ,\n",
       "         -0.34898534, -0.40348884, -0.2764429 , -0.13031594, -0.4608968 ,\n",
       "          0.09454478, -0.12320648, -0.14078723, -0.50851965, -0.06684607,\n",
       "         -0.07885461, -0.09246565,  0.5226969 ,  0.37759784,  0.18832804,\n",
       "          0.18208395, -0.20287226,  0.20437823,  0.22667634,  0.18665692,\n",
       "          0.2708183 ,  0.2804361 ,  0.34533143, -0.11907806,  0.09647851,\n",
       "          0.43727618,  0.2601664 ,  0.22633888,  0.4874975 ,  0.2596139 ,\n",
       "          0.13572945,  0.23944975,  0.15515062,  0.04127954,  0.14906216,\n",
       "          0.05631853, -0.00303849,  0.36197385,  0.01787709,  0.09384653,\n",
       "          0.6354218 ,  0.64578915,  0.10771915,  0.10140957, -0.14440705,\n",
       "         -0.22620782, -0.10533272, -0.11428218, -0.02876891,  0.10665055,\n",
       "         -0.17301227, -0.00083023, -0.10227488,  0.07814051,  0.17935392,\n",
       "          0.0654626 , -0.13228887,  0.06726605,  0.12877236, -0.05697676,\n",
       "         -0.13809717, -0.12645732,  0.04134174, -0.19147852,  0.00666039,\n",
       "          0.14151347,  0.07479314,  0.2662459 ,  0.24411346, -0.1924188 ,\n",
       "          0.11549749, -0.14238355, -0.23818439, -0.05476218,  0.16595699,\n",
       "          0.28630757]], dtype=float32)>]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset ted_hrlr_translate/pt_to_en/1.0.0 (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /Users/omernivron/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fc3b31a23c447ba3768b9f04c2bba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdc0beb8cd84536ae01701300c40d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bff6948c5614fc6baf83c16f8d7e1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Extraction completed...', max=1, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /Users/omernivron/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteCN89RD/ted_hrlr_translate-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1185a2853bf94cfa98a76e519622d468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51785), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /Users/omernivron/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteCN89RD/ted_hrlr_translate-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe47927118c44868f923ded09a8f382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1193), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /Users/omernivron/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteCN89RD/ted_hrlr_translate-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45a94752bb8413195bcf94c8187534a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1803), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to /Users/omernivron/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\r"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string: Transformer is awesome.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7915 ----> T\n",
      "1248 ----> ran\n",
      "7946 ----> s\n",
      "7194 ----> former \n",
      "13 ----> is \n",
      "2799 ----> awesome\n",
      "7877 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "\n",
    "    return result_pt, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
       " array([[8214,  342, 3032, ...,    0,    0,    0],\n",
       "        [8214,   95,  198, ...,    0,    0,    0],\n",
       "        [8214, 4479, 7990, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,  584,   12, ...,    0,    0,    0],\n",
       "        [8214,   59, 1548, ...,    0,    0,    0],\n",
       "        [8214,  118,   34, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8087,   98,   25, ...,    0,    0,    0],\n",
       "        [8087,   12,   20, ...,    0,    0,    0],\n",
       "        [8087,   12, 5453, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,   18, 2059, ...,    0,    0,    0],\n",
       "        [8087,   16, 1436, ...,    0,    0,    0],\n",
       "        [8087,   15,   57, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mask indicates which entries should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.25745153, 0.22060382, 0.59445953]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        \n",
    "#         print('value: ', v)\n",
    "#         print('key: ', k)\n",
    "#         print('query: ', q)\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value:  tf.Tensor(\n",
      "[[[0.93007994 0.07386124 0.12029994 ... 0.7008438  0.0717392  0.26032686]\n",
      "  [0.48606646 0.7353846  0.41520226 ... 0.35724092 0.27602732 0.86372983]\n",
      "  [0.41987157 0.8600621  0.10850787 ... 0.467916   0.99416685 0.17329168]\n",
      "  ...\n",
      "  [0.12135828 0.24125612 0.59066045 ... 0.6352788  0.778121   0.11662745]\n",
      "  [0.4706217  0.46683395 0.69353175 ... 0.56884396 0.90624857 0.3799808 ]\n",
      "  [0.5592438  0.9700223  0.72765493 ... 0.13796568 0.9331293  0.17192411]]], shape=(1, 60, 512), dtype=float32)\n",
      "key:  tf.Tensor(\n",
      "[[[0.93007994 0.07386124 0.12029994 ... 0.7008438  0.0717392  0.26032686]\n",
      "  [0.48606646 0.7353846  0.41520226 ... 0.35724092 0.27602732 0.86372983]\n",
      "  [0.41987157 0.8600621  0.10850787 ... 0.467916   0.99416685 0.17329168]\n",
      "  ...\n",
      "  [0.12135828 0.24125612 0.59066045 ... 0.6352788  0.778121   0.11662745]\n",
      "  [0.4706217  0.46683395 0.69353175 ... 0.56884396 0.90624857 0.3799808 ]\n",
      "  [0.5592438  0.9700223  0.72765493 ... 0.13796568 0.9331293  0.17192411]]], shape=(1, 60, 512), dtype=float32)\n",
      "query:  tf.Tensor(\n",
      "[[[0.93007994 0.07386124 0.12029994 ... 0.7008438  0.0717392  0.26032686]\n",
      "  [0.48606646 0.7353846  0.41520226 ... 0.35724092 0.27602732 0.86372983]\n",
      "  [0.41987157 0.8600621  0.10850787 ... 0.467916   0.99416685 0.17329168]\n",
      "  ...\n",
      "  [0.12135828 0.24125612 0.59066045 ... 0.6352788  0.778121   0.11662745]\n",
      "  [0.4706217  0.46683395 0.69353175 ... 0.56884396 0.90624857 0.3799808 ]\n",
      "  [0.5592438  0.9700223  0.72765493 ... 0.13796568 0.9331293  0.17192411]]], shape=(1, 60, 512), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input sentence is passed through N encoder layers \n",
    "# that generates an output for each word/token in the sequence.\n",
    "\n",
    "# The decoder attends on the encoder's output and\n",
    "# its own input (self-attention) to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder layer\n",
    "\n",
    "# Each encoder layer consists of sublayers:\n",
    "\n",
    "# Multi-head attention (with padding mask)\n",
    "# Point wise feed forward networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder layer\n",
    "\n",
    "# Each decoder layer consists of sublayers:\n",
    "\n",
    "# Masked multi-head attention (with look ahead mask and padding mask)\n",
    "# Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer.\n",
    "# Point wise feed forward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "#         print('encoder_x_inp: ', x)\n",
    "#         print('encoder_mask: ', mask)\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "#         print('encoded_x_after_embedding: ', x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    #a call method, the layer's forward pass\n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "        \n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every n epochs.\n",
    "\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. \n",
    "# tar_real is that same input shifted by 1: \n",
    "#     At each location in tar_input, tar_real contains the next token that should be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, sentence = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "# tar_inp = \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "# tar_real = \"A lion in the jungle is sleeping EOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher forcing is passing the true output to the next time step\n",
    "# regardless of what the model predicts at the current time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent the model from peaking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    print('enc_padding_mask: ', enc_padding_mask)\n",
    "    print('dec_padding_mask: ', dec_padding_mask)\n",
    "\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(37,), dtype=int64, numpy=\n",
       "array([8214,   42,    8,  131, 3491, 7990,  155,    2, 8215,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38,), dtype=int64, numpy=\n",
       "array([8087,   16,   13,   18, 7375,  492,  111,    2, 8088,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(37,), dtype=int64, numpy=\n",
       "array([8214,   32, 7095,  136,   72,   95,    3, 5768, 8072, 8003,   16,\n",
       "         22, 6014, 7990,    6,  400,    3, 2155, 8055,   93,  125,   11,\n",
       "          3, 2637,    1, 1837, 7106,  232,    1,  500,   14, 1944,   37,\n",
       "       8215,    0,    0,    0])>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38,), dtype=int64, numpy=\n",
       "array([8087,   62, 3257, 5032, 7863,   20,  114, 6540, 7863,   11, 3029,\n",
       "          1,    4,   65,  298,    5,  197, 3802,  116,  190,    8,    9,\n",
       "        120,    5, 1763,  231, 7012, 6455, 5112,    1, 1515,  403,   75,\n",
       "       8088,    0,    0,    0,    0])>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:  tf.Tensor(\n",
      "[[8214    9 1356 ...    0    0    0]\n",
      " [8214   77   42 ...    0    0    0]\n",
      " [8214  610 8003 ...  589 8215    0]\n",
      " ...\n",
      " [8214   64 7265 ...    0    0    0]\n",
      " [8214   78   16 ...    0    0    0]\n",
      " [8214   27 1389 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087    7  855 ...    0    0    0]\n",
      " [8087   18   16 ...    0    0    0]\n",
      " [8087  229   81 ... 8088    0    0]\n",
      " ...\n",
      " [8087   14  134 ...    0    0    0]\n",
      " [8087  115   44 ...    0    0    0]\n",
      " [8087   12   20 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "Epoch 1 Batch 0 Loss 8.9562 Accuracy 0.0008\n",
      "inp:  tf.Tensor(\n",
      "[[8214    7 1591 ...    0    0    0]\n",
      " [8214 5130  180 ...    0    0    0]\n",
      " [8214   56  131 ...    0    0    0]\n",
      " ...\n",
      " [8214    8 6487 ...    0    0    0]\n",
      " [8214 1455  543 ...    0    0    0]\n",
      " [8214   24  402 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087 1173 1493 ...    0    0    0]\n",
      " [8087  134   16 ...    0    0    0]\n",
      " [8087   17   20 ... 8088    0    0]\n",
      " ...\n",
      " [8087   14  101 ...    0    0    0]\n",
      " [8087  120 6470 ...    0    0    0]\n",
      " [8087   19    8 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214 8066 8057 ...    0    0    0]\n",
      " [8214 1176   54 ...    0    0    0]\n",
      " [8214    8   21 ...    0    0    0]\n",
      " ...\n",
      " [8214 2030   39 ...    0    0    0]\n",
      " [8214   25  294 ...    0    0    0]\n",
      " [8214    3  438 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087 7939 7930 ...    0    0    0]\n",
      " [8087  169    1 ...    0    0    0]\n",
      " [8087   19    8 ...    0    0    0]\n",
      " ...\n",
      " [8087 1588   56 ...    0    0    0]\n",
      " [8087   23  444 ...    0    0    0]\n",
      " [8087    3  336 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214 1668 2926 ... 8215    0    0]\n",
      " [8214   56   80 ...    0    0    0]\n",
      " [8214  630   24 ...    0    0    0]\n",
      " ...\n",
      " [8214  852    5 ...    0    0    0]\n",
      " [8214   54    8 ...    0    0    0]\n",
      " [8214 2822 2252 ...    0    0    0]], shape=(64, 37), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087  395    6 ...    0    0    0]\n",
      " [8087   17   20 ...    0    0    0]\n",
      " [8087   84   35 ...    0    0    0]\n",
      " ...\n",
      " [8087   12  300 ...    0    0    0]\n",
      " [8087   16  653 ...    0    0    0]\n",
      " [8087 2284 1979 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214    6  355 ...    0    0    0]\n",
      " [8214   77    7 ...    0    0    0]\n",
      " [8214 7091   20 ...    0    0    0]\n",
      " ...\n",
      " [8214  116 3455 ...    0    0    0]\n",
      " [8214   23  953 ...    0    0    0]\n",
      " [8214  195    1 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087    4   64 ...    0    0    0]\n",
      " [8087   18   29 ...    0    0    0]\n",
      " [8087 5763   28 ...    0    0    0]\n",
      " ...\n",
      " [8087   83  229 ...    0    0    0]\n",
      " [8087 2009    3 ...    0    0    0]\n",
      " [8087   23   25 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  299   47 ...   37 8215    0]\n",
      " [8214  149  405 ...    0    0    0]\n",
      " [8214    6 2381 ...    0    0    0]\n",
      " ...\n",
      " [8214  264  159 ...    0    0    0]\n",
      " [8214  104    1 ... 3969    2 8215]\n",
      " [8214   64  323 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   62  154 ...    0    0    0]\n",
      " [8087   62   14 ...    0    0    0]\n",
      " [8087    4   40 ...    0    0    0]\n",
      " ...\n",
      " [8087    7  232 ...    0    0    0]\n",
      " [8087  133    1 ...    0    0    0]\n",
      " [8087   14  101 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214 1417 8003 ...    0    0    0]\n",
      " [8214  235    1 ...    0    0    0]\n",
      " [8214   21 4670 ...    0    0    0]\n",
      " ...\n",
      " [8214  116    7 ...    0    0    0]\n",
      " [8214   64 5531 ...    0    0    0]\n",
      " [8214 1985 3901 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   12  466 ...    0    0    0]\n",
      " [8087  525    1 ...    0    0    0]\n",
      " [8087    4   37 ...    0    0    0]\n",
      " ...\n",
      " [8087   83   12 ...    0    0    0]\n",
      " [8087   14  279 ...    0    0    0]\n",
      " [8087 7449 2701 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214    3   69 ...    0    0    0]\n",
      " [8214    3  313 ...    0    0    0]\n",
      " [8214  235   28 ...    0    0    0]\n",
      " ...\n",
      " [8214   53    7 ...    0    0    0]\n",
      " [8214  119    1 ...    0    0    0]\n",
      " [8214   77   52 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   32  646 ...    0    0    0]\n",
      " [8087  338   13 ...    0    0    0]\n",
      " [8087  525   52 ...    0    0    0]\n",
      " ...\n",
      " [8087   10   20 ...    0    0    0]\n",
      " [8087   18   12 ...    0    0    0]\n",
      " [8087   18   40 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  510 1926 ...    0    0    0]\n",
      " [8214  149 1291 ...    0    0    0]\n",
      " [8214  149   78 ...    0    0    0]\n",
      " ...\n",
      " [8214   46   84 ...    0    0    0]\n",
      " [8214    3  209 ...    0    0    0]\n",
      " [8214    8 2421 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087    7 1058 ...    0    0    0]\n",
      " [8087  478   13 ...    0    0    0]\n",
      " [8087   62 2677 ... 7946   75 8088]\n",
      " ...\n",
      " [8087   79    8 ...    0    0    0]\n",
      " [8087  140   57 ...    0    0    0]\n",
      " [8087    4   19 ...    0    0    0]], shape=(64, 37), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   41 4328 ...    2 8215    0]\n",
      " [8214   66 2245 ...    0    0    0]\n",
      " [8214   32    7 ...    0    0    0]\n",
      " ...\n",
      " [8214   33 2813 ...    0    0    0]\n",
      " [8214  119    1 ...    0    0    0]\n",
      " [8214   41 2849 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087 4152   69 ...    2 8088    0]\n",
      " [8087   25  267 ...    0    0    0]\n",
      " [8087   62    3 ...    0    0    0]\n",
      " ...\n",
      " [8087   68  232 ...    0    0    0]\n",
      " [8087    4  169 ...    0    0    0]\n",
      " [8087  983  217 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   42   34 ...    0    0    0]\n",
      " [8214   13    8 ...    0    0    0]\n",
      " [8214  192   31 ...    0    0    0]\n",
      " ...\n",
      " [8214    6   64 ...    0    0    0]\n",
      " [8214    9  733 ...    0    0    0]\n",
      " [8214  136 2432 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   16   13 ...    0    0    0]\n",
      " [8087   40    8 ...    0    0    0]\n",
      " [8087   18   14 ...    0    0    0]\n",
      " ...\n",
      " [8087    4   14 ...    0    0    0]\n",
      " [8087  227    8 ...    0    0    0]\n",
      " [8087  110 6780 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214    3  252 ...    0    0    0]\n",
      " [8214  174  101 ...    0    0    0]\n",
      " [8214    6 1701 ...    0    0    0]\n",
      " ...\n",
      " [8214  265 1540 ...    0    0    0]\n",
      " [8214    3  252 ...    0    0    0]\n",
      " [8214   16 3378 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087    3  124 ...    0    0    0]\n",
      " [8087 1604    1 ...    0    0    0]\n",
      " [8087    4   17 ...    0    0    0]\n",
      " ...\n",
      " [8087   25  282 ...    0    0    0]\n",
      " [8087    3  124 ...    0    0    0]\n",
      " [8087 6216    3 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  306  896 ...    0    0    0]\n",
      " [8214 7263 7990 ...    0    0    0]\n",
      " [8214    6   61 ...    0    0    0]\n",
      " ...\n",
      " [8214   42   49 ...    0    0    0]\n",
      " [8214   11  504 ...    0    0    0]\n",
      " [8214    7  785 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   17   71 ...    0    0    0]\n",
      " [8087   25  293 ...    0    0    0]\n",
      " [8087    4  243 ...    0    0    0]\n",
      " ...\n",
      " [8087   16   13 ...    0    0    0]\n",
      " [8087   18 1147 ...    0    0    0]\n",
      " [8087    3  720 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   89   41 ...    0    0    0]\n",
      " [8214   18  827 ...    0    0    0]\n",
      " [8214    7 6069 ...    0    0    0]\n",
      " ...\n",
      " [8214   67   92 ...    0    0    0]\n",
      " [8214    7   73 ...    0    0    0]\n",
      " [8214 3031    9 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087    4   96 ...    0    0    0]\n",
      " [8087  868   22 ...    0    0    0]\n",
      " [8087    3 3784 ...    0    0    0]\n",
      " ...\n",
      " [8087   94  119 ...    0    0    0]\n",
      " [8087   32 1327 ...    0    0    0]\n",
      " [8087   12  761 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214    3  249 ...    0    0    0]\n",
      " [8214   23   52 ...    0    0    0]\n",
      " [8214   35  511 ...    0    0    0]\n",
      " ...\n",
      " [8214   12  162 ...    0    0    0]\n",
      " [8214  381    5 ...    0    0    0]\n",
      " [8214 2424 2808 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   20    3 ...    0    0    0]\n",
      " [8087   18   12 ...    0    0    0]\n",
      " [8087   50   30 ...    0    0    0]\n",
      " ...\n",
      " [8087  176  126 ...    0    0    0]\n",
      " [8087   25  397 ...    0    0    0]\n",
      " [8087  409  564 ...    0    0    0]], shape=(64, 37), dtype=int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:  tf.Tensor(\n",
      "[[8214  229    4 ...    0    0    0]\n",
      " [8214   13  324 ...    0    0    0]\n",
      " [8214  195    1 ...    0    0    0]\n",
      " ...\n",
      " [8214   46  174 ...    0    0    0]\n",
      " [8214  194    1 ...    0    0    0]\n",
      " [8214   12  954 ...    0    0    0]], shape=(64, 35), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   14   90 ...    0    0    0]\n",
      " [8087   64   20 ...    0    0    0]\n",
      " [8087  803    1 ...    0    0    0]\n",
      " ...\n",
      " [8087   18   64 ...    0    0    0]\n",
      " [8087   18 6985 ...    0    0    0]\n",
      " [8087    3 1040 ...    0    0    0]], shape=(64, 33), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   67   92 ... 6703    2 8215]\n",
      " [8214 1830    5 ...    0    0    0]\n",
      " [8214   47   66 ...    0    0    0]\n",
      " ...\n",
      " [8214  282  210 ...    0    0    0]\n",
      " [8214   54   56 ...    0    0    0]\n",
      " [8214    6   14 ...    2 8215    0]], shape=(64, 38), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   94  119 ...    2 8088    0]\n",
      " [8087   25   70 ...    0    0    0]\n",
      " [8087   66   25 ...    0    0    0]\n",
      " ...\n",
      " [8087   12  333 ...    0    0    0]\n",
      " [8087   10   20 ...    0    0    0]\n",
      " [8087    4   11 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   47 2245 ...    0    0    0]\n",
      " [8214  138  176 ...    0    0    0]\n",
      " [8214   13  852 ...    0    0    0]\n",
      " ...\n",
      " [8214  347    1 ...    0    0    0]\n",
      " [8214  324 6898 ...    0    0    0]\n",
      " [8214   25  128 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   19    8 ...    0    0    0]\n",
      " [8087   58  126 ...    0    0    0]\n",
      " [8087   12   30 ...    0    0    0]\n",
      " ...\n",
      " [8087  359    1 ...    0    0    0]\n",
      " [8087   64   20 ...    0    0    0]\n",
      " [8087   23   58 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   23  239 ...    0    0    0]\n",
      " [8214    6   59 ...    0    0    0]\n",
      " [8214 2306    3 ... 8079    2 8215]\n",
      " ...\n",
      " [8214  130   46 ...    0    0    0]\n",
      " [8214   14 3416 ...    0    0    0]\n",
      " [8214  303 5268 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   11  109 ...    0    0    0]\n",
      " [8087    4   16 ...    0    0    0]\n",
      " [8087 7000    4 ...    0    0    0]\n",
      " ...\n",
      " [8087   89    6 ...    0    0    0]\n",
      " [8087   11 5113 ...    0    0    0]\n",
      " [8087   17 5264 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214 1128 8003 ...    0    0    0]\n",
      " [8214 1673   24 ...    0    0    0]\n",
      " [8214   25   84 ...    0    0    0]\n",
      " ...\n",
      " [8214  149   16 ...    0    0    0]\n",
      " [8214    6   77 ...    0    0    0]\n",
      " [8214  229    4 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   15   24 ...    0    0    0]\n",
      " [8087   14  271 ...    0    0    0]\n",
      " [8087   23  145 ...    0    0    0]\n",
      " ...\n",
      " [8087   44 1879 ...    0    0    0]\n",
      " [8087    4   18 ...    0    0    0]\n",
      " [8087   14  132 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  843    2 ...    0    0    0]\n",
      " [8214   42    8 ...    0    0    0]\n",
      " [8214    3   98 ...    0    0    0]\n",
      " ...\n",
      " [8214   60   42 ...    0    0    0]\n",
      " [8214   32  265 ...    0    0    0]\n",
      " [8214   21    8 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   39    6 ...    0    0    0]\n",
      " [8087   19    8 ...    0    0    0]\n",
      " [8087   43 6174 ...    0    0    0]\n",
      " ...\n",
      " [8087    4   64 ...    0    0    0]\n",
      " [8087   25  191 ...    0    0    0]\n",
      " [8087   61   71 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   13 1140 ...    0    0    0]\n",
      " [8214   23  251 ...    0    0    0]\n",
      " [8214    8 2640 ...    0    0    0]\n",
      " ...\n",
      " [8214  166    3 ...    0    0    0]\n",
      " [8214   27  227 ...    0    0    0]\n",
      " [8214   23   88 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   65  210 ...    0    0    0]\n",
      " [8087   18   21 ...    0    0    0]\n",
      " [8087   17   13 ...    0    0    0]\n",
      " ...\n",
      " [8087   39   43 ...    0    0    0]\n",
      " [8087    4   12 ...    0    0    0]\n",
      " [8087   18 4943 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  194    1 ...    0    0    0]\n",
      " [8214  179    3 ...    0    0    0]\n",
      " [8214  185    8 ...    0    0    0]\n",
      " ...\n",
      " [8214    6   52 ...    0    0    0]\n",
      " [8214    8    3 ...    0    0    0]\n",
      " [8214 1029 6759 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   18   27 ...    0    0    0]\n",
      " [8087    4 7936 ...    0    0    0]\n",
      " [8087   40    8 ...    0    0    0]\n",
      " ...\n",
      " [8087    4   40 ...    0    0    0]\n",
      " [8087   19    8 ...    0    0    0]\n",
      " [8087   14 3375 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214    6   16 ...    0    0    0]\n",
      " [8214  200   39 ...    0    0    0]\n",
      " [8214   14  823 ...    0    0    0]\n",
      " ...\n",
      " [8214   74   79 ...    0    0    0]\n",
      " [8214   25  155 ...    0    0    0]\n",
      " [8214    7  122 ...    0    0    0]], shape=(64, 37), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087    4   44 ...    0    0    0]\n",
      " [8087  768   56 ...    0    0    0]\n",
      " [8087   27 4725 ...    0    0    0]\n",
      " ...\n",
      " [8087   51    8 ...    0    0    0]\n",
      " [8087   23  111 ...    0    0    0]\n",
      " [8087   43  124 ...    0    0    0]], shape=(64, 36), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  426 1044 ...    0    0    0]\n",
      " [8214   87   80 ...    0    0    0]\n",
      " [8214   13   87 ...    0    0    0]\n",
      " ...\n",
      " [8214   10  133 ...    0    0    0]\n",
      " [8214  299 7647 ...    0    0    0]\n",
      " [8214    7 1083 ...    0    0    0]], shape=(64, 35), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087  110   47 ...    0    0    0]\n",
      " [8087   25   24 ...    0    0    0]\n",
      " [8087   25   30 ...    0    0    0]\n",
      " ...\n",
      " [8087   47 2161 ...    0    0    0]\n",
      " [8087  190    8 ...    0    0    0]\n",
      " [8087  832    5 ...    0    0    0]], shape=(64, 34), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  638 1438 ...    0    0    0]\n",
      " [8214    3 1379 ...    0    0    0]\n",
      " [8214  155    1 ...    0    0    0]\n",
      " ...\n",
      " [8214   32    6 ...  351  362 8215]\n",
      " [8214    3 3084 ... 8215    0    0]\n",
      " [8214   25   10 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087  126 1058 ...    0    0    0]\n",
      " [8087    3 6696 ...    0    0    0]\n",
      " [8087  111    1 ...    0    0    0]\n",
      " ...\n",
      " [8087   62    4 ...  297 8088    0]\n",
      " [8087 2227  439 ...    0    0    0]\n",
      " [8087   23   45 ...    0    0    0]], shape=(64, 36), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  164    9 ...    0    0    0]\n",
      " [8214    7 2673 ...    0    0    0]\n",
      " [8214   25   48 ...    0    0    0]\n",
      " ...\n",
      " [8214 1060  211 ...    0    0    0]\n",
      " [8214    7    5 ...    0    0    0]\n",
      " [8214  212    2 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   12  141 ...    0    0    0]\n",
      " [8087    3 5749 ...    0    0    0]\n",
      " [8087   23    5 ...    0    0    0]\n",
      " ...\n",
      " [8087  142  840 ...    0    0    0]\n",
      " [8087   29   20 ...    0    0    0]\n",
      " [8087  153   51 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214    3  438 ...    0    0    0]\n",
      " [8214   25    5 ...    0    0    0]\n",
      " [8214   53 1167 ...    0    0    0]\n",
      " ...\n",
      " [8214   23   52 ...    0    0    0]\n",
      " [8214    6 3615 ...    0    0    0]\n",
      " [8214  125    4 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   18    3 ...    0    0    0]\n",
      " [8087   23    5 ...    0    0    0]\n",
      " [8087   40    8 ...    0    0    0]\n",
      " ...\n",
      " [8087   18   14 ...    0    0    0]\n",
      " [8087    4   12 ...    0    0    0]\n",
      " [8087   15  120 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  445  452 ... 8215    0    0]\n",
      " [8214   13  324 ...    0    0    0]\n",
      " [8214   63   12 ...    0    0    0]\n",
      " ...\n",
      " [8214   52    8 ...    0    0    0]\n",
      " [8214   52 1582 ...    0    0    0]\n",
      " [8214 3828 7990 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   12   88 ...   51    2 8088]\n",
      " [8087   64   72 ...    0    0    0]\n",
      " [8087  146  852 ...    0    0    0]\n",
      " ...\n",
      " [8087    4   40 ...    0    0    0]\n",
      " [8087  111    1 ...    0    0    0]\n",
      " [8087 4913 3366 ...    0    0    0]], shape=(64, 37), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214    7    5 ...    0    0    0]\n",
      " [8214 5687 7990 ...    0    0    0]\n",
      " [8214   21   74 ...    0    0    0]\n",
      " ...\n",
      " [8214    8    9 ...    0    0    0]\n",
      " [8214   23   52 ...    0    0    0]\n",
      " [8214    6  105 ...    0    0    0]], shape=(64, 36), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   29    3 ...    0    0    0]\n",
      " [8087   17  672 ...    0    0    0]\n",
      " [8087   18   15 ...    0    0    0]\n",
      " ...\n",
      " [8087  243    8 ...    0    0    0]\n",
      " [8087   18   19 ...    0    0    0]\n",
      " [8087    4   54 ...    0    0    0]], shape=(64, 38), dtype=int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:  tf.Tensor(\n",
      "[[8214 5893   57 ...    0    0    0]\n",
      " [8214    7    5 ...    0    0    0]\n",
      " [8214    8  347 ...    0    0    0]\n",
      " ...\n",
      " [8214   25 3217 ...    0    0    0]\n",
      " [8214   25  140 ...    0    0    0]\n",
      " [8214   64   76 ...    0    0    0]], shape=(64, 35), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087 2405   77 ...    0    0    0]\n",
      " [8087  196    8 ...    0    0    0]\n",
      " [8087    6  278 ...    0    0    0]\n",
      " ...\n",
      " [8087   23   12 ...    0    0    0]\n",
      " [8087   23   11 ...    0    0    0]\n",
      " [8087   14   31 ...    0    0    0]], shape=(64, 37), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214 2685   79 ...    0    0    0]\n",
      " [8214   54    8 ...    0    0    0]\n",
      " [8214  191 5079 ...    0    0    0]\n",
      " ...\n",
      " [8214  568 8003 ...    0    0    0]\n",
      " [8214   42 1965 ...    0    0    0]\n",
      " [8214    5 1092 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   14   97 ...    0    0    0]\n",
      " [8087   16   13 ...    0    0    0]\n",
      " [8087   18  255 ...    0    0    0]\n",
      " ...\n",
      " [8087   14   22 ...    0    0    0]\n",
      " [8087   16  195 ...    0    0    0]\n",
      " [8087   29 1472 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214 2171  449 ...    0    0    0]\n",
      " [8214   40   27 ...    0    0    0]\n",
      " [8214   34  343 ...    0    0    0]\n",
      " ...\n",
      " [8214   52   82 ...    0    0    0]\n",
      " [8214    3  627 ...    0    0    0]\n",
      " [8214    8    9 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087 7329 7863 ...    0    0    0]\n",
      " [8087   59   12 ...    0    0    0]\n",
      " [8087   25   22 ...    0    0    0]\n",
      " ...\n",
      " [8087   10   31 ...    0    0    0]\n",
      " [8087    3  617 ...    0    0    0]\n",
      " [8087   19    8 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   32   25 ...    0    0    0]\n",
      " [8214    6 5688 ...    0    0    0]\n",
      " [8214  372    2 ...    0    0    0]\n",
      " ...\n",
      " [8214  123    7 ...    0    0    0]\n",
      " [8214 1712   39 ...    0    0    0]\n",
      " [8214   27   56 ...    0    0    0]], shape=(64, 36), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   23   12 ...    0    0    0]\n",
      " [8087   96 7936 ...    0    0    0]\n",
      " [8087  153   51 ...    0    0    0]\n",
      " ...\n",
      " [8087  308   10 ...    0    0    0]\n",
      " [8087 2734   56 ...    0    0    0]\n",
      " [8087   12   20 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  892    4 ...    0    0    0]\n",
      " [8214   18  322 ...    0    0    0]\n",
      " [8214   27  629 ...    0    0    0]\n",
      " ...\n",
      " [8214  188  488 ...    0    0    0]\n",
      " [8214   77   41 ...    0    0    0]\n",
      " [8214   12  266 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   17  672 ...    0    0    0]\n",
      " [8087    4  423 ...    0    0    0]\n",
      " [8087   18   12 ...    0    0    0]\n",
      " ...\n",
      " [8087   19    8 ...    0    0    0]\n",
      " [8087   18   37 ...    0    0    0]\n",
      " [8087   32  791 ...    0    0    0]], shape=(64, 35), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  405 5757 ...    0    0    0]\n",
      " [8214   46  460 ...    0    0    0]\n",
      " [8214   25   16 ...    0    0    0]\n",
      " ...\n",
      " [8214  188   16 ...    0    0    0]\n",
      " [8214  116   16 ...    0    0    0]\n",
      " [8214  167  224 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   14  102 ...    0    0    0]\n",
      " [8087  341  148 ...    0    0    0]\n",
      " [8087   23   44 ...    0    0    0]\n",
      " ...\n",
      " [8087   15  207 ...    0    0    0]\n",
      " [8087   83   44 ... 8088    0    0]\n",
      " [8087   12  542 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   47    8 ...    0    0    0]\n",
      " [8214  194    1 ...    0    0    0]\n",
      " [8214 3772 1575 ...    0    0    0]\n",
      " ...\n",
      " [8214  194    1 ...    0    0    0]\n",
      " [8214  113 2213 ...    0    0    0]\n",
      " [8214   12 7004 ... 8215    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087  154   30 ...    0    0    0]\n",
      " [8087  169    1 ...    0    0    0]\n",
      " [8087 1977   12 ...    0    0    0]\n",
      " ...\n",
      " [8087   18 2403 ...    0    0    0]\n",
      " [8087   63    8 ...    0    0    0]\n",
      " [8087 2223 6765 ...    0    0    0]], shape=(64, 38), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   14  260 ...    0    0    0]\n",
      " [8214  290    1 ...    2 8215    0]\n",
      " [8214    8   21 ...    0    0    0]\n",
      " ...\n",
      " [8214  282  210 ...    0    0    0]\n",
      " [8214   67  107 ...    0    0    0]\n",
      " [8214   27   13 ... 8215    0    0]], shape=(64, 37), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   11  121 ...    0    0    0]\n",
      " [8087  331    1 ...    0    0    0]\n",
      " [8087   19    8 ...    0    0    0]\n",
      " ...\n",
      " [8087   12  333 ...    0    0    0]\n",
      " [8087   94  136 ...    0    0    0]\n",
      " [8087   12   30 ...   51 8088    0]], shape=(64, 40), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   10   43 ...    0    0    0]\n",
      " [8214    3  222 ...    0    0    0]\n",
      " [8214 1209    1 ... 8215    0    0]\n",
      " ...\n",
      " [8214  610 8003 ...    0    0    0]\n",
      " [8214   16 2461 ...    0    0    0]\n",
      " [8214   23    5 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   18   47 ...    0    0    0]\n",
      " [8087    3  332 ...    0    0    0]\n",
      " [8087  888    1 ...    0    0    0]\n",
      " ...\n",
      " [8087  229   81 ...    0    0    0]\n",
      " [8087   44   12 ...    0    0    0]\n",
      " [8087   18  154 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214  556 2094 ...    0    0    0]\n",
      " [8214   32    6 ...   88  238 8215]\n",
      " [8214  138   34 ...    0    0    0]\n",
      " ...\n",
      " [8214   71    3 ...    0    0    0]\n",
      " [8214  192  644 ...    0    0    0]\n",
      " [8214   14  208 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   14 5252 ...    0    0    0]\n",
      " [8087    4 7936 ...    2 8088    0]\n",
      " [8087   58   22 ...    0    0    0]\n",
      " ...\n",
      " [8087  105    8 ...    0    0    0]\n",
      " [8087   18   14 ...    0    0    0]\n",
      " [8087   11  206 ...    0    0    0]], shape=(64, 37), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   25  137 ...    0    0    0]\n",
      " [8214   67  107 ...    0    0    0]\n",
      " [8214    6    7 ...    0    0    0]\n",
      " ...\n",
      " [8214   32    7 ...    0    0    0]\n",
      " [8214    7    5 ...    0    0    0]\n",
      " [8214   24 7923 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   23   19 ...    0    0    0]\n",
      " [8087   94  136 ...    0    0    0]\n",
      " [8087    4  196 ...    0    0    0]\n",
      " ...\n",
      " [8087   62    3 ...    0    0    0]\n",
      " [8087   29   16 ...    0    0    0]\n",
      " [8087   11 5475 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "inp:  tf.Tensor(\n",
      "[[8214   42    8 ...    0    0    0]\n",
      " [8214   32 7095 ...    0    0    0]\n",
      " [8214    7    5 ...    0    0    0]\n",
      " ...\n",
      " [8214  149   25 ...    0    0    0]\n",
      " [8214 1155  200 ...    0    0    0]\n",
      " [8214    6    8 ...    0    0    0]], shape=(64, 37), dtype=int64)\n",
      "tar:  tf.Tensor(\n",
      "[[8087   16   13 ...    0    0    0]\n",
      " [8087   62 3257 ...    0    0    0]\n",
      " [8087   29   13 ...    0    0    0]\n",
      " ...\n",
      " [8087   62   23 ...    0    0    0]\n",
      " [8087   25 1095 ...    0    0    0]\n",
      " [8087    4   16 ...    0    0    0]], shape=(64, 38), dtype=int64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-9266a508c9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inp: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tar: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        print('inp: ', inp)\n",
    "        print('tar: ', tar)\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "    # inp sentence is portuguese, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este é um problema que temos que resolver.\n",
      "Predicted translation: \n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every v1.Session.run call should be replaced by a Python function.\n",
    "# The feed_dict and v1.placeholders become function arguments.\n",
    "# The fetches become the function's return value.\n",
    "# After that add a tf.function decorator to make it run efficiently in graph. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
