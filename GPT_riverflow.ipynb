{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import river_model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from inference import infer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_numeric = pd.read_csv('/Users/omernivron/Downloads/Data_riverflow/att_numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_numeric.iloc[:, 1:] = (attributes_numeric.iloc[:, 1:] - np.mean(attributes_numeric.iloc[:, 1:], axis = 0)) / np.std(attributes_numeric.iloc[:, 1:], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pp = np.load('/Users/omernivron/Downloads/river_processed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pe = np.load('/Users/omernivron/Downloads/river_processed_te.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_river_mha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, train_loss, m_tr, token_pos, time_pos, time_pos2, pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_log_sig = decoder(token_pos, time_pos, time_pos2, pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_log_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_log_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, test_loss, m_te, token_pos_te, time_pos_te, time_pos2_te, pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_log_sig_te = decoder(token_pos_te, time_pos_te, time_pos2_te, pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_log_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_log_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_tr = arr_pp[1::5]; t2_tr = arr_pp[2::5]\n",
    "tar_tr = arr_pp[0::5];\n",
    "token_tr = arr_pp[3::5]; basin_l_tr = arr_pp[4::5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_te = arr_pe[1::5]; t2_te = arr_pe[2::5]\n",
    "tar_te = arr_pe[0::5];\n",
    "token_te = arr_pe[3::5]; basin_l_te = arr_pe[4::5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Restored from /Users/omernivron/Downloads/GPT_river_mha/ckpt/check_0/ckpt-219\n",
      "Epoch 0 batch 0 train Loss 1.7003 test Loss 1.5920 with MSE metric 2.9140\n",
      "Epoch 0 batch 100 train Loss 1.9576 test Loss 1.5821 with MSE metric 3.0140\n",
      "Time taken for 1 epoch: 68.68693995475769 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 1.7496 test Loss 2.0357 with MSE metric 10.5334\n",
      "Epoch 1 batch 100 train Loss 1.7829 test Loss 1.7077 with MSE metric 5.3546\n",
      "Time taken for 1 epoch: 65.68280506134033 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 2.0039 test Loss 1.6149 with MSE metric 3.1979\n",
      "Epoch 2 batch 100 train Loss 1.9334 test Loss 1.8944 with MSE metric 8.1001\n",
      "Time taken for 1 epoch: 66.18736481666565 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 1.8210 test Loss 1.6747 with MSE metric 3.7088\n",
      "Epoch 3 batch 100 train Loss 1.7148 test Loss 1.5868 with MSE metric 3.3897\n",
      "Time taken for 1 epoch: 66.06295394897461 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 1.9319 test Loss 1.5569 with MSE metric 1.9793\n",
      "Epoch 4 batch 100 train Loss 1.9670 test Loss 1.7996 with MSE metric 6.6490\n",
      "Time taken for 1 epoch: 65.82426381111145 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 1.7200 test Loss 1.6190 with MSE metric 4.0742\n",
      "Epoch 5 batch 100 train Loss 1.6576 test Loss 1.5991 with MSE metric 2.2927\n",
      "Time taken for 1 epoch: 65.79992079734802 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 1.6361 test Loss 1.6329 with MSE metric 3.7090\n",
      "Epoch 6 batch 100 train Loss 1.6030 test Loss 1.5883 with MSE metric 3.5186\n",
      "Time taken for 1 epoch: 65.54132413864136 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 1.6525 test Loss 1.5200 with MSE metric 2.6046\n",
      "Epoch 7 batch 100 train Loss 1.8296 test Loss 1.4843 with MSE metric 1.8925\n",
      "Time taken for 1 epoch: 65.62896013259888 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 2.1127 test Loss 1.5863 with MSE metric 3.0225\n",
      "Epoch 8 batch 100 train Loss 1.9334 test Loss 1.7789 with MSE metric 6.2700\n",
      "Time taken for 1 epoch: 65.28608775138855 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 1.9522 test Loss 1.5728 with MSE metric 2.8668\n",
      "Epoch 9 batch 100 train Loss 2.0860 test Loss 1.7350 with MSE metric 5.4172\n",
      "Time taken for 1 epoch: 66.03665089607239 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 1.7597 test Loss 1.5641 with MSE metric 3.2838\n",
      "Epoch 10 batch 100 train Loss 2.1627 test Loss 1.6849 with MSE metric 4.9138\n",
      "Time taken for 1 epoch: 65.47571516036987 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 1.6228 test Loss 2.1696 with MSE metric 12.5096\n",
      "Epoch 11 batch 100 train Loss 1.6982 test Loss 1.8293 with MSE metric 7.0249\n",
      "Time taken for 1 epoch: 65.81098818778992 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 2.1369 test Loss 1.5429 with MSE metric 3.1252\n",
      "Epoch 12 batch 100 train Loss 1.7633 test Loss 1.5283 with MSE metric 1.9791\n",
      "Time taken for 1 epoch: 64.92729783058167 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 1.8613 test Loss 1.8878 with MSE metric 7.9835\n",
      "Epoch 13 batch 100 train Loss 1.6833 test Loss 1.8499 with MSE metric 7.4206\n",
      "Time taken for 1 epoch: 64.93321204185486 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 1.7241 test Loss 1.5139 with MSE metric 2.0274\n",
      "Epoch 14 batch 100 train Loss 1.8921 test Loss 1.7011 with MSE metric 4.9616\n",
      "Time taken for 1 epoch: 64.57962393760681 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 1.7649 test Loss 1.4855 with MSE metric 1.9486\n",
      "Epoch 15 batch 100 train Loss 1.6775 test Loss 1.6217 with MSE metric 3.8076\n",
      "Time taken for 1 epoch: 64.4631359577179 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 1.7215 test Loss 2.0545 with MSE metric 10.5316\n",
      "Epoch 16 batch 100 train Loss 1.6264 test Loss 1.5910 with MSE metric 3.3104\n",
      "Time taken for 1 epoch: 64.53578591346741 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 1.9071 test Loss 1.6603 with MSE metric 4.7869\n",
      "Epoch 17 batch 100 train Loss 1.7367 test Loss 1.5109 with MSE metric 2.0486\n",
      "Time taken for 1 epoch: 69.51687693595886 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 1.9890 test Loss 1.8217 with MSE metric 6.9022\n",
      "Epoch 18 batch 100 train Loss 2.0445 test Loss 1.8221 with MSE metric 6.9248\n",
      "Time taken for 1 epoch: 73.6934380531311 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 1.7116 test Loss 1.9959 with MSE metric 9.9178\n",
      "Epoch 19 batch 100 train Loss 1.7478 test Loss 1.5270 with MSE metric 2.2579\n",
      "Time taken for 1 epoch: 75.56167340278625 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 2.2233 test Loss 1.6416 with MSE metric 4.2349\n",
      "Epoch 20 batch 100 train Loss 1.7596 test Loss 2.1526 with MSE metric 11.6853\n",
      "Time taken for 1 epoch: 71.95159006118774 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 1.7100 test Loss 1.5294 with MSE metric 1.8340\n",
      "Epoch 21 batch 100 train Loss 1.7032 test Loss 1.6616 with MSE metric 4.4632\n",
      "Time taken for 1 epoch: 71.25164318084717 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 1.9152 test Loss 1.6761 with MSE metric 4.4086\n",
      "Epoch 22 batch 100 train Loss 1.7275 test Loss 1.6613 with MSE metric 4.4631\n",
      "Time taken for 1 epoch: 71.58017325401306 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 1.8735 test Loss 1.5267 with MSE metric 2.0683\n",
      "Epoch 23 batch 100 train Loss 1.9935 test Loss 1.5013 with MSE metric 1.7441\n",
      "Time taken for 1 epoch: 73.23346304893494 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 1.9595 test Loss 1.5039 with MSE metric 2.0397\n",
      "Epoch 24 batch 100 train Loss 1.7623 test Loss 1.6087 with MSE metric 3.3845\n",
      "Time taken for 1 epoch: 64.61010503768921 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 2.0662 test Loss 1.5200 with MSE metric 2.5272\n",
      "Epoch 25 batch 100 train Loss 1.8278 test Loss 1.5543 with MSE metric 2.3744\n",
      "Time taken for 1 epoch: 64.89545321464539 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 1.7909 test Loss 1.6465 with MSE metric 4.2005\n",
      "Epoch 26 batch 100 train Loss 1.7122 test Loss 1.4824 with MSE metric 1.4830\n",
      "Time taken for 1 epoch: 65.82886576652527 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 2.3392 test Loss 1.6046 with MSE metric 3.5530\n",
      "Epoch 27 batch 100 train Loss 1.8834 test Loss 1.6673 with MSE metric 4.2194\n",
      "Time taken for 1 epoch: 65.9259672164917 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 1.7769 test Loss 1.5529 with MSE metric 2.4946\n",
      "Epoch 28 batch 100 train Loss 1.7145 test Loss 1.9768 with MSE metric 9.4832\n",
      "Time taken for 1 epoch: 65.83308410644531 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 1.9354 test Loss 1.5705 with MSE metric 3.0861\n",
      "Epoch 29 batch 100 train Loss 1.6987 test Loss 1.8126 with MSE metric 6.8394\n",
      "Time taken for 1 epoch: 65.97654581069946 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 1.7864 test Loss 2.0870 with MSE metric 11.1028\n",
      "Epoch 30 batch 100 train Loss 1.7890 test Loss 1.5616 with MSE metric 2.0430\n",
      "Time taken for 1 epoch: 65.72694277763367 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 1.7243 test Loss 1.5954 with MSE metric 3.2975\n",
      "Epoch 31 batch 100 train Loss 1.6596 test Loss 1.8603 with MSE metric 7.5766\n",
      "Time taken for 1 epoch: 66.05436205863953 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 1.8006 test Loss 1.6423 with MSE metric 4.1280\n",
      "Epoch 32 batch 100 train Loss 1.6489 test Loss 1.6181 with MSE metric 4.0058\n",
      "Time taken for 1 epoch: 65.50306296348572 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 2.1395 test Loss 1.5604 with MSE metric 2.8442\n",
      "Epoch 33 batch 100 train Loss 1.7414 test Loss 1.5075 with MSE metric 2.3251\n",
      "Time taken for 1 epoch: 64.90228295326233 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 1.8981 test Loss 2.1671 with MSE metric 12.4768\n",
      "Epoch 34 batch 100 train Loss 2.3991 test Loss 1.5318 with MSE metric 2.1663\n",
      "Time taken for 1 epoch: 64.94391322135925 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 1.7773 test Loss 1.5777 with MSE metric 3.2519\n",
      "Epoch 35 batch 100 train Loss 1.6697 test Loss 1.8681 with MSE metric 7.5978\n",
      "Time taken for 1 epoch: 69.86402773857117 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 1.6492 test Loss 1.5122 with MSE metric 2.1496\n",
      "Epoch 36 batch 100 train Loss 2.0323 test Loss 1.6521 with MSE metric 4.7142\n",
      "Time taken for 1 epoch: 82.9497537612915 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 2.1344 test Loss 1.6500 with MSE metric 4.1300\n",
      "Epoch 37 batch 100 train Loss 1.6460 test Loss 1.5188 with MSE metric 2.2341\n",
      "Time taken for 1 epoch: 71.10777688026428 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 1.7616 test Loss 1.6289 with MSE metric 3.8840\n",
      "Epoch 38 batch 100 train Loss 1.8308 test Loss 1.7138 with MSE metric 5.0872\n",
      "Time taken for 1 epoch: 75.41159510612488 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 2.0372 test Loss 1.4848 with MSE metric 1.5536\n",
      "Epoch 39 batch 100 train Loss 1.6123 test Loss 1.5379 with MSE metric 2.1418\n",
      "Time taken for 1 epoch: 71.66312503814697 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 1.7926 test Loss 1.4577 with MSE metric 1.5791\n",
      "Epoch 40 batch 100 train Loss 1.8229 test Loss 1.5075 with MSE metric 2.5039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 67.43184781074524 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 1.7189 test Loss 2.1640 with MSE metric 12.2645\n",
      "Epoch 41 batch 100 train Loss 1.8803 test Loss 1.6014 with MSE metric 3.5047\n",
      "Time taken for 1 epoch: 64.68967700004578 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 2.2002 test Loss 1.6405 with MSE metric 3.8881\n",
      "Epoch 42 batch 100 train Loss 1.9717 test Loss 1.4749 with MSE metric 1.7314\n",
      "Time taken for 1 epoch: 65.55017495155334 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 1.7075 test Loss 1.6975 with MSE metric 4.8254\n",
      "Epoch 43 batch 100 train Loss 1.8016 test Loss 1.7483 with MSE metric 5.6994\n",
      "Time taken for 1 epoch: 66.66386198997498 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 2.0605 test Loss 1.5582 with MSE metric 2.5423\n",
      "Epoch 44 batch 100 train Loss 1.6530 test Loss 1.7476 with MSE metric 5.9125\n",
      "Time taken for 1 epoch: 66.63962292671204 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 1.8703 test Loss 1.5001 with MSE metric 1.7336\n",
      "Epoch 45 batch 100 train Loss 1.9103 test Loss 1.5535 with MSE metric 2.5684\n",
      "Time taken for 1 epoch: 66.72168016433716 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 1.7595 test Loss 1.4822 with MSE metric 2.5067\n",
      "Epoch 46 batch 100 train Loss 2.1492 test Loss 1.7853 with MSE metric 6.4736\n",
      "Time taken for 1 epoch: 66.89830493927002 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 1.7489 test Loss 2.1613 with MSE metric 11.8122\n",
      "Epoch 47 batch 100 train Loss 1.7139 test Loss 1.5114 with MSE metric 2.7151\n",
      "Time taken for 1 epoch: 66.65522289276123 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 1.8140 test Loss 1.9594 with MSE metric 9.2449\n",
      "Epoch 48 batch 100 train Loss 1.9946 test Loss 1.5612 with MSE metric 2.5058\n",
      "Time taken for 1 epoch: 65.37581896781921 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 1.7184 test Loss 1.5381 with MSE metric 2.3956\n",
      "Epoch 49 batch 100 train Loss 1.9726 test Loss 1.5716 with MSE metric 2.5784\n",
      "Time taken for 1 epoch: 65.19872522354126 secs\n",
      "\n",
      "Epoch 50 batch 0 train Loss 1.8641 test Loss 1.9492 with MSE metric 8.7814\n",
      "Epoch 50 batch 100 train Loss 1.9278 test Loss 1.6357 with MSE metric 4.0014\n",
      "Time taken for 1 epoch: 67.20731282234192 secs\n",
      "\n",
      "Epoch 51 batch 0 train Loss 1.7449 test Loss 1.5345 with MSE metric 2.3567\n",
      "Epoch 51 batch 100 train Loss 1.8162 test Loss 1.4612 with MSE metric 2.0164\n",
      "Time taken for 1 epoch: 67.3935067653656 secs\n",
      "\n",
      "Epoch 52 batch 0 train Loss 1.8330 test Loss 1.5516 with MSE metric 2.7388\n",
      "Epoch 52 batch 100 train Loss 2.3029 test Loss 1.5821 with MSE metric 3.1671\n",
      "Time taken for 1 epoch: 64.94600582122803 secs\n",
      "\n",
      "Epoch 53 batch 0 train Loss 1.8216 test Loss 1.6864 with MSE metric 3.9763\n",
      "Epoch 53 batch 100 train Loss 1.7127 test Loss 1.5061 with MSE metric 2.3917\n",
      "Time taken for 1 epoch: 67.23861622810364 secs\n",
      "\n",
      "Epoch 54 batch 0 train Loss 1.8175 test Loss 1.4414 with MSE metric 1.7215\n",
      "Epoch 54 batch 100 train Loss 1.8279 test Loss 1.4711 with MSE metric 1.6278\n",
      "Time taken for 1 epoch: 68.33430910110474 secs\n",
      "\n",
      "Epoch 55 batch 0 train Loss 1.6984 test Loss 1.4868 with MSE metric 1.4668\n",
      "Epoch 55 batch 100 train Loss 1.8469 test Loss 2.0028 with MSE metric 9.6996\n",
      "Time taken for 1 epoch: 66.98355889320374 secs\n",
      "\n",
      "Epoch 56 batch 0 train Loss 1.8145 test Loss 1.4868 with MSE metric 2.0220\n",
      "Epoch 56 batch 100 train Loss 1.7276 test Loss 1.6422 with MSE metric 4.3852\n",
      "Time taken for 1 epoch: 66.88877511024475 secs\n",
      "\n",
      "Epoch 57 batch 0 train Loss 1.9698 test Loss 1.6365 with MSE metric 3.7909\n",
      "Epoch 57 batch 100 train Loss 2.3698 test Loss 1.5513 with MSE metric 2.1054\n",
      "Time taken for 1 epoch: 67.01206684112549 secs\n",
      "\n",
      "Epoch 58 batch 0 train Loss 1.7783 test Loss 1.5270 with MSE metric 2.4968\n",
      "Epoch 58 batch 100 train Loss 1.8861 test Loss 1.7478 with MSE metric 5.8804\n",
      "Time taken for 1 epoch: 67.80810403823853 secs\n",
      "\n",
      "Epoch 59 batch 0 train Loss 1.6131 test Loss 1.9389 with MSE metric 8.6930\n",
      "Epoch 59 batch 100 train Loss 1.7391 test Loss 1.5046 with MSE metric 1.8353\n",
      "Time taken for 1 epoch: 67.5546932220459 secs\n",
      "\n",
      "Epoch 60 batch 0 train Loss 1.6742 test Loss 1.6857 with MSE metric 4.8411\n",
      "Epoch 60 batch 100 train Loss 1.7186 test Loss 1.6995 with MSE metric 4.8244\n",
      "Time taken for 1 epoch: 67.19504308700562 secs\n",
      "\n",
      "Epoch 61 batch 0 train Loss 2.1663 test Loss 1.6410 with MSE metric 3.7437\n",
      "Epoch 61 batch 100 train Loss 1.9546 test Loss 1.6271 with MSE metric 3.5177\n",
      "Time taken for 1 epoch: 65.96809911727905 secs\n",
      "\n",
      "Epoch 62 batch 0 train Loss 1.8076 test Loss 1.6036 with MSE metric 3.9734\n",
      "Epoch 62 batch 100 train Loss 2.0703 test Loss 1.4903 with MSE metric 2.3897\n",
      "Time taken for 1 epoch: 66.82825899124146 secs\n",
      "\n",
      "Epoch 63 batch 0 train Loss 1.6878 test Loss 1.5764 with MSE metric 3.1350\n",
      "Epoch 63 batch 100 train Loss 1.7165 test Loss 1.5430 with MSE metric 2.0601\n",
      "Time taken for 1 epoch: 67.8072829246521 secs\n",
      "\n",
      "Epoch 64 batch 0 train Loss 1.5425 test Loss 2.1398 with MSE metric 11.3538\n",
      "Epoch 64 batch 100 train Loss 2.6353 test Loss 1.5349 with MSE metric 2.3582\n",
      "Time taken for 1 epoch: 66.35114526748657 secs\n",
      "\n",
      "Epoch 65 batch 0 train Loss 1.8552 test Loss 1.5404 with MSE metric 3.1847\n",
      "Epoch 65 batch 100 train Loss 1.8142 test Loss 1.7570 with MSE metric 5.7661\n",
      "Time taken for 1 epoch: 67.20643997192383 secs\n",
      "\n",
      "Epoch 66 batch 0 train Loss 1.7816 test Loss 1.6337 with MSE metric 4.2974\n",
      "Epoch 66 batch 100 train Loss 1.5963 test Loss 1.5241 with MSE metric 2.8115\n",
      "Time taken for 1 epoch: 67.11426186561584 secs\n",
      "\n",
      "Epoch 67 batch 0 train Loss 1.7628 test Loss 2.0483 with MSE metric 10.6414\n",
      "Epoch 67 batch 100 train Loss 2.0044 test Loss 1.5154 with MSE metric 2.4088\n",
      "Time taken for 1 epoch: 66.13099002838135 secs\n",
      "\n",
      "Epoch 68 batch 0 train Loss 1.8579 test Loss 1.5882 with MSE metric 2.9487\n",
      "Epoch 68 batch 100 train Loss 1.6696 test Loss 1.5303 with MSE metric 2.3629\n",
      "Time taken for 1 epoch: 66.30893802642822 secs\n",
      "\n",
      "Epoch 69 batch 0 train Loss 1.8765 test Loss 1.4814 with MSE metric 2.1996\n",
      "Epoch 69 batch 100 train Loss 1.7683 test Loss 1.4941 with MSE metric 2.0093\n",
      "Time taken for 1 epoch: 65.63510918617249 secs\n",
      "\n",
      "Epoch 70 batch 0 train Loss 2.0013 test Loss 1.7587 with MSE metric 6.1233\n",
      "Epoch 70 batch 100 train Loss 1.7484 test Loss 1.5661 with MSE metric 2.3923\n",
      "Time taken for 1 epoch: 66.44581484794617 secs\n",
      "\n",
      "Epoch 71 batch 0 train Loss 1.8296 test Loss 1.7098 with MSE metric 5.1833\n",
      "Epoch 71 batch 100 train Loss 1.9187 test Loss 1.6942 with MSE metric 5.2694\n",
      "Time taken for 1 epoch: 67.0799469947815 secs\n",
      "\n",
      "Epoch 72 batch 0 train Loss 2.2731 test Loss 1.5539 with MSE metric 2.5272\n",
      "Epoch 72 batch 100 train Loss 1.5726 test Loss 1.4250 with MSE metric 2.0191\n",
      "Time taken for 1 epoch: 65.53280305862427 secs\n",
      "\n",
      "Epoch 73 batch 0 train Loss 1.7375 test Loss 1.8088 with MSE metric 6.7852\n",
      "Epoch 73 batch 100 train Loss 1.7577 test Loss 1.6314 with MSE metric 4.1053\n",
      "Time taken for 1 epoch: 66.54201817512512 secs\n",
      "\n",
      "Epoch 74 batch 0 train Loss 1.7747 test Loss 1.5394 with MSE metric 2.7665\n",
      "Epoch 74 batch 100 train Loss 1.6350 test Loss 1.4971 with MSE metric 2.6644\n",
      "Time taken for 1 epoch: 66.67298197746277 secs\n",
      "\n",
      "Epoch 75 batch 0 train Loss 1.8861 test Loss 1.4823 with MSE metric 1.8455\n",
      "Epoch 75 batch 100 train Loss 1.6851 test Loss 1.6881 with MSE metric 4.5882\n",
      "Time taken for 1 epoch: 66.02322006225586 secs\n",
      "\n",
      "Epoch 76 batch 0 train Loss 1.7097 test Loss 1.5192 with MSE metric 2.0208\n",
      "Epoch 76 batch 100 train Loss 1.7584 test Loss 1.8149 with MSE metric 6.8938\n",
      "Time taken for 1 epoch: 66.46689796447754 secs\n",
      "\n",
      "Epoch 77 batch 0 train Loss 2.1131 test Loss 1.4997 with MSE metric 2.2148\n",
      "Epoch 77 batch 100 train Loss 1.6899 test Loss 1.5085 with MSE metric 2.4868\n",
      "Time taken for 1 epoch: 66.3879508972168 secs\n",
      "\n",
      "Epoch 78 batch 0 train Loss 1.7731 test Loss 1.5105 with MSE metric 1.8372\n",
      "Epoch 78 batch 100 train Loss 1.6690 test Loss 1.4987 with MSE metric 1.4685\n",
      "Time taken for 1 epoch: 67.82100319862366 secs\n",
      "\n",
      "Epoch 79 batch 0 train Loss 1.5847 test Loss 1.5165 with MSE metric 2.5046\n",
      "Epoch 79 batch 100 train Loss 1.7101 test Loss 1.5202 with MSE metric 1.9179\n",
      "Time taken for 1 epoch: 67.20690584182739 secs\n",
      "\n",
      "Epoch 80 batch 0 train Loss 1.6703 test Loss 1.9546 with MSE metric 9.0790\n",
      "Epoch 80 batch 100 train Loss 1.9203 test Loss 1.5234 with MSE metric 2.0992\n",
      "Time taken for 1 epoch: 66.05132293701172 secs\n",
      "\n",
      "Epoch 81 batch 0 train Loss 1.7781 test Loss 1.5292 with MSE metric 3.0183\n",
      "Epoch 81 batch 100 train Loss 1.6680 test Loss 1.4851 with MSE metric 1.7643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 64.99147915840149 secs\n",
      "\n",
      "Epoch 82 batch 0 train Loss 1.8911 test Loss 1.6046 with MSE metric 3.6536\n",
      "Epoch 82 batch 100 train Loss 1.6564 test Loss 2.2140 with MSE metric 13.9296\n",
      "Time taken for 1 epoch: 65.14191389083862 secs\n",
      "\n",
      "Epoch 83 batch 0 train Loss 2.3785 test Loss 2.0898 with MSE metric 10.7116\n",
      "Epoch 83 batch 100 train Loss 1.7044 test Loss 1.5297 with MSE metric 1.8545\n",
      "Time taken for 1 epoch: 65.07447814941406 secs\n",
      "\n",
      "Epoch 84 batch 0 train Loss 1.8248 test Loss 1.5480 with MSE metric 3.3737\n",
      "Epoch 84 batch 100 train Loss 1.6089 test Loss 1.4718 with MSE metric 2.0628\n",
      "Time taken for 1 epoch: 65.38776898384094 secs\n",
      "\n",
      "Epoch 85 batch 0 train Loss 1.6787 test Loss 1.5656 with MSE metric 3.0915\n",
      "Epoch 85 batch 100 train Loss 2.0155 test Loss 1.4797 with MSE metric 2.3750\n",
      "Time taken for 1 epoch: 66.9913101196289 secs\n",
      "\n",
      "Epoch 86 batch 0 train Loss 1.8378 test Loss 1.6528 with MSE metric 4.3781\n",
      "Epoch 86 batch 100 train Loss 1.6997 test Loss 1.4512 with MSE metric 1.5587\n",
      "Time taken for 1 epoch: 67.04645705223083 secs\n",
      "\n",
      "Epoch 87 batch 0 train Loss 1.6762 test Loss 1.6546 with MSE metric 4.1047\n",
      "Epoch 87 batch 100 train Loss 1.5988 test Loss 1.7424 with MSE metric 5.9252\n",
      "Time taken for 1 epoch: 65.42439103126526 secs\n",
      "\n",
      "Epoch 88 batch 0 train Loss 1.7884 test Loss 1.7075 with MSE metric 4.7369\n",
      "Epoch 88 batch 100 train Loss 1.9863 test Loss 1.8928 with MSE metric 8.0679\n",
      "Time taken for 1 epoch: 67.74682688713074 secs\n",
      "\n",
      "Epoch 89 batch 0 train Loss 1.8590 test Loss 1.4691 with MSE metric 1.8289\n",
      "Epoch 89 batch 100 train Loss 1.7096 test Loss 1.6920 with MSE metric 5.2452\n",
      "Time taken for 1 epoch: 68.26062679290771 secs\n",
      "\n",
      "Epoch 90 batch 0 train Loss 1.6151 test Loss 1.5914 with MSE metric 2.8918\n",
      "Epoch 90 batch 100 train Loss 2.0038 test Loss 1.9258 with MSE metric 8.5692\n",
      "Time taken for 1 epoch: 67.57787704467773 secs\n",
      "\n",
      "Epoch 91 batch 0 train Loss 1.6832 test Loss 1.4835 with MSE metric 1.6955\n",
      "Epoch 91 batch 100 train Loss 1.6295 test Loss 1.4344 with MSE metric 1.6591\n",
      "Time taken for 1 epoch: 67.77334928512573 secs\n",
      "\n",
      "Epoch 92 batch 0 train Loss 1.7026 test Loss 1.6187 with MSE metric 3.4338\n",
      "Epoch 92 batch 100 train Loss 1.8731 test Loss 1.6342 with MSE metric 4.0051\n",
      "Time taken for 1 epoch: 67.10133981704712 secs\n",
      "\n",
      "Epoch 93 batch 0 train Loss 1.6578 test Loss 1.6929 with MSE metric 4.8739\n",
      "Epoch 93 batch 100 train Loss 2.0352 test Loss 1.5631 with MSE metric 3.2889\n",
      "Time taken for 1 epoch: 67.53418898582458 secs\n",
      "\n",
      "Epoch 94 batch 0 train Loss 1.8140 test Loss 1.5384 with MSE metric 2.8225\n",
      "Epoch 94 batch 100 train Loss 1.8628 test Loss 1.6321 with MSE metric 4.0192\n",
      "Time taken for 1 epoch: 68.09125709533691 secs\n",
      "\n",
      "Epoch 95 batch 0 train Loss 1.7307 test Loss 1.5434 with MSE metric 2.5632\n",
      "Epoch 95 batch 100 train Loss 1.6975 test Loss 1.5005 with MSE metric 1.7826\n",
      "Time taken for 1 epoch: 68.33497095108032 secs\n",
      "\n",
      "Epoch 96 batch 0 train Loss 1.6360 test Loss 1.5087 with MSE metric 2.1794\n",
      "Epoch 96 batch 100 train Loss 1.8938 test Loss 1.5712 with MSE metric 2.8773\n",
      "Time taken for 1 epoch: 67.79215693473816 secs\n",
      "\n",
      "Epoch 97 batch 0 train Loss 1.8338 test Loss 1.5855 with MSE metric 3.1304\n",
      "Epoch 97 batch 100 train Loss 1.5755 test Loss 1.4924 with MSE metric 2.1141\n",
      "Time taken for 1 epoch: 67.49069786071777 secs\n",
      "\n",
      "Epoch 98 batch 0 train Loss 2.1497 test Loss 2.0348 with MSE metric 10.5318\n",
      "Epoch 98 batch 100 train Loss 1.6660 test Loss 2.0022 with MSE metric 9.5134\n",
      "Time taken for 1 epoch: 67.53268623352051 secs\n",
      "\n",
      "Epoch 99 batch 0 train Loss 1.7395 test Loss 1.8397 with MSE metric 7.2447\n",
      "Epoch 99 batch 100 train Loss 1.7256 test Loss 1.5979 with MSE metric 3.8865\n",
      "Time taken for 1 epoch: 67.68594980239868 secs\n",
      "\n",
      "Epoch 100 batch 0 train Loss 1.6158 test Loss 1.6410 with MSE metric 4.1237\n",
      "Epoch 100 batch 100 train Loss 2.0575 test Loss 1.5293 with MSE metric 2.2126\n",
      "Time taken for 1 epoch: 67.89452409744263 secs\n",
      "\n",
      "Epoch 101 batch 0 train Loss 2.0188 test Loss 1.5378 with MSE metric 2.6568\n",
      "Epoch 101 batch 100 train Loss 2.0337 test Loss 1.4757 with MSE metric 1.4952\n",
      "Time taken for 1 epoch: 68.72862195968628 secs\n",
      "\n",
      "Epoch 102 batch 0 train Loss 1.9320 test Loss 1.6405 with MSE metric 4.5114\n",
      "Epoch 102 batch 100 train Loss 1.6367 test Loss 1.4996 with MSE metric 2.6666\n",
      "Time taken for 1 epoch: 66.60909414291382 secs\n",
      "\n",
      "Epoch 103 batch 0 train Loss 2.9275 test Loss 1.4881 with MSE metric 1.5292\n",
      "Epoch 103 batch 100 train Loss 1.5718 test Loss 1.6051 with MSE metric 3.9821\n",
      "Time taken for 1 epoch: 68.38798594474792 secs\n",
      "\n",
      "Epoch 104 batch 0 train Loss 1.6751 test Loss 1.5899 with MSE metric 3.7284\n",
      "Epoch 104 batch 100 train Loss 1.9870 test Loss 1.5732 with MSE metric 2.6465\n",
      "Time taken for 1 epoch: 67.47736692428589 secs\n",
      "\n",
      "Epoch 105 batch 0 train Loss 1.8507 test Loss 1.6061 with MSE metric 4.2409\n",
      "Epoch 105 batch 100 train Loss 1.9260 test Loss 1.6599 with MSE metric 4.0594\n",
      "Time taken for 1 epoch: 65.17875409126282 secs\n",
      "\n",
      "Epoch 106 batch 0 train Loss 1.9909 test Loss 1.5063 with MSE metric 2.4084\n",
      "Epoch 106 batch 100 train Loss 1.8022 test Loss 1.6037 with MSE metric 3.8044\n",
      "Time taken for 1 epoch: 65.14906287193298 secs\n",
      "\n",
      "Epoch 107 batch 0 train Loss 1.8776 test Loss 1.5073 with MSE metric 1.8889\n",
      "Epoch 107 batch 100 train Loss 3.2407 test Loss 1.8457 with MSE metric 7.3546\n",
      "Time taken for 1 epoch: 65.15606188774109 secs\n",
      "\n",
      "Epoch 108 batch 0 train Loss 1.7373 test Loss 1.5751 with MSE metric 2.7341\n",
      "Epoch 108 batch 100 train Loss 2.1546 test Loss 2.1884 with MSE metric 12.6701\n",
      "Time taken for 1 epoch: 65.1939902305603 secs\n",
      "\n",
      "Epoch 109 batch 0 train Loss 1.8057 test Loss 1.6140 with MSE metric 4.0376\n",
      "Epoch 109 batch 100 train Loss 1.5980 test Loss 1.4741 with MSE metric 1.9756\n",
      "Time taken for 1 epoch: 66.09972810745239 secs\n",
      "\n",
      "Epoch 110 batch 0 train Loss 2.1452 test Loss 1.5154 with MSE metric 2.0423\n",
      "Epoch 110 batch 100 train Loss 2.5907 test Loss 1.4359 with MSE metric 1.3060\n",
      "Time taken for 1 epoch: 65.94534993171692 secs\n",
      "\n",
      "Epoch 111 batch 0 train Loss 1.7941 test Loss 1.6008 with MSE metric 3.9616\n",
      "Epoch 111 batch 100 train Loss 1.9090 test Loss 1.5712 with MSE metric 2.9745\n",
      "Time taken for 1 epoch: 66.67476391792297 secs\n",
      "\n",
      "Epoch 112 batch 0 train Loss 1.6484 test Loss 1.6581 with MSE metric 4.8233\n",
      "Epoch 112 batch 100 train Loss 1.9834 test Loss 1.5827 with MSE metric 2.8327\n",
      "Time taken for 1 epoch: 67.38883900642395 secs\n",
      "\n",
      "Epoch 113 batch 0 train Loss 2.0510 test Loss 2.1022 with MSE metric 11.1990\n",
      "Epoch 113 batch 100 train Loss 1.8567 test Loss 1.5302 with MSE metric 2.3518\n",
      "Time taken for 1 epoch: 67.14141201972961 secs\n",
      "\n",
      "Epoch 114 batch 0 train Loss 1.7444 test Loss 1.4749 with MSE metric 2.3070\n",
      "Epoch 114 batch 100 train Loss 1.8457 test Loss 1.4674 with MSE metric 2.3426\n",
      "Time taken for 1 epoch: 68.66840505599976 secs\n",
      "\n",
      "Epoch 115 batch 0 train Loss 1.6694 test Loss 1.6708 with MSE metric 4.3488\n",
      "Epoch 115 batch 100 train Loss 1.7759 test Loss 2.0097 with MSE metric 10.0115\n",
      "Time taken for 1 epoch: 66.25010514259338 secs\n",
      "\n",
      "Epoch 116 batch 0 train Loss 1.5658 test Loss 1.4652 with MSE metric 1.6975\n",
      "Epoch 116 batch 100 train Loss 2.2774 test Loss 1.5310 with MSE metric 2.5901\n",
      "Time taken for 1 epoch: 65.3270320892334 secs\n",
      "\n",
      "Epoch 117 batch 0 train Loss 1.9031 test Loss 1.6917 with MSE metric 4.8649\n",
      "Epoch 117 batch 100 train Loss 1.7133 test Loss 1.5262 with MSE metric 2.3519\n",
      "Time taken for 1 epoch: 66.53474569320679 secs\n",
      "\n",
      "Epoch 118 batch 0 train Loss 2.5527 test Loss 2.0497 with MSE metric 10.2320\n",
      "Epoch 118 batch 100 train Loss 1.8855 test Loss 1.4640 with MSE metric 2.2854\n",
      "Time taken for 1 epoch: 65.69320511817932 secs\n",
      "\n",
      "Epoch 119 batch 0 train Loss 1.8089 test Loss 1.7409 with MSE metric 5.8339\n",
      "Epoch 119 batch 100 train Loss 1.7566 test Loss 1.8416 with MSE metric 7.3075\n",
      "Time taken for 1 epoch: 65.37722182273865 secs\n",
      "\n",
      "Epoch 120 batch 0 train Loss 1.9609 test Loss 1.5626 with MSE metric 3.5712\n",
      "Epoch 120 batch 100 train Loss 1.7076 test Loss 1.5202 with MSE metric 2.3851\n",
      "Time taken for 1 epoch: 65.42034792900085 secs\n",
      "\n",
      "Epoch 121 batch 0 train Loss 1.8659 test Loss 1.8890 with MSE metric 7.9377\n",
      "Epoch 121 batch 100 train Loss 1.8549 test Loss 1.8050 with MSE metric 6.7282\n",
      "Time taken for 1 epoch: 65.52140378952026 secs\n",
      "\n",
      "Epoch 122 batch 0 train Loss 1.7763 test Loss 1.5628 with MSE metric 3.6903\n",
      "Epoch 122 batch 100 train Loss 1.8137 test Loss 2.0690 with MSE metric 10.4778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 65.50670123100281 secs\n",
      "\n",
      "Epoch 123 batch 0 train Loss 2.5393 test Loss 1.6965 with MSE metric 5.0370\n",
      "Epoch 123 batch 100 train Loss 1.6616 test Loss 1.4926 with MSE metric 1.8381\n",
      "Time taken for 1 epoch: 65.44403100013733 secs\n",
      "\n",
      "Epoch 124 batch 0 train Loss 1.8633 test Loss 1.5024 with MSE metric 2.1573\n",
      "Epoch 124 batch 100 train Loss 1.8734 test Loss 1.5034 with MSE metric 1.5040\n",
      "Time taken for 1 epoch: 65.45074510574341 secs\n",
      "\n",
      "Epoch 125 batch 0 train Loss 1.6104 test Loss 1.9068 with MSE metric 8.3260\n",
      "Epoch 125 batch 100 train Loss 1.6162 test Loss 1.6224 with MSE metric 4.3298\n",
      "Time taken for 1 epoch: 65.29555988311768 secs\n",
      "\n",
      "Epoch 126 batch 0 train Loss 1.8754 test Loss 1.8883 with MSE metric 7.9510\n",
      "Epoch 126 batch 100 train Loss 1.6091 test Loss 1.4982 with MSE metric 2.2169\n",
      "Time taken for 1 epoch: 65.35960078239441 secs\n",
      "\n",
      "Epoch 127 batch 0 train Loss 1.7532 test Loss 1.6907 with MSE metric 4.8703\n",
      "Epoch 127 batch 100 train Loss 1.5517 test Loss 1.5480 with MSE metric 3.2615\n",
      "Time taken for 1 epoch: 65.51332020759583 secs\n",
      "\n",
      "Epoch 128 batch 0 train Loss 1.7410 test Loss 1.9101 with MSE metric 8.3552\n",
      "Epoch 128 batch 100 train Loss 1.5296 test Loss 2.0462 with MSE metric 9.8167\n",
      "Time taken for 1 epoch: 65.42801117897034 secs\n",
      "\n",
      "Epoch 129 batch 0 train Loss 1.8465 test Loss 1.4299 with MSE metric 1.4402\n",
      "Epoch 129 batch 100 train Loss 1.9149 test Loss 1.7494 with MSE metric 6.0581\n",
      "Time taken for 1 epoch: 65.50336003303528 secs\n",
      "\n",
      "Epoch 130 batch 0 train Loss 1.5183 test Loss 1.5227 with MSE metric 2.8343\n",
      "Epoch 130 batch 100 train Loss 1.7052 test Loss 1.8965 with MSE metric 8.0322\n",
      "Time taken for 1 epoch: 67.04146790504456 secs\n",
      "\n",
      "Epoch 131 batch 0 train Loss 1.9232 test Loss 1.5855 with MSE metric 2.4783\n",
      "Epoch 131 batch 100 train Loss 1.6087 test Loss 1.6300 with MSE metric 4.1498\n",
      "Time taken for 1 epoch: 66.6377170085907 secs\n",
      "\n",
      "Epoch 132 batch 0 train Loss 1.7243 test Loss 1.5053 with MSE metric 1.9549\n",
      "Epoch 132 batch 100 train Loss 1.8972 test Loss 1.8459 with MSE metric 7.3746\n",
      "Time taken for 1 epoch: 66.31851482391357 secs\n",
      "\n",
      "Epoch 133 batch 0 train Loss 2.0393 test Loss 1.4865 with MSE metric 2.0630\n",
      "Epoch 133 batch 100 train Loss 1.7848 test Loss 1.4773 with MSE metric 1.9005\n",
      "Time taken for 1 epoch: 66.52163600921631 secs\n",
      "\n",
      "Epoch 134 batch 0 train Loss 1.7351 test Loss 1.6883 with MSE metric 4.7648\n",
      "Epoch 134 batch 100 train Loss 1.6100 test Loss 1.5821 with MSE metric 3.5539\n",
      "Time taken for 1 epoch: 67.53721404075623 secs\n",
      "\n",
      "Epoch 135 batch 0 train Loss 1.5195 test Loss 1.6049 with MSE metric 4.0397\n",
      "Epoch 135 batch 100 train Loss 1.7925 test Loss 1.5654 with MSE metric 3.1630\n",
      "Time taken for 1 epoch: 66.51261901855469 secs\n",
      "\n",
      "Epoch 136 batch 0 train Loss 1.6246 test Loss 1.5407 with MSE metric 2.3926\n",
      "Epoch 136 batch 100 train Loss 1.9509 test Loss 1.8118 with MSE metric 6.8411\n",
      "Time taken for 1 epoch: 66.71226978302002 secs\n",
      "\n",
      "Epoch 137 batch 0 train Loss 1.6496 test Loss 1.5026 with MSE metric 2.1320\n",
      "Epoch 137 batch 100 train Loss 1.8052 test Loss 1.4835 with MSE metric 1.6000\n",
      "Time taken for 1 epoch: 66.08948922157288 secs\n",
      "\n",
      "Epoch 138 batch 0 train Loss 1.5305 test Loss 1.4407 with MSE metric 2.2071\n",
      "Epoch 138 batch 100 train Loss 1.9977 test Loss 1.5008 with MSE metric 2.4508\n",
      "Time taken for 1 epoch: 65.76588678359985 secs\n",
      "\n",
      "Epoch 139 batch 0 train Loss 1.5744 test Loss 1.5893 with MSE metric 3.5380\n",
      "Epoch 139 batch 100 train Loss 2.0712 test Loss 1.4877 with MSE metric 2.5219\n",
      "Time taken for 1 epoch: 67.28722977638245 secs\n",
      "\n",
      "Epoch 140 batch 0 train Loss 2.1217 test Loss 1.6676 with MSE metric 4.8637\n",
      "Epoch 140 batch 100 train Loss 1.9395 test Loss 1.4690 with MSE metric 1.2679\n",
      "Time taken for 1 epoch: 66.07584714889526 secs\n",
      "\n",
      "Epoch 141 batch 0 train Loss 1.9927 test Loss 1.6514 with MSE metric 4.4999\n",
      "Epoch 141 batch 100 train Loss 1.9657 test Loss 1.5581 with MSE metric 2.6384\n",
      "Time taken for 1 epoch: 66.30979609489441 secs\n",
      "\n",
      "Epoch 142 batch 0 train Loss 1.6519 test Loss 1.5416 with MSE metric 3.1037\n",
      "Epoch 142 batch 100 train Loss 1.7124 test Loss 1.7677 with MSE metric 6.2000\n",
      "Time taken for 1 epoch: 66.30510306358337 secs\n",
      "\n",
      "Epoch 143 batch 0 train Loss 1.7810 test Loss 1.4417 with MSE metric 1.4534\n",
      "Epoch 143 batch 100 train Loss 1.7880 test Loss 1.6583 with MSE metric 4.6502\n",
      "Time taken for 1 epoch: 66.40118026733398 secs\n",
      "\n",
      "Epoch 144 batch 0 train Loss 1.6802 test Loss 1.5775 with MSE metric 2.6746\n",
      "Epoch 144 batch 100 train Loss 1.5305 test Loss 1.9892 with MSE metric 9.4999\n",
      "Time taken for 1 epoch: 66.90747690200806 secs\n",
      "\n",
      "Epoch 145 batch 0 train Loss 1.8771 test Loss 1.4966 with MSE metric 2.1737\n",
      "Epoch 145 batch 100 train Loss 1.9407 test Loss 1.8528 with MSE metric 7.4754\n",
      "Time taken for 1 epoch: 66.22898197174072 secs\n",
      "\n",
      "Epoch 146 batch 0 train Loss 1.8493 test Loss 1.8845 with MSE metric 7.7937\n",
      "Epoch 146 batch 100 train Loss 1.9640 test Loss 2.0914 with MSE metric 10.8614\n",
      "Time taken for 1 epoch: 68.56162595748901 secs\n",
      "\n",
      "Epoch 147 batch 0 train Loss 2.2561 test Loss 1.9645 with MSE metric 8.8296\n",
      "Epoch 147 batch 100 train Loss 1.9021 test Loss 2.1829 with MSE metric 12.0357\n",
      "Time taken for 1 epoch: 66.90017318725586 secs\n",
      "\n",
      "Epoch 148 batch 0 train Loss 1.9615 test Loss 1.8637 with MSE metric 7.6408\n",
      "Epoch 148 batch 100 train Loss 1.6749 test Loss 1.5367 with MSE metric 2.2894\n",
      "Time taken for 1 epoch: 66.0122401714325 secs\n",
      "\n",
      "Epoch 149 batch 0 train Loss 1.6589 test Loss 1.7024 with MSE metric 5.2522\n",
      "Epoch 149 batch 100 train Loss 1.5399 test Loss 1.4130 with MSE metric 1.6460\n",
      "Time taken for 1 epoch: 66.18936324119568 secs\n",
      "\n",
      "Epoch 150 batch 0 train Loss 2.0558 test Loss 1.4978 with MSE metric 2.1794\n",
      "Epoch 150 batch 100 train Loss 1.7219 test Loss 1.6982 with MSE metric 4.5950\n",
      "Time taken for 1 epoch: 66.57159519195557 secs\n",
      "\n",
      "Epoch 151 batch 0 train Loss 1.6943 test Loss 1.6900 with MSE metric 5.0023\n",
      "Epoch 151 batch 100 train Loss 1.5521 test Loss 1.6014 with MSE metric 4.1301\n",
      "Time taken for 1 epoch: 66.51941204071045 secs\n",
      "\n",
      "Epoch 152 batch 0 train Loss 1.6335 test Loss 1.8114 with MSE metric 6.8653\n",
      "Epoch 152 batch 100 train Loss 1.8391 test Loss 1.4840 with MSE metric 2.0251\n",
      "Time taken for 1 epoch: 69.03846669197083 secs\n",
      "\n",
      "Epoch 153 batch 0 train Loss 1.9206 test Loss 2.1437 with MSE metric 11.9772\n",
      "Epoch 153 batch 100 train Loss 1.5722 test Loss 1.8702 with MSE metric 7.7388\n",
      "Time taken for 1 epoch: 68.86372184753418 secs\n",
      "\n",
      "Epoch 154 batch 0 train Loss 1.8018 test Loss 1.4891 with MSE metric 2.1555\n",
      "Epoch 154 batch 100 train Loss 1.8323 test Loss 1.5316 with MSE metric 2.9714\n",
      "Time taken for 1 epoch: 67.65875267982483 secs\n",
      "\n",
      "Epoch 155 batch 0 train Loss 1.6275 test Loss 1.7630 with MSE metric 6.1065\n",
      "Epoch 155 batch 100 train Loss 1.6277 test Loss 1.6056 with MSE metric 3.9854\n",
      "Time taken for 1 epoch: 69.15446591377258 secs\n",
      "\n",
      "Epoch 156 batch 0 train Loss 1.6885 test Loss 1.5545 with MSE metric 3.4452\n",
      "Epoch 156 batch 100 train Loss 1.5948 test Loss 1.5313 with MSE metric 2.7827\n",
      "Time taken for 1 epoch: 67.67835116386414 secs\n",
      "\n",
      "Epoch 157 batch 0 train Loss 1.6315 test Loss 1.4538 with MSE metric 2.0779\n",
      "Epoch 157 batch 100 train Loss 1.6023 test Loss 1.6850 with MSE metric 5.0688\n",
      "Time taken for 1 epoch: 67.86223602294922 secs\n",
      "\n",
      "Epoch 158 batch 0 train Loss 1.6795 test Loss 1.5924 with MSE metric 3.4299\n",
      "Epoch 158 batch 100 train Loss 1.9163 test Loss 2.0871 with MSE metric 10.9849\n",
      "Time taken for 1 epoch: 68.28580594062805 secs\n",
      "\n",
      "Epoch 159 batch 0 train Loss 1.7437 test Loss 1.6999 with MSE metric 5.2714\n",
      "Epoch 159 batch 100 train Loss 2.2539 test Loss 1.6315 with MSE metric 4.2839\n",
      "Time taken for 1 epoch: 67.63575196266174 secs\n",
      "\n",
      "Epoch 160 batch 0 train Loss 1.9139 test Loss 1.5195 with MSE metric 2.5340\n",
      "Epoch 160 batch 100 train Loss 1.7142 test Loss 1.5658 with MSE metric 3.1348\n",
      "Time taken for 1 epoch: 69.66560006141663 secs\n",
      "\n",
      "Epoch 161 batch 0 train Loss 2.4881 test Loss 1.6171 with MSE metric 3.6237\n",
      "Epoch 161 batch 100 train Loss 1.6318 test Loss 1.5892 with MSE metric 3.7505\n",
      "Time taken for 1 epoch: 67.7814552783966 secs\n",
      "\n",
      "Epoch 162 batch 0 train Loss 1.6688 test Loss 1.5503 with MSE metric 3.3111\n",
      "Epoch 162 batch 100 train Loss 1.7113 test Loss 1.8457 with MSE metric 7.3492\n",
      "Time taken for 1 epoch: 69.30676794052124 secs\n",
      "\n",
      "Epoch 163 batch 0 train Loss 2.1491 test Loss 1.8501 with MSE metric 7.4359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163 batch 100 train Loss 2.0245 test Loss 1.4164 with MSE metric 1.6491\n",
      "Time taken for 1 epoch: 67.72444891929626 secs\n",
      "\n",
      "Epoch 164 batch 0 train Loss 1.7513 test Loss 1.5969 with MSE metric 3.6399\n",
      "Epoch 164 batch 100 train Loss 1.6079 test Loss 1.5140 with MSE metric 2.4270\n",
      "Time taken for 1 epoch: 66.69918322563171 secs\n",
      "\n",
      "Epoch 165 batch 0 train Loss 1.6897 test Loss 1.4730 with MSE metric 1.5551\n",
      "Epoch 165 batch 100 train Loss 1.7291 test Loss 1.5715 with MSE metric 3.4866\n",
      "Time taken for 1 epoch: 65.77011299133301 secs\n",
      "\n",
      "Epoch 166 batch 0 train Loss 1.7233 test Loss 1.5141 with MSE metric 2.3748\n",
      "Epoch 166 batch 100 train Loss 1.5567 test Loss 1.4909 with MSE metric 2.6818\n",
      "Time taken for 1 epoch: 67.3675811290741 secs\n",
      "\n",
      "Epoch 167 batch 0 train Loss 1.8544 test Loss 1.6479 with MSE metric 4.0020\n",
      "Epoch 167 batch 100 train Loss 1.7361 test Loss 1.6131 with MSE metric 3.5311\n",
      "Time taken for 1 epoch: 65.65760588645935 secs\n",
      "\n",
      "Epoch 168 batch 0 train Loss 1.7952 test Loss 1.6032 with MSE metric 4.1087\n",
      "Epoch 168 batch 100 train Loss 2.0602 test Loss 1.6467 with MSE metric 4.3124\n",
      "Time taken for 1 epoch: 66.07265996932983 secs\n",
      "\n",
      "Epoch 169 batch 0 train Loss 1.5983 test Loss 1.5050 with MSE metric 2.2014\n",
      "Epoch 169 batch 100 train Loss 2.1664 test Loss 1.5719 with MSE metric 3.3072\n",
      "Time taken for 1 epoch: 66.03509879112244 secs\n",
      "\n",
      "Epoch 170 batch 0 train Loss 2.1715 test Loss 1.6684 with MSE metric 4.8382\n",
      "Epoch 170 batch 100 train Loss 1.5740 test Loss 1.5235 with MSE metric 2.0421\n",
      "Time taken for 1 epoch: 65.77094197273254 secs\n",
      "\n",
      "Epoch 171 batch 0 train Loss 1.7975 test Loss 1.5201 with MSE metric 3.1307\n",
      "Epoch 171 batch 100 train Loss 1.6827 test Loss 2.0043 with MSE metric 9.7533\n",
      "Time taken for 1 epoch: 65.61362767219543 secs\n",
      "\n",
      "Epoch 172 batch 0 train Loss 2.3099 test Loss 1.4496 with MSE metric 2.0134\n",
      "Epoch 172 batch 100 train Loss 1.7047 test Loss 1.5346 with MSE metric 3.2807\n",
      "Time taken for 1 epoch: 65.26939702033997 secs\n",
      "\n",
      "Epoch 173 batch 0 train Loss 1.6889 test Loss 1.5406 with MSE metric 3.1618\n",
      "Epoch 173 batch 100 train Loss 1.9183 test Loss 1.5666 with MSE metric 3.1603\n",
      "Time taken for 1 epoch: 69.10268306732178 secs\n",
      "\n",
      "Epoch 174 batch 0 train Loss 1.8551 test Loss 1.5238 with MSE metric 2.7138\n",
      "Epoch 174 batch 100 train Loss 1.6170 test Loss 1.6659 with MSE metric 4.9503\n",
      "Time taken for 1 epoch: 70.62898898124695 secs\n",
      "\n",
      "Epoch 175 batch 0 train Loss 1.6564 test Loss 1.4446 with MSE metric 1.5283\n",
      "Epoch 175 batch 100 train Loss 1.7137 test Loss 1.5246 with MSE metric 2.3286\n",
      "Time taken for 1 epoch: 71.3027491569519 secs\n",
      "\n",
      "Epoch 176 batch 0 train Loss 2.0828 test Loss 1.4447 with MSE metric 1.8513\n",
      "Epoch 176 batch 100 train Loss 1.6147 test Loss 1.4537 with MSE metric 1.9458\n",
      "Time taken for 1 epoch: 69.56981921195984 secs\n",
      "\n",
      "Epoch 177 batch 0 train Loss 1.8216 test Loss 1.9467 with MSE metric 8.7521\n",
      "Epoch 177 batch 100 train Loss 1.7039 test Loss 1.5619 with MSE metric 3.5421\n",
      "Time taken for 1 epoch: 74.1247308254242 secs\n",
      "\n",
      "Epoch 178 batch 0 train Loss 1.5564 test Loss 2.2458 with MSE metric 12.6755\n",
      "Epoch 178 batch 100 train Loss 1.7160 test Loss 1.6297 with MSE metric 3.7965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-315808fb7bf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mbatch_tok_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tim_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tim_pos_tr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tar_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batch_river\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasin_l_tr\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mattributes_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mbatch_pos_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tok_pos_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mtar_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_log_sig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tok_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tim_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tim_pos_tr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tar_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_n\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam(0.0004)\n",
    "    decoder = river_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 64\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(tar_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_river_mha/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tim_pos_tr2, batch_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_river(token_tr, t1_tr, t2_tr, basin_l_tr,  attributes_numeric, tar_tr, batch_s=64)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_log_sig, mask = train_step(decoder, optimizer_c, train_loss, m_tr, batch_tok_pos_tr, batch_tim_pos_tr, batch_tim_pos_tr2, batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 100 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tim_pos_te2, batch_pos_te, batch_tar_te, _ = batch_creator.create_batch_river(token_te, t1_te, t2_te, basin_l_te,  attributes_numeric, tar_te, batch_s=64)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_log_sig_te, t_mask = test_step(decoder, test_loss, m_te, batch_tok_pos_te, batch_tim_pos_te, batch_tim_pos_te2, batch_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_te.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/omernivron/Downloads/Data_riverflow/river_flow_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin = pd.read_csv('/Users/omernivron/Downloads/Data_riverflow/basin_list.txt', header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>11482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>11522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>11523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>11528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>11532500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0     1022500\n",
       "1     1031500\n",
       "2     1047000\n",
       "3     1052500\n",
       "4     1054200\n",
       "..        ...\n",
       "526  11482500\n",
       "527  11522500\n",
       "528  11523200\n",
       "529  11528700\n",
       "530  11532500\n",
       "\n",
       "[531 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.181555968690538"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((df[df['basin'] == 1022500]['OBS_RUN'] - np.mean(df[df['basin'] == 1022500]['OBS_RUN']))**2) / len(df[df['basin'] == 1022500]['OBS_RUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1- 4.4 / (sum((df[df['basin'] == 1022500]['OBS_RUN'] - np.mean(df[df['basin'] == 1022500]['OBS_RUN']))**2) / len(df[df['basin'] == 1022500]['OBS_RUN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
