{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import river_model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from inference import infer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv('/Users/omernivron/Downloads/attibutes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_numeric = attributes[['gauge_id', \n",
    "               'p_mean', 'pet_mean', 'p_seasonality', 'frac_snow',\n",
    "               'aridity', 'high_prec_freq', 'high_prec_dur',\n",
    "                #'high_prec_timing',\n",
    "               'low_prec_freq', 'low_prec_dur',\n",
    "                #'low_prec_timing', 'geol_1st_class', 'glim_1st_class_frac', 'geol_2nd_class', 'glim_2nd_class_frac',\n",
    "               'carbonate_rocks_frac', #'geol_porostiy',\n",
    "                'geol_permeability',\n",
    "                #'q_mean','runoff_ratio', 'slope_fdc', 'baseflow_index', 'stream_elas', 'q5',\n",
    "               #'q95', 'high_q_freq', 'high_q_dur', 'low_q_freq', 'low_q_dur',\n",
    "               #'zero_q_freq', 'hfd_mean', 'huc_02', 'gauge_name',\n",
    "               'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity','soil_conductivity',\n",
    "                'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac',\n",
    "                #'water_frac', 'organic_frac', 'other_frac',\n",
    "                #'gauge_lat','gauge_lon',\n",
    "                'elev_mean', 'slope_mean',\n",
    "                #'area_gages2',\n",
    "                'area_geospa_fabric',\n",
    "                'frac_forest', 'lai_max', 'lai_diff', 'gvf_max','gvf_diff',\n",
    "                #'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50','root_depth_99', \n",
    "#                                  'hru08'\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_basins = pd.read_csv('/Users/omernivron/Downloads/basin_list.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/omernivron/Downloads/daymet_data_seed05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_channels = ['OBS_RUN',\n",
    "                    'doy_cos','doy_sin',\n",
    "                    'prcp(mm/day)', \n",
    "                    'srad(W/m2)',  \n",
    "                    'tmax(C)',\n",
    "                    'tmin(C)', \n",
    "                    'vp(Pa)'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_channels2 = ['prcp(mm/day)', \n",
    "                    'srad(W/m2)',  \n",
    "                    'tmax(C)',\n",
    "                    'tmin(C)', \n",
    "                    'vp(Pa)'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_drop = ['MNTH', 'DY', 'hru02', 'hru04', 'RAIM', 'TAIR', 'PET', 'ET', 'SWE', 'swe(mm)', 'PRCP', 'seed', 'id_lag', 'HR', 'dayl(s)', 'YR', 'MOD_RUN', 'id', 'DOY', 'DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns= list_to_drop, inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = df[df['OBS_RUN'] >= 0]\n",
    "cols = df.columns.to_list()\n",
    "cols = [cols[5]] + cols[:5] + cols[6:]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_tr = np.zeros((200 * len(selected_basins), 50, 9))\n",
    "arr_te = np.zeros((200 * (len(df.basin.unique()) - len(selected_basins)), 50, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_tr = 0\n",
    "step_te = 0\n",
    "for bas in df.basin.unique():\n",
    "    df_temp = (df[df['basin'] == bas]).reset_index()\n",
    "    att_num = np.where(attributes_numeric['hru08'] == df_temp['hru08'].unique()[0])[0]\n",
    "    rep_att_num = np.repeat(att_num, 50).reshape(-1, 1)\n",
    "    for obs in range(200):\n",
    "        rows = np.random.choice(np.arange(0, df_temp.shape[0], 1), 50)\n",
    "        if (np.isin(bas, selected_basins)):\n",
    "            arr_tr[step_tr, :, :] = np.concatenate((df_temp.loc[rows, context_channels], rep_att_num), axis = 1)\n",
    "            step_tr += 1\n",
    "        else:\n",
    "            arr_te[step_te, :, :] = np.concatenate((df_temp.loc[rows, context_channels], rep_att_num), axis = 1)\n",
    "            step_te += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/omernivron/Downloads/river_flow_tr', arr_tr)\n",
    "np.save('/Users/omernivron/Downloads/river_flow_te', arr_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pp = np.zeros((5 * 10000, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(context_channels2)):\n",
    "    for row in range(0 + 10000 * i, 10000 * (i + 1), 5):\n",
    "        basin = np.random.choice(selected_basins[0], 2)\n",
    "        df_temp = (df[df['basin'] == basin[0]]).reset_index()\n",
    "        df_riv = (df[df['basin'] == basin[1]]).reset_index()\n",
    "\n",
    "        idx_i = np.random.choice(np.arange(0, df_temp.shape[0], 1), 25)\n",
    "        idx_riv = np.random.choice(np.arange(0, df_riv.shape[0], 1), 25)\n",
    "        \n",
    "        arr_pp[row, :] = np.concatenate((df_temp.loc[idx_i, [context_channels2[i]]], df_riv.loc[idx_riv, ['OBS_RUN']]), axis = 0).reshape(-1)\n",
    "        arr_pp[row + 1, :] = np.concatenate((df_temp.loc[idx_i, ['doy_cos']], df_riv.loc[idx_riv, ['doy_cos']]), axis = 0).reshape(-1)\n",
    "        arr_pp[row + 2, :] = np.concatenate((df_temp.loc[idx_i, ['doy_sin']], df_riv.loc[idx_riv, ['doy_sin']]), axis = 0).reshape(-1)\n",
    "        arr_pp[row + 3, :] = np.concatenate((np.ones(25) * i, np.ones(25) * 9))\n",
    "        arr_pp[row + 4, :] = np.concatenate((df_temp.loc[idx_i, ['basin']], df_riv.loc[idx_riv, ['basin']]), axis = 0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/omernivron/Downloads/river_processed', arr_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pp = np.load('/Users/omernivron/Downloads/river_processed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_river'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, train_loss, m_tr, token_pos, time_pos, time_pos2, pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, time_pos2, pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, test_loss, m_te, token_pos_te, time_pos_te, pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = arr_pp[1::5]; t2 = arr_pp[2::5]\n",
    "tar = arr_pp[0::5];\n",
    "token = arr_pp[3::5]; basin_l = arr_pp[4::5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Initializing from scratch.\n",
      "Tensor(\"decoder_2/embedding_2/embedding_lookup/Identity:0\", shape=(32, 50, 8), dtype=float64)\n",
      "Tensor(\"decoder_2/concat:0\", shape=(32, 50, 9), dtype=float64)\n",
      "Tensor(\"decoder_2/embedding_2/embedding_lookup/Identity:0\", shape=(32, 50, 8), dtype=float64)\n",
      "Tensor(\"decoder_2/concat:0\", shape=(32, 50, 9), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = river_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(tar.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_river/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tim_pos_tr2, batch_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_river(token, t1, t2, basin_l,  attributes_numeric, tar, batch_s=32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_sig, mask = train_step(decoder, optimizer_c, train_loss, m_tr, batch_tok_pos_tr, batch_tim_pos_tr, batch_tim_pos_tr2, batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "#                 if batch_n % 10000 == 0:\n",
    "#                     batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_river(token, t1, t2, basin_l,  att, tar, batch_s=32)\n",
    "#                     batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "#                     tar_real_te, pred_te, pred_sig_te, t_mask = test_step(batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "#                     helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "#                     helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "#                     manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>p_mean</th>\n",
       "      <th>pet_mean</th>\n",
       "      <th>p_seasonality</th>\n",
       "      <th>frac_snow</th>\n",
       "      <th>aridity</th>\n",
       "      <th>high_prec_freq</th>\n",
       "      <th>high_prec_dur</th>\n",
       "      <th>low_prec_freq</th>\n",
       "      <th>low_prec_dur</th>\n",
       "      <th>...</th>\n",
       "      <th>silt_frac</th>\n",
       "      <th>clay_frac</th>\n",
       "      <th>elev_mean</th>\n",
       "      <th>slope_mean</th>\n",
       "      <th>area_geospa_fabric</th>\n",
       "      <th>frac_forest</th>\n",
       "      <th>lai_max</th>\n",
       "      <th>lai_diff</th>\n",
       "      <th>gvf_max</th>\n",
       "      <th>gvf_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1013500</td>\n",
       "      <td>3.126679</td>\n",
       "      <td>1.971555</td>\n",
       "      <td>0.187940</td>\n",
       "      <td>0.313440</td>\n",
       "      <td>0.630559</td>\n",
       "      <td>12.95</td>\n",
       "      <td>1.348958</td>\n",
       "      <td>202.20</td>\n",
       "      <td>3.427119</td>\n",
       "      <td>...</td>\n",
       "      <td>55.156940</td>\n",
       "      <td>16.275732</td>\n",
       "      <td>250.31</td>\n",
       "      <td>21.64152</td>\n",
       "      <td>2303.95</td>\n",
       "      <td>0.9063</td>\n",
       "      <td>4.167304</td>\n",
       "      <td>3.340732</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.371648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1022500</td>\n",
       "      <td>3.608126</td>\n",
       "      <td>2.119256</td>\n",
       "      <td>-0.114530</td>\n",
       "      <td>0.245259</td>\n",
       "      <td>0.587356</td>\n",
       "      <td>20.55</td>\n",
       "      <td>1.205279</td>\n",
       "      <td>233.65</td>\n",
       "      <td>3.662226</td>\n",
       "      <td>...</td>\n",
       "      <td>28.080937</td>\n",
       "      <td>12.037646</td>\n",
       "      <td>92.68</td>\n",
       "      <td>17.79072</td>\n",
       "      <td>620.38</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>4.871392</td>\n",
       "      <td>3.746692</td>\n",
       "      <td>0.863936</td>\n",
       "      <td>0.337712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1030500</td>\n",
       "      <td>3.274405</td>\n",
       "      <td>2.043594</td>\n",
       "      <td>0.047358</td>\n",
       "      <td>0.277018</td>\n",
       "      <td>0.624111</td>\n",
       "      <td>17.15</td>\n",
       "      <td>1.207746</td>\n",
       "      <td>215.60</td>\n",
       "      <td>3.514262</td>\n",
       "      <td>...</td>\n",
       "      <td>51.779182</td>\n",
       "      <td>14.776824</td>\n",
       "      <td>143.80</td>\n",
       "      <td>12.79195</td>\n",
       "      <td>3676.09</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>4.685200</td>\n",
       "      <td>3.665543</td>\n",
       "      <td>0.858502</td>\n",
       "      <td>0.351393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1031500</td>\n",
       "      <td>3.522957</td>\n",
       "      <td>2.071324</td>\n",
       "      <td>0.104091</td>\n",
       "      <td>0.291836</td>\n",
       "      <td>0.587950</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1.148936</td>\n",
       "      <td>227.35</td>\n",
       "      <td>3.473644</td>\n",
       "      <td>...</td>\n",
       "      <td>50.841232</td>\n",
       "      <td>12.654125</td>\n",
       "      <td>247.80</td>\n",
       "      <td>29.56035</td>\n",
       "      <td>766.53</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>4.903259</td>\n",
       "      <td>3.990843</td>\n",
       "      <td>0.870668</td>\n",
       "      <td>0.398619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1047000</td>\n",
       "      <td>3.323146</td>\n",
       "      <td>2.090024</td>\n",
       "      <td>0.147776</td>\n",
       "      <td>0.280118</td>\n",
       "      <td>0.628929</td>\n",
       "      <td>20.10</td>\n",
       "      <td>1.165217</td>\n",
       "      <td>235.90</td>\n",
       "      <td>3.691706</td>\n",
       "      <td>...</td>\n",
       "      <td>34.185443</td>\n",
       "      <td>10.303622</td>\n",
       "      <td>310.38</td>\n",
       "      <td>49.92122</td>\n",
       "      <td>904.94</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>5.086811</td>\n",
       "      <td>4.300978</td>\n",
       "      <td>0.891383</td>\n",
       "      <td>0.445473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>14309500</td>\n",
       "      <td>4.977781</td>\n",
       "      <td>3.122204</td>\n",
       "      <td>-0.995847</td>\n",
       "      <td>0.061255</td>\n",
       "      <td>0.627228</td>\n",
       "      <td>15.10</td>\n",
       "      <td>1.776471</td>\n",
       "      <td>222.65</td>\n",
       "      <td>6.893189</td>\n",
       "      <td>...</td>\n",
       "      <td>38.879406</td>\n",
       "      <td>23.213862</td>\n",
       "      <td>709.83</td>\n",
       "      <td>110.42527</td>\n",
       "      <td>226.31</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.227902</td>\n",
       "      <td>1.986325</td>\n",
       "      <td>0.883414</td>\n",
       "      <td>0.115741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>14316700</td>\n",
       "      <td>4.543400</td>\n",
       "      <td>2.277630</td>\n",
       "      <td>-0.821172</td>\n",
       "      <td>0.176337</td>\n",
       "      <td>0.501305</td>\n",
       "      <td>14.75</td>\n",
       "      <td>1.446078</td>\n",
       "      <td>214.85</td>\n",
       "      <td>6.018207</td>\n",
       "      <td>...</td>\n",
       "      <td>38.519396</td>\n",
       "      <td>24.363634</td>\n",
       "      <td>952.26</td>\n",
       "      <td>119.08920</td>\n",
       "      <td>588.01</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.859652</td>\n",
       "      <td>2.828735</td>\n",
       "      <td>0.914354</td>\n",
       "      <td>0.171176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>14325000</td>\n",
       "      <td>6.297437</td>\n",
       "      <td>2.434652</td>\n",
       "      <td>-0.952055</td>\n",
       "      <td>0.030203</td>\n",
       "      <td>0.386610</td>\n",
       "      <td>14.60</td>\n",
       "      <td>1.467337</td>\n",
       "      <td>219.05</td>\n",
       "      <td>6.240741</td>\n",
       "      <td>...</td>\n",
       "      <td>40.860260</td>\n",
       "      <td>20.068726</td>\n",
       "      <td>656.53</td>\n",
       "      <td>124.96889</td>\n",
       "      <td>444.92</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.150730</td>\n",
       "      <td>1.867148</td>\n",
       "      <td>0.873517</td>\n",
       "      <td>0.115977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>14362250</td>\n",
       "      <td>2.781676</td>\n",
       "      <td>3.325188</td>\n",
       "      <td>-0.985486</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>1.195390</td>\n",
       "      <td>20.45</td>\n",
       "      <td>1.786026</td>\n",
       "      <td>260.35</td>\n",
       "      <td>7.354520</td>\n",
       "      <td>...</td>\n",
       "      <td>39.602460</td>\n",
       "      <td>22.404372</td>\n",
       "      <td>875.67</td>\n",
       "      <td>109.93127</td>\n",
       "      <td>43.88</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.430338</td>\n",
       "      <td>2.451489</td>\n",
       "      <td>0.868294</td>\n",
       "      <td>0.117102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>14400000</td>\n",
       "      <td>5.556071</td>\n",
       "      <td>2.279668</td>\n",
       "      <td>-1.015946</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.410302</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.537849</td>\n",
       "      <td>237.00</td>\n",
       "      <td>6.909621</td>\n",
       "      <td>...</td>\n",
       "      <td>42.214515</td>\n",
       "      <td>27.805122</td>\n",
       "      <td>625.31</td>\n",
       "      <td>98.81802</td>\n",
       "      <td>703.37</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.218271</td>\n",
       "      <td>1.430863</td>\n",
       "      <td>0.743440</td>\n",
       "      <td>0.097405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gauge_id    p_mean  pet_mean  p_seasonality  frac_snow   aridity  \\\n",
       "0     1013500  3.126679  1.971555       0.187940   0.313440  0.630559   \n",
       "1     1022500  3.608126  2.119256      -0.114530   0.245259  0.587356   \n",
       "2     1030500  3.274405  2.043594       0.047358   0.277018  0.624111   \n",
       "3     1031500  3.522957  2.071324       0.104091   0.291836  0.587950   \n",
       "4     1047000  3.323146  2.090024       0.147776   0.280118  0.628929   \n",
       "..        ...       ...       ...            ...        ...       ...   \n",
       "666  14309500  4.977781  3.122204      -0.995847   0.061255  0.627228   \n",
       "667  14316700  4.543400  2.277630      -0.821172   0.176337  0.501305   \n",
       "668  14325000  6.297437  2.434652      -0.952055   0.030203  0.386610   \n",
       "669  14362250  2.781676  3.325188      -0.985486   0.141500  1.195390   \n",
       "670  14400000  5.556071  2.279668      -1.015946   0.024330  0.410302   \n",
       "\n",
       "     high_prec_freq  high_prec_dur  low_prec_freq  low_prec_dur  ...  \\\n",
       "0             12.95       1.348958         202.20      3.427119  ...   \n",
       "1             20.55       1.205279         233.65      3.662226  ...   \n",
       "2             17.15       1.207746         215.60      3.514262  ...   \n",
       "3             18.90       1.148936         227.35      3.473644  ...   \n",
       "4             20.10       1.165217         235.90      3.691706  ...   \n",
       "..              ...            ...            ...           ...  ...   \n",
       "666           15.10       1.776471         222.65      6.893189  ...   \n",
       "667           14.75       1.446078         214.85      6.018207  ...   \n",
       "668           14.60       1.467337         219.05      6.240741  ...   \n",
       "669           20.45       1.786026         260.35      7.354520  ...   \n",
       "670           19.30       1.537849         237.00      6.909621  ...   \n",
       "\n",
       "     silt_frac  clay_frac  elev_mean  slope_mean  area_geospa_fabric  \\\n",
       "0    55.156940  16.275732     250.31    21.64152             2303.95   \n",
       "1    28.080937  12.037646      92.68    17.79072              620.38   \n",
       "2    51.779182  14.776824     143.80    12.79195             3676.09   \n",
       "3    50.841232  12.654125     247.80    29.56035              766.53   \n",
       "4    34.185443  10.303622     310.38    49.92122              904.94   \n",
       "..         ...        ...        ...         ...                 ...   \n",
       "666  38.879406  23.213862     709.83   110.42527              226.31   \n",
       "667  38.519396  24.363634     952.26   119.08920              588.01   \n",
       "668  40.860260  20.068726     656.53   124.96889              444.92   \n",
       "669  39.602460  22.404372     875.67   109.93127               43.88   \n",
       "670  42.214515  27.805122     625.31    98.81802              703.37   \n",
       "\n",
       "     frac_forest   lai_max  lai_diff   gvf_max  gvf_diff  \n",
       "0         0.9063  4.167304  3.340732  0.804567  0.371648  \n",
       "1         0.9232  4.871392  3.746692  0.863936  0.337712  \n",
       "2         0.8782  4.685200  3.665543  0.858502  0.351393  \n",
       "3         0.9548  4.903259  3.990843  0.870668  0.398619  \n",
       "4         0.9906  5.086811  4.300978  0.891383  0.445473  \n",
       "..           ...       ...       ...       ...       ...  \n",
       "666       1.0000  4.227902  1.986325  0.883414  0.115741  \n",
       "667       1.0000  4.859652  2.828735  0.914354  0.171176  \n",
       "668       1.0000  4.150730  1.867148  0.873517  0.115977  \n",
       "669       1.0000  4.430338  2.451489  0.868294  0.117102  \n",
       "670       1.0000  3.218271  1.430863  0.743440  0.097405  \n",
       "\n",
       "[671 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1586610.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[i, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.30466119096509, 2.54404796714579, 0.08026925136315599,\n",
       "       0.088413478566147, 0.769836246481542, 22.4, 1.20107238605898,\n",
       "       'son', 249.1, 4.2948275862069, 'son', 'Metamorphics',\n",
       "       0.961900930450197, 'Basic volcanic rocks', 0.0380990695498032, 0.0,\n",
       "       0.013, -14.039, 1.16319464933894, 0.35198605306925296,\n",
       "       1.49041360174604, 0.661737774386697, 1.22591311720456,\n",
       "       0.235571235472374, 3.02877302750195, 3.5, 1.12903225806452, 18.1,\n",
       "       9.28205128205128, 0.0, 168.5, 2, 'MORGAN RUN NEAR LOUISVILLE, MD',\n",
       "       1.01680672268908, 1.35970209358477, 0.455757721489515,\n",
       "       1.16555746012633, 0.576391109817708, 29.0265605450293,\n",
       "       49.23108413474521, 16.0880342853558, 0.0, 0.0, 5.611916852655572,\n",
       "       39.45189, -76.95531, 196.88, 11.85286, 72.7, 72.81, 0.3247,\n",
       "       2.51629324989542, 2.0915919074814, 0.7268470895212249,\n",
       "       0.40583064071036296, 1.0, '    cropland/natural vegetation mosaic',\n",
       "       0.18, 1.5, 'hru_01586610'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(attributes.iloc[np.where([attributes['gauge_id'] == xx[i, 0]])[1][0], 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pos_tr = np.zeros((32, attributes_numeric.shape[1] - 1, tar.shape[1]))\n",
    "xx = basin_l[:32, :]\n",
    "for i in range(xx.shape[0]):\n",
    "    batch_pos_tr[i, :, :] = np.concatenate(((np.repeat(np.array(attributes_numeric.iloc[np.where\n",
    "                                                            ([attributes_numeric['gauge_id'] == xx[i, 0]])[1][0], 1:]).reshape(1, -1), 25)).reshape(-1, 25), (np.repeat(np.array(attributes_numeric.iloc[np.where\n",
    "                                                                                                                                                                                                        ([attributes_numeric['gauge_id'] == xx[i, -1]])[1][0], 1:]).reshape(1, -1), 25)).reshape(-1, 25)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
