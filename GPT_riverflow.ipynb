{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import river_model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from inference import infer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_numeric = pd.read_csv('/Users/omernivron/Downloads/att_numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pp = np.load('/Users/omernivron/Downloads/river_processed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pe = np.load('/Users/omernivron/Downloads/river_processed_te.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_river'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, train_loss, m_tr, token_pos, time_pos, time_pos2, pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_log_sig = decoder(token_pos, time_pos, time_pos2, pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_log_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_log_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, test_loss, m_te, token_pos_te, time_pos_te, time_pos2_te, pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_log_sig_te = decoder(token_pos_te, time_pos_te, time_pos2_te, pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_log_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_log_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_tr = arr_pp[1::5]; t2_tr = arr_pp[2::5]\n",
    "tar_tr = arr_pp[0::5];\n",
    "token_tr = arr_pp[3::5]; basin_l_tr = arr_pp[4::5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_te = arr_pe[1::5]; t2_te = arr_pe[2::5]\n",
    "tar_te = arr_pe[0::5];\n",
    "token_te = arr_pe[3::5]; basin_l_te = arr_pe[4::5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Restored from /Users/omernivron/Downloads/GPT_river/ckpt/check_0/ckpt-41\n",
      "Epoch 0 batch 0 train Loss 83.1630 test Loss 8.6789 with MSE metric 160927.1250\n",
      "Epoch 0 batch 100 train Loss 58.8439 test Loss 7.9786 with MSE metric 250305.1875\n",
      "Epoch 0 batch 200 train Loss 41.8898 test Loss 8.3989 with MSE metric 240539.0312\n",
      "Epoch 0 batch 300 train Loss 236.0662 test Loss 8.1602 with MSE metric 105619.0469\n",
      "Time taken for 1 epoch: 63.16648006439209 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 76.7884 test Loss 8.1410 with MSE metric 100045.8750\n",
      "Epoch 1 batch 100 train Loss 150.8243 test Loss 8.1329 with MSE metric 159931.4531\n",
      "Epoch 1 batch 200 train Loss 40.7146 test Loss 8.5260 with MSE metric 109778.3125\n",
      "Epoch 1 batch 300 train Loss 10.7738 test Loss 8.5501 with MSE metric 47570.4883\n",
      "Time taken for 1 epoch: 61.53780198097229 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 78.3459 test Loss 7.9623 with MSE metric 125895.0156\n",
      "Epoch 2 batch 100 train Loss 14.4509 test Loss 8.6691 with MSE metric 107578.5000\n",
      "Epoch 2 batch 200 train Loss 13.4063 test Loss 8.3893 with MSE metric 46347.6992\n",
      "Epoch 2 batch 300 train Loss 29.5005 test Loss 8.3111 with MSE metric 228121.1719\n",
      "Time taken for 1 epoch: 61.56362271308899 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 17.7052 test Loss 8.8715 with MSE metric 168599.8438\n",
      "Epoch 3 batch 100 train Loss 13.2567 test Loss 8.5051 with MSE metric 192830.8594\n",
      "Epoch 3 batch 200 train Loss 46.6247 test Loss 9.3243 with MSE metric 136555.4531\n",
      "Epoch 3 batch 300 train Loss 9.2248 test Loss 8.5570 with MSE metric 63302.3906\n",
      "Time taken for 1 epoch: 61.74731993675232 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 13.2024 test Loss 8.5311 with MSE metric 119713.9375\n",
      "Epoch 4 batch 100 train Loss 23.9311 test Loss 8.4176 with MSE metric 208724.8125\n",
      "Epoch 4 batch 200 train Loss 11.6304 test Loss 8.2454 with MSE metric 175356.5312\n",
      "Epoch 4 batch 300 train Loss 12.9689 test Loss 8.6390 with MSE metric 140777.7344\n",
      "Time taken for 1 epoch: 61.845256090164185 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 24.8607 test Loss 8.5428 with MSE metric 157196.2969\n",
      "Epoch 5 batch 100 train Loss 10.5812 test Loss 8.4863 with MSE metric 191322.1562\n",
      "Epoch 5 batch 200 train Loss 13.6318 test Loss 8.6706 with MSE metric 151202.4062\n",
      "Epoch 5 batch 300 train Loss 10.4170 test Loss 8.9435 with MSE metric 152997.2812\n",
      "Time taken for 1 epoch: 63.17829990386963 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 25.5368 test Loss 8.5641 with MSE metric 155643.4062\n",
      "Epoch 6 batch 100 train Loss 15.7982 test Loss 8.6979 with MSE metric 207329.3125\n",
      "Epoch 6 batch 200 train Loss 9.6783 test Loss 8.7579 with MSE metric 131163.4062\n",
      "Epoch 6 batch 300 train Loss 85.8309 test Loss 8.7217 with MSE metric 160816.9062\n",
      "Time taken for 1 epoch: 62.93301320075989 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 15.3442 test Loss 8.5664 with MSE metric 200243.7344\n",
      "Epoch 7 batch 100 train Loss 12.4908 test Loss 8.7352 with MSE metric 152965.3906\n",
      "Epoch 7 batch 200 train Loss 8.5205 test Loss 8.8400 with MSE metric 67307.3203\n",
      "Epoch 7 batch 300 train Loss 9.3072 test Loss 8.9683 with MSE metric 137416.9688\n",
      "Time taken for 1 epoch: 62.7177369594574 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 15.2517 test Loss 9.3577 with MSE metric 203004.0781\n",
      "Epoch 8 batch 100 train Loss 8.3106 test Loss 9.0932 with MSE metric 82852.3516\n",
      "Epoch 8 batch 200 train Loss 15.1105 test Loss 9.0378 with MSE metric 98000.5469\n",
      "Epoch 8 batch 300 train Loss 25.6297 test Loss 8.9791 with MSE metric 179860.0938\n",
      "Time taken for 1 epoch: 63.09479904174805 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 10.8544 test Loss 8.8548 with MSE metric 169107.9531\n",
      "Epoch 9 batch 100 train Loss 14.1708 test Loss 9.0931 with MSE metric 110337.8828\n",
      "Epoch 9 batch 200 train Loss 9.1727 test Loss 8.8533 with MSE metric 107631.9844\n",
      "Epoch 9 batch 300 train Loss 9.3918 test Loss 8.8037 with MSE metric 86899.7500\n",
      "Time taken for 1 epoch: 63.210155963897705 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 15.0379 test Loss 9.1177 with MSE metric 141779.1562\n",
      "Epoch 10 batch 100 train Loss 12.4378 test Loss 8.9684 with MSE metric 56382.3633\n",
      "Epoch 10 batch 200 train Loss 20.0490 test Loss 8.9101 with MSE metric 170613.9375\n",
      "Epoch 10 batch 300 train Loss 17.5645 test Loss 9.4152 with MSE metric 108112.4844\n",
      "Time taken for 1 epoch: 63.01667499542236 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 10.9257 test Loss 8.8130 with MSE metric 119544.1406\n",
      "Epoch 11 batch 100 train Loss 9.9955 test Loss 9.4279 with MSE metric 139549.3125\n",
      "Epoch 11 batch 200 train Loss 8.8163 test Loss 9.2229 with MSE metric 158497.0781\n",
      "Epoch 11 batch 300 train Loss 9.1317 test Loss 9.1781 with MSE metric 123231.9531\n",
      "Time taken for 1 epoch: 63.23599195480347 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 8.4922 test Loss 9.2706 with MSE metric 114321.0078\n",
      "Epoch 12 batch 100 train Loss 10.8872 test Loss 9.6224 with MSE metric 199451.0938\n",
      "Epoch 12 batch 200 train Loss 8.7791 test Loss 9.5285 with MSE metric 87576.8984\n",
      "Epoch 12 batch 300 train Loss 10.6133 test Loss 9.7044 with MSE metric 114100.6562\n",
      "Time taken for 1 epoch: 63.08723497390747 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 7.9693 test Loss 9.3410 with MSE metric 131988.2812\n",
      "Epoch 13 batch 100 train Loss 7.9017 test Loss 9.7085 with MSE metric 77769.0469\n",
      "Epoch 13 batch 200 train Loss 8.0095 test Loss 9.2425 with MSE metric 106675.3672\n",
      "Epoch 13 batch 300 train Loss 12.3750 test Loss 9.8716 with MSE metric 268481.8125\n",
      "Time taken for 1 epoch: 63.23317909240723 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 9.7340 test Loss 9.5012 with MSE metric 154576.6875\n",
      "Epoch 14 batch 100 train Loss 8.5103 test Loss 9.9640 with MSE metric 182419.5781\n",
      "Epoch 14 batch 200 train Loss 8.1333 test Loss 9.6029 with MSE metric 125609.2500\n",
      "Epoch 14 batch 300 train Loss 8.5604 test Loss 9.9175 with MSE metric 143568.2500\n",
      "Time taken for 1 epoch: 62.802704095840454 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 9.0355 test Loss 9.5277 with MSE metric 127439.2188\n",
      "Epoch 15 batch 100 train Loss 8.5026 test Loss 9.4301 with MSE metric 182004.8125\n",
      "Epoch 15 batch 200 train Loss 9.2670 test Loss 9.6832 with MSE metric 257743.5000\n",
      "Epoch 15 batch 300 train Loss 8.5717 test Loss 9.7434 with MSE metric 235757.7500\n",
      "Time taken for 1 epoch: 62.56944274902344 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 8.9126 test Loss 9.2774 with MSE metric 121242.0547\n",
      "Epoch 16 batch 100 train Loss 8.0302 test Loss 9.6026 with MSE metric 133178.4062\n",
      "Epoch 16 batch 200 train Loss 7.9620 test Loss 9.8661 with MSE metric 145722.4375\n",
      "Epoch 16 batch 300 train Loss 9.0250 test Loss 9.7810 with MSE metric 148149.4688\n",
      "Time taken for 1 epoch: 61.51349902153015 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 8.7135 test Loss 9.9312 with MSE metric 120747.6719\n",
      "Epoch 17 batch 100 train Loss 8.3851 test Loss 10.1290 with MSE metric 184980.3594\n",
      "Epoch 17 batch 200 train Loss 8.0652 test Loss 10.0798 with MSE metric 101305.0703\n",
      "Epoch 17 batch 300 train Loss 8.1152 test Loss 9.7417 with MSE metric 132932.6562\n",
      "Time taken for 1 epoch: 63.66710805892944 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 8.5847 test Loss 9.8068 with MSE metric 134053.9219\n",
      "Epoch 18 batch 100 train Loss 7.7470 test Loss 9.4670 with MSE metric 50736.3164\n",
      "Epoch 18 batch 200 train Loss 8.3915 test Loss 10.1335 with MSE metric 203701.2344\n",
      "Epoch 18 batch 300 train Loss 7.8641 test Loss 9.2665 with MSE metric 59129.4648\n",
      "Time taken for 1 epoch: 67.06323885917664 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 9.2875 test Loss 10.0225 with MSE metric 133192.4062\n",
      "Epoch 19 batch 100 train Loss 7.9787 test Loss 9.7445 with MSE metric 80578.0625\n",
      "Epoch 19 batch 200 train Loss 9.1815 test Loss 9.3530 with MSE metric 150201.3594\n",
      "Epoch 19 batch 300 train Loss 7.9028 test Loss 9.8138 with MSE metric 45031.9609\n",
      "Time taken for 1 epoch: 64.31188201904297 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 8.0046 test Loss 10.1873 with MSE metric 91623.8828\n",
      "Epoch 20 batch 100 train Loss 8.4546 test Loss 9.4104 with MSE metric 199841.3281\n",
      "Epoch 20 batch 200 train Loss 8.0660 test Loss 9.8785 with MSE metric 71054.3203\n",
      "Epoch 20 batch 300 train Loss 7.9466 test Loss 9.9364 with MSE metric 117244.1641\n",
      "Time taken for 1 epoch: 65.50989532470703 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 8.5286 test Loss 10.0028 with MSE metric 153398.6250\n",
      "Epoch 21 batch 100 train Loss 8.1302 test Loss 9.9179 with MSE metric 121595.0547\n",
      "Epoch 21 batch 200 train Loss 8.1319 test Loss 9.6418 with MSE metric 141015.8125\n",
      "Epoch 21 batch 300 train Loss 8.2985 test Loss 9.6631 with MSE metric 270738.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 64.52017211914062 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 8.5261 test Loss 9.8804 with MSE metric 199715.1562\n",
      "Epoch 22 batch 100 train Loss 7.8528 test Loss 10.1375 with MSE metric 29103.1953\n",
      "Epoch 22 batch 200 train Loss 7.7125 test Loss 10.1581 with MSE metric 30892.5918\n",
      "Epoch 22 batch 300 train Loss 8.0753 test Loss 9.7710 with MSE metric 182269.4531\n",
      "Time taken for 1 epoch: 64.57459998130798 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 9.0407 test Loss 9.9144 with MSE metric 164771.1250\n",
      "Epoch 23 batch 100 train Loss 9.0602 test Loss 9.6778 with MSE metric 249782.6250\n",
      "Epoch 23 batch 200 train Loss 15.4989 test Loss 9.9605 with MSE metric 232768.0625\n",
      "Epoch 23 batch 300 train Loss 8.3160 test Loss 9.8987 with MSE metric 163710.0938\n",
      "Time taken for 1 epoch: 62.251121282577515 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 8.2905 test Loss 9.6634 with MSE metric 272372.7500\n",
      "Epoch 24 batch 100 train Loss 9.0512 test Loss 9.7734 with MSE metric 88452.9062\n",
      "Epoch 24 batch 200 train Loss 8.1791 test Loss 9.7042 with MSE metric 188092.9375\n",
      "Epoch 24 batch 300 train Loss 8.0690 test Loss 10.1206 with MSE metric 107911.8594\n",
      "Time taken for 1 epoch: 62.2952241897583 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 8.1312 test Loss 9.6480 with MSE metric 188980.8125\n",
      "Epoch 25 batch 100 train Loss 8.9261 test Loss 9.8722 with MSE metric 170913.7188\n",
      "Epoch 25 batch 200 train Loss 7.7959 test Loss 9.3965 with MSE metric 88852.6172\n",
      "Epoch 25 batch 300 train Loss 9.4199 test Loss 9.5168 with MSE metric 78632.7031\n",
      "Time taken for 1 epoch: 62.25585579872131 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 8.8182 test Loss 9.6432 with MSE metric 107429.5078\n",
      "Epoch 26 batch 100 train Loss 8.0692 test Loss 9.2661 with MSE metric 143314.3125\n",
      "Epoch 26 batch 200 train Loss 7.9705 test Loss 9.4271 with MSE metric 123280.7031\n",
      "Epoch 26 batch 300 train Loss 7.9319 test Loss 9.4505 with MSE metric 133707.4062\n",
      "Time taken for 1 epoch: 63.073951959609985 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 7.8931 test Loss 9.7606 with MSE metric 86787.4688\n",
      "Epoch 27 batch 100 train Loss 7.8897 test Loss 9.4717 with MSE metric 97484.5000\n",
      "Epoch 27 batch 200 train Loss 8.2148 test Loss 9.6184 with MSE metric 205475.6406\n",
      "Epoch 27 batch 300 train Loss 7.9315 test Loss 9.4177 with MSE metric 157804.9375\n",
      "Time taken for 1 epoch: 62.20002317428589 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 8.0967 test Loss 9.4608 with MSE metric 164243.7188\n",
      "Epoch 28 batch 100 train Loss 7.8695 test Loss 9.1956 with MSE metric 131724.0156\n",
      "Epoch 28 batch 200 train Loss 11.2875 test Loss 9.2488 with MSE metric 367715.6562\n",
      "Epoch 28 batch 300 train Loss 8.1268 test Loss 9.5489 with MSE metric 192072.0625\n",
      "Time taken for 1 epoch: 62.837936878204346 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 7.8176 test Loss 9.1040 with MSE metric 110894.8438\n",
      "Epoch 29 batch 100 train Loss 7.8905 test Loss 8.9306 with MSE metric 94969.6953\n",
      "Epoch 29 batch 200 train Loss 8.0370 test Loss 9.1446 with MSE metric 108247.1016\n",
      "Epoch 29 batch 300 train Loss 7.8671 test Loss 9.3859 with MSE metric 136855.4844\n",
      "Time taken for 1 epoch: 62.529123067855835 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 7.9115 test Loss 9.2752 with MSE metric 183567.0938\n",
      "Epoch 30 batch 100 train Loss 7.7630 test Loss 8.9141 with MSE metric 139184.3750\n",
      "Epoch 30 batch 200 train Loss 8.0090 test Loss 9.3939 with MSE metric 173672.3750\n",
      "Epoch 30 batch 300 train Loss 8.0299 test Loss 9.6267 with MSE metric 202078.4688\n",
      "Time taken for 1 epoch: 62.15745306015015 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 7.8340 test Loss 8.8229 with MSE metric 126486.3281\n",
      "Epoch 31 batch 100 train Loss 7.5463 test Loss 9.1047 with MSE metric 48688.4375\n",
      "Epoch 31 batch 200 train Loss 7.7203 test Loss 9.2126 with MSE metric 146618.6562\n",
      "Epoch 31 batch 300 train Loss 7.5381 test Loss 9.1601 with MSE metric 75871.9688\n",
      "Time taken for 1 epoch: 62.71426820755005 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 7.7221 test Loss 9.0660 with MSE metric 120296.7578\n",
      "Epoch 32 batch 100 train Loss 7.6574 test Loss 9.2042 with MSE metric 85008.8438\n",
      "Epoch 32 batch 200 train Loss 7.4894 test Loss 8.9546 with MSE metric 61574.4609\n",
      "Epoch 32 batch 300 train Loss 7.3571 test Loss 9.2358 with MSE metric 46216.6328\n",
      "Time taken for 1 epoch: 62.41387987136841 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 7.8501 test Loss 9.1264 with MSE metric 148486.1250\n",
      "Epoch 33 batch 100 train Loss 7.9198 test Loss 9.0943 with MSE metric 186956.5469\n",
      "Epoch 33 batch 200 train Loss 7.8867 test Loss 9.6845 with MSE metric 160478.2969\n",
      "Epoch 33 batch 300 train Loss 8.1819 test Loss 9.2963 with MSE metric 202958.1562\n",
      "Time taken for 1 epoch: 63.211352825164795 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 7.7129 test Loss 8.6537 with MSE metric 129901.2266\n",
      "Epoch 34 batch 100 train Loss 7.3957 test Loss 8.7655 with MSE metric 58197.6641\n",
      "Epoch 34 batch 200 train Loss 8.1353 test Loss 8.7844 with MSE metric 302161.5625\n",
      "Epoch 34 batch 300 train Loss 7.5580 test Loss 8.8636 with MSE metric 91266.3984\n",
      "Time taken for 1 epoch: 62.224446058273315 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 7.7216 test Loss 8.5728 with MSE metric 129131.0312\n",
      "Epoch 35 batch 100 train Loss 7.3389 test Loss 8.7480 with MSE metric 62149.6367\n",
      "Epoch 35 batch 200 train Loss 7.8779 test Loss 8.4969 with MSE metric 193817.1719\n",
      "Epoch 35 batch 300 train Loss 7.6100 test Loss 8.9774 with MSE metric 141396.5938\n",
      "Time taken for 1 epoch: 62.97720813751221 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 7.5328 test Loss 8.5108 with MSE metric 122248.2188\n",
      "Epoch 36 batch 100 train Loss 7.5090 test Loss 8.4058 with MSE metric 130095.2969\n",
      "Epoch 36 batch 200 train Loss 7.3363 test Loss 8.6136 with MSE metric 78955.4922\n",
      "Epoch 36 batch 300 train Loss 7.5155 test Loss 8.4451 with MSE metric 115004.2812\n",
      "Time taken for 1 epoch: 62.8551287651062 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 7.2103 test Loss 8.3675 with MSE metric 64513.1211\n",
      "Epoch 37 batch 100 train Loss 7.2210 test Loss 8.8241 with MSE metric 69173.9062\n",
      "Epoch 37 batch 200 train Loss 7.3415 test Loss 8.5644 with MSE metric 78058.0938\n",
      "Epoch 37 batch 300 train Loss 7.8461 test Loss 9.3886 with MSE metric 167042.9062\n",
      "Time taken for 1 epoch: 67.38240313529968 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 7.5673 test Loss 8.4271 with MSE metric 180367.6719\n",
      "Epoch 38 batch 100 train Loss 7.5244 test Loss 8.4329 with MSE metric 158468.5781\n",
      "Epoch 38 batch 200 train Loss 7.3718 test Loss 8.4417 with MSE metric 111903.4219\n",
      "Epoch 38 batch 300 train Loss 7.7793 test Loss 8.4673 with MSE metric 223474.5312\n",
      "Time taken for 1 epoch: 63.84957003593445 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 7.4409 test Loss 8.8043 with MSE metric 146403.7500\n",
      "Epoch 39 batch 100 train Loss 7.6168 test Loss 8.4763 with MSE metric 182525.1094\n",
      "Epoch 39 batch 200 train Loss 7.6074 test Loss 8.1740 with MSE metric 212455.3438\n",
      "Epoch 39 batch 300 train Loss 7.2482 test Loss 8.3865 with MSE metric 97572.2812\n",
      "Time taken for 1 epoch: 62.577528953552246 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 7.6051 test Loss 8.4176 with MSE metric 213509.4688\n",
      "Epoch 40 batch 100 train Loss 7.2158 test Loss 8.3539 with MSE metric 95722.2266\n",
      "Epoch 40 batch 200 train Loss 7.9065 test Loss 8.3171 with MSE metric 239652.3281\n",
      "Epoch 40 batch 300 train Loss 7.2090 test Loss 8.3783 with MSE metric 124574.2812\n",
      "Time taken for 1 epoch: 63.13227820396423 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 7.3951 test Loss 8.2654 with MSE metric 150726.3281\n",
      "Epoch 41 batch 100 train Loss 7.4272 test Loss 8.4124 with MSE metric 185283.8125\n",
      "Epoch 41 batch 200 train Loss 7.4946 test Loss 8.2360 with MSE metric 177927.4688\n",
      "Epoch 41 batch 300 train Loss 7.1734 test Loss 8.1732 with MSE metric 118159.0000\n",
      "Time taken for 1 epoch: 63.30610513687134 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 7.7812 test Loss 8.2476 with MSE metric 288640.4375\n",
      "Epoch 42 batch 100 train Loss 7.5730 test Loss 8.3509 with MSE metric 220226.1875\n",
      "Epoch 42 batch 200 train Loss 7.2708 test Loss 8.1516 with MSE metric 156021.3438\n",
      "Epoch 42 batch 300 train Loss 7.2853 test Loss 8.0324 with MSE metric 138897.5469\n",
      "Time taken for 1 epoch: 63.62627291679382 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 7.4430 test Loss 8.1487 with MSE metric 219811.3281\n",
      "Epoch 43 batch 100 train Loss 7.1389 test Loss 8.0740 with MSE metric 120101.3125\n",
      "Epoch 43 batch 200 train Loss 7.8349 test Loss 8.3113 with MSE metric 259452.4688\n",
      "Epoch 43 batch 300 train Loss 7.3223 test Loss 8.3888 with MSE metric 201626.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 62.88431692123413 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 6.8936 test Loss 8.0245 with MSE metric 64302.7812\n",
      "Epoch 44 batch 100 train Loss 7.4135 test Loss 8.0743 with MSE metric 232925.3750\n",
      "Epoch 44 batch 200 train Loss 7.3593 test Loss 7.9226 with MSE metric 202448.1562\n",
      "Epoch 44 batch 300 train Loss 7.0075 test Loss 8.2144 with MSE metric 114110.9375\n",
      "Time taken for 1 epoch: 64.08842325210571 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 7.2085 test Loss 8.1187 with MSE metric 178431.4375\n",
      "Epoch 45 batch 100 train Loss 7.0109 test Loss 7.9522 with MSE metric 102456.6719\n",
      "Epoch 45 batch 200 train Loss 6.9413 test Loss 7.8502 with MSE metric 94884.8906\n",
      "Epoch 45 batch 300 train Loss 7.5711 test Loss 7.7912 with MSE metric 278274.5312\n",
      "Time taken for 1 epoch: 62.89453387260437 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 7.0751 test Loss 7.8994 with MSE metric 143181.0781\n",
      "Epoch 46 batch 100 train Loss 7.2929 test Loss 7.9254 with MSE metric 211754.5312\n",
      "Epoch 46 batch 200 train Loss 6.9889 test Loss 7.9065 with MSE metric 119487.1406\n",
      "Epoch 46 batch 300 train Loss 6.9162 test Loss 7.7878 with MSE metric 109479.7031\n",
      "Time taken for 1 epoch: 62.91996383666992 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 7.3065 test Loss 7.7226 with MSE metric 206542.7969\n",
      "Epoch 47 batch 100 train Loss 7.1876 test Loss 7.7321 with MSE metric 182283.7812\n",
      "Epoch 47 batch 200 train Loss 7.1974 test Loss 7.7733 with MSE metric 190907.8438\n",
      "Epoch 47 batch 300 train Loss 6.6560 test Loss 7.6112 with MSE metric 63071.9766\n",
      "Time taken for 1 epoch: 63.31044912338257 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 7.3858 test Loss 7.4922 with MSE metric 234111.5156\n",
      "Epoch 48 batch 100 train Loss 6.9661 test Loss 7.7263 with MSE metric 128902.5781\n",
      "Epoch 48 batch 200 train Loss 6.7015 test Loss 7.6581 with MSE metric 76514.6875\n",
      "Epoch 48 batch 300 train Loss 7.5204 test Loss 7.6545 with MSE metric 308770.5312\n",
      "Time taken for 1 epoch: 62.79880499839783 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 6.8864 test Loss 7.7186 with MSE metric 123026.0234\n",
      "Epoch 49 batch 100 train Loss 7.2519 test Loss 7.5828 with MSE metric 219451.3281\n",
      "Epoch 49 batch 200 train Loss 6.8339 test Loss 7.6950 with MSE metric 113146.5156\n",
      "Epoch 49 batch 300 train Loss 6.7708 test Loss 7.6707 with MSE metric 98116.6875\n",
      "Time taken for 1 epoch: 63.45310401916504 secs\n",
      "\n",
      "Epoch 50 batch 0 train Loss 6.8310 test Loss 7.7012 with MSE metric 113829.1719\n",
      "Epoch 50 batch 100 train Loss 7.0826 test Loss 7.5110 with MSE metric 160405.0938\n",
      "Epoch 50 batch 200 train Loss 6.8353 test Loss 7.7136 with MSE metric 113713.7656\n",
      "Epoch 50 batch 300 train Loss 6.9936 test Loss 7.6642 with MSE metric 167615.8438\n",
      "Time taken for 1 epoch: 62.46979904174805 secs\n",
      "\n",
      "Epoch 51 batch 0 train Loss 7.0376 test Loss 7.6158 with MSE metric 181806.4375\n",
      "Epoch 51 batch 100 train Loss 6.9214 test Loss 7.6348 with MSE metric 146984.2344\n",
      "Epoch 51 batch 200 train Loss 6.7983 test Loss 7.6115 with MSE metric 112963.3594\n",
      "Epoch 51 batch 300 train Loss 6.6848 test Loss 7.4432 with MSE metric 86237.5469\n",
      "Time taken for 1 epoch: 63.241443157196045 secs\n",
      "\n",
      "Epoch 52 batch 0 train Loss 6.6558 test Loss 7.4600 with MSE metric 84765.7266\n",
      "Epoch 52 batch 100 train Loss 7.0635 test Loss 7.5990 with MSE metric 190524.4531\n",
      "Epoch 52 batch 200 train Loss 6.7444 test Loss 7.5627 with MSE metric 104661.6094\n",
      "Epoch 52 batch 300 train Loss 7.0034 test Loss 7.4627 with MSE metric 175194.4531\n",
      "Time taken for 1 epoch: 62.61426091194153 secs\n",
      "\n",
      "Epoch 53 batch 0 train Loss 6.7809 test Loss 7.5214 with MSE metric 116904.1328\n",
      "Epoch 53 batch 100 train Loss 6.8171 test Loss 7.5078 with MSE metric 126115.4766\n",
      "Epoch 53 batch 200 train Loss 7.0686 test Loss 7.5306 with MSE metric 194163.4375\n",
      "Epoch 53 batch 300 train Loss 6.8309 test Loss 7.3886 with MSE metric 133357.4844\n",
      "Time taken for 1 epoch: 62.536373138427734 secs\n",
      "\n",
      "Epoch 54 batch 0 train Loss 6.6184 test Loss 7.4105 with MSE metric 75647.0000\n",
      "Epoch 54 batch 100 train Loss 6.6378 test Loss 7.2769 with MSE metric 88908.5938\n",
      "Epoch 54 batch 200 train Loss 6.6155 test Loss 7.5009 with MSE metric 73839.2969\n",
      "Epoch 54 batch 300 train Loss 7.0453 test Loss 7.4155 with MSE metric 197037.2344\n",
      "Time taken for 1 epoch: 63.55638027191162 secs\n",
      "\n",
      "Epoch 55 batch 0 train Loss 6.9402 test Loss 7.4964 with MSE metric 166823.9062\n",
      "Epoch 55 batch 100 train Loss 6.7127 test Loss 7.4688 with MSE metric 100170.8281\n",
      "Epoch 55 batch 200 train Loss 6.8985 test Loss 7.4253 with MSE metric 156998.7188\n",
      "Epoch 55 batch 300 train Loss 6.7042 test Loss 7.3344 with MSE metric 107656.0078\n",
      "Time taken for 1 epoch: 63.03302192687988 secs\n",
      "\n",
      "Epoch 56 batch 0 train Loss 7.1191 test Loss 7.3919 with MSE metric 207095.6406\n",
      "Epoch 56 batch 100 train Loss 6.8116 test Loss 7.3311 with MSE metric 133831.8750\n",
      "Epoch 56 batch 200 train Loss 6.7258 test Loss 7.2498 with MSE metric 114804.9219\n",
      "Epoch 56 batch 300 train Loss 6.6434 test Loss 7.3984 with MSE metric 80632.8906\n",
      "Time taken for 1 epoch: 63.46632695198059 secs\n",
      "\n",
      "Epoch 57 batch 0 train Loss 6.7390 test Loss 7.3547 with MSE metric 117000.1641\n",
      "Epoch 57 batch 100 train Loss 6.9244 test Loss 7.3396 with MSE metric 169075.4219\n",
      "Epoch 57 batch 200 train Loss 6.6512 test Loss 7.3082 with MSE metric 94580.4062\n",
      "Epoch 57 batch 300 train Loss 6.8121 test Loss 7.3095 with MSE metric 137783.5938\n",
      "Time taken for 1 epoch: 63.14702081680298 secs\n",
      "\n",
      "Epoch 58 batch 0 train Loss 6.8569 test Loss 7.2903 with MSE metric 150147.5000\n",
      "Epoch 58 batch 100 train Loss 6.9547 test Loss 7.5122 with MSE metric 180768.3594\n",
      "Epoch 58 batch 200 train Loss 6.5289 test Loss 7.4937 with MSE metric 47502.9062\n",
      "Epoch 58 batch 300 train Loss 6.6505 test Loss 7.2485 with MSE metric 95030.5547\n",
      "Time taken for 1 epoch: 62.15764784812927 secs\n",
      "\n",
      "Epoch 59 batch 0 train Loss 6.9219 test Loss 7.2207 with MSE metric 160790.3750\n",
      "Epoch 59 batch 100 train Loss 6.6579 test Loss 7.4365 with MSE metric 89351.5312\n",
      "Epoch 59 batch 200 train Loss 7.3499 test Loss 7.2833 with MSE metric 266390.5625\n",
      "Epoch 59 batch 300 train Loss 6.6685 test Loss 7.3165 with MSE metric 99452.9297\n",
      "Time taken for 1 epoch: 62.06078004837036 secs\n",
      "\n",
      "Epoch 60 batch 0 train Loss 6.8082 test Loss 7.3875 with MSE metric 137865.2500\n",
      "Epoch 60 batch 100 train Loss 6.8590 test Loss 7.2425 with MSE metric 151064.0469\n",
      "Epoch 60 batch 200 train Loss 6.7778 test Loss 7.3105 with MSE metric 131496.1719\n",
      "Epoch 60 batch 300 train Loss 6.7588 test Loss 7.4053 with MSE metric 122613.3516\n",
      "Time taken for 1 epoch: 61.85860800743103 secs\n",
      "\n",
      "Epoch 61 batch 0 train Loss 6.8290 test Loss 7.2522 with MSE metric 144484.3125\n",
      "Epoch 61 batch 100 train Loss 6.8362 test Loss 7.1807 with MSE metric 146878.8594\n",
      "Epoch 61 batch 200 train Loss 6.7051 test Loss 7.3457 with MSE metric 106917.8750\n",
      "Epoch 61 batch 300 train Loss 6.5649 test Loss 7.3089 with MSE metric 65139.1797\n",
      "Time taken for 1 epoch: 62.140217304229736 secs\n",
      "\n",
      "Epoch 62 batch 0 train Loss 6.5521 test Loss 7.1465 with MSE metric 80693.5781\n",
      "Epoch 62 batch 100 train Loss 6.6680 test Loss 7.3001 with MSE metric 100848.5391\n",
      "Epoch 62 batch 200 train Loss 7.0690 test Loss 7.2339 with MSE metric 194812.7188\n",
      "Epoch 62 batch 300 train Loss 6.6870 test Loss 7.1703 with MSE metric 111241.2500\n",
      "Time taken for 1 epoch: 61.83950424194336 secs\n",
      "\n",
      "Epoch 63 batch 0 train Loss 6.9008 test Loss 7.1364 with MSE metric 148529.0625\n",
      "Epoch 63 batch 100 train Loss 6.5992 test Loss 7.2576 with MSE metric 83997.2656\n",
      "Epoch 63 batch 200 train Loss 6.6913 test Loss 7.0529 with MSE metric 110547.1250\n",
      "Epoch 63 batch 300 train Loss 7.3825 test Loss 7.3165 with MSE metric 270203.2188\n",
      "Time taken for 1 epoch: 62.955288887023926 secs\n",
      "\n",
      "Epoch 64 batch 0 train Loss 6.5661 test Loss 7.2660 with MSE metric 65344.7578\n",
      "Epoch 64 batch 100 train Loss 6.3251 test Loss 7.1311 with MSE metric 23242.9570\n",
      "Epoch 64 batch 200 train Loss 6.9699 test Loss 7.3902 with MSE metric 191895.1562\n",
      "Epoch 64 batch 300 train Loss 6.5211 test Loss 7.1136 with MSE metric 73537.4531\n",
      "Time taken for 1 epoch: 63.143556118011475 secs\n",
      "\n",
      "Epoch 65 batch 0 train Loss 6.5548 test Loss 7.1852 with MSE metric 76565.3125\n",
      "Epoch 65 batch 100 train Loss 6.5816 test Loss 7.2842 with MSE metric 81592.1562\n",
      "Epoch 65 batch 200 train Loss 6.8997 test Loss 7.2840 with MSE metric 166806.0625\n",
      "Epoch 65 batch 300 train Loss 6.5854 test Loss 7.3768 with MSE metric 60970.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 63.904216051101685 secs\n",
      "\n",
      "Epoch 66 batch 0 train Loss 7.0131 test Loss 7.1855 with MSE metric 193302.4531\n",
      "Epoch 66 batch 100 train Loss 6.8216 test Loss 7.3470 with MSE metric 146461.2188\n",
      "Epoch 66 batch 200 train Loss 6.5990 test Loss 7.0628 with MSE metric 91737.5469\n",
      "Epoch 66 batch 300 train Loss 6.7137 test Loss 7.2263 with MSE metric 112858.1484\n",
      "Time taken for 1 epoch: 62.20226192474365 secs\n",
      "\n",
      "Epoch 67 batch 0 train Loss 6.3593 test Loss 7.1180 with MSE metric 41285.7344\n",
      "Epoch 67 batch 100 train Loss 6.6005 test Loss 6.9896 with MSE metric 95269.5469\n",
      "Epoch 67 batch 200 train Loss 7.3718 test Loss 7.0102 with MSE metric 246492.2031\n",
      "Epoch 67 batch 300 train Loss 6.9538 test Loss 7.3461 with MSE metric 190247.0312\n",
      "Time taken for 1 epoch: 61.64045429229736 secs\n",
      "\n",
      "Epoch 68 batch 0 train Loss 7.0259 test Loss 7.1513 with MSE metric 204596.1406\n",
      "Epoch 68 batch 100 train Loss 6.6426 test Loss 7.1499 with MSE metric 100391.3750\n",
      "Epoch 68 batch 200 train Loss 6.8268 test Loss 7.1664 with MSE metric 148162.9844\n",
      "Epoch 68 batch 300 train Loss 7.1582 test Loss 7.0156 with MSE metric 215003.1719\n",
      "Time taken for 1 epoch: 60.67782711982727 secs\n",
      "\n",
      "Epoch 69 batch 0 train Loss 6.8893 test Loss 7.3704 with MSE metric 162086.4062\n",
      "Epoch 69 batch 100 train Loss 6.9697 test Loss 7.3654 with MSE metric 197843.2656\n",
      "Epoch 69 batch 200 train Loss 6.6286 test Loss 7.1851 with MSE metric 89303.5391\n",
      "Epoch 69 batch 300 train Loss 6.4093 test Loss 7.0562 with MSE metric 35630.7109\n",
      "Time taken for 1 epoch: 61.08700370788574 secs\n",
      "\n",
      "Epoch 70 batch 0 train Loss 7.3307 test Loss 7.0401 with MSE metric 260506.8594\n",
      "Epoch 70 batch 100 train Loss 6.5565 test Loss 7.0336 with MSE metric 80775.0391\n",
      "Epoch 70 batch 200 train Loss 6.9293 test Loss 7.0755 with MSE metric 166508.8750\n",
      "Epoch 70 batch 300 train Loss 6.6075 test Loss 7.0821 with MSE metric 75974.6094\n",
      "Time taken for 1 epoch: 61.670161962509155 secs\n",
      "\n",
      "Epoch 71 batch 0 train Loss 6.5546 test Loss 7.0335 with MSE metric 82725.3750\n",
      "Epoch 71 batch 100 train Loss 7.1097 test Loss 7.0537 with MSE metric 202457.7188\n",
      "Epoch 71 batch 200 train Loss 6.8711 test Loss 7.0551 with MSE metric 162369.4062\n",
      "Epoch 71 batch 300 train Loss 6.6899 test Loss 6.9885 with MSE metric 112445.4844\n",
      "Time taken for 1 epoch: 61.849485874176025 secs\n",
      "\n",
      "Epoch 72 batch 0 train Loss 6.7172 test Loss 6.9463 with MSE metric 121067.7344\n",
      "Epoch 72 batch 100 train Loss 6.6695 test Loss 7.0615 with MSE metric 109720.0156\n",
      "Epoch 72 batch 200 train Loss 6.5079 test Loss 6.9950 with MSE metric 63076.4141\n",
      "Epoch 72 batch 300 train Loss 6.6056 test Loss 7.0473 with MSE metric 78461.3281\n",
      "Time taken for 1 epoch: 61.23563098907471 secs\n",
      "\n",
      "Epoch 73 batch 0 train Loss 6.7210 test Loss 6.9365 with MSE metric 119511.0312\n",
      "Epoch 73 batch 100 train Loss 6.7228 test Loss 7.0332 with MSE metric 120711.3281\n",
      "Epoch 73 batch 200 train Loss 6.6948 test Loss 7.1218 with MSE metric 111703.6719\n",
      "Epoch 73 batch 300 train Loss 6.8003 test Loss 7.0036 with MSE metric 143070.8125\n",
      "Time taken for 1 epoch: 62.4385621547699 secs\n",
      "\n",
      "Epoch 74 batch 0 train Loss 6.6189 test Loss 6.8718 with MSE metric 99221.7188\n",
      "Epoch 74 batch 100 train Loss 6.8495 test Loss 6.8754 with MSE metric 154792.5156\n",
      "Epoch 74 batch 200 train Loss 6.8605 test Loss 6.9653 with MSE metric 147113.8125\n",
      "Epoch 74 batch 300 train Loss 6.7504 test Loss 6.9803 with MSE metric 130332.2031\n",
      "Time taken for 1 epoch: 59.789599895477295 secs\n",
      "\n",
      "Epoch 75 batch 0 train Loss 6.8974 test Loss 6.9138 with MSE metric 159408.5625\n",
      "Epoch 75 batch 100 train Loss 6.6658 test Loss 6.7128 with MSE metric 105498.7344\n",
      "Epoch 75 batch 200 train Loss 6.8390 test Loss 6.9770 with MSE metric 154864.8906\n",
      "Epoch 75 batch 300 train Loss 7.0982 test Loss 6.8778 with MSE metric 213092.8906\n",
      "Time taken for 1 epoch: 63.230380058288574 secs\n",
      "\n",
      "Epoch 76 batch 0 train Loss 7.3838 test Loss 6.8529 with MSE metric 276929.3750\n",
      "Epoch 76 batch 100 train Loss 6.7924 test Loss 7.0270 with MSE metric 121600.8906\n",
      "Epoch 76 batch 200 train Loss 6.3345 test Loss 6.8087 with MSE metric 23351.7969\n",
      "Epoch 76 batch 300 train Loss 6.8463 test Loss 6.9736 with MSE metric 150836.2188\n",
      "Time taken for 1 epoch: 62.845484018325806 secs\n",
      "\n",
      "Epoch 77 batch 0 train Loss 6.6309 test Loss 7.0115 with MSE metric 69327.5312\n",
      "Epoch 77 batch 100 train Loss 6.7309 test Loss 6.6548 with MSE metric 124427.0156\n",
      "Epoch 77 batch 200 train Loss 6.5613 test Loss 6.7842 with MSE metric 81125.2656\n",
      "Epoch 77 batch 300 train Loss 6.7409 test Loss 7.0051 with MSE metric 126104.2266\n",
      "Time taken for 1 epoch: 68.10093069076538 secs\n",
      "\n",
      "Epoch 78 batch 0 train Loss 6.7650 test Loss 6.8817 with MSE metric 134223.4062\n",
      "Epoch 78 batch 100 train Loss 6.7710 test Loss 6.8542 with MSE metric 133660.9844\n",
      "Epoch 78 batch 200 train Loss 6.6528 test Loss 6.8631 with MSE metric 107569.8750\n",
      "Epoch 78 batch 300 train Loss 6.8453 test Loss 7.0356 with MSE metric 154701.5938\n",
      "Time taken for 1 epoch: 68.20289206504822 secs\n",
      "\n",
      "Epoch 79 batch 0 train Loss 6.7394 test Loss 6.8149 with MSE metric 128173.3906\n",
      "Epoch 79 batch 100 train Loss 6.6015 test Loss 7.1388 with MSE metric 55184.4258\n",
      "Epoch 79 batch 200 train Loss 6.6470 test Loss 6.8468 with MSE metric 100428.3047\n",
      "Epoch 79 batch 300 train Loss 6.8404 test Loss 7.1242 with MSE metric 147830.7969\n",
      "Time taken for 1 epoch: 66.98261308670044 secs\n",
      "\n",
      "Epoch 80 batch 0 train Loss 6.8044 test Loss 6.7417 with MSE metric 139747.2969\n",
      "Epoch 80 batch 100 train Loss 6.5570 test Loss 6.8218 with MSE metric 88786.2344\n",
      "Epoch 80 batch 200 train Loss 6.4328 test Loss 6.5437 with MSE metric 67297.3047\n",
      "Epoch 80 batch 300 train Loss 6.4760 test Loss 6.8815 with MSE metric 68683.7344\n",
      "Time taken for 1 epoch: 65.31973576545715 secs\n",
      "\n",
      "Epoch 81 batch 0 train Loss 6.7752 test Loss 6.6642 with MSE metric 135684.3594\n",
      "Epoch 81 batch 100 train Loss 6.7085 test Loss 6.6898 with MSE metric 120745.9219\n",
      "Epoch 81 batch 200 train Loss 7.4968 test Loss 6.6848 with MSE metric 259325.8594\n",
      "Epoch 81 batch 300 train Loss 6.9708 test Loss 6.8006 with MSE metric 180403.9375\n",
      "Time taken for 1 epoch: 66.84284400939941 secs\n",
      "\n",
      "Epoch 82 batch 0 train Loss 6.7889 test Loss 6.8024 with MSE metric 140717.9375\n",
      "Epoch 82 batch 100 train Loss 7.3862 test Loss 6.6427 with MSE metric 227055.2031\n",
      "Epoch 82 batch 200 train Loss 7.2011 test Loss 6.6487 with MSE metric 242286.5000\n",
      "Epoch 82 batch 300 train Loss 6.7002 test Loss 6.8249 with MSE metric 117354.2500\n",
      "Time taken for 1 epoch: 65.75467491149902 secs\n",
      "\n",
      "Epoch 83 batch 0 train Loss 6.6901 test Loss 6.5677 with MSE metric 115628.8047\n",
      "Epoch 83 batch 100 train Loss 6.6942 test Loss 6.6620 with MSE metric 116103.6875\n",
      "Epoch 83 batch 200 train Loss 6.8680 test Loss 6.7416 with MSE metric 148261.7188\n",
      "Epoch 83 batch 300 train Loss 6.7660 test Loss 6.7158 with MSE metric 134587.5312\n",
      "Time taken for 1 epoch: 65.44796085357666 secs\n",
      "\n",
      "Epoch 84 batch 0 train Loss 6.5001 test Loss 6.6467 with MSE metric 79817.3672\n",
      "Epoch 84 batch 100 train Loss 6.9891 test Loss 6.5762 with MSE metric 177849.0625\n",
      "Epoch 84 batch 200 train Loss 6.9587 test Loss 6.6353 with MSE metric 170085.7812\n",
      "Epoch 84 batch 300 train Loss 6.5764 test Loss 6.7413 with MSE metric 80715.5781\n",
      "Time taken for 1 epoch: 65.48670792579651 secs\n",
      "\n",
      "Epoch 85 batch 0 train Loss 6.7784 test Loss 6.7695 with MSE metric 137953.8438\n",
      "Epoch 85 batch 100 train Loss 6.8487 test Loss 6.6336 with MSE metric 137810.2344\n",
      "Epoch 85 batch 200 train Loss 6.6638 test Loss 6.7731 with MSE metric 108842.0391\n",
      "Epoch 85 batch 300 train Loss 6.5345 test Loss 6.7235 with MSE metric 81648.6562\n",
      "Time taken for 1 epoch: 64.43333292007446 secs\n",
      "\n",
      "Epoch 86 batch 0 train Loss 6.7530 test Loss 6.6823 with MSE metric 131672.0625\n",
      "Epoch 86 batch 100 train Loss 6.6257 test Loss 6.4512 with MSE metric 101771.5312\n",
      "Epoch 86 batch 200 train Loss 6.5769 test Loss 6.7474 with MSE metric 92593.5000\n",
      "Epoch 86 batch 300 train Loss 6.4625 test Loss 6.4029 with MSE metric 70479.9531\n",
      "Time taken for 1 epoch: 64.13668298721313 secs\n",
      "\n",
      "Epoch 87 batch 0 train Loss 6.4224 test Loss 6.3999 with MSE metric 67325.9688\n",
      "Epoch 87 batch 100 train Loss 6.2834 test Loss 6.6063 with MSE metric 39155.6875\n",
      "Epoch 87 batch 200 train Loss 6.4704 test Loss 6.5304 with MSE metric 73930.2578\n",
      "Epoch 87 batch 300 train Loss 6.2259 test Loss 6.3656 with MSE metric 38096.5781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 63.57934308052063 secs\n",
      "\n",
      "Epoch 88 batch 0 train Loss 6.6474 test Loss 6.5572 with MSE metric 103754.4297\n",
      "Epoch 88 batch 100 train Loss 6.4543 test Loss 6.6516 with MSE metric 72944.3125\n",
      "Epoch 88 batch 200 train Loss 6.2891 test Loss 6.2621 with MSE metric 49623.0234\n",
      "Epoch 88 batch 300 train Loss 6.2073 test Loss 6.1694 with MSE metric 43677.0469\n",
      "Time taken for 1 epoch: 64.34267067909241 secs\n",
      "\n",
      "Epoch 89 batch 0 train Loss 6.3063 test Loss 6.4855 with MSE metric 54348.6250\n",
      "Epoch 89 batch 100 train Loss 6.6600 test Loss 6.3215 with MSE metric 89443.8281\n",
      "Epoch 89 batch 200 train Loss 6.3111 test Loss 6.2264 with MSE metric 54401.4141\n",
      "Epoch 89 batch 300 train Loss 6.3388 test Loss 6.4903 with MSE metric 57435.0703\n",
      "Time taken for 1 epoch: 63.98026514053345 secs\n",
      "\n",
      "Epoch 90 batch 0 train Loss 6.2201 test Loss 6.3858 with MSE metric 43236.0352\n",
      "Epoch 90 batch 100 train Loss 6.5018 test Loss 6.4711 with MSE metric 76504.5000\n",
      "Epoch 90 batch 200 train Loss 6.6334 test Loss 6.1304 with MSE metric 84076.1172\n",
      "Epoch 90 batch 300 train Loss 6.1871 test Loss 6.2297 with MSE metric 35121.7578\n",
      "Time taken for 1 epoch: 63.194252014160156 secs\n",
      "\n",
      "Epoch 91 batch 0 train Loss 6.6230 test Loss 6.3194 with MSE metric 91631.2031\n",
      "Epoch 91 batch 100 train Loss 6.0765 test Loss 6.2327 with MSE metric 30338.8633\n",
      "Epoch 91 batch 200 train Loss 6.3874 test Loss 6.3437 with MSE metric 63921.0664\n",
      "Epoch 91 batch 300 train Loss 6.5696 test Loss 6.3187 with MSE metric 69830.3984\n",
      "Time taken for 1 epoch: 64.05864810943604 secs\n",
      "\n",
      "Epoch 92 batch 0 train Loss 5.9762 test Loss 6.1656 with MSE metric 17879.8223\n",
      "Epoch 92 batch 100 train Loss 6.0835 test Loss 6.3405 with MSE metric 27243.4238\n",
      "Epoch 92 batch 200 train Loss 6.1876 test Loss 6.6227 with MSE metric 42309.3125\n",
      "Epoch 92 batch 300 train Loss 6.2923 test Loss 6.3742 with MSE metric 52251.0547\n",
      "Time taken for 1 epoch: 62.42713403701782 secs\n",
      "\n",
      "Epoch 93 batch 0 train Loss 6.2667 test Loss 6.3571 with MSE metric 50264.0586\n",
      "Epoch 93 batch 100 train Loss 6.3288 test Loss 6.4445 with MSE metric 56998.7578\n",
      "Epoch 93 batch 200 train Loss 6.4189 test Loss 6.5259 with MSE metric 67913.2344\n",
      "Epoch 93 batch 300 train Loss 6.1408 test Loss 6.4242 with MSE metric 38599.7031\n",
      "Time taken for 1 epoch: 63.60940098762512 secs\n",
      "\n",
      "Epoch 94 batch 0 train Loss 6.1783 test Loss 6.4518 with MSE metric 40925.0234\n",
      "Epoch 94 batch 100 train Loss 6.3399 test Loss 6.2234 with MSE metric 57813.7852\n",
      "Epoch 94 batch 200 train Loss 6.4147 test Loss 6.1528 with MSE metric 67309.2812\n",
      "Epoch 94 batch 300 train Loss 6.2733 test Loss 6.3156 with MSE metric 51096.0469\n",
      "Time taken for 1 epoch: 62.014437198638916 secs\n",
      "\n",
      "Epoch 95 batch 0 train Loss 6.3344 test Loss 6.1798 with MSE metric 57365.5938\n",
      "Epoch 95 batch 100 train Loss 6.1107 test Loss 6.2110 with MSE metric 33152.5078\n",
      "Epoch 95 batch 200 train Loss 5.9741 test Loss 6.1749 with MSE metric 25562.5273\n",
      "Epoch 95 batch 300 train Loss 6.4280 test Loss 6.2901 with MSE metric 67506.1875\n",
      "Time taken for 1 epoch: 61.994446992874146 secs\n",
      "\n",
      "Epoch 96 batch 0 train Loss 6.3537 test Loss 6.5624 with MSE metric 54580.1172\n",
      "Epoch 96 batch 100 train Loss 6.3011 test Loss 6.4146 with MSE metric 53564.0312\n",
      "Epoch 96 batch 200 train Loss 6.3228 test Loss 6.3665 with MSE metric 56455.5469\n",
      "Epoch 96 batch 300 train Loss 6.0199 test Loss 6.3162 with MSE metric 25010.3730\n",
      "Time taken for 1 epoch: 61.988893032073975 secs\n",
      "\n",
      "Epoch 97 batch 0 train Loss 5.9133 test Loss 6.1030 with MSE metric 19418.8594\n",
      "Epoch 97 batch 100 train Loss 6.0729 test Loss 6.4118 with MSE metric 26047.3887\n",
      "Epoch 97 batch 200 train Loss 5.8936 test Loss 6.1496 with MSE metric 20588.4238\n",
      "Epoch 97 batch 300 train Loss 6.1541 test Loss 6.1587 with MSE metric 37802.8164\n",
      "Time taken for 1 epoch: 61.887125968933105 secs\n",
      "\n",
      "Epoch 98 batch 0 train Loss 6.3698 test Loss 6.4926 with MSE metric 60976.9922\n",
      "Epoch 98 batch 100 train Loss 6.3781 test Loss 5.9389 with MSE metric 59552.7344\n",
      "Epoch 98 batch 200 train Loss 6.1393 test Loss 6.2011 with MSE metric 38953.4688\n",
      "Epoch 98 batch 300 train Loss 6.1412 test Loss 6.1032 with MSE metric 38634.2617\n",
      "Time taken for 1 epoch: 61.82365703582764 secs\n",
      "\n",
      "Epoch 99 batch 0 train Loss 6.1068 test Loss 6.6282 with MSE metric 36173.8828\n",
      "Epoch 99 batch 100 train Loss 6.0321 test Loss 6.4116 with MSE metric 29308.0586\n",
      "Epoch 99 batch 200 train Loss 6.5835 test Loss 6.3226 with MSE metric 80454.7891\n",
      "Epoch 99 batch 300 train Loss 6.2140 test Loss 6.2102 with MSE metric 39771.2500\n",
      "Time taken for 1 epoch: 62.06964111328125 secs\n",
      "\n",
      "Epoch 100 batch 0 train Loss 6.2232 test Loss 6.1723 with MSE metric 46268.7188\n",
      "Epoch 100 batch 100 train Loss 6.6477 test Loss 6.2023 with MSE metric 77743.0312\n",
      "Epoch 100 batch 200 train Loss 6.7360 test Loss 6.3242 with MSE metric 100119.3594\n",
      "Epoch 100 batch 300 train Loss 5.9107 test Loss 6.3914 with MSE metric 11575.5215\n",
      "Time taken for 1 epoch: 61.26182699203491 secs\n",
      "\n",
      "Epoch 101 batch 0 train Loss 5.9252 test Loss 5.9380 with MSE metric 24352.3691\n",
      "Epoch 101 batch 100 train Loss 5.8649 test Loss 6.0535 with MSE metric 20802.1523\n",
      "Epoch 101 batch 200 train Loss 6.4368 test Loss 6.3103 with MSE metric 70962.0000\n",
      "Epoch 101 batch 300 train Loss 6.0368 test Loss 6.1696 with MSE metric 7456.3730\n",
      "Time taken for 1 epoch: 60.62875294685364 secs\n",
      "\n",
      "Epoch 102 batch 0 train Loss 6.1692 test Loss 6.1492 with MSE metric 40981.8164\n",
      "Epoch 102 batch 100 train Loss 6.6050 test Loss 6.2374 with MSE metric 73970.9297\n",
      "Epoch 102 batch 200 train Loss 6.2119 test Loss 6.0771 with MSE metric 45324.5547\n",
      "Epoch 102 batch 300 train Loss 6.2008 test Loss 6.5720 with MSE metric 43597.1484\n",
      "Time taken for 1 epoch: 60.594064235687256 secs\n",
      "\n",
      "Epoch 103 batch 0 train Loss 6.4065 test Loss 6.0599 with MSE metric 61500.5469\n",
      "Epoch 103 batch 100 train Loss 6.3430 test Loss 6.1377 with MSE metric 58997.0547\n",
      "Epoch 103 batch 200 train Loss 6.2812 test Loss 6.4610 with MSE metric 49849.4258\n",
      "Epoch 103 batch 300 train Loss 6.2116 test Loss 6.6596 with MSE metric 45295.7891\n",
      "Time taken for 1 epoch: 60.55704879760742 secs\n",
      "\n",
      "Epoch 104 batch 0 train Loss 6.0433 test Loss 6.0978 with MSE metric 29087.9590\n",
      "Epoch 104 batch 100 train Loss 5.9907 test Loss 6.4660 with MSE metric 27129.1230\n",
      "Epoch 104 batch 200 train Loss 6.1120 test Loss 6.1098 with MSE metric 32696.2070\n",
      "Epoch 104 batch 300 train Loss 6.3155 test Loss 6.3539 with MSE metric 53385.7148\n",
      "Time taken for 1 epoch: 60.816848278045654 secs\n",
      "\n",
      "Epoch 105 batch 0 train Loss 6.2824 test Loss 6.1950 with MSE metric 52079.0703\n",
      "Epoch 105 batch 100 train Loss 6.2106 test Loss 6.2306 with MSE metric 44564.5117\n",
      "Epoch 105 batch 200 train Loss 6.2637 test Loss 6.5280 with MSE metric 48171.1953\n",
      "Epoch 105 batch 300 train Loss 6.0340 test Loss 6.0475 with MSE metric 31689.4277\n",
      "Time taken for 1 epoch: 60.786771059036255 secs\n",
      "\n",
      "Epoch 106 batch 0 train Loss 6.0880 test Loss 6.2049 with MSE metric 33096.3672\n",
      "Epoch 106 batch 100 train Loss 6.6446 test Loss 5.6939 with MSE metric 77537.1641\n",
      "Epoch 106 batch 200 train Loss 5.9664 test Loss 6.0747 with MSE metric 25967.9570\n",
      "Epoch 106 batch 300 train Loss 6.4749 test Loss 5.8681 with MSE metric 66735.3750\n",
      "Time taken for 1 epoch: 60.73673391342163 secs\n",
      "\n",
      "Epoch 107 batch 0 train Loss 6.0805 test Loss 6.3775 with MSE metric 24581.4219\n",
      "Epoch 107 batch 100 train Loss 6.0115 test Loss 6.1433 with MSE metric 29045.7832\n",
      "Epoch 107 batch 200 train Loss 6.0275 test Loss 5.8616 with MSE metric 29218.4531\n",
      "Epoch 107 batch 300 train Loss 6.2586 test Loss 6.5844 with MSE metric 49657.2812\n",
      "Time taken for 1 epoch: 60.541869163513184 secs\n",
      "\n",
      "Epoch 108 batch 0 train Loss 6.5458 test Loss 6.2805 with MSE metric 81936.0547\n",
      "Epoch 108 batch 100 train Loss 6.0763 test Loss 6.0001 with MSE metric 32894.8633\n",
      "Epoch 108 batch 200 train Loss 6.0891 test Loss 6.0192 with MSE metric 34490.7188\n",
      "Epoch 108 batch 300 train Loss 6.3609 test Loss 6.0377 with MSE metric 58236.2578\n",
      "Time taken for 1 epoch: 60.823484897613525 secs\n",
      "\n",
      "Epoch 109 batch 0 train Loss 6.1167 test Loss 6.5124 with MSE metric 37458.1602\n",
      "Epoch 109 batch 100 train Loss 6.1324 test Loss 6.0449 with MSE metric 30397.9004\n",
      "Epoch 109 batch 200 train Loss 5.9539 test Loss 6.5367 with MSE metric 24727.0078\n",
      "Epoch 109 batch 300 train Loss 5.9321 test Loss 6.1804 with MSE metric 21935.8184\n",
      "Time taken for 1 epoch: 60.615378856658936 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110 batch 0 train Loss 6.1951 test Loss 6.2308 with MSE metric 43689.7031\n",
      "Epoch 110 batch 100 train Loss 5.9848 test Loss 6.1912 with MSE metric 23197.3242\n",
      "Epoch 110 batch 200 train Loss 6.3971 test Loss 5.9651 with MSE metric 65086.2656\n",
      "Epoch 110 batch 300 train Loss 6.1842 test Loss 6.2280 with MSE metric 42771.2930\n",
      "Time taken for 1 epoch: 61.127249002456665 secs\n",
      "\n",
      "Epoch 111 batch 0 train Loss 6.1194 test Loss 6.1247 with MSE metric 35733.3125\n",
      "Epoch 111 batch 100 train Loss 6.1296 test Loss 6.0337 with MSE metric 37839.0312\n",
      "Epoch 111 batch 200 train Loss 6.1737 test Loss 6.2345 with MSE metric 40810.5469\n",
      "Epoch 111 batch 300 train Loss 5.9905 test Loss 6.0412 with MSE metric 21286.5234\n",
      "Time taken for 1 epoch: 60.36514091491699 secs\n",
      "\n",
      "Epoch 112 batch 0 train Loss 5.9594 test Loss 6.2906 with MSE metric 26095.0039\n",
      "Epoch 112 batch 100 train Loss 6.2949 test Loss 6.1780 with MSE metric 53433.8047\n",
      "Epoch 112 batch 200 train Loss 6.3612 test Loss 6.3141 with MSE metric 61156.6367\n",
      "Epoch 112 batch 300 train Loss 6.2269 test Loss 6.0024 with MSE metric 46195.8359\n",
      "Time taken for 1 epoch: 60.58829188346863 secs\n",
      "\n",
      "Epoch 113 batch 0 train Loss 6.1823 test Loss 6.0725 with MSE metric 40879.5195\n",
      "Epoch 113 batch 100 train Loss 5.8891 test Loss 5.8214 with MSE metric 23701.5195\n",
      "Epoch 113 batch 200 train Loss 6.1278 test Loss 6.1142 with MSE metric 33729.3594\n",
      "Epoch 113 batch 300 train Loss 6.2971 test Loss 6.1972 with MSE metric 52309.7500\n",
      "Time taken for 1 epoch: 64.95628905296326 secs\n",
      "\n",
      "Epoch 114 batch 0 train Loss 6.0305 test Loss 6.1045 with MSE metric 28940.3984\n",
      "Epoch 114 batch 100 train Loss 5.9723 test Loss 6.0279 with MSE metric 26077.1348\n",
      "Epoch 114 batch 200 train Loss 5.9639 test Loss 6.3934 with MSE metric 15686.9844\n",
      "Epoch 114 batch 300 train Loss 6.1535 test Loss 6.2560 with MSE metric 40513.2461\n",
      "Time taken for 1 epoch: 64.79824900627136 secs\n",
      "\n",
      "Epoch 115 batch 0 train Loss 6.1139 test Loss 6.0433 with MSE metric 33528.6484\n",
      "Epoch 115 batch 100 train Loss 6.3811 test Loss 5.7958 with MSE metric 59329.1328\n",
      "Epoch 115 batch 200 train Loss 6.2319 test Loss 6.0141 with MSE metric 45472.9531\n",
      "Epoch 115 batch 300 train Loss 6.2616 test Loss 6.2902 with MSE metric 48678.8438\n",
      "Time taken for 1 epoch: 63.74796414375305 secs\n",
      "\n",
      "Epoch 116 batch 0 train Loss 6.1434 test Loss 6.0815 with MSE metric 39711.7031\n",
      "Epoch 116 batch 100 train Loss 5.9355 test Loss 6.2602 with MSE metric 24163.0215\n",
      "Epoch 116 batch 200 train Loss 5.8264 test Loss 6.3347 with MSE metric 12859.9336\n",
      "Epoch 116 batch 300 train Loss 6.1882 test Loss 6.7085 with MSE metric 42636.4531\n",
      "Time taken for 1 epoch: 63.584733963012695 secs\n",
      "\n",
      "Epoch 117 batch 0 train Loss 6.2617 test Loss 6.1934 with MSE metric 50033.7422\n",
      "Epoch 117 batch 100 train Loss 6.1388 test Loss 6.1727 with MSE metric 38155.9141\n",
      "Epoch 117 batch 200 train Loss 6.1662 test Loss 6.0829 with MSE metric 34096.5391\n",
      "Epoch 117 batch 300 train Loss 6.0844 test Loss 6.3080 with MSE metric 35253.8477\n",
      "Time taken for 1 epoch: 63.71886897087097 secs\n",
      "\n",
      "Epoch 118 batch 0 train Loss 6.1074 test Loss 5.8775 with MSE metric 36066.0586\n",
      "Epoch 118 batch 100 train Loss 5.9772 test Loss 6.4063 with MSE metric 27360.8457\n",
      "Epoch 118 batch 200 train Loss 6.3396 test Loss 6.1633 with MSE metric 56107.9062\n",
      "Epoch 118 batch 300 train Loss 6.0372 test Loss 6.2974 with MSE metric 28632.3535\n",
      "Time taken for 1 epoch: 61.767062187194824 secs\n",
      "\n",
      "Epoch 119 batch 0 train Loss 6.1773 test Loss 6.2285 with MSE metric 41298.9727\n",
      "Epoch 119 batch 100 train Loss 6.5561 test Loss 6.2988 with MSE metric 70365.1719\n",
      "Epoch 119 batch 200 train Loss 6.0950 test Loss 6.0950 with MSE metric 30724.7383\n",
      "Epoch 119 batch 300 train Loss 6.2361 test Loss 6.1282 with MSE metric 47553.5234\n",
      "Time taken for 1 epoch: 63.002315044403076 secs\n",
      "\n",
      "Epoch 120 batch 0 train Loss 6.1380 test Loss 6.0515 with MSE metric 37600.5391\n",
      "Epoch 120 batch 100 train Loss 5.9548 test Loss 5.8650 with MSE metric 23717.3984\n",
      "Epoch 120 batch 200 train Loss 6.2091 test Loss 6.0008 with MSE metric 45272.4883\n",
      "Epoch 120 batch 300 train Loss 6.4736 test Loss 5.9768 with MSE metric 71103.4141\n",
      "Time taken for 1 epoch: 63.87608075141907 secs\n",
      "\n",
      "Epoch 121 batch 0 train Loss 6.0445 test Loss 6.5209 with MSE metric 32248.2520\n",
      "Epoch 121 batch 100 train Loss 6.4210 test Loss 5.9519 with MSE metric 62699.5078\n",
      "Epoch 121 batch 200 train Loss 6.2911 test Loss 6.4267 with MSE metric 51293.0312\n",
      "Epoch 121 batch 300 train Loss 5.9817 test Loss 6.2508 with MSE metric 25676.9336\n",
      "Time taken for 1 epoch: 63.814478158950806 secs\n",
      "\n",
      "Epoch 122 batch 0 train Loss 6.4075 test Loss 6.2106 with MSE metric 62958.3750\n",
      "Epoch 122 batch 100 train Loss 6.2010 test Loss 5.9169 with MSE metric 44644.1719\n",
      "Epoch 122 batch 200 train Loss 5.9974 test Loss 6.1084 with MSE metric 22348.9062\n",
      "Epoch 122 batch 300 train Loss 6.1840 test Loss 6.1261 with MSE metric 42915.7969\n",
      "Time taken for 1 epoch: 64.87836718559265 secs\n",
      "\n",
      "Epoch 123 batch 0 train Loss 6.3808 test Loss 6.2557 with MSE metric 63621.2109\n",
      "Epoch 123 batch 100 train Loss 6.1855 test Loss 6.1024 with MSE metric 43112.5781\n",
      "Epoch 123 batch 200 train Loss 6.3624 test Loss 6.0156 with MSE metric 60290.4766\n",
      "Epoch 123 batch 300 train Loss 6.6131 test Loss 5.8940 with MSE metric 74202.6172\n",
      "Time taken for 1 epoch: 64.14563727378845 secs\n",
      "\n",
      "Epoch 124 batch 0 train Loss 6.1350 test Loss 6.1113 with MSE metric 33654.7930\n",
      "Epoch 124 batch 100 train Loss 6.6269 test Loss 6.5760 with MSE metric 70282.1484\n",
      "Epoch 124 batch 200 train Loss 6.0563 test Loss 6.4257 with MSE metric 29677.4727\n",
      "Epoch 124 batch 300 train Loss 6.2383 test Loss 6.3799 with MSE metric 45466.0859\n",
      "Time taken for 1 epoch: 64.00115609169006 secs\n",
      "\n",
      "Epoch 125 batch 0 train Loss 6.2656 test Loss 6.1176 with MSE metric 50116.2070\n",
      "Epoch 125 batch 100 train Loss 6.5691 test Loss 6.5653 with MSE metric 79984.2266\n",
      "Epoch 125 batch 200 train Loss 6.1630 test Loss 6.1515 with MSE metric 40212.9844\n",
      "Epoch 125 batch 300 train Loss 6.3714 test Loss 6.1872 with MSE metric 59419.9023\n",
      "Time taken for 1 epoch: 63.75512218475342 secs\n",
      "\n",
      "Epoch 126 batch 0 train Loss 6.2398 test Loss 6.5509 with MSE metric 48059.2188\n",
      "Epoch 126 batch 100 train Loss 6.0923 test Loss 6.2346 with MSE metric 31946.3008\n",
      "Epoch 126 batch 200 train Loss 6.0491 test Loss 6.0885 with MSE metric 32289.9082\n",
      "Epoch 126 batch 300 train Loss 5.9925 test Loss 5.9534 with MSE metric 25629.6172\n",
      "Time taken for 1 epoch: 63.23891091346741 secs\n",
      "\n",
      "Epoch 127 batch 0 train Loss 6.6507 test Loss 5.9551 with MSE metric 79685.5156\n",
      "Epoch 127 batch 100 train Loss 6.2837 test Loss 5.8403 with MSE metric 52201.2188\n",
      "Epoch 127 batch 200 train Loss 6.2440 test Loss 6.5759 with MSE metric 47400.6758\n",
      "Epoch 127 batch 300 train Loss 6.1191 test Loss 6.4000 with MSE metric 37654.2461\n",
      "Time taken for 1 epoch: 63.12784004211426 secs\n",
      "\n",
      "Epoch 128 batch 0 train Loss 6.1088 test Loss 6.3185 with MSE metric 29669.2383\n",
      "Epoch 128 batch 100 train Loss 6.2004 test Loss 5.9291 with MSE metric 44401.1719\n",
      "Epoch 128 batch 200 train Loss 6.1914 test Loss 6.2286 with MSE metric 43272.3398\n",
      "Epoch 128 batch 300 train Loss 6.3155 test Loss 6.0593 with MSE metric 55170.0664\n",
      "Time taken for 1 epoch: 63.50055932998657 secs\n",
      "\n",
      "Epoch 129 batch 0 train Loss 6.1373 test Loss 5.8995 with MSE metric 38666.5703\n",
      "Epoch 129 batch 100 train Loss 6.3890 test Loss 6.1291 with MSE metric 61058.4844\n",
      "Epoch 129 batch 200 train Loss 6.1464 test Loss 6.3278 with MSE metric 39873.6719\n",
      "Epoch 129 batch 300 train Loss 6.5178 test Loss 6.1397 with MSE metric 63213.5430\n",
      "Time taken for 1 epoch: 63.2835431098938 secs\n",
      "\n",
      "Epoch 130 batch 0 train Loss 6.2962 test Loss 6.2358 with MSE metric 53953.4766\n",
      "Epoch 130 batch 100 train Loss 6.5173 test Loss 6.2172 with MSE metric 70621.0156\n",
      "Epoch 130 batch 200 train Loss 6.0567 test Loss 6.0377 with MSE metric 31839.9336\n",
      "Epoch 130 batch 300 train Loss 6.3505 test Loss 6.3139 with MSE metric 58961.4219\n",
      "Time taken for 1 epoch: 63.664509296417236 secs\n",
      "\n",
      "Epoch 131 batch 0 train Loss 6.0867 test Loss 6.4276 with MSE metric 32864.0820\n",
      "Epoch 131 batch 100 train Loss 5.9343 test Loss 6.3387 with MSE metric 20699.1680\n",
      "Epoch 131 batch 200 train Loss 5.9182 test Loss 6.3410 with MSE metric 15521.2734\n",
      "Epoch 131 batch 300 train Loss 6.4868 test Loss 6.1062 with MSE metric 68652.1094\n",
      "Time taken for 1 epoch: 62.97245121002197 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132 batch 0 train Loss 6.3195 test Loss 6.4942 with MSE metric 53576.4219\n",
      "Epoch 132 batch 100 train Loss 6.4951 test Loss 6.3670 with MSE metric 69256.4531\n",
      "Epoch 132 batch 200 train Loss 6.0382 test Loss 6.0554 with MSE metric 27548.4688\n",
      "Epoch 132 batch 300 train Loss 6.7778 test Loss 6.5213 with MSE metric 95780.5781\n",
      "Time taken for 1 epoch: 64.7581832408905 secs\n",
      "\n",
      "Epoch 133 batch 0 train Loss 5.8110 test Loss 6.2293 with MSE metric 8998.9072\n",
      "Epoch 133 batch 100 train Loss 6.2307 test Loss 6.2792 with MSE metric 47416.1250\n",
      "Epoch 133 batch 200 train Loss 6.7006 test Loss 6.7044 with MSE metric 96699.8125\n",
      "Epoch 133 batch 300 train Loss 6.5096 test Loss 5.9848 with MSE metric 75449.9844\n",
      "Time taken for 1 epoch: 64.17077779769897 secs\n",
      "\n",
      "Epoch 134 batch 0 train Loss 6.0270 test Loss 6.2357 with MSE metric 27324.8555\n",
      "Epoch 134 batch 100 train Loss 5.9631 test Loss 6.0108 with MSE metric 24973.0156\n",
      "Epoch 134 batch 200 train Loss 6.2372 test Loss 6.2365 with MSE metric 46685.2734\n",
      "Epoch 134 batch 300 train Loss 5.9769 test Loss 6.0250 with MSE metric 25518.2012\n",
      "Time taken for 1 epoch: 64.33550477027893 secs\n",
      "\n",
      "Epoch 135 batch 0 train Loss 5.9903 test Loss 6.1477 with MSE metric 27324.8613\n",
      "Epoch 135 batch 100 train Loss 6.2432 test Loss 6.2244 with MSE metric 48237.3945\n",
      "Epoch 135 batch 200 train Loss 6.1786 test Loss 6.6273 with MSE metric 42611.6523\n",
      "Epoch 135 batch 300 train Loss 6.2836 test Loss 6.1956 with MSE metric 51231.3516\n",
      "Time taken for 1 epoch: 63.07242393493652 secs\n",
      "\n",
      "Epoch 136 batch 0 train Loss 6.2210 test Loss 6.4009 with MSE metric 46493.8047\n",
      "Epoch 136 batch 100 train Loss 6.2596 test Loss 6.3280 with MSE metric 49840.4766\n",
      "Epoch 136 batch 200 train Loss 5.9682 test Loss 6.0352 with MSE metric 24818.2227\n",
      "Epoch 136 batch 300 train Loss 5.9945 test Loss 5.8123 with MSE metric 28264.9883\n",
      "Time taken for 1 epoch: 64.47413086891174 secs\n",
      "\n",
      "Epoch 137 batch 0 train Loss 6.0610 test Loss 6.2704 with MSE metric 32873.2070\n",
      "Epoch 137 batch 100 train Loss 5.8998 test Loss 6.1455 with MSE metric 16101.5664\n",
      "Epoch 137 batch 200 train Loss 6.2111 test Loss 6.0750 with MSE metric 45233.5000\n",
      "Epoch 137 batch 300 train Loss 5.9915 test Loss 6.1351 with MSE metric 25949.2852\n",
      "Time taken for 1 epoch: 62.6273078918457 secs\n",
      "\n",
      "Epoch 138 batch 0 train Loss 6.2651 test Loss 6.8688 with MSE metric 49892.4531\n",
      "Epoch 138 batch 100 train Loss 6.1400 test Loss 5.8561 with MSE metric 38733.4688\n",
      "Epoch 138 batch 200 train Loss 6.1695 test Loss 6.0554 with MSE metric 41039.0781\n",
      "Epoch 138 batch 300 train Loss 6.1340 test Loss 6.2809 with MSE metric 39096.7344\n",
      "Time taken for 1 epoch: 62.55174279212952 secs\n",
      "\n",
      "Epoch 139 batch 0 train Loss 6.1296 test Loss 6.5243 with MSE metric 37614.3906\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = river_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(tar_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_river/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tim_pos_tr2, batch_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_river(token_tr, t1_tr, t2_tr, basin_l_tr,  attributes_numeric, tar_tr, batch_s=32)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_log_sig, mask = train_step(decoder, optimizer_c, train_loss, m_tr, batch_tok_pos_tr, batch_tim_pos_tr, batch_tim_pos_tr2, batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 100 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tim_pos_te2, batch_pos_te, batch_tar_te, _ = batch_creator.create_batch_river(token_te, t1_te, t2_te, basin_l_te,  attributes_numeric, tar_te, batch_s=32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_log_sig_te, t_mask = test_step(decoder, test_loss, m_te, batch_tok_pos_te, batch_tim_pos_te, batch_tim_pos_te2, batch_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
