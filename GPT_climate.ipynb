{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, data_combine, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, t, token = data_combine.climate_data_to_model_input('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Restored from /Users/omernivron/Downloads/GPT_climate/ckpt/check_0/ckpt-12500\n",
      "Epoch 0 batch 0 train Loss 5.2739 test Loss 5.3960 with MSE metric 7004.3525\n",
      "Epoch 0 batch 10 train Loss 5.3193 test Loss 5.2997 with MSE metric 7671.2256\n",
      "Epoch 0 batch 20 train Loss 5.2761 test Loss 5.3289 with MSE metric 7031.9185\n",
      "Epoch 0 batch 30 train Loss 5.2238 test Loss 5.3786 with MSE metric 6280.2842\n",
      "Epoch 0 batch 40 train Loss 5.3137 test Loss 5.2315 with MSE metric 7581.8154\n",
      "Epoch 0 batch 50 train Loss 5.3869 test Loss 5.3905 with MSE metric 8709.8418\n",
      "Epoch 0 batch 60 train Loss 5.2766 test Loss 5.4403 with MSE metric 7043.8154\n",
      "Epoch 0 batch 70 train Loss 5.2409 test Loss 5.3325 with MSE metric 6418.5020\n",
      "Epoch 0 batch 80 train Loss 5.3103 test Loss 5.4162 with MSE metric 7533.3545\n",
      "Epoch 0 batch 90 train Loss 5.2801 test Loss 5.4246 with MSE metric 7092.9111\n",
      "Epoch 0 batch 100 train Loss 5.3501 test Loss 5.3274 with MSE metric 8150.3296\n",
      "Epoch 0 batch 110 train Loss 5.3044 test Loss 5.3094 with MSE metric 7445.3154\n",
      "Epoch 0 batch 120 train Loss 5.3496 test Loss 5.3368 with MSE metric 8065.3135\n",
      "Epoch 0 batch 130 train Loss 5.3184 test Loss 5.3486 with MSE metric 7659.5874\n",
      "Epoch 0 batch 140 train Loss 5.2336 test Loss 5.3169 with MSE metric 6280.4395\n",
      "Epoch 0 batch 150 train Loss 5.2216 test Loss 5.4669 with MSE metric 6246.4639\n",
      "Epoch 0 batch 160 train Loss 5.3310 test Loss 5.3184 with MSE metric 7848.3657\n",
      "Epoch 0 batch 170 train Loss 5.3085 test Loss 5.3733 with MSE metric 7505.8203\n",
      "Epoch 0 batch 180 train Loss 5.2684 test Loss 5.3201 with MSE metric 6913.8057\n",
      "Epoch 0 batch 190 train Loss 5.2724 test Loss 5.3970 with MSE metric 6976.2305\n",
      "Epoch 0 batch 200 train Loss 5.2780 test Loss 5.2705 with MSE metric 7051.8628\n",
      "Epoch 0 batch 210 train Loss 5.1818 test Loss 5.3918 with MSE metric 5651.3164\n",
      "Epoch 0 batch 220 train Loss 5.2896 test Loss 5.3692 with MSE metric 7200.4277\n",
      "Epoch 0 batch 230 train Loss 5.2815 test Loss 5.4744 with MSE metric 7113.6143\n",
      "Epoch 0 batch 240 train Loss 5.3138 test Loss 5.3446 with MSE metric 7588.3447\n",
      "Time taken for 1 epoch: 27.315759897232056 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 5.2980 test Loss 5.2883 with MSE metric 7296.1523\n",
      "Epoch 1 batch 10 train Loss 5.3635 test Loss 5.3808 with MSE metric 8230.4707\n",
      "Epoch 1 batch 20 train Loss 5.3966 test Loss 5.4025 with MSE metric 8891.7090\n",
      "Epoch 1 batch 30 train Loss 5.2746 test Loss 5.3536 with MSE metric 7006.1826\n",
      "Epoch 1 batch 40 train Loss 5.3149 test Loss 5.4153 with MSE metric 7601.8301\n",
      "Epoch 1 batch 50 train Loss 5.3799 test Loss 5.3623 with MSE metric 8635.6562\n",
      "Epoch 1 batch 60 train Loss 5.1799 test Loss 5.2579 with MSE metric 5621.1318\n",
      "Epoch 1 batch 70 train Loss 5.3605 test Loss 5.3030 with MSE metric 8255.0156\n",
      "Epoch 1 batch 80 train Loss 5.2775 test Loss 5.4000 with MSE metric 6894.2285\n",
      "Epoch 1 batch 90 train Loss 5.1520 test Loss 5.3502 with MSE metric 5324.9868\n",
      "Epoch 1 batch 100 train Loss 5.2447 test Loss 5.3707 with MSE metric 6594.7578\n",
      "Epoch 1 batch 110 train Loss 5.3527 test Loss 5.3649 with MSE metric 8095.5967\n",
      "Epoch 1 batch 120 train Loss 5.3028 test Loss 5.3033 with MSE metric 7414.0098\n",
      "Epoch 1 batch 130 train Loss 5.3013 test Loss 5.4405 with MSE metric 7398.9531\n",
      "Epoch 1 batch 140 train Loss 5.2948 test Loss 5.4197 with MSE metric 7304.6309\n",
      "Epoch 1 batch 150 train Loss 5.2510 test Loss 5.3120 with MSE metric 6647.1333\n",
      "Epoch 1 batch 160 train Loss 5.3363 test Loss 5.3978 with MSE metric 7870.2432\n",
      "Epoch 1 batch 170 train Loss 5.3285 test Loss 5.3377 with MSE metric 7808.3975\n",
      "Epoch 1 batch 180 train Loss 5.2385 test Loss 5.3849 with MSE metric 6496.5845\n",
      "Epoch 1 batch 190 train Loss 5.3350 test Loss 5.2887 with MSE metric 7823.5068\n",
      "Epoch 1 batch 200 train Loss 5.3576 test Loss 5.3589 with MSE metric 8273.3555\n",
      "Epoch 1 batch 210 train Loss 5.2474 test Loss 5.4360 with MSE metric 6562.5283\n",
      "Epoch 1 batch 220 train Loss 5.3305 test Loss 5.3562 with MSE metric 7772.6011\n",
      "Epoch 1 batch 230 train Loss 5.2938 test Loss 5.3205 with MSE metric 7280.5625\n",
      "Epoch 1 batch 240 train Loss 5.3855 test Loss 5.3255 with MSE metric 8664.3086\n",
      "Time taken for 1 epoch: 25.225006103515625 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 5.2597 test Loss 5.3828 with MSE metric 6774.4258\n",
      "Epoch 2 batch 10 train Loss 5.2547 test Loss 5.4424 with MSE metric 6690.8306\n",
      "Epoch 2 batch 20 train Loss 5.1674 test Loss 5.3583 with MSE metric 5372.5986\n",
      "Epoch 2 batch 30 train Loss 5.3008 test Loss 5.2750 with MSE metric 7391.4092\n",
      "Epoch 2 batch 40 train Loss 5.2883 test Loss 5.4303 with MSE metric 7164.4639\n",
      "Epoch 2 batch 50 train Loss 5.2573 test Loss 5.3513 with MSE metric 6759.0381\n",
      "Epoch 2 batch 60 train Loss 5.3050 test Loss 5.4212 with MSE metric 7454.2803\n",
      "Epoch 2 batch 70 train Loss 5.3435 test Loss 5.4605 with MSE metric 8049.9331\n",
      "Epoch 2 batch 80 train Loss 5.3334 test Loss 5.3286 with MSE metric 7831.1304\n",
      "Epoch 2 batch 90 train Loss 5.3678 test Loss 5.3929 with MSE metric 8446.4404\n",
      "Epoch 2 batch 100 train Loss 5.2640 test Loss 5.3211 with MSE metric 6809.8608\n",
      "Epoch 2 batch 110 train Loss 5.2437 test Loss 5.3377 with MSE metric 6584.2822\n",
      "Epoch 2 batch 120 train Loss 5.3321 test Loss 5.3317 with MSE metric 7859.3784\n",
      "Epoch 2 batch 130 train Loss 5.2708 test Loss 5.3013 with MSE metric 6939.8789\n",
      "Epoch 2 batch 140 train Loss 5.2657 test Loss 5.3639 with MSE metric 6892.9062\n",
      "Epoch 2 batch 150 train Loss 5.2401 test Loss 5.3346 with MSE metric 6517.0254\n",
      "Epoch 2 batch 160 train Loss 5.3023 test Loss 5.3722 with MSE metric 7375.4580\n",
      "Epoch 2 batch 170 train Loss 5.3104 test Loss 5.4486 with MSE metric 7536.0508\n",
      "Epoch 2 batch 180 train Loss 5.3598 test Loss 5.3577 with MSE metric 8300.5098\n",
      "Epoch 2 batch 190 train Loss 5.3022 test Loss 5.4578 with MSE metric 7407.6123\n",
      "Epoch 2 batch 200 train Loss 5.2813 test Loss 5.3612 with MSE metric 7096.0854\n",
      "Epoch 2 batch 210 train Loss 5.3682 test Loss 5.2800 with MSE metric 8431.8760\n",
      "Epoch 2 batch 220 train Loss 5.3817 test Loss 5.3816 with MSE metric 8607.5596\n",
      "Epoch 2 batch 230 train Loss 5.2803 test Loss 5.3768 with MSE metric 7097.4033\n",
      "Epoch 2 batch 240 train Loss 5.2084 test Loss 5.4662 with MSE metric 6000.0112\n",
      "Time taken for 1 epoch: 24.646179676055908 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 5.4091 test Loss 5.3480 with MSE metric 8886.2324\n",
      "Epoch 3 batch 10 train Loss 5.3296 test Loss 5.2954 with MSE metric 7793.9541\n",
      "Epoch 3 batch 20 train Loss 5.2828 test Loss 5.3683 with MSE metric 7125.9287\n",
      "Epoch 3 batch 30 train Loss 5.2368 test Loss 5.2865 with MSE metric 6451.4458\n",
      "Epoch 3 batch 40 train Loss 5.2421 test Loss 5.3479 with MSE metric 6509.8008\n",
      "Epoch 3 batch 50 train Loss 5.2802 test Loss 5.3949 with MSE metric 7092.6602\n",
      "Epoch 3 batch 60 train Loss 5.3181 test Loss 5.3367 with MSE metric 7648.0015\n",
      "Epoch 3 batch 70 train Loss 5.3148 test Loss 5.2935 with MSE metric 7594.5361\n",
      "Epoch 3 batch 80 train Loss 5.3274 test Loss 5.4374 with MSE metric 7773.1201\n",
      "Epoch 3 batch 90 train Loss 5.2188 test Loss 5.3380 with MSE metric 6112.8774\n",
      "Epoch 3 batch 100 train Loss 5.3563 test Loss 5.4222 with MSE metric 8258.6328\n",
      "Epoch 3 batch 110 train Loss 5.2692 test Loss 5.3051 with MSE metric 6938.2861\n",
      "Epoch 3 batch 120 train Loss 5.4655 test Loss 5.3935 with MSE metric 9732.5938\n",
      "Epoch 3 batch 130 train Loss 5.3056 test Loss 5.2402 with MSE metric 7464.8394\n",
      "Epoch 3 batch 140 train Loss 5.3072 test Loss 5.3693 with MSE metric 7487.2036\n",
      "Epoch 3 batch 150 train Loss 5.3832 test Loss 5.4238 with MSE metric 8639.2744\n",
      "Epoch 3 batch 160 train Loss 5.2575 test Loss 5.3229 with MSE metric 6775.4443\n",
      "Epoch 3 batch 170 train Loss 5.3423 test Loss 5.4717 with MSE metric 7938.3477\n",
      "Epoch 3 batch 180 train Loss 5.3402 test Loss 5.3598 with MSE metric 7984.8232\n",
      "Epoch 3 batch 190 train Loss 5.3358 test Loss 5.4607 with MSE metric 7890.8447\n",
      "Epoch 3 batch 200 train Loss 5.3521 test Loss 5.3783 with MSE metric 8184.7812\n",
      "Epoch 3 batch 210 train Loss 5.3251 test Loss 5.2858 with MSE metric 7754.5879\n",
      "Epoch 3 batch 220 train Loss 5.3561 test Loss 5.3015 with MSE metric 8209.9717\n",
      "Epoch 3 batch 230 train Loss 5.3247 test Loss 5.3965 with MSE metric 7749.2383\n",
      "Epoch 3 batch 240 train Loss 5.3492 test Loss 5.3429 with MSE metric 7964.7266\n",
      "Time taken for 1 epoch: 26.228026151657104 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 5.3778 test Loss 5.3563 with MSE metric 8566.4668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 batch 10 train Loss 5.2046 test Loss 5.3782 with MSE metric 5885.1895\n",
      "Epoch 4 batch 20 train Loss 5.2004 test Loss 5.4595 with MSE metric 5833.0474\n",
      "Epoch 4 batch 30 train Loss 5.3310 test Loss 5.3161 with MSE metric 7845.3711\n",
      "Epoch 4 batch 40 train Loss 5.2773 test Loss 5.3506 with MSE metric 7018.6499\n",
      "Epoch 4 batch 50 train Loss 5.2539 test Loss 5.2978 with MSE metric 6723.7568\n",
      "Epoch 4 batch 60 train Loss 5.3238 test Loss 5.3999 with MSE metric 7739.4102\n",
      "Epoch 4 batch 70 train Loss 5.3751 test Loss 5.3683 with MSE metric 8478.8232\n",
      "Epoch 4 batch 80 train Loss 5.3449 test Loss 5.4134 with MSE metric 8035.2354\n",
      "Epoch 4 batch 90 train Loss 5.2883 test Loss 5.3809 with MSE metric 7212.1602\n",
      "Epoch 4 batch 100 train Loss 5.3226 test Loss 5.4721 with MSE metric 7688.2002\n",
      "Epoch 4 batch 110 train Loss 5.2845 test Loss 5.4208 with MSE metric 7059.3350\n",
      "Epoch 4 batch 120 train Loss 5.3521 test Loss 5.3900 with MSE metric 8157.3882\n",
      "Epoch 4 batch 130 train Loss 5.3484 test Loss 5.4910 with MSE metric 8000.5605\n",
      "Epoch 4 batch 140 train Loss 5.2735 test Loss 5.4012 with MSE metric 6987.8887\n",
      "Epoch 4 batch 150 train Loss 5.2803 test Loss 5.3329 with MSE metric 7058.2314\n",
      "Epoch 4 batch 160 train Loss 5.3915 test Loss 5.4233 with MSE metric 8583.1484\n",
      "Epoch 4 batch 170 train Loss 5.3937 test Loss 5.3768 with MSE metric 8853.0439\n",
      "Epoch 4 batch 180 train Loss 5.2998 test Loss 5.2926 with MSE metric 7378.0249\n",
      "Epoch 4 batch 190 train Loss 5.3561 test Loss 5.3697 with MSE metric 8203.6064\n",
      "Epoch 4 batch 200 train Loss 5.1971 test Loss 5.3054 with MSE metric 5812.2554\n",
      "Epoch 4 batch 210 train Loss 5.3705 test Loss 5.3392 with MSE metric 8379.4023\n",
      "Epoch 4 batch 220 train Loss 5.3432 test Loss 5.4115 with MSE metric 8044.2891\n",
      "Epoch 4 batch 230 train Loss 5.3000 test Loss 5.3896 with MSE metric 7370.3052\n",
      "Epoch 4 batch 240 train Loss 5.1876 test Loss 5.3043 with MSE metric 5772.5459\n",
      "Time taken for 1 epoch: 25.541282892227173 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 5.2985 test Loss 5.3828 with MSE metric 7360.7568\n",
      "Epoch 5 batch 10 train Loss 5.2273 test Loss 5.3404 with MSE metric 6270.2524\n",
      "Epoch 5 batch 20 train Loss 5.3241 test Loss 5.3088 with MSE metric 7684.2598\n",
      "Epoch 5 batch 30 train Loss 5.2593 test Loss 5.3283 with MSE metric 6736.8042\n",
      "Epoch 5 batch 40 train Loss 5.2170 test Loss 5.4245 with MSE metric 6220.3540\n",
      "Epoch 5 batch 50 train Loss 5.2696 test Loss 5.4345 with MSE metric 6942.9229\n",
      "Epoch 5 batch 60 train Loss 5.2580 test Loss 5.3570 with MSE metric 6734.8018\n",
      "Epoch 5 batch 70 train Loss 5.2603 test Loss 5.4163 with MSE metric 6818.1343\n",
      "Epoch 5 batch 80 train Loss 5.3607 test Loss 5.3315 with MSE metric 8320.3867\n",
      "Epoch 5 batch 90 train Loss 5.3042 test Loss 5.3014 with MSE metric 7437.3740\n",
      "Epoch 5 batch 100 train Loss 5.2711 test Loss 5.4699 with MSE metric 6946.3740\n",
      "Epoch 5 batch 110 train Loss 5.3446 test Loss 5.4052 with MSE metric 8060.9771\n",
      "Epoch 5 batch 120 train Loss 5.1978 test Loss 5.4716 with MSE metric 5881.7334\n",
      "Epoch 5 batch 130 train Loss 5.4009 test Loss 5.3987 with MSE metric 8891.3975\n",
      "Epoch 5 batch 140 train Loss 5.2641 test Loss 5.4037 with MSE metric 6833.5649\n",
      "Epoch 5 batch 150 train Loss 5.2873 test Loss 5.3696 with MSE metric 7179.1304\n",
      "Epoch 5 batch 160 train Loss 5.2771 test Loss 5.4250 with MSE metric 6917.8340\n",
      "Epoch 5 batch 170 train Loss 5.2834 test Loss 5.4006 with MSE metric 7139.6733\n",
      "Epoch 5 batch 180 train Loss 5.3323 test Loss 5.4025 with MSE metric 7867.9526\n",
      "Epoch 5 batch 190 train Loss 5.3996 test Loss 5.3472 with MSE metric 8659.8945\n",
      "Epoch 5 batch 200 train Loss 5.3482 test Loss 5.3666 with MSE metric 8125.2500\n",
      "Epoch 5 batch 210 train Loss 5.3758 test Loss 5.4670 with MSE metric 8466.5762\n",
      "Epoch 5 batch 220 train Loss 5.2131 test Loss 5.3836 with MSE metric 6164.9385\n",
      "Epoch 5 batch 230 train Loss 5.3094 test Loss 5.3518 with MSE metric 7510.5728\n",
      "Epoch 5 batch 240 train Loss 5.2728 test Loss 5.3012 with MSE metric 6937.2285\n",
      "Time taken for 1 epoch: 27.357419967651367 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 5.3083 test Loss 5.4232 with MSE metric 7483.5874\n",
      "Epoch 6 batch 10 train Loss 5.3758 test Loss 5.2409 with MSE metric 8411.2363\n",
      "Epoch 6 batch 20 train Loss 5.3462 test Loss 5.3746 with MSE metric 8093.6934\n",
      "Epoch 6 batch 30 train Loss 5.3024 test Loss 5.4013 with MSE metric 7410.7725\n",
      "Epoch 6 batch 40 train Loss 5.2584 test Loss 5.3694 with MSE metric 6754.8335\n",
      "Epoch 6 batch 50 train Loss 5.3521 test Loss 5.3521 with MSE metric 8108.3892\n",
      "Epoch 6 batch 60 train Loss 5.3297 test Loss 5.2893 with MSE metric 7803.0361\n",
      "Epoch 6 batch 70 train Loss 5.2590 test Loss 5.4495 with MSE metric 6769.9932\n",
      "Epoch 6 batch 80 train Loss 5.2127 test Loss 5.3743 with MSE metric 6159.8223\n",
      "Epoch 6 batch 90 train Loss 5.3366 test Loss 5.3886 with MSE metric 7940.0859\n",
      "Epoch 6 batch 100 train Loss 5.3286 test Loss 5.2908 with MSE metric 7792.2129\n",
      "Epoch 6 batch 110 train Loss 5.3133 test Loss 5.3202 with MSE metric 7577.5771\n",
      "Epoch 6 batch 120 train Loss 5.1895 test Loss 5.3321 with MSE metric 5787.5342\n",
      "Epoch 6 batch 130 train Loss 5.4462 test Loss 5.3951 with MSE metric 9560.2168\n",
      "Epoch 6 batch 140 train Loss 5.4139 test Loss 5.2809 with MSE metric 9107.0127\n",
      "Epoch 6 batch 150 train Loss 5.2363 test Loss 5.4209 with MSE metric 6398.1030\n",
      "Epoch 6 batch 160 train Loss 5.3794 test Loss 5.3848 with MSE metric 8323.5303\n",
      "Epoch 6 batch 170 train Loss 5.3217 test Loss 5.4221 with MSE metric 7664.2202\n",
      "Epoch 6 batch 180 train Loss 5.3459 test Loss 5.3697 with MSE metric 8043.5972\n",
      "Epoch 6 batch 190 train Loss 5.3163 test Loss 5.2568 with MSE metric 7595.3320\n",
      "Epoch 6 batch 200 train Loss 5.2397 test Loss 5.3983 with MSE metric 6403.7383\n",
      "Epoch 6 batch 210 train Loss 5.2688 test Loss 5.2978 with MSE metric 6931.1460\n",
      "Epoch 6 batch 220 train Loss 5.2532 test Loss 5.2570 with MSE metric 6702.8682\n",
      "Epoch 6 batch 230 train Loss 5.3875 test Loss 5.3252 with MSE metric 8705.0205\n",
      "Epoch 6 batch 240 train Loss 5.2972 test Loss 5.3903 with MSE metric 7335.7012\n",
      "Time taken for 1 epoch: 27.728526830673218 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 5.3233 test Loss 5.3604 with MSE metric 7698.2427\n",
      "Epoch 7 batch 10 train Loss 5.3189 test Loss 5.3265 with MSE metric 7656.5889\n",
      "Epoch 7 batch 20 train Loss 5.2920 test Loss 5.4240 with MSE metric 7261.5029\n",
      "Epoch 7 batch 30 train Loss 5.3504 test Loss 5.4014 with MSE metric 8156.8159\n",
      "Epoch 7 batch 40 train Loss 5.3222 test Loss 5.3315 with MSE metric 7715.4775\n",
      "Epoch 7 batch 50 train Loss 5.3081 test Loss 5.3390 with MSE metric 7497.8501\n",
      "Epoch 7 batch 60 train Loss 5.2331 test Loss 5.3667 with MSE metric 6337.6973\n",
      "Epoch 7 batch 70 train Loss 5.2929 test Loss 5.3728 with MSE metric 7272.2256\n",
      "Epoch 7 batch 80 train Loss 5.3294 test Loss 5.3510 with MSE metric 7829.5884\n",
      "Epoch 7 batch 90 train Loss 5.4066 test Loss 5.3443 with MSE metric 9080.7969\n",
      "Epoch 7 batch 100 train Loss 5.3547 test Loss 5.4092 with MSE metric 8134.3418\n",
      "Epoch 7 batch 110 train Loss 5.3277 test Loss 5.3517 with MSE metric 7802.5269\n",
      "Epoch 7 batch 120 train Loss 5.3165 test Loss 5.3018 with MSE metric 7626.3315\n",
      "Epoch 7 batch 130 train Loss 5.3385 test Loss 5.2979 with MSE metric 7938.7983\n",
      "Epoch 7 batch 140 train Loss 5.3512 test Loss 5.3949 with MSE metric 8145.0244\n",
      "Epoch 7 batch 150 train Loss 5.2983 test Loss 5.4058 with MSE metric 7357.4712\n",
      "Epoch 7 batch 160 train Loss 5.2787 test Loss 5.2792 with MSE metric 7064.4619\n",
      "Epoch 7 batch 170 train Loss 5.2831 test Loss 5.3818 with MSE metric 7119.8682\n",
      "Epoch 7 batch 180 train Loss 5.3122 test Loss 5.3862 with MSE metric 7553.6670\n",
      "Epoch 7 batch 190 train Loss 5.2132 test Loss 5.3262 with MSE metric 6074.6948\n",
      "Epoch 7 batch 200 train Loss 5.2510 test Loss 5.3419 with MSE metric 6565.1143\n",
      "Epoch 7 batch 210 train Loss 5.3164 test Loss 5.3261 with MSE metric 7619.6201\n",
      "Epoch 7 batch 220 train Loss 5.2994 test Loss 5.4119 with MSE metric 7325.4526\n",
      "Epoch 7 batch 230 train Loss 5.2948 test Loss 5.3499 with MSE metric 7294.4014\n",
      "Epoch 7 batch 240 train Loss 5.2610 test Loss 5.3188 with MSE metric 6796.5864\n",
      "Time taken for 1 epoch: 27.99218988418579 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 5.2593 test Loss 5.3887 with MSE metric 6775.9009\n",
      "Epoch 8 batch 10 train Loss 5.1157 test Loss 5.4663 with MSE metric 4821.5625\n",
      "Epoch 8 batch 20 train Loss 5.2852 test Loss 5.3701 with MSE metric 7163.7002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 batch 30 train Loss 5.3523 test Loss 5.3570 with MSE metric 8130.1357\n",
      "Epoch 8 batch 40 train Loss 5.3379 test Loss 5.4210 with MSE metric 7960.9014\n",
      "Epoch 8 batch 50 train Loss 5.2990 test Loss 5.3978 with MSE metric 7366.9414\n",
      "Epoch 8 batch 60 train Loss 5.4245 test Loss 5.4119 with MSE metric 9181.5176\n",
      "Epoch 8 batch 70 train Loss 5.4154 test Loss 5.3382 with MSE metric 9231.1846\n",
      "Epoch 8 batch 80 train Loss 5.3191 test Loss 5.3831 with MSE metric 7659.1997\n",
      "Epoch 8 batch 90 train Loss 5.2329 test Loss 5.4139 with MSE metric 6442.1250\n",
      "Epoch 8 batch 100 train Loss 5.3064 test Loss 5.2479 with MSE metric 7468.5527\n",
      "Epoch 8 batch 110 train Loss 5.2248 test Loss 5.3372 with MSE metric 6130.6709\n",
      "Epoch 8 batch 120 train Loss 5.4192 test Loss 5.3596 with MSE metric 9036.5684\n",
      "Epoch 8 batch 130 train Loss 5.2147 test Loss 5.3605 with MSE metric 5992.6211\n",
      "Epoch 8 batch 140 train Loss 5.4226 test Loss 5.2635 with MSE metric 9048.9121\n",
      "Epoch 8 batch 150 train Loss 5.3399 test Loss 5.3707 with MSE metric 7992.8564\n",
      "Epoch 8 batch 160 train Loss 5.2488 test Loss 5.3980 with MSE metric 6514.1289\n",
      "Epoch 8 batch 170 train Loss 5.3183 test Loss 5.3600 with MSE metric 7620.5034\n",
      "Epoch 8 batch 180 train Loss 5.3097 test Loss 5.2994 with MSE metric 7513.2051\n",
      "Epoch 8 batch 190 train Loss 5.3183 test Loss 5.3474 with MSE metric 7647.5195\n",
      "Epoch 8 batch 200 train Loss 5.2920 test Loss 5.3665 with MSE metric 7264.1709\n",
      "Epoch 8 batch 210 train Loss 5.2689 test Loss 5.3784 with MSE metric 6903.3447\n",
      "Epoch 8 batch 220 train Loss 5.4156 test Loss 5.3208 with MSE metric 8964.5273\n",
      "Epoch 8 batch 230 train Loss 5.2901 test Loss 5.3154 with MSE metric 7163.6650\n",
      "Epoch 8 batch 240 train Loss 5.2895 test Loss 5.3425 with MSE metric 7226.6768\n",
      "Time taken for 1 epoch: 27.89631175994873 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 5.3351 test Loss 5.3041 with MSE metric 7917.1406\n",
      "Epoch 9 batch 10 train Loss 5.2618 test Loss 5.4052 with MSE metric 6837.5122\n",
      "Epoch 9 batch 20 train Loss 5.2856 test Loss 5.3394 with MSE metric 7171.9966\n",
      "Epoch 9 batch 30 train Loss 5.2867 test Loss 5.3051 with MSE metric 7185.6084\n",
      "Epoch 9 batch 40 train Loss 5.2809 test Loss 5.5091 with MSE metric 7098.3179\n",
      "Epoch 9 batch 50 train Loss 5.2765 test Loss 5.3616 with MSE metric 7030.8008\n",
      "Epoch 9 batch 60 train Loss 5.3290 test Loss 5.4100 with MSE metric 7772.8906\n",
      "Epoch 9 batch 70 train Loss 5.2931 test Loss 5.2937 with MSE metric 7245.4111\n",
      "Epoch 9 batch 80 train Loss 5.3169 test Loss 5.4048 with MSE metric 7632.6123\n",
      "Epoch 9 batch 90 train Loss 5.2263 test Loss 5.2881 with MSE metric 6351.9590\n",
      "Epoch 9 batch 100 train Loss 5.3366 test Loss 5.3118 with MSE metric 7918.3096\n",
      "Epoch 9 batch 110 train Loss 5.2524 test Loss 5.3377 with MSE metric 6641.8911\n",
      "Epoch 9 batch 120 train Loss 5.4017 test Loss 5.3431 with MSE metric 8794.7891\n",
      "Epoch 9 batch 130 train Loss 5.3164 test Loss 5.4360 with MSE metric 7623.1846\n",
      "Epoch 9 batch 140 train Loss 5.3570 test Loss 5.3409 with MSE metric 8213.3232\n",
      "Epoch 9 batch 150 train Loss 5.3041 test Loss 5.3582 with MSE metric 7417.7168\n",
      "Epoch 9 batch 160 train Loss 5.2788 test Loss 5.2701 with MSE metric 7074.6963\n",
      "Epoch 9 batch 170 train Loss 5.3742 test Loss 5.3450 with MSE metric 8483.4443\n",
      "Epoch 9 batch 180 train Loss 5.2496 test Loss 5.3124 with MSE metric 6470.4014\n",
      "Epoch 9 batch 190 train Loss 5.2244 test Loss 5.3972 with MSE metric 6303.7715\n",
      "Epoch 9 batch 200 train Loss 5.3394 test Loss 5.3994 with MSE metric 7895.5376\n",
      "Epoch 9 batch 210 train Loss 5.2992 test Loss 5.2720 with MSE metric 7338.8628\n",
      "Epoch 9 batch 220 train Loss 5.2288 test Loss 5.3643 with MSE metric 6390.7578\n",
      "Epoch 9 batch 230 train Loss 5.3182 test Loss 5.3302 with MSE metric 7656.5137\n",
      "Epoch 9 batch 240 train Loss 5.3250 test Loss 5.3468 with MSE metric 7744.1387\n",
      "Time taken for 1 epoch: 27.529977083206177 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 5.3054 test Loss 5.3953 with MSE metric 7392.4746\n",
      "Epoch 10 batch 10 train Loss 5.3243 test Loss 5.3884 with MSE metric 7722.6797\n",
      "Epoch 10 batch 20 train Loss 5.3855 test Loss 5.3338 with MSE metric 8635.2920\n",
      "Epoch 10 batch 30 train Loss 5.2989 test Loss 5.3739 with MSE metric 7361.1265\n",
      "Epoch 10 batch 40 train Loss 5.2484 test Loss 5.3148 with MSE metric 6635.9380\n",
      "Epoch 10 batch 50 train Loss 5.3059 test Loss 5.2723 with MSE metric 7459.1001\n",
      "Epoch 10 batch 60 train Loss 5.3410 test Loss 5.4483 with MSE metric 8011.4595\n",
      "Epoch 10 batch 70 train Loss 5.3469 test Loss 5.4732 with MSE metric 8061.0059\n",
      "Epoch 10 batch 80 train Loss 5.3086 test Loss 5.3385 with MSE metric 7497.9727\n",
      "Epoch 10 batch 90 train Loss 5.3255 test Loss 5.3829 with MSE metric 7729.3101\n",
      "Epoch 10 batch 100 train Loss 5.2900 test Loss 5.3368 with MSE metric 7235.1289\n",
      "Epoch 10 batch 110 train Loss 5.3249 test Loss 5.3412 with MSE metric 7715.2041\n",
      "Epoch 10 batch 120 train Loss 5.3953 test Loss 5.3518 with MSE metric 8768.2832\n",
      "Epoch 10 batch 130 train Loss 5.3882 test Loss 5.4367 with MSE metric 8766.3037\n",
      "Epoch 10 batch 140 train Loss 5.4067 test Loss 5.2854 with MSE metric 8982.8809\n",
      "Epoch 10 batch 150 train Loss 5.3251 test Loss 5.3385 with MSE metric 7750.7949\n",
      "Epoch 10 batch 160 train Loss 5.2545 test Loss 5.2738 with MSE metric 6718.3892\n",
      "Epoch 10 batch 170 train Loss 5.2440 test Loss 5.4511 with MSE metric 6556.4121\n",
      "Epoch 10 batch 180 train Loss 5.3116 test Loss 5.2509 with MSE metric 7459.4248\n",
      "Epoch 10 batch 190 train Loss 5.3162 test Loss 5.3455 with MSE metric 7608.7129\n",
      "Epoch 10 batch 200 train Loss 5.2728 test Loss 5.3689 with MSE metric 6989.4678\n",
      "Epoch 10 batch 210 train Loss 5.3485 test Loss 5.3224 with MSE metric 8047.3154\n",
      "Epoch 10 batch 220 train Loss 5.2418 test Loss 5.3533 with MSE metric 6451.7212\n",
      "Epoch 10 batch 230 train Loss 5.3138 test Loss 5.3659 with MSE metric 7551.0801\n",
      "Epoch 10 batch 240 train Loss 5.2692 test Loss 5.3735 with MSE metric 6913.1367\n",
      "Time taken for 1 epoch: 27.693137884140015 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 5.2862 test Loss 5.3226 with MSE metric 7118.7593\n",
      "Epoch 11 batch 10 train Loss 5.2974 test Loss 5.3966 with MSE metric 7342.9712\n",
      "Epoch 11 batch 20 train Loss 5.2864 test Loss 5.3673 with MSE metric 7158.7964\n",
      "Epoch 11 batch 30 train Loss 5.3646 test Loss 5.3799 with MSE metric 8372.6367\n",
      "Epoch 11 batch 40 train Loss 5.2552 test Loss 5.2834 with MSE metric 6747.9395\n",
      "Epoch 11 batch 50 train Loss 5.3438 test Loss 5.3379 with MSE metric 8009.6060\n",
      "Epoch 11 batch 60 train Loss 5.3424 test Loss 5.3393 with MSE metric 7975.4800\n",
      "Epoch 11 batch 70 train Loss 5.2546 test Loss 5.3798 with MSE metric 6712.6909\n",
      "Epoch 11 batch 80 train Loss 5.2663 test Loss 5.4022 with MSE metric 6888.8340\n",
      "Epoch 11 batch 90 train Loss 5.3407 test Loss 5.3798 with MSE metric 7999.2598\n",
      "Epoch 11 batch 100 train Loss 5.3158 test Loss 5.3242 with MSE metric 7589.5283\n",
      "Epoch 11 batch 110 train Loss 5.3474 test Loss 5.4360 with MSE metric 7993.9180\n",
      "Epoch 11 batch 120 train Loss 5.2794 test Loss 5.4199 with MSE metric 7052.4229\n",
      "Epoch 11 batch 130 train Loss 5.4510 test Loss 5.4528 with MSE metric 9316.5039\n",
      "Epoch 11 batch 140 train Loss 5.2721 test Loss 5.3589 with MSE metric 6954.0796\n",
      "Epoch 11 batch 150 train Loss 5.3765 test Loss 5.2961 with MSE metric 8482.6875\n",
      "Epoch 11 batch 160 train Loss 5.2978 test Loss 5.4217 with MSE metric 7333.4707\n",
      "Epoch 11 batch 170 train Loss 5.2027 test Loss 5.4623 with MSE metric 6015.6030\n",
      "Epoch 11 batch 180 train Loss 5.3082 test Loss 5.2714 with MSE metric 7498.1904\n",
      "Epoch 11 batch 190 train Loss 5.3372 test Loss 5.4437 with MSE metric 7949.8857\n",
      "Epoch 11 batch 200 train Loss 5.3591 test Loss 5.3714 with MSE metric 8222.0684\n",
      "Epoch 11 batch 210 train Loss 5.2451 test Loss 5.3883 with MSE metric 6578.3628\n",
      "Epoch 11 batch 220 train Loss 5.2622 test Loss 5.3794 with MSE metric 6807.8242\n",
      "Epoch 11 batch 230 train Loss 5.3234 test Loss 5.2833 with MSE metric 7733.5068\n",
      "Epoch 11 batch 240 train Loss 5.2294 test Loss 5.3605 with MSE metric 6244.8354\n",
      "Time taken for 1 epoch: 27.689010858535767 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 5.3567 test Loss 5.3366 with MSE metric 8182.5527\n",
      "Epoch 12 batch 10 train Loss 5.2905 test Loss 5.4561 with MSE metric 7243.7163\n",
      "Epoch 12 batch 20 train Loss 5.1639 test Loss 5.2802 with MSE metric 5164.7725\n",
      "Epoch 12 batch 30 train Loss 5.3425 test Loss 5.3636 with MSE metric 8020.3037\n",
      "Epoch 12 batch 40 train Loss 5.3141 test Loss 5.3409 with MSE metric 7593.4795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 batch 50 train Loss 5.2323 test Loss 5.4770 with MSE metric 6420.9409\n",
      "Epoch 12 batch 60 train Loss 5.2427 test Loss 5.3771 with MSE metric 6472.5073\n",
      "Epoch 12 batch 70 train Loss 5.3223 test Loss 5.3763 with MSE metric 7712.1934\n",
      "Epoch 12 batch 80 train Loss 5.4112 test Loss 5.3833 with MSE metric 9044.3457\n",
      "Epoch 12 batch 90 train Loss 5.2680 test Loss 5.4529 with MSE metric 6919.2607\n",
      "Epoch 12 batch 100 train Loss 5.2028 test Loss 5.4810 with MSE metric 5981.0146\n",
      "Epoch 12 batch 110 train Loss 5.3260 test Loss 5.4718 with MSE metric 7757.2139\n",
      "Epoch 12 batch 120 train Loss 5.3832 test Loss 5.2961 with MSE metric 8608.3809\n",
      "Epoch 12 batch 130 train Loss 5.2799 test Loss 5.3087 with MSE metric 7036.4336\n",
      "Epoch 12 batch 140 train Loss 5.3311 test Loss 5.3596 with MSE metric 7788.6904\n",
      "Epoch 12 batch 150 train Loss 5.3213 test Loss 5.3949 with MSE metric 7667.3364\n",
      "Epoch 12 batch 160 train Loss 5.2538 test Loss 5.3747 with MSE metric 6689.4326\n",
      "Epoch 12 batch 170 train Loss 5.2982 test Loss 5.4047 with MSE metric 7326.6182\n",
      "Epoch 12 batch 180 train Loss 5.3584 test Loss 5.2998 with MSE metric 8292.3809\n",
      "Epoch 12 batch 190 train Loss 5.3427 test Loss 5.3414 with MSE metric 8011.8716\n",
      "Epoch 12 batch 200 train Loss 5.2824 test Loss 5.3826 with MSE metric 7116.5908\n",
      "Epoch 12 batch 210 train Loss 5.2789 test Loss 5.3699 with MSE metric 7061.9453\n",
      "Epoch 12 batch 220 train Loss 5.3109 test Loss 5.4130 with MSE metric 7522.8892\n",
      "Epoch 12 batch 230 train Loss 5.3298 test Loss 5.3230 with MSE metric 7819.9844\n",
      "Epoch 12 batch 240 train Loss 5.3466 test Loss 5.2717 with MSE metric 8050.5430\n",
      "Time taken for 1 epoch: 27.690535068511963 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 5.3542 test Loss 5.3554 with MSE metric 8187.3423\n",
      "Epoch 13 batch 10 train Loss 5.2543 test Loss 5.4303 with MSE metric 6687.4395\n",
      "Epoch 13 batch 20 train Loss 5.2855 test Loss 5.5153 with MSE metric 7171.7061\n",
      "Epoch 13 batch 30 train Loss 5.3048 test Loss 5.2726 with MSE metric 7451.7612\n",
      "Epoch 13 batch 40 train Loss 5.3897 test Loss 5.2623 with MSE metric 8699.4404\n",
      "Epoch 13 batch 50 train Loss 5.3577 test Loss 5.2471 with MSE metric 8256.8887\n",
      "Epoch 13 batch 60 train Loss 5.1205 test Loss 5.4375 with MSE metric 4878.4106\n",
      "Epoch 13 batch 70 train Loss 5.2140 test Loss 5.3790 with MSE metric 6179.0186\n",
      "Epoch 13 batch 80 train Loss 5.3148 test Loss 5.2718 with MSE metric 7592.4590\n",
      "Epoch 13 batch 90 train Loss 5.2304 test Loss 5.4269 with MSE metric 6361.3057\n",
      "Epoch 13 batch 100 train Loss 5.2433 test Loss 5.3396 with MSE metric 6588.4653\n",
      "Epoch 13 batch 110 train Loss 5.3600 test Loss 5.3978 with MSE metric 8277.2080\n",
      "Epoch 13 batch 120 train Loss 5.2229 test Loss 5.2891 with MSE metric 6226.8350\n",
      "Epoch 13 batch 130 train Loss 5.3616 test Loss 5.4014 with MSE metric 8183.0781\n",
      "Epoch 13 batch 140 train Loss 5.3358 test Loss 5.4011 with MSE metric 7889.4258\n",
      "Epoch 13 batch 150 train Loss 5.3078 test Loss 5.3846 with MSE metric 7494.4248\n",
      "Epoch 13 batch 160 train Loss 5.3830 test Loss 5.4119 with MSE metric 8695.2891\n",
      "Epoch 13 batch 170 train Loss 5.3153 test Loss 5.3337 with MSE metric 7610.3203\n",
      "Epoch 13 batch 180 train Loss 5.3197 test Loss 5.3398 with MSE metric 7656.1611\n",
      "Epoch 13 batch 190 train Loss 5.3090 test Loss 5.3269 with MSE metric 7510.6572\n",
      "Epoch 13 batch 200 train Loss 5.2381 test Loss 5.3452 with MSE metric 6350.8774\n",
      "Epoch 13 batch 210 train Loss 5.3241 test Loss 5.2645 with MSE metric 7742.4639\n",
      "Epoch 13 batch 220 train Loss 5.3165 test Loss 5.3598 with MSE metric 7618.8003\n",
      "Epoch 13 batch 230 train Loss 5.3989 test Loss 5.3714 with MSE metric 8894.8018\n",
      "Epoch 13 batch 240 train Loss 5.3099 test Loss 5.4173 with MSE metric 7527.3540\n",
      "Time taken for 1 epoch: 27.8957200050354 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 5.3389 test Loss 5.4333 with MSE metric 7962.6123\n",
      "Epoch 14 batch 10 train Loss 5.2688 test Loss 5.3824 with MSE metric 6913.7373\n",
      "Epoch 14 batch 20 train Loss 5.3512 test Loss 5.3566 with MSE metric 8032.9380\n",
      "Epoch 14 batch 30 train Loss 5.3053 test Loss 5.2966 with MSE metric 7459.3818\n",
      "Epoch 14 batch 40 train Loss 5.3711 test Loss 5.2871 with MSE metric 8504.5273\n",
      "Epoch 14 batch 50 train Loss 5.3455 test Loss 5.3727 with MSE metric 8041.7183\n",
      "Epoch 14 batch 60 train Loss 5.3255 test Loss 5.4426 with MSE metric 7705.2617\n",
      "Epoch 14 batch 70 train Loss 5.1668 test Loss 5.3767 with MSE metric 5476.8633\n",
      "Epoch 14 batch 80 train Loss 5.2680 test Loss 5.3144 with MSE metric 6918.7510\n",
      "Epoch 14 batch 90 train Loss 5.3473 test Loss 5.3854 with MSE metric 8093.5869\n",
      "Epoch 14 batch 100 train Loss 5.3668 test Loss 5.2737 with MSE metric 8351.7627\n",
      "Epoch 14 batch 110 train Loss 5.3760 test Loss 5.2958 with MSE metric 8571.9746\n",
      "Epoch 14 batch 120 train Loss 5.3243 test Loss 5.3041 with MSE metric 7723.9219\n",
      "Epoch 14 batch 130 train Loss 5.2724 test Loss 5.3750 with MSE metric 6976.0806\n",
      "Epoch 14 batch 140 train Loss 5.2865 test Loss 5.4422 with MSE metric 7163.3477\n",
      "Epoch 14 batch 150 train Loss 5.3330 test Loss 5.2875 with MSE metric 7884.0015\n",
      "Epoch 14 batch 160 train Loss 5.2118 test Loss 5.3776 with MSE metric 6080.0107\n",
      "Epoch 14 batch 170 train Loss 5.3861 test Loss 5.2864 with MSE metric 8621.6660\n",
      "Epoch 14 batch 180 train Loss 5.3235 test Loss 5.3482 with MSE metric 7732.0796\n",
      "Epoch 14 batch 190 train Loss 5.3382 test Loss 5.4149 with MSE metric 7876.6201\n",
      "Epoch 14 batch 200 train Loss 5.2272 test Loss 5.3639 with MSE metric 6330.4531\n",
      "Epoch 14 batch 210 train Loss 5.3730 test Loss 5.2580 with MSE metric 8509.4678\n",
      "Epoch 14 batch 220 train Loss 5.2932 test Loss 5.3053 with MSE metric 7275.2622\n",
      "Epoch 14 batch 230 train Loss 5.3235 test Loss 5.3275 with MSE metric 7726.6475\n",
      "Epoch 14 batch 240 train Loss 5.3560 test Loss 5.3655 with MSE metric 8191.4414\n",
      "Time taken for 1 epoch: 24.781702995300293 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 5.3505 test Loss 5.3879 with MSE metric 8149.2983\n",
      "Epoch 15 batch 10 train Loss 5.4791 test Loss 5.3664 with MSE metric 10037.8311\n",
      "Epoch 15 batch 20 train Loss 5.2977 test Loss 5.3344 with MSE metric 7341.9326\n",
      "Epoch 15 batch 30 train Loss 5.2394 test Loss 5.3092 with MSE metric 6445.4565\n",
      "Epoch 15 batch 40 train Loss 5.3016 test Loss 5.3576 with MSE metric 7389.9429\n",
      "Epoch 15 batch 50 train Loss 5.3394 test Loss 5.3310 with MSE metric 7964.5625\n",
      "Epoch 15 batch 60 train Loss 5.2208 test Loss 5.4064 with MSE metric 6271.8096\n",
      "Epoch 15 batch 70 train Loss 5.3234 test Loss 5.3020 with MSE metric 7735.3672\n",
      "Epoch 15 batch 80 train Loss 5.3359 test Loss 5.3174 with MSE metric 7931.0171\n",
      "Epoch 15 batch 90 train Loss 5.3809 test Loss 5.4476 with MSE metric 8608.6377\n",
      "Epoch 15 batch 100 train Loss 5.3233 test Loss 5.5115 with MSE metric 7698.6821\n",
      "Epoch 15 batch 110 train Loss 5.3914 test Loss 5.4390 with MSE metric 8786.8359\n",
      "Epoch 15 batch 120 train Loss 5.3272 test Loss 5.4083 with MSE metric 7794.1279\n",
      "Epoch 15 batch 130 train Loss 5.3249 test Loss 5.4902 with MSE metric 7654.3984\n",
      "Epoch 15 batch 140 train Loss 5.1872 test Loss 5.3770 with MSE metric 5623.6260\n",
      "Epoch 15 batch 150 train Loss 5.3657 test Loss 5.2809 with MSE metric 8335.6211\n",
      "Epoch 15 batch 160 train Loss 5.3221 test Loss 5.3913 with MSE metric 7714.9971\n",
      "Epoch 15 batch 170 train Loss 5.3780 test Loss 5.2940 with MSE metric 8571.5127\n",
      "Epoch 15 batch 180 train Loss 5.2833 test Loss 5.2601 with MSE metric 7117.2842\n",
      "Epoch 15 batch 190 train Loss 5.2897 test Loss 5.4236 with MSE metric 7231.0791\n",
      "Epoch 15 batch 200 train Loss 5.2289 test Loss 5.4161 with MSE metric 6284.5269\n",
      "Epoch 15 batch 210 train Loss 5.2941 test Loss 5.2152 with MSE metric 7282.3076\n",
      "Epoch 15 batch 220 train Loss 5.3850 test Loss 5.2943 with MSE metric 8682.2207\n",
      "Epoch 15 batch 230 train Loss 5.3071 test Loss 5.4044 with MSE metric 7484.5879\n",
      "Epoch 15 batch 240 train Loss 5.3870 test Loss 5.4018 with MSE metric 8480.4053\n",
      "Time taken for 1 epoch: 23.905124187469482 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 5.3450 test Loss 5.3797 with MSE metric 8028.5762\n",
      "Epoch 16 batch 10 train Loss 5.3067 test Loss 5.3951 with MSE metric 7457.2656\n",
      "Epoch 16 batch 20 train Loss 5.2840 test Loss 5.2847 with MSE metric 7126.6719\n",
      "Epoch 16 batch 30 train Loss 5.2488 test Loss 5.2916 with MSE metric 6594.8174\n",
      "Epoch 16 batch 40 train Loss 5.2677 test Loss 5.3451 with MSE metric 6901.7397\n",
      "Epoch 16 batch 50 train Loss 5.3361 test Loss 5.2451 with MSE metric 7878.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 batch 60 train Loss 5.3081 test Loss 5.4167 with MSE metric 7501.0283\n",
      "Epoch 16 batch 70 train Loss 5.3401 test Loss 5.3055 with MSE metric 7996.6289\n",
      "Epoch 16 batch 80 train Loss 5.2296 test Loss 5.3125 with MSE metric 6339.5635\n",
      "Epoch 16 batch 90 train Loss 5.3085 test Loss 5.2942 with MSE metric 7493.1260\n",
      "Epoch 16 batch 100 train Loss 5.2698 test Loss 5.3992 with MSE metric 6927.7588\n",
      "Epoch 16 batch 110 train Loss 5.2844 test Loss 5.3589 with MSE metric 7150.3516\n",
      "Epoch 16 batch 120 train Loss 5.3036 test Loss 5.3022 with MSE metric 7426.6602\n",
      "Epoch 16 batch 130 train Loss 5.2113 test Loss 5.3171 with MSE metric 6098.0010\n",
      "Epoch 16 batch 140 train Loss 5.3042 test Loss 5.5114 with MSE metric 7410.3408\n",
      "Epoch 16 batch 150 train Loss 5.2805 test Loss 5.3687 with MSE metric 7014.1953\n",
      "Epoch 16 batch 160 train Loss 5.2917 test Loss 5.3728 with MSE metric 7257.1143\n",
      "Epoch 16 batch 170 train Loss 5.2513 test Loss 5.4339 with MSE metric 6672.2949\n",
      "Epoch 16 batch 180 train Loss 5.3040 test Loss 5.3506 with MSE metric 7434.5303\n",
      "Epoch 16 batch 190 train Loss 5.2291 test Loss 5.3166 with MSE metric 6290.4678\n",
      "Epoch 16 batch 200 train Loss 5.3006 test Loss 5.3276 with MSE metric 7360.0576\n",
      "Epoch 16 batch 210 train Loss 5.3545 test Loss 5.3524 with MSE metric 8167.5747\n",
      "Epoch 16 batch 220 train Loss 5.2970 test Loss 5.3454 with MSE metric 7288.2725\n",
      "Epoch 16 batch 230 train Loss 5.3512 test Loss 5.3319 with MSE metric 8107.3174\n",
      "Epoch 16 batch 240 train Loss 5.1986 test Loss 5.4149 with MSE metric 5947.3926\n",
      "Time taken for 1 epoch: 26.261240005493164 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 5.2979 test Loss 5.3973 with MSE metric 7343.7568\n",
      "Epoch 17 batch 10 train Loss 5.2404 test Loss 5.4010 with MSE metric 6507.6289\n",
      "Epoch 17 batch 20 train Loss 5.1668 test Loss 5.3624 with MSE metric 5442.5127\n",
      "Epoch 17 batch 30 train Loss 5.3117 test Loss 5.4610 with MSE metric 7555.5498\n",
      "Epoch 17 batch 40 train Loss 5.3846 test Loss 5.3337 with MSE metric 8674.5244\n",
      "Epoch 17 batch 50 train Loss 5.1700 test Loss 5.3418 with MSE metric 5521.8979\n",
      "Epoch 17 batch 60 train Loss 5.2802 test Loss 5.4387 with MSE metric 7064.7363\n",
      "Epoch 17 batch 70 train Loss 5.3559 test Loss 5.2928 with MSE metric 8245.9180\n",
      "Epoch 17 batch 80 train Loss 5.3514 test Loss 5.3708 with MSE metric 8115.4331\n",
      "Epoch 17 batch 90 train Loss 5.3137 test Loss 5.3079 with MSE metric 7584.6768\n",
      "Epoch 17 batch 100 train Loss 5.3834 test Loss 5.4033 with MSE metric 8587.9375\n",
      "Epoch 17 batch 110 train Loss 5.3833 test Loss 5.3480 with MSE metric 8624.8457\n",
      "Epoch 17 batch 120 train Loss 5.4002 test Loss 5.4066 with MSE metric 8875.2031\n",
      "Epoch 17 batch 130 train Loss 5.3183 test Loss 5.3755 with MSE metric 7611.3936\n",
      "Epoch 17 batch 140 train Loss 5.3596 test Loss 5.3622 with MSE metric 8207.0840\n",
      "Epoch 17 batch 150 train Loss 5.3154 test Loss 5.3950 with MSE metric 7613.1533\n",
      "Epoch 17 batch 160 train Loss 5.3828 test Loss 5.2750 with MSE metric 8604.4707\n",
      "Epoch 17 batch 170 train Loss 5.3400 test Loss 5.3209 with MSE metric 7996.5439\n",
      "Epoch 17 batch 180 train Loss 5.3067 test Loss 5.3848 with MSE metric 7476.0400\n",
      "Epoch 17 batch 190 train Loss 5.2039 test Loss 5.4080 with MSE metric 6003.3477\n",
      "Epoch 17 batch 200 train Loss 5.3090 test Loss 5.2972 with MSE metric 7515.1138\n",
      "Epoch 17 batch 210 train Loss 5.2140 test Loss 5.3322 with MSE metric 6129.6855\n",
      "Epoch 17 batch 220 train Loss 5.2625 test Loss 5.3774 with MSE metric 6770.1475\n",
      "Epoch 17 batch 230 train Loss 5.3351 test Loss 5.3712 with MSE metric 7845.6807\n",
      "Epoch 17 batch 240 train Loss 5.4187 test Loss 5.3818 with MSE metric 9305.3076\n",
      "Time taken for 1 epoch: 26.640191793441772 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 5.3112 test Loss 5.3920 with MSE metric 7535.0000\n",
      "Epoch 18 batch 10 train Loss 5.3177 test Loss 5.2694 with MSE metric 7646.1182\n",
      "Epoch 18 batch 20 train Loss 5.2693 test Loss 5.3654 with MSE metric 6919.6562\n",
      "Epoch 18 batch 30 train Loss 5.3314 test Loss 5.3003 with MSE metric 7853.6699\n",
      "Epoch 18 batch 40 train Loss 5.2839 test Loss 5.2730 with MSE metric 7130.6226\n",
      "Epoch 18 batch 50 train Loss 5.2491 test Loss 5.4793 with MSE metric 6628.7974\n",
      "Epoch 18 batch 60 train Loss 5.2658 test Loss 5.3592 with MSE metric 6866.7900\n",
      "Epoch 18 batch 70 train Loss 5.3614 test Loss 5.3632 with MSE metric 8256.6406\n",
      "Epoch 18 batch 80 train Loss 5.3582 test Loss 5.4683 with MSE metric 8171.1831\n",
      "Epoch 18 batch 90 train Loss 5.3219 test Loss 5.3788 with MSE metric 7712.1797\n",
      "Epoch 18 batch 100 train Loss 5.2267 test Loss 5.4326 with MSE metric 6269.2373\n",
      "Epoch 18 batch 110 train Loss 5.3244 test Loss 5.3164 with MSE metric 7620.3384\n",
      "Epoch 18 batch 120 train Loss 5.2417 test Loss 5.3455 with MSE metric 6476.6753\n",
      "Epoch 18 batch 130 train Loss 5.2672 test Loss 5.2597 with MSE metric 6908.2139\n",
      "Epoch 18 batch 140 train Loss 5.1411 test Loss 5.3797 with MSE metric 5157.8789\n",
      "Epoch 18 batch 150 train Loss 5.3278 test Loss 5.3329 with MSE metric 7801.7705\n",
      "Epoch 18 batch 160 train Loss 5.2665 test Loss 5.3028 with MSE metric 6773.2598\n",
      "Epoch 18 batch 170 train Loss 5.2877 test Loss 5.3688 with MSE metric 7202.4087\n",
      "Epoch 18 batch 180 train Loss 5.3466 test Loss 5.3531 with MSE metric 8066.6182\n",
      "Epoch 18 batch 190 train Loss 5.3268 test Loss 5.4492 with MSE metric 7787.8594\n",
      "Epoch 18 batch 200 train Loss 5.3811 test Loss 5.3220 with MSE metric 8617.2754\n",
      "Epoch 18 batch 210 train Loss 5.3487 test Loss 5.3311 with MSE metric 8117.6787\n",
      "Epoch 18 batch 220 train Loss 5.3148 test Loss 5.3292 with MSE metric 7599.7788\n",
      "Epoch 18 batch 230 train Loss 5.2550 test Loss 5.3245 with MSE metric 6670.6963\n",
      "Epoch 18 batch 240 train Loss 5.3527 test Loss 5.4264 with MSE metric 8202.0391\n",
      "Time taken for 1 epoch: 24.569818019866943 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 5.3267 test Loss 5.3012 with MSE metric 7775.7705\n",
      "Epoch 19 batch 10 train Loss 5.3418 test Loss 5.3675 with MSE metric 8014.1797\n",
      "Epoch 19 batch 20 train Loss 5.3231 test Loss 5.3671 with MSE metric 7728.9766\n",
      "Epoch 19 batch 30 train Loss 5.2834 test Loss 5.4320 with MSE metric 7139.3369\n",
      "Epoch 19 batch 40 train Loss 5.3596 test Loss 5.3847 with MSE metric 8314.4062\n",
      "Epoch 19 batch 50 train Loss 5.3050 test Loss 5.2865 with MSE metric 7430.4199\n",
      "Epoch 19 batch 60 train Loss 5.2260 test Loss 5.3908 with MSE metric 6250.9238\n",
      "Epoch 19 batch 70 train Loss 5.3492 test Loss 5.3302 with MSE metric 8144.0889\n",
      "Epoch 19 batch 80 train Loss 5.3383 test Loss 5.2500 with MSE metric 7934.5684\n",
      "Epoch 19 batch 90 train Loss 5.3385 test Loss 5.3133 with MSE metric 7952.6577\n",
      "Epoch 19 batch 100 train Loss 5.3005 test Loss 5.4157 with MSE metric 7389.4673\n",
      "Epoch 19 batch 110 train Loss 5.3010 test Loss 5.4436 with MSE metric 7388.6123\n",
      "Epoch 19 batch 120 train Loss 5.3067 test Loss 5.4231 with MSE metric 7475.7114\n",
      "Epoch 19 batch 130 train Loss 5.2678 test Loss 5.4258 with MSE metric 6914.7207\n",
      "Epoch 19 batch 140 train Loss 5.4267 test Loss 5.3752 with MSE metric 9154.3574\n",
      "Epoch 19 batch 150 train Loss 5.2667 test Loss 5.2590 with MSE metric 6805.7827\n",
      "Epoch 19 batch 160 train Loss 5.2390 test Loss 5.3061 with MSE metric 6526.0537\n",
      "Epoch 19 batch 170 train Loss 5.3090 test Loss 5.3697 with MSE metric 7514.0352\n",
      "Epoch 19 batch 180 train Loss 5.2928 test Loss 5.3080 with MSE metric 7273.2046\n",
      "Epoch 19 batch 190 train Loss 5.3279 test Loss 5.3036 with MSE metric 7797.1567\n",
      "Epoch 19 batch 200 train Loss 5.2348 test Loss 5.3551 with MSE metric 6289.1455\n",
      "Epoch 19 batch 210 train Loss 5.4184 test Loss 5.3082 with MSE metric 9178.7412\n",
      "Epoch 19 batch 220 train Loss 5.2879 test Loss 5.3501 with MSE metric 7204.7832\n",
      "Epoch 19 batch 230 train Loss 5.3540 test Loss 5.2808 with MSE metric 8173.3926\n",
      "Epoch 19 batch 240 train Loss 5.3947 test Loss 5.2313 with MSE metric 8851.8906\n",
      "Time taken for 1 epoch: 24.392998218536377 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 5.2521 test Loss 5.2631 with MSE metric 6672.1064\n",
      "Epoch 20 batch 10 train Loss 5.3041 test Loss 5.3422 with MSE metric 7442.6050\n",
      "Epoch 20 batch 20 train Loss 5.3292 test Loss 5.3914 with MSE metric 7823.5894\n",
      "Epoch 20 batch 30 train Loss 5.3379 test Loss 5.3036 with MSE metric 7962.9761\n",
      "Epoch 20 batch 40 train Loss 5.3073 test Loss 5.4395 with MSE metric 7490.3525\n",
      "Epoch 20 batch 50 train Loss 5.2784 test Loss 5.3921 with MSE metric 7064.9756\n",
      "Epoch 20 batch 60 train Loss 5.2624 test Loss 5.3507 with MSE metric 6839.6509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 batch 70 train Loss 5.3765 test Loss 5.4545 with MSE metric 8501.7148\n",
      "Epoch 20 batch 80 train Loss 5.3774 test Loss 5.3016 with MSE metric 8579.6816\n",
      "Epoch 20 batch 90 train Loss 5.2769 test Loss 5.3781 with MSE metric 6971.7314\n",
      "Epoch 20 batch 100 train Loss 5.2834 test Loss 5.2555 with MSE metric 7140.6270\n",
      "Epoch 20 batch 110 train Loss 5.3379 test Loss 5.4733 with MSE metric 7902.1211\n",
      "Epoch 20 batch 120 train Loss 5.3165 test Loss 5.2908 with MSE metric 7629.6182\n",
      "Epoch 20 batch 130 train Loss 5.3233 test Loss 5.4684 with MSE metric 7729.4155\n",
      "Epoch 20 batch 140 train Loss 5.3484 test Loss 5.3744 with MSE metric 8063.7632\n",
      "Epoch 20 batch 150 train Loss 5.2689 test Loss 5.3994 with MSE metric 6908.4932\n",
      "Epoch 20 batch 160 train Loss 5.4169 test Loss 5.4031 with MSE metric 8961.4062\n",
      "Epoch 20 batch 170 train Loss 5.2086 test Loss 5.2898 with MSE metric 5927.1987\n",
      "Epoch 20 batch 180 train Loss 5.3014 test Loss 5.3365 with MSE metric 7355.6338\n",
      "Epoch 20 batch 190 train Loss 5.3013 test Loss 5.3387 with MSE metric 7398.9189\n",
      "Epoch 20 batch 200 train Loss 5.3730 test Loss 5.3994 with MSE metric 8527.2939\n",
      "Epoch 20 batch 210 train Loss 5.2838 test Loss 5.3530 with MSE metric 7139.7783\n",
      "Epoch 20 batch 220 train Loss 5.3133 test Loss 5.3295 with MSE metric 7581.0439\n",
      "Epoch 20 batch 230 train Loss 5.3660 test Loss 5.3501 with MSE metric 8413.8848\n",
      "Epoch 20 batch 240 train Loss 5.2482 test Loss 5.3516 with MSE metric 6635.1016\n",
      "Time taken for 1 epoch: 24.3204288482666 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 5.1848 test Loss 5.3731 with MSE metric 5728.2188\n",
      "Epoch 21 batch 10 train Loss 5.1545 test Loss 5.5613 with MSE metric 5319.0054\n",
      "Epoch 21 batch 20 train Loss 5.4456 test Loss 5.3914 with MSE metric 9467.3389\n",
      "Epoch 21 batch 30 train Loss 5.2694 test Loss 5.3624 with MSE metric 6870.3696\n",
      "Epoch 21 batch 40 train Loss 5.4053 test Loss 5.4136 with MSE metric 8631.0527\n",
      "Epoch 21 batch 50 train Loss 5.2899 test Loss 5.4365 with MSE metric 7175.8271\n",
      "Epoch 21 batch 60 train Loss 5.2177 test Loss 5.3072 with MSE metric 6192.2021\n",
      "Epoch 21 batch 70 train Loss 5.2974 test Loss 5.3887 with MSE metric 7334.5107\n",
      "Epoch 21 batch 80 train Loss 5.3357 test Loss 5.2470 with MSE metric 7928.1104\n",
      "Epoch 21 batch 90 train Loss 5.3297 test Loss 5.2631 with MSE metric 7816.0366\n",
      "Epoch 21 batch 100 train Loss 5.3661 test Loss 5.3218 with MSE metric 8329.0166\n",
      "Epoch 21 batch 110 train Loss 5.3203 test Loss 5.4066 with MSE metric 7677.6714\n",
      "Epoch 21 batch 120 train Loss 5.3194 test Loss 5.3423 with MSE metric 7661.4097\n",
      "Epoch 21 batch 130 train Loss 5.3656 test Loss 5.3424 with MSE metric 8415.3633\n",
      "Epoch 21 batch 140 train Loss 5.2510 test Loss 5.4278 with MSE metric 6675.5107\n",
      "Epoch 21 batch 150 train Loss 5.3891 test Loss 5.3581 with MSE metric 8763.9395\n",
      "Epoch 21 batch 160 train Loss 5.4064 test Loss 5.2838 with MSE metric 9046.0078\n",
      "Epoch 21 batch 170 train Loss 5.3358 test Loss 5.5053 with MSE metric 7817.2651\n",
      "Epoch 21 batch 180 train Loss 5.2459 test Loss 5.3898 with MSE metric 6495.0410\n",
      "Epoch 21 batch 190 train Loss 5.2605 test Loss 5.3104 with MSE metric 6815.9434\n",
      "Epoch 21 batch 200 train Loss 5.2971 test Loss 5.2732 with MSE metric 7309.0215\n",
      "Epoch 21 batch 210 train Loss 5.2804 test Loss 5.3430 with MSE metric 7096.1060\n",
      "Epoch 21 batch 220 train Loss 5.3555 test Loss 5.4008 with MSE metric 8154.8237\n",
      "Epoch 21 batch 230 train Loss 5.3048 test Loss 5.3072 with MSE metric 7453.1318\n",
      "Epoch 21 batch 240 train Loss 5.3040 test Loss 5.3477 with MSE metric 7433.5239\n",
      "Time taken for 1 epoch: 24.35892605781555 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 5.2443 test Loss 5.4208 with MSE metric 6603.3135\n",
      "Epoch 22 batch 10 train Loss 5.3332 test Loss 5.4490 with MSE metric 7874.5815\n",
      "Epoch 22 batch 20 train Loss 5.3736 test Loss 5.3973 with MSE metric 8495.3047\n",
      "Epoch 22 batch 30 train Loss 5.1811 test Loss 5.3678 with MSE metric 5454.9893\n",
      "Epoch 22 batch 40 train Loss 5.3199 test Loss 5.4205 with MSE metric 7644.7236\n",
      "Epoch 22 batch 50 train Loss 5.2914 test Loss 5.2962 with MSE metric 7253.7974\n",
      "Epoch 22 batch 60 train Loss 5.2873 test Loss 5.4951 with MSE metric 7178.9434\n",
      "Epoch 22 batch 70 train Loss 5.3058 test Loss 5.2390 with MSE metric 7467.5400\n",
      "Epoch 22 batch 80 train Loss 5.3746 test Loss 5.2924 with MSE metric 8555.5898\n",
      "Epoch 22 batch 90 train Loss 5.3085 test Loss 5.4092 with MSE metric 7508.8311\n",
      "Epoch 22 batch 100 train Loss 5.2575 test Loss 5.3451 with MSE metric 6749.2480\n",
      "Epoch 22 batch 110 train Loss 5.3352 test Loss 5.3787 with MSE metric 7911.4937\n",
      "Epoch 22 batch 120 train Loss 5.1131 test Loss 5.3370 with MSE metric 4627.0498\n",
      "Epoch 22 batch 130 train Loss 5.1762 test Loss 5.4232 with MSE metric 5589.5752\n",
      "Epoch 22 batch 140 train Loss 5.3003 test Loss 5.3927 with MSE metric 7381.8989\n",
      "Epoch 22 batch 150 train Loss 5.3313 test Loss 5.4086 with MSE metric 7858.6270\n",
      "Epoch 22 batch 160 train Loss 5.3912 test Loss 5.3120 with MSE metric 8665.9062\n",
      "Epoch 22 batch 170 train Loss 5.3135 test Loss 5.4407 with MSE metric 7566.6890\n",
      "Epoch 22 batch 180 train Loss 5.2422 test Loss 5.3009 with MSE metric 6495.5518\n",
      "Epoch 22 batch 190 train Loss 5.1701 test Loss 5.3951 with MSE metric 5541.3975\n",
      "Epoch 22 batch 200 train Loss 5.2876 test Loss 5.4898 with MSE metric 7200.3086\n",
      "Epoch 22 batch 210 train Loss 5.2783 test Loss 5.3881 with MSE metric 7060.3721\n",
      "Epoch 22 batch 220 train Loss 5.2534 test Loss 5.4649 with MSE metric 6719.3135\n",
      "Epoch 22 batch 230 train Loss 5.3048 test Loss 5.3963 with MSE metric 7406.5649\n",
      "Epoch 22 batch 240 train Loss 5.2562 test Loss 5.3270 with MSE metric 6657.6357\n",
      "Time taken for 1 epoch: 25.567179203033447 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 5.2090 test Loss 5.3973 with MSE metric 6128.4502\n",
      "Epoch 23 batch 10 train Loss 5.2527 test Loss 5.4206 with MSE metric 6599.4111\n",
      "Epoch 23 batch 20 train Loss 5.2751 test Loss 5.2721 with MSE metric 7008.7793\n",
      "Epoch 23 batch 30 train Loss 5.3652 test Loss 5.4777 with MSE metric 8278.0830\n",
      "Epoch 23 batch 40 train Loss 5.3073 test Loss 5.3763 with MSE metric 7490.2007\n",
      "Epoch 23 batch 50 train Loss 5.1766 test Loss 5.3606 with MSE metric 5510.5459\n",
      "Epoch 23 batch 60 train Loss 5.2449 test Loss 5.3207 with MSE metric 6540.6904\n",
      "Epoch 23 batch 70 train Loss 5.3467 test Loss 5.3704 with MSE metric 8102.9878\n",
      "Epoch 23 batch 80 train Loss 5.3434 test Loss 5.3120 with MSE metric 8019.2920\n",
      "Epoch 23 batch 90 train Loss 5.3341 test Loss 5.3910 with MSE metric 7868.5527\n",
      "Epoch 23 batch 100 train Loss 5.3920 test Loss 5.3558 with MSE metric 8690.1328\n",
      "Epoch 23 batch 110 train Loss 5.3175 test Loss 5.2072 with MSE metric 7617.6865\n",
      "Epoch 23 batch 120 train Loss 5.2865 test Loss 5.3726 with MSE metric 7141.2881\n",
      "Epoch 23 batch 130 train Loss 5.2478 test Loss 5.3698 with MSE metric 6584.3789\n",
      "Epoch 23 batch 140 train Loss 5.2401 test Loss 5.3406 with MSE metric 6526.6309\n",
      "Epoch 23 batch 150 train Loss 5.2832 test Loss 5.3578 with MSE metric 7136.1836\n",
      "Epoch 23 batch 160 train Loss 5.2455 test Loss 5.4110 with MSE metric 6552.3252\n",
      "Epoch 23 batch 170 train Loss 5.2071 test Loss 5.3808 with MSE metric 5992.5518\n",
      "Epoch 23 batch 180 train Loss 5.3257 test Loss 5.4697 with MSE metric 7769.0879\n",
      "Epoch 23 batch 190 train Loss 5.2848 test Loss 5.3484 with MSE metric 7158.0967\n",
      "Epoch 23 batch 200 train Loss 5.3636 test Loss 5.3062 with MSE metric 8325.0000\n",
      "Epoch 23 batch 210 train Loss 5.3188 test Loss 5.2583 with MSE metric 7664.6074\n",
      "Epoch 23 batch 220 train Loss 5.2136 test Loss 5.3974 with MSE metric 6145.0493\n",
      "Epoch 23 batch 230 train Loss 5.2246 test Loss 5.3462 with MSE metric 6245.4971\n",
      "Epoch 23 batch 240 train Loss 5.2768 test Loss 5.3671 with MSE metric 7038.7529\n",
      "Time taken for 1 epoch: 25.103767156600952 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 5.3227 test Loss 5.4298 with MSE metric 7723.7188\n",
      "Epoch 24 batch 10 train Loss 5.2830 test Loss 5.3946 with MSE metric 7123.3936\n",
      "Epoch 24 batch 20 train Loss 5.3143 test Loss 5.3545 with MSE metric 7581.5679\n",
      "Epoch 24 batch 30 train Loss 5.2963 test Loss 5.3191 with MSE metric 7292.0850\n",
      "Epoch 24 batch 40 train Loss 5.1740 test Loss 5.4021 with MSE metric 5397.3467\n",
      "Epoch 24 batch 50 train Loss 5.3269 test Loss 5.2895 with MSE metric 7716.9243\n",
      "Epoch 24 batch 60 train Loss 5.3474 test Loss 5.3710 with MSE metric 8097.2510\n",
      "Epoch 24 batch 70 train Loss 5.2373 test Loss 5.3229 with MSE metric 6403.2603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 batch 80 train Loss 5.3227 test Loss 5.3723 with MSE metric 7722.0688\n",
      "Epoch 24 batch 90 train Loss 5.2788 test Loss 5.4071 with MSE metric 7052.8052\n",
      "Epoch 24 batch 100 train Loss 5.3226 test Loss 5.2790 with MSE metric 7625.8306\n",
      "Epoch 24 batch 110 train Loss 5.3138 test Loss 5.3249 with MSE metric 7587.9473\n",
      "Epoch 24 batch 120 train Loss 5.2280 test Loss 5.3580 with MSE metric 6222.6162\n",
      "Epoch 24 batch 130 train Loss 5.2136 test Loss 5.5158 with MSE metric 6096.2202\n",
      "Epoch 24 batch 140 train Loss 5.3582 test Loss 5.3409 with MSE metric 8267.5576\n",
      "Epoch 24 batch 150 train Loss 5.2662 test Loss 5.4692 with MSE metric 6897.3403\n",
      "Epoch 24 batch 160 train Loss 5.2827 test Loss 5.3919 with MSE metric 7130.6953\n",
      "Epoch 24 batch 170 train Loss 5.3766 test Loss 5.3910 with MSE metric 8543.7656\n",
      "Epoch 24 batch 180 train Loss 5.3430 test Loss 5.3930 with MSE metric 8042.4297\n",
      "Epoch 24 batch 190 train Loss 5.3402 test Loss 5.2976 with MSE metric 7900.7344\n",
      "Epoch 24 batch 200 train Loss 5.3143 test Loss 5.3380 with MSE metric 7594.6216\n",
      "Epoch 24 batch 210 train Loss 5.3179 test Loss 5.3286 with MSE metric 7576.2080\n",
      "Epoch 24 batch 220 train Loss 5.3365 test Loss 5.3197 with MSE metric 7928.0576\n",
      "Epoch 24 batch 230 train Loss 5.3641 test Loss 5.3991 with MSE metric 8328.0684\n",
      "Epoch 24 batch 240 train Loss 5.3861 test Loss 5.3366 with MSE metric 8562.6836\n",
      "Time taken for 1 epoch: 25.735514879226685 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 5.2864 test Loss 5.3552 with MSE metric 7144.5205\n",
      "Epoch 25 batch 10 train Loss 5.3111 test Loss 5.3812 with MSE metric 7540.7026\n",
      "Epoch 25 batch 20 train Loss 5.3952 test Loss 5.4068 with MSE metric 8673.5957\n",
      "Epoch 25 batch 30 train Loss 5.2858 test Loss 5.2666 with MSE metric 7160.2959\n",
      "Epoch 25 batch 40 train Loss 5.4281 test Loss 5.4115 with MSE metric 9279.6270\n",
      "Epoch 25 batch 50 train Loss 5.4095 test Loss 5.3234 with MSE metric 8942.3203\n",
      "Epoch 25 batch 60 train Loss 5.2434 test Loss 5.3264 with MSE metric 6501.0527\n",
      "Epoch 25 batch 70 train Loss 5.2773 test Loss 5.3004 with MSE metric 7052.4551\n",
      "Epoch 25 batch 80 train Loss 5.3122 test Loss 5.3181 with MSE metric 7485.8711\n",
      "Epoch 25 batch 90 train Loss 5.2331 test Loss 5.3175 with MSE metric 6451.3740\n",
      "Epoch 25 batch 100 train Loss 5.2417 test Loss 5.2032 with MSE metric 6560.5610\n",
      "Epoch 25 batch 110 train Loss 5.3144 test Loss 5.3809 with MSE metric 7590.2983\n",
      "Epoch 25 batch 120 train Loss 5.3177 test Loss 5.2811 with MSE metric 7647.4717\n",
      "Epoch 25 batch 130 train Loss 5.2535 test Loss 5.3628 with MSE metric 6723.1172\n",
      "Epoch 25 batch 140 train Loss 5.3321 test Loss 5.3623 with MSE metric 7860.5674\n",
      "Epoch 25 batch 150 train Loss 5.2006 test Loss 5.3648 with MSE metric 5846.3047\n",
      "Epoch 25 batch 160 train Loss 5.2794 test Loss 5.2679 with MSE metric 7081.0649\n",
      "Epoch 25 batch 170 train Loss 5.2790 test Loss 5.4428 with MSE metric 7073.8467\n",
      "Epoch 25 batch 180 train Loss 5.3427 test Loss 5.4094 with MSE metric 8035.6445\n",
      "Epoch 25 batch 190 train Loss 5.2825 test Loss 5.3827 with MSE metric 7128.6562\n",
      "Epoch 25 batch 200 train Loss 5.3138 test Loss 5.4396 with MSE metric 7583.5918\n",
      "Epoch 25 batch 210 train Loss 5.3075 test Loss 5.4418 with MSE metric 7482.1387\n",
      "Epoch 25 batch 220 train Loss 5.3807 test Loss 5.4306 with MSE metric 8636.4990\n",
      "Epoch 25 batch 230 train Loss 5.3453 test Loss 5.2608 with MSE metric 8074.2949\n",
      "Epoch 25 batch 240 train Loss 5.3489 test Loss 5.2213 with MSE metric 8099.7080\n",
      "Time taken for 1 epoch: 27.893143892288208 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 5.2654 test Loss 5.3780 with MSE metric 6886.5718\n",
      "Epoch 26 batch 10 train Loss 5.2745 test Loss 5.3216 with MSE metric 6787.7349\n",
      "Epoch 26 batch 20 train Loss 5.4061 test Loss 5.3503 with MSE metric 8826.7676\n",
      "Epoch 26 batch 30 train Loss 5.3140 test Loss 5.4449 with MSE metric 7584.4028\n",
      "Epoch 26 batch 40 train Loss 5.2286 test Loss 5.2995 with MSE metric 6292.5898\n",
      "Epoch 26 batch 50 train Loss 5.2448 test Loss 5.2127 with MSE metric 6594.4697\n",
      "Epoch 26 batch 60 train Loss 5.3345 test Loss 5.3847 with MSE metric 7899.4824\n",
      "Epoch 26 batch 70 train Loss 5.3661 test Loss 5.3332 with MSE metric 8315.7412\n",
      "Epoch 26 batch 80 train Loss 5.2948 test Loss 5.3141 with MSE metric 7286.1582\n",
      "Epoch 26 batch 90 train Loss 5.3000 test Loss 5.3382 with MSE metric 7381.1250\n",
      "Epoch 26 batch 100 train Loss 5.3186 test Loss 5.3874 with MSE metric 7657.6045\n",
      "Epoch 26 batch 110 train Loss 5.4310 test Loss 5.3912 with MSE metric 9368.0303\n",
      "Epoch 26 batch 120 train Loss 5.3033 test Loss 5.3140 with MSE metric 7424.5493\n",
      "Epoch 26 batch 130 train Loss 5.3136 test Loss 5.3882 with MSE metric 7497.6416\n",
      "Epoch 26 batch 140 train Loss 5.2559 test Loss 5.3025 with MSE metric 6655.0674\n",
      "Epoch 26 batch 150 train Loss 5.3404 test Loss 5.3503 with MSE metric 7944.5981\n",
      "Epoch 26 batch 160 train Loss 5.2466 test Loss 5.3505 with MSE metric 6503.2285\n",
      "Epoch 26 batch 170 train Loss 5.3230 test Loss 5.3289 with MSE metric 7713.9219\n",
      "Epoch 26 batch 180 train Loss 5.2477 test Loss 5.3321 with MSE metric 6528.8311\n",
      "Epoch 26 batch 190 train Loss 5.3374 test Loss 5.3468 with MSE metric 7927.4023\n",
      "Epoch 26 batch 200 train Loss 5.2604 test Loss 5.4353 with MSE metric 6774.0825\n",
      "Epoch 26 batch 210 train Loss 5.2435 test Loss 5.3410 with MSE metric 6545.6987\n",
      "Epoch 26 batch 220 train Loss 5.3455 test Loss 5.2674 with MSE metric 8050.7759\n",
      "Epoch 26 batch 230 train Loss 5.3056 test Loss 5.4813 with MSE metric 7464.8350\n",
      "Epoch 26 batch 240 train Loss 5.2933 test Loss 5.4044 with MSE metric 7264.5537\n",
      "Time taken for 1 epoch: 27.25740694999695 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 5.3609 test Loss 5.3594 with MSE metric 8212.8496\n",
      "Epoch 27 batch 10 train Loss 5.2572 test Loss 5.2568 with MSE metric 6700.5537\n",
      "Epoch 27 batch 20 train Loss 5.2624 test Loss 5.3582 with MSE metric 6833.3237\n",
      "Epoch 27 batch 30 train Loss 5.3233 test Loss 5.3329 with MSE metric 7709.0088\n",
      "Epoch 27 batch 40 train Loss 5.3346 test Loss 5.3141 with MSE metric 7802.6748\n",
      "Epoch 27 batch 50 train Loss 5.4380 test Loss 5.3299 with MSE metric 9340.5225\n",
      "Epoch 27 batch 60 train Loss 5.2690 test Loss 5.3304 with MSE metric 6745.1055\n",
      "Epoch 27 batch 70 train Loss 5.2880 test Loss 5.3817 with MSE metric 7189.1367\n",
      "Epoch 27 batch 80 train Loss 5.3735 test Loss 5.2844 with MSE metric 8415.5576\n",
      "Epoch 27 batch 90 train Loss 5.3954 test Loss 5.2633 with MSE metric 8838.5840\n",
      "Epoch 27 batch 100 train Loss 5.2452 test Loss 5.3451 with MSE metric 6505.7939\n",
      "Epoch 27 batch 110 train Loss 5.3587 test Loss 5.3561 with MSE metric 8280.9180\n",
      "Epoch 27 batch 120 train Loss 5.3297 test Loss 5.3697 with MSE metric 7822.7817\n",
      "Epoch 27 batch 130 train Loss 5.3011 test Loss 5.2872 with MSE metric 7396.5596\n",
      "Epoch 27 batch 140 train Loss 5.2943 test Loss 5.2896 with MSE metric 7245.3862\n",
      "Epoch 27 batch 150 train Loss 5.1982 test Loss 5.2473 with MSE metric 5801.3965\n",
      "Epoch 27 batch 160 train Loss 5.4380 test Loss 5.3866 with MSE metric 9030.2627\n",
      "Epoch 27 batch 170 train Loss 5.3077 test Loss 5.3832 with MSE metric 7369.4814\n",
      "Epoch 27 batch 180 train Loss 5.3813 test Loss 5.3270 with MSE metric 8384.9395\n",
      "Epoch 27 batch 190 train Loss 5.3959 test Loss 5.3151 with MSE metric 8895.3008\n",
      "Epoch 27 batch 200 train Loss 5.2187 test Loss 5.2596 with MSE metric 6174.8877\n",
      "Epoch 27 batch 210 train Loss 5.3802 test Loss 5.3592 with MSE metric 8456.2246\n",
      "Epoch 27 batch 220 train Loss 5.2675 test Loss 5.4349 with MSE metric 6858.9126\n",
      "Epoch 27 batch 230 train Loss 5.3791 test Loss 5.4011 with MSE metric 8536.0029\n",
      "Epoch 27 batch 240 train Loss 5.3187 test Loss 5.3698 with MSE metric 7638.5244\n",
      "Time taken for 1 epoch: 26.96002197265625 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 5.2528 test Loss 5.3415 with MSE metric 6653.0605\n",
      "Epoch 28 batch 10 train Loss 5.2169 test Loss 5.4307 with MSE metric 6169.9106\n",
      "Epoch 28 batch 20 train Loss 5.2881 test Loss 5.3688 with MSE metric 7207.7661\n",
      "Epoch 28 batch 30 train Loss 5.3199 test Loss 5.4073 with MSE metric 7681.5869\n",
      "Epoch 28 batch 40 train Loss 5.3947 test Loss 5.3580 with MSE metric 8803.7871\n",
      "Epoch 28 batch 50 train Loss 5.2497 test Loss 5.3876 with MSE metric 6675.5264\n",
      "Epoch 28 batch 60 train Loss 5.3115 test Loss 5.2749 with MSE metric 7553.2109\n",
      "Epoch 28 batch 70 train Loss 5.2550 test Loss 5.4525 with MSE metric 6580.4453\n",
      "Epoch 28 batch 80 train Loss 5.1719 test Loss 5.2506 with MSE metric 5623.8354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 batch 90 train Loss 5.3966 test Loss 5.2274 with MSE metric 8701.2422\n",
      "Epoch 28 batch 100 train Loss 5.4307 test Loss 5.2877 with MSE metric 9476.3291\n",
      "Epoch 28 batch 110 train Loss 5.3141 test Loss 5.4105 with MSE metric 7590.2349\n",
      "Epoch 28 batch 120 train Loss 5.3634 test Loss 5.4162 with MSE metric 8325.0186\n",
      "Epoch 28 batch 130 train Loss 5.3004 test Loss 5.4366 with MSE metric 7370.6855\n",
      "Epoch 28 batch 140 train Loss 5.3311 test Loss 5.4390 with MSE metric 7856.0454\n",
      "Epoch 28 batch 150 train Loss 5.3819 test Loss 5.3760 with MSE metric 8464.3955\n",
      "Epoch 28 batch 160 train Loss 5.2506 test Loss 5.4584 with MSE metric 6634.3037\n",
      "Epoch 28 batch 170 train Loss 5.3849 test Loss 5.3949 with MSE metric 8690.5293\n",
      "Epoch 28 batch 180 train Loss 5.3343 test Loss 5.3440 with MSE metric 7875.7554\n",
      "Epoch 28 batch 190 train Loss 5.2767 test Loss 5.3288 with MSE metric 6973.7007\n",
      "Epoch 28 batch 200 train Loss 5.1749 test Loss 5.2244 with MSE metric 5623.4897\n",
      "Epoch 28 batch 210 train Loss 5.2180 test Loss 5.3058 with MSE metric 6180.0664\n",
      "Epoch 28 batch 220 train Loss 5.2422 test Loss 5.2796 with MSE metric 6414.1538\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s=32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 10 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s= 32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
