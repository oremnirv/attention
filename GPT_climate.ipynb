{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, data_combine, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, t, token = data_combine.climate_data_to_model_input('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Initializing from scratch.\n",
      "Epoch 0 batch 0 train Loss 276635.7271 test Loss 92122.0270 with MSE metric 45551.5664\n",
      "Epoch 0 batch 10 train Loss 147707.3831 test Loss 46122.4647 with MSE metric 46528.5883\n",
      "Epoch 0 batch 20 train Loss 88084.5578 test Loss 30754.6986 with MSE metric 46206.7365\n",
      "Epoch 0 batch 30 train Loss 62429.0003 test Loss 23068.2816 with MSE metric 45921.8479\n",
      "Epoch 0 batch 40 train Loss 48274.3682 test Loss 18456.1269 with MSE metric 46211.5025\n",
      "Epoch 0 batch 50 train Loss 39442.4337 test Loss 15381.2496 with MSE metric 46455.9027\n",
      "Epoch 0 batch 60 train Loss 33508.3575 test Loss 13184.8707 with MSE metric 46346.1084\n",
      "Epoch 0 batch 70 train Loss 29167.5388 test Loss 11537.5780 with MSE metric 46335.6242\n",
      "Epoch 0 batch 80 train Loss 25797.9345 test Loss 10256.3357 with MSE metric 46273.0657\n",
      "Epoch 0 batch 90 train Loss 23146.2318 test Loss 9231.3473 with MSE metric 46227.9679\n",
      "Epoch 0 batch 100 train Loss 21060.0951 test Loss 8392.7229 with MSE metric 46165.1620\n",
      "Epoch 0 batch 110 train Loss 19309.1080 test Loss 7693.8508 with MSE metric 46050.8190\n",
      "Epoch 0 batch 120 train Loss 17813.1833 test Loss 7102.5077 with MSE metric 46037.1066\n",
      "Epoch 0 batch 130 train Loss 16545.6666 test Loss 6595.6374 with MSE metric 45987.3798\n",
      "Epoch 0 batch 140 train Loss 15436.1051 test Loss 6156.3540 with MSE metric 45965.9638\n",
      "Epoch 0 batch 150 train Loss 14490.9189 test Loss 5771.9818 with MSE metric 45837.2313\n",
      "Epoch 0 batch 160 train Loss 13648.2320 test Loss 5432.8268 with MSE metric 45738.6189\n",
      "Epoch 0 batch 170 train Loss 12894.9693 test Loss 5131.3574 with MSE metric 45722.8218\n",
      "Epoch 0 batch 180 train Loss 12231.2676 test Loss 4861.6208 with MSE metric 45704.8512\n",
      "Epoch 0 batch 190 train Loss 11666.5050 test Loss 4618.8589 with MSE metric 45681.4122\n",
      "Epoch 0 batch 200 train Loss 11118.5316 test Loss 4399.2165 with MSE metric 45638.4006\n",
      "Epoch 0 batch 210 train Loss 10623.2994 test Loss 4199.5465 with MSE metric 45616.0264\n",
      "Epoch 0 batch 220 train Loss 10173.6591 test Loss 4017.2419 with MSE metric 45623.0627\n",
      "Epoch 0 batch 230 train Loss 9873.5946 test Loss 3850.1275 with MSE metric 45636.4663\n",
      "Epoch 0 batch 240 train Loss 9480.7455 test Loss 3696.3865 with MSE metric 45682.9595\n",
      "Time taken for 1 epoch: 29.80279779434204 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 9124.2280 test Loss 3554.4753 with MSE metric 45660.4712\n",
      "Epoch 1 batch 10 train Loss 8791.8516 test Loss 3423.0775 with MSE metric 45631.3107\n",
      "Epoch 1 batch 20 train Loss 8483.4931 test Loss 3301.0655 with MSE metric 45624.8822\n",
      "Epoch 1 batch 30 train Loss 8197.5235 test Loss 3187.4693 with MSE metric 45595.5737\n",
      "Epoch 1 batch 40 train Loss 7926.3725 test Loss 3081.4480 with MSE metric 45558.1109\n",
      "Epoch 1 batch 50 train Loss 7672.9370 test Loss 2982.2665 with MSE metric 45564.8212\n",
      "Epoch 1 batch 60 train Loss 7435.2876 test Loss 2889.2848 with MSE metric 45548.5692\n",
      "Epoch 1 batch 70 train Loss 7216.6531 test Loss 2801.9395 with MSE metric 45565.6623\n",
      "Epoch 1 batch 80 train Loss 7013.3297 test Loss 2719.7330 with MSE metric 45543.4595\n",
      "Epoch 1 batch 90 train Loss 6815.8027 test Loss 2642.2251 with MSE metric 45544.6597\n",
      "Epoch 1 batch 100 train Loss 6630.0385 test Loss 2569.0271 with MSE metric 45511.0475\n",
      "Epoch 1 batch 110 train Loss 6454.1176 test Loss 2499.7850 with MSE metric 45552.1871\n",
      "Epoch 1 batch 120 train Loss 6286.4269 test Loss 2434.1891 with MSE metric 45541.7238\n",
      "Epoch 1 batch 130 train Loss 6127.5972 test Loss 2371.9559 with MSE metric 45516.7344\n",
      "Epoch 1 batch 140 train Loss 5977.9277 test Loss 2312.8366 with MSE metric 45525.2810\n",
      "Epoch 1 batch 150 train Loss 5834.2226 test Loss 2256.6018 with MSE metric 45501.9900\n",
      "Epoch 1 batch 160 train Loss 5696.9706 test Loss 2203.0444 with MSE metric 45504.5150\n",
      "Epoch 1 batch 170 train Loss 5567.8414 test Loss 2151.9788 with MSE metric 45511.1050\n",
      "Epoch 1 batch 180 train Loss 5444.3873 test Loss 2103.2348 with MSE metric 45505.7568\n",
      "Epoch 1 batch 190 train Loss 5327.1111 test Loss 2056.6562 with MSE metric 45486.8465\n",
      "Epoch 1 batch 200 train Loss 5215.3255 test Loss 2012.1051 with MSE metric 45456.5968\n",
      "Epoch 1 batch 210 train Loss 5107.2656 test Loss 1969.4498 with MSE metric 45438.7616\n",
      "Epoch 1 batch 220 train Loss 5004.6703 test Loss 1928.5730 with MSE metric 45416.6755\n",
      "Epoch 1 batch 230 train Loss 4904.7037 test Loss 1889.3640 with MSE metric 45411.7511\n",
      "Epoch 1 batch 240 train Loss 4808.9931 test Loss 1851.7242 with MSE metric 45396.3384\n",
      "Time taken for 1 epoch: 30.204960107803345 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 4715.8155 test Loss 1815.5617 with MSE metric 45372.3463\n",
      "Epoch 2 batch 10 train Loss 4626.4286 test Loss 1780.7894 with MSE metric 45407.1597\n",
      "Epoch 2 batch 20 train Loss 4540.6385 test Loss 1747.3304 with MSE metric 45395.9634\n",
      "Epoch 2 batch 30 train Loss 4458.9864 test Loss 1715.1112 with MSE metric 45368.7647\n",
      "Epoch 2 batch 40 train Loss 4378.9445 test Loss 1684.0633 with MSE metric 45358.9707\n",
      "Epoch 2 batch 50 train Loss 4301.6845 test Loss 1654.1259 with MSE metric 45358.0889\n",
      "Epoch 2 batch 60 train Loss 4226.7513 test Loss 1625.2389 with MSE metric 45356.1640\n",
      "Epoch 2 batch 70 train Loss 4155.5887 test Loss 1597.3483 with MSE metric 45337.4026\n",
      "Epoch 2 batch 80 train Loss 4085.8513 test Loss 1570.4030 with MSE metric 45320.4106\n",
      "Epoch 2 batch 90 train Loss 4018.8824 test Loss 1544.3560 with MSE metric 45301.5502\n",
      "Epoch 2 batch 100 train Loss 3953.5983 test Loss 1519.1634 with MSE metric 45312.4614\n",
      "Epoch 2 batch 110 train Loss 3907.1685 test Loss 1494.7844 with MSE metric 45320.0233\n",
      "Epoch 2 batch 120 train Loss 3846.0601 test Loss 1471.1793 with MSE metric 45321.6546\n",
      "Epoch 2 batch 130 train Loss 3786.8848 test Loss 1448.3114 with MSE metric 45310.0291\n",
      "Epoch 2 batch 140 train Loss 3730.0600 test Loss 1426.1475 with MSE metric 45307.0465\n",
      "Epoch 2 batch 150 train Loss 3674.7688 test Loss 1404.6558 with MSE metric 45301.0975\n",
      "Epoch 2 batch 160 train Loss 3622.3668 test Loss 1383.8053 with MSE metric 45284.5422\n",
      "Epoch 2 batch 170 train Loss 3570.4887 test Loss 1363.5687 with MSE metric 45264.1534\n",
      "Epoch 2 batch 180 train Loss 3519.4906 test Loss 1343.9183 with MSE metric 45255.2268\n",
      "Epoch 2 batch 190 train Loss 3469.9788 test Loss 1324.8305 with MSE metric 45242.3446\n",
      "Epoch 2 batch 200 train Loss 3421.7977 test Loss 1306.2798 with MSE metric 45234.8210\n",
      "Epoch 2 batch 210 train Loss 3374.9198 test Loss 1288.2458 with MSE metric 45216.6375\n",
      "Epoch 2 batch 220 train Loss 3329.2466 test Loss 1270.7057 with MSE metric 45224.7367\n",
      "Epoch 2 batch 230 train Loss 3290.2829 test Loss 1253.6399 with MSE metric 45212.4944\n",
      "Epoch 2 batch 240 train Loss 3247.0155 test Loss 1237.0290 with MSE metric 45213.0627\n",
      "Time taken for 1 epoch: 31.725919008255005 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 3204.6864 test Loss 1220.8559 with MSE metric 45215.3255\n",
      "Epoch 3 batch 10 train Loss 3164.4331 test Loss 1205.1031 with MSE metric 45203.2527\n",
      "Epoch 3 batch 20 train Loss 3124.5183 test Loss 1189.7540 with MSE metric 45187.2403\n",
      "Epoch 3 batch 30 train Loss 3085.7194 test Loss 1174.7932 with MSE metric 45186.5168\n",
      "Epoch 3 batch 40 train Loss 3047.8729 test Loss 1160.2082 with MSE metric 45193.8979\n",
      "Epoch 3 batch 50 train Loss 3010.8848 test Loss 1145.9832 with MSE metric 45201.4190\n",
      "Epoch 3 batch 60 train Loss 2974.5083 test Loss 1132.1055 with MSE metric 45197.5738\n",
      "Epoch 3 batch 70 train Loss 2939.4955 test Loss 1118.5618 with MSE metric 45179.5291\n",
      "Epoch 3 batch 80 train Loss 2904.8209 test Loss 1105.3416 with MSE metric 45177.5222\n",
      "Epoch 3 batch 90 train Loss 2871.3609 test Loss 1092.4312 with MSE metric 45154.4694\n",
      "Epoch 3 batch 100 train Loss 2838.4304 test Loss 1079.8221 with MSE metric 45135.9870\n",
      "Epoch 3 batch 110 train Loss 2806.0842 test Loss 1067.5027 with MSE metric 45127.8282\n",
      "Epoch 3 batch 120 train Loss 2774.9793 test Loss 1055.4634 with MSE metric 45137.2418\n",
      "Epoch 3 batch 130 train Loss 2744.1656 test Loss 1043.6953 with MSE metric 45140.2590\n",
      "Epoch 3 batch 140 train Loss 2714.0698 test Loss 1032.1884 with MSE metric 45125.9614\n",
      "Epoch 3 batch 150 train Loss 2684.6101 test Loss 1020.9345 with MSE metric 45127.0517\n",
      "Epoch 3 batch 160 train Loss 2655.7088 test Loss 1009.9257 with MSE metric 45106.0034\n",
      "Epoch 3 batch 170 train Loss 2627.6455 test Loss 999.1540 with MSE metric 45100.1974\n",
      "Epoch 3 batch 180 train Loss 2600.1196 test Loss 988.6107 with MSE metric 45096.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 batch 190 train Loss 2573.7582 test Loss 978.2907 with MSE metric 45097.1019\n",
      "Epoch 3 batch 200 train Loss 2547.3261 test Loss 968.1856 with MSE metric 45086.2424\n",
      "Epoch 3 batch 210 train Loss 2521.5919 test Loss 958.2892 with MSE metric 45083.5910\n",
      "Epoch 3 batch 220 train Loss 2496.2625 test Loss 948.5948 with MSE metric 45077.8469\n",
      "Epoch 3 batch 230 train Loss 2471.7906 test Loss 939.0961 with MSE metric 45075.7935\n",
      "Epoch 3 batch 240 train Loss 2447.3417 test Loss 929.7878 with MSE metric 45074.1080\n",
      "Time taken for 1 epoch: 30.357666015625 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 2423.4229 test Loss 920.6635 with MSE metric 45070.6918\n",
      "Epoch 4 batch 10 train Loss 2399.9280 test Loss 911.7182 with MSE metric 45057.1591\n",
      "Epoch 4 batch 20 train Loss 2376.9605 test Loss 902.9480 with MSE metric 45060.1760\n",
      "Epoch 4 batch 30 train Loss 2354.4834 test Loss 894.3457 with MSE metric 45053.5382\n",
      "Epoch 4 batch 40 train Loss 2332.3808 test Loss 885.9081 with MSE metric 45056.1516\n",
      "Epoch 4 batch 50 train Loss 2310.8005 test Loss 877.6294 with MSE metric 45048.3208\n",
      "Epoch 4 batch 60 train Loss 2289.5329 test Loss 869.5059 with MSE metric 45050.3329\n",
      "Epoch 4 batch 70 train Loss 2268.5803 test Loss 861.5328 with MSE metric 45049.0500\n",
      "Epoch 4 batch 80 train Loss 2248.0512 test Loss 853.7055 with MSE metric 45050.0251\n",
      "Epoch 4 batch 90 train Loss 2228.1946 test Loss 846.0206 with MSE metric 45040.2955\n",
      "Epoch 4 batch 100 train Loss 2208.3677 test Loss 838.4739 with MSE metric 45038.7982\n",
      "Epoch 4 batch 110 train Loss 2189.0255 test Loss 831.0628 with MSE metric 45039.2967\n",
      "Epoch 4 batch 120 train Loss 2169.8542 test Loss 823.7827 with MSE metric 45040.0957\n",
      "Epoch 4 batch 130 train Loss 2151.1224 test Loss 816.6308 with MSE metric 45031.2315\n",
      "Epoch 4 batch 140 train Loss 2132.6633 test Loss 809.6035 with MSE metric 45028.1465\n",
      "Epoch 4 batch 150 train Loss 2114.6679 test Loss 802.6972 with MSE metric 45029.7922\n",
      "Epoch 4 batch 160 train Loss 2096.8272 test Loss 795.9090 with MSE metric 45009.7464\n",
      "Epoch 4 batch 170 train Loss 2079.2895 test Loss 789.2361 with MSE metric 45003.7349\n",
      "Epoch 4 batch 180 train Loss 2062.1005 test Loss 782.6755 with MSE metric 45006.3125\n",
      "Epoch 4 batch 190 train Loss 2045.0445 test Loss 776.2245 with MSE metric 45009.7786\n",
      "Epoch 4 batch 200 train Loss 2028.3304 test Loss 769.8799 with MSE metric 45013.1025\n",
      "Epoch 4 batch 210 train Loss 2012.2038 test Loss 763.6393 with MSE metric 45015.9939\n",
      "Epoch 4 batch 220 train Loss 1996.0434 test Loss 757.4997 with MSE metric 45015.8314\n",
      "Epoch 4 batch 230 train Loss 1980.1839 test Loss 751.4598 with MSE metric 45021.4358\n",
      "Epoch 4 batch 240 train Loss 1964.5745 test Loss 745.5168 with MSE metric 45019.8027\n",
      "Time taken for 1 epoch: 26.902130365371704 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 1949.3237 test Loss 739.6682 with MSE metric 45015.0101\n",
      "Epoch 5 batch 10 train Loss 1934.1405 test Loss 733.9112 with MSE metric 45005.8931\n",
      "Epoch 5 batch 20 train Loss 1919.2996 test Loss 728.2447 with MSE metric 44992.6769\n",
      "Epoch 5 batch 30 train Loss 1904.5790 test Loss 722.6661 with MSE metric 44985.9718\n",
      "Epoch 5 batch 40 train Loss 1890.1015 test Loss 717.1732 with MSE metric 44980.7272\n",
      "Epoch 5 batch 50 train Loss 1875.8885 test Loss 711.7641 with MSE metric 44976.6084\n",
      "Epoch 5 batch 60 train Loss 1861.7850 test Loss 706.4370 with MSE metric 44972.1892\n",
      "Epoch 5 batch 70 train Loss 1848.0188 test Loss 701.1902 with MSE metric 44966.2768\n",
      "Epoch 5 batch 80 train Loss 1834.3533 test Loss 696.0219 with MSE metric 44961.8813\n",
      "Epoch 5 batch 90 train Loss 1820.9810 test Loss 690.9305 with MSE metric 44958.3249\n",
      "Epoch 5 batch 100 train Loss 1807.7856 test Loss 685.9142 with MSE metric 44957.5026\n",
      "Epoch 5 batch 110 train Loss 1794.7100 test Loss 680.9711 with MSE metric 44954.3753\n",
      "Epoch 5 batch 120 train Loss 1781.8545 test Loss 676.0997 with MSE metric 44950.0262\n",
      "Epoch 5 batch 130 train Loss 1769.5992 test Loss 671.2974 with MSE metric 44953.6941\n",
      "Epoch 5 batch 140 train Loss 1757.0611 test Loss 666.5647 with MSE metric 44950.4413\n",
      "Epoch 5 batch 150 train Loss 1744.7342 test Loss 661.8990 with MSE metric 44945.1746\n",
      "Epoch 5 batch 160 train Loss 1732.5453 test Loss 657.2992 with MSE metric 44942.7267\n",
      "Epoch 5 batch 170 train Loss 1720.5524 test Loss 652.7641 with MSE metric 44939.7389\n",
      "Epoch 5 batch 180 train Loss 1708.8043 test Loss 648.2915 with MSE metric 44932.7510\n",
      "Epoch 5 batch 190 train Loss 1697.1367 test Loss 643.8807 with MSE metric 44930.0002\n",
      "Epoch 5 batch 200 train Loss 1685.6918 test Loss 639.5310 with MSE metric 44923.7291\n",
      "Epoch 5 batch 210 train Loss 1674.3916 test Loss 635.2398 with MSE metric 44923.1230\n",
      "Epoch 5 batch 220 train Loss 1663.1726 test Loss 631.0065 with MSE metric 44920.7822\n",
      "Epoch 5 batch 230 train Loss 1652.1218 test Loss 626.8302 with MSE metric 44918.4070\n",
      "Epoch 5 batch 240 train Loss 1641.2626 test Loss 622.7098 with MSE metric 44907.1517\n",
      "Time taken for 1 epoch: 26.943596124649048 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 1630.5116 test Loss 618.6443 with MSE metric 44908.3888\n",
      "Epoch 6 batch 10 train Loss 1619.9084 test Loss 614.6322 with MSE metric 44907.9875\n",
      "Epoch 6 batch 20 train Loss 1609.4370 test Loss 610.6725 with MSE metric 44904.9789\n",
      "Epoch 6 batch 30 train Loss 1599.1753 test Loss 606.7645 with MSE metric 44898.2220\n",
      "Epoch 6 batch 40 train Loss 1589.4027 test Loss 602.9062 with MSE metric 44898.4453\n",
      "Epoch 6 batch 50 train Loss 1579.8025 test Loss 599.0980 with MSE metric 44895.3265\n",
      "Epoch 6 batch 60 train Loss 1570.1713 test Loss 595.3385 with MSE metric 44895.1295\n",
      "Epoch 6 batch 70 train Loss 1560.3152 test Loss 591.6264 with MSE metric 44889.9835\n",
      "Epoch 6 batch 80 train Loss 1550.6231 test Loss 587.9608 with MSE metric 44877.7429\n",
      "Epoch 6 batch 90 train Loss 1540.9968 test Loss 584.3415 with MSE metric 44868.1489\n",
      "Epoch 6 batch 100 train Loss 1531.5152 test Loss 580.7673 with MSE metric 44863.2595\n",
      "Epoch 6 batch 110 train Loss 1522.2312 test Loss 577.2369 with MSE metric 44860.4943\n",
      "Epoch 6 batch 120 train Loss 1512.9760 test Loss 573.7503 with MSE metric 44855.4648\n",
      "Epoch 6 batch 130 train Loss 1503.8501 test Loss 570.3058 with MSE metric 44853.5367\n",
      "Epoch 6 batch 140 train Loss 1494.8289 test Loss 566.9034 with MSE metric 44852.7956\n",
      "Epoch 6 batch 150 train Loss 1485.9243 test Loss 563.5417 with MSE metric 44845.7155\n",
      "Epoch 6 batch 160 train Loss 1477.0828 test Loss 560.2209 with MSE metric 44849.0325\n",
      "Epoch 6 batch 170 train Loss 1468.3799 test Loss 556.9394 with MSE metric 44841.9069\n",
      "Epoch 6 batch 180 train Loss 1459.7514 test Loss 553.6971 with MSE metric 44841.4974\n",
      "Epoch 6 batch 190 train Loss 1451.2333 test Loss 550.4925 with MSE metric 44835.1646\n",
      "Epoch 6 batch 200 train Loss 1442.8199 test Loss 547.3256 with MSE metric 44837.8007\n",
      "Epoch 6 batch 210 train Loss 1434.5088 test Loss 544.1954 with MSE metric 44837.0209\n",
      "Epoch 6 batch 220 train Loss 1426.3004 test Loss 541.1014 with MSE metric 44837.4252\n",
      "Epoch 6 batch 230 train Loss 1418.2052 test Loss 538.0432 with MSE metric 44834.3945\n",
      "Epoch 6 batch 240 train Loss 1410.2322 test Loss 535.0199 with MSE metric 44837.7895\n",
      "Time taken for 1 epoch: 28.42843222618103 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 1402.3128 test Loss 532.0310 with MSE metric 44838.7347\n",
      "Epoch 7 batch 10 train Loss 1394.8938 test Loss 529.0759 with MSE metric 44837.8097\n",
      "Epoch 7 batch 20 train Loss 1387.1896 test Loss 526.1541 with MSE metric 44836.9873\n",
      "Epoch 7 batch 30 train Loss 1379.5406 test Loss 523.2653 with MSE metric 44838.1247\n",
      "Epoch 7 batch 40 train Loss 1371.9384 test Loss 520.4084 with MSE metric 44842.7618\n",
      "Epoch 7 batch 50 train Loss 1364.4487 test Loss 517.5830 with MSE metric 44842.7576\n",
      "Epoch 7 batch 60 train Loss 1357.0505 test Loss 514.7889 with MSE metric 44840.7958\n",
      "Epoch 7 batch 70 train Loss 1349.6931 test Loss 512.0254 with MSE metric 44841.6104\n",
      "Epoch 7 batch 80 train Loss 1342.4208 test Loss 509.2917 with MSE metric 44838.3105\n",
      "Epoch 7 batch 90 train Loss 1335.2635 test Loss 506.5879 with MSE metric 44837.1282\n",
      "Epoch 7 batch 100 train Loss 1328.2923 test Loss 503.9134 with MSE metric 44828.0234\n",
      "Epoch 7 batch 110 train Loss 1321.3661 test Loss 501.2670 with MSE metric 44822.9268\n",
      "Epoch 7 batch 120 train Loss 1314.4068 test Loss 498.6491 with MSE metric 44821.2390\n",
      "Epoch 7 batch 130 train Loss 1307.5278 test Loss 496.0590 with MSE metric 44819.3020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 batch 140 train Loss 1300.7095 test Loss 493.4961 with MSE metric 44818.2943\n",
      "Epoch 7 batch 150 train Loss 1294.0040 test Loss 490.9602 with MSE metric 44817.6849\n",
      "Epoch 7 batch 160 train Loss 1287.3511 test Loss 488.4506 with MSE metric 44813.3637\n",
      "Epoch 7 batch 170 train Loss 1280.7618 test Loss 485.9669 with MSE metric 44810.6646\n",
      "Epoch 7 batch 180 train Loss 1274.3010 test Loss 483.5091 with MSE metric 44811.2059\n",
      "Epoch 7 batch 190 train Loss 1267.8261 test Loss 481.0765 with MSE metric 44810.5494\n",
      "Epoch 7 batch 200 train Loss 1261.4127 test Loss 478.6689 with MSE metric 44809.8589\n",
      "Epoch 7 batch 210 train Loss 1255.0965 test Loss 476.2854 with MSE metric 44813.5021\n",
      "Epoch 7 batch 220 train Loss 1248.8292 test Loss 473.9266 with MSE metric 44812.7825\n",
      "Epoch 7 batch 230 train Loss 1242.6034 test Loss 471.5913 with MSE metric 44808.9969\n",
      "Epoch 7 batch 240 train Loss 1236.4422 test Loss 469.2792 with MSE metric 44808.6132\n",
      "Time taken for 1 epoch: 29.421195030212402 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 1230.3906 test Loss 466.9904 with MSE metric 44808.7149\n",
      "Epoch 8 batch 10 train Loss 1224.4285 test Loss 464.7240 with MSE metric 44806.7802\n",
      "Epoch 8 batch 20 train Loss 1218.6171 test Loss 462.4802 with MSE metric 44807.5621\n",
      "Epoch 8 batch 30 train Loss 1212.7419 test Loss 460.2585 with MSE metric 44804.1905\n",
      "Epoch 8 batch 40 train Loss 1206.9594 test Loss 458.0585 with MSE metric 44804.8762\n",
      "Epoch 8 batch 50 train Loss 1201.1744 test Loss 455.8799 with MSE metric 44804.1438\n",
      "Epoch 8 batch 60 train Loss 1195.4953 test Loss 453.7222 with MSE metric 44800.6782\n",
      "Epoch 8 batch 70 train Loss 1189.8997 test Loss 451.5857 with MSE metric 44800.6376\n",
      "Epoch 8 batch 80 train Loss 1184.2786 test Loss 449.4694 with MSE metric 44794.7038\n",
      "Epoch 8 batch 90 train Loss 1178.7679 test Loss 447.3732 with MSE metric 44790.2379\n",
      "Epoch 8 batch 100 train Loss 1173.2369 test Loss 445.2970 with MSE metric 44784.2181\n",
      "Epoch 8 batch 110 train Loss 1167.7877 test Loss 443.2406 with MSE metric 44779.5893\n",
      "Epoch 8 batch 120 train Loss 1162.3806 test Loss 441.2032 with MSE metric 44783.8443\n",
      "Epoch 8 batch 130 train Loss 1156.9951 test Loss 439.1849 with MSE metric 44780.4207\n",
      "Epoch 8 batch 140 train Loss 1151.6675 test Loss 437.1854 with MSE metric 44780.1581\n",
      "Epoch 8 batch 150 train Loss 1146.3919 test Loss 435.2046 with MSE metric 44781.6780\n",
      "Epoch 8 batch 160 train Loss 1141.1671 test Loss 433.2420 with MSE metric 44780.8104\n",
      "Epoch 8 batch 170 train Loss 1135.9960 test Loss 431.2975 with MSE metric 44782.0080\n",
      "Epoch 8 batch 180 train Loss 1130.8632 test Loss 429.3708 with MSE metric 44782.8237\n",
      "Epoch 8 batch 190 train Loss 1125.7662 test Loss 427.4617 with MSE metric 44784.3824\n",
      "Epoch 8 batch 200 train Loss 1120.7611 test Loss 425.5697 with MSE metric 44780.5057\n",
      "Epoch 8 batch 210 train Loss 1115.8185 test Loss 423.6950 with MSE metric 44778.1294\n",
      "Epoch 8 batch 220 train Loss 1110.8698 test Loss 421.8371 with MSE metric 44776.9149\n",
      "Epoch 8 batch 230 train Loss 1105.9861 test Loss 419.9959 with MSE metric 44780.6554\n",
      "Epoch 8 batch 240 train Loss 1101.1204 test Loss 418.1710 with MSE metric 44781.0386\n",
      "Time taken for 1 epoch: 28.709784269332886 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 1096.3074 test Loss 416.3621 with MSE metric 44782.9400\n",
      "Epoch 9 batch 10 train Loss 1091.5239 test Loss 414.5694 with MSE metric 44785.1685\n",
      "Epoch 9 batch 20 train Loss 1086.7956 test Loss 412.7924 with MSE metric 44780.8820\n",
      "Epoch 9 batch 30 train Loss 1082.1020 test Loss 411.0307 with MSE metric 44778.6129\n",
      "Epoch 9 batch 40 train Loss 1077.4411 test Loss 409.2844 with MSE metric 44776.2438\n",
      "Epoch 9 batch 50 train Loss 1072.8193 test Loss 407.5533 with MSE metric 44781.0837\n",
      "Epoch 9 batch 60 train Loss 1068.2353 test Loss 405.8372 with MSE metric 44782.0508\n",
      "Epoch 9 batch 70 train Loss 1063.6886 test Loss 404.1360 with MSE metric 44780.8341\n",
      "Epoch 9 batch 80 train Loss 1059.1905 test Loss 402.4494 with MSE metric 44781.4039\n",
      "Epoch 9 batch 90 train Loss 1054.7699 test Loss 400.7771 with MSE metric 44787.7362\n",
      "Epoch 9 batch 100 train Loss 1050.3931 test Loss 399.1191 with MSE metric 44785.3247\n",
      "Epoch 9 batch 110 train Loss 1046.0192 test Loss 397.4750 with MSE metric 44784.5071\n",
      "Epoch 9 batch 120 train Loss 1041.6685 test Loss 395.8446 with MSE metric 44786.4044\n",
      "Epoch 9 batch 130 train Loss 1037.3602 test Loss 394.2278 with MSE metric 44781.6735\n",
      "Epoch 9 batch 140 train Loss 1033.0749 test Loss 392.6245 with MSE metric 44776.6956\n",
      "Epoch 9 batch 150 train Loss 1028.8282 test Loss 391.0346 with MSE metric 44776.2279\n",
      "Epoch 9 batch 160 train Loss 1024.6156 test Loss 389.4578 with MSE metric 44767.4653\n",
      "Epoch 9 batch 170 train Loss 1020.4697 test Loss 387.8941 with MSE metric 44768.6716\n",
      "Epoch 9 batch 180 train Loss 1016.3315 test Loss 386.3434 with MSE metric 44766.8421\n",
      "Epoch 9 batch 190 train Loss 1012.2648 test Loss 384.8051 with MSE metric 44765.5127\n",
      "Epoch 9 batch 200 train Loss 1008.1914 test Loss 383.2797 with MSE metric 44766.9645\n",
      "Epoch 9 batch 210 train Loss 1004.1462 test Loss 381.7665 with MSE metric 44762.0403\n",
      "Epoch 9 batch 220 train Loss 1000.1362 test Loss 380.2652 with MSE metric 44756.9332\n",
      "Epoch 9 batch 230 train Loss 996.1593 test Loss 378.7764 with MSE metric 44755.8938\n",
      "Epoch 9 batch 240 train Loss 992.2195 test Loss 377.2995 with MSE metric 44757.9395\n",
      "Time taken for 1 epoch: 26.26639413833618 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 988.3106 test Loss 375.8343 with MSE metric 44752.6474\n",
      "Epoch 10 batch 10 train Loss 984.4457 test Loss 374.3808 with MSE metric 44752.1910\n",
      "Epoch 10 batch 20 train Loss 980.5911 test Loss 372.9387 with MSE metric 44749.4991\n",
      "Epoch 10 batch 30 train Loss 976.7615 test Loss 371.5080 with MSE metric 44745.4841\n",
      "Epoch 10 batch 40 train Loss 972.9814 test Loss 370.0887 with MSE metric 44743.7243\n",
      "Epoch 10 batch 50 train Loss 969.2190 test Loss 368.6804 with MSE metric 44738.4643\n",
      "Epoch 10 batch 60 train Loss 965.4848 test Loss 367.2831 with MSE metric 44735.0319\n",
      "Epoch 10 batch 70 train Loss 961.7912 test Loss 365.8969 with MSE metric 44737.4210\n",
      "Epoch 10 batch 80 train Loss 958.1134 test Loss 364.5213 with MSE metric 44735.4580\n",
      "Epoch 10 batch 90 train Loss 954.4746 test Loss 363.1560 with MSE metric 44734.0153\n",
      "Epoch 10 batch 100 train Loss 951.0032 test Loss 361.8011 with MSE metric 44734.3537\n",
      "Epoch 10 batch 110 train Loss 947.4074 test Loss 360.4568 with MSE metric 44735.1766\n",
      "Epoch 10 batch 120 train Loss 943.8441 test Loss 359.1229 with MSE metric 44730.9161\n",
      "Epoch 10 batch 130 train Loss 940.3111 test Loss 357.7991 with MSE metric 44727.8943\n",
      "Epoch 10 batch 140 train Loss 936.8113 test Loss 356.4852 with MSE metric 44727.6135\n",
      "Epoch 10 batch 150 train Loss 933.3255 test Loss 355.1812 with MSE metric 44725.6370\n",
      "Epoch 10 batch 160 train Loss 929.8681 test Loss 353.8870 with MSE metric 44724.3893\n",
      "Epoch 10 batch 170 train Loss 926.4301 test Loss 352.6024 with MSE metric 44724.1716\n",
      "Epoch 10 batch 180 train Loss 923.0297 test Loss 351.3273 with MSE metric 44721.5506\n",
      "Epoch 10 batch 190 train Loss 919.7079 test Loss 350.0621 with MSE metric 44718.6745\n",
      "Epoch 10 batch 200 train Loss 916.3466 test Loss 348.8059 with MSE metric 44717.5225\n",
      "Epoch 10 batch 210 train Loss 913.0072 test Loss 347.5592 with MSE metric 44715.3538\n",
      "Epoch 10 batch 220 train Loss 909.6953 test Loss 346.3217 with MSE metric 44713.4314\n",
      "Epoch 10 batch 230 train Loss 906.4107 test Loss 345.0931 with MSE metric 44709.1637\n",
      "Epoch 10 batch 240 train Loss 903.1823 test Loss 343.8733 with MSE metric 44708.9765\n",
      "Time taken for 1 epoch: 28.26937174797058 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 899.9468 test Loss 342.6624 with MSE metric 44709.2745\n",
      "Epoch 11 batch 10 train Loss 896.7564 test Loss 341.4603 with MSE metric 44710.2116\n",
      "Epoch 11 batch 20 train Loss 893.5612 test Loss 340.2668 with MSE metric 44711.2036\n",
      "Epoch 11 batch 30 train Loss 890.3962 test Loss 339.0820 with MSE metric 44711.3188\n",
      "Epoch 11 batch 40 train Loss 887.2443 test Loss 337.9054 with MSE metric 44713.8833\n",
      "Epoch 11 batch 50 train Loss 884.1199 test Loss 336.7374 with MSE metric 44711.1275\n",
      "Epoch 11 batch 60 train Loss 881.0252 test Loss 335.5779 with MSE metric 44708.5708\n",
      "Epoch 11 batch 70 train Loss 877.9422 test Loss 334.4263 with MSE metric 44707.4125\n",
      "Epoch 11 batch 80 train Loss 874.8821 test Loss 333.2829 with MSE metric 44706.8448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 batch 90 train Loss 871.8436 test Loss 332.1477 with MSE metric 44707.9802\n",
      "Epoch 11 batch 100 train Loss 868.8273 test Loss 331.0205 with MSE metric 44706.4711\n",
      "Epoch 11 batch 110 train Loss 865.8288 test Loss 329.9011 with MSE metric 44702.7393\n",
      "Epoch 11 batch 120 train Loss 862.8777 test Loss 328.7893 with MSE metric 44698.5043\n",
      "Epoch 11 batch 130 train Loss 859.9492 test Loss 327.6854 with MSE metric 44698.4619\n",
      "Epoch 11 batch 140 train Loss 857.0172 test Loss 326.5892 with MSE metric 44696.4532\n",
      "Epoch 11 batch 150 train Loss 854.1239 test Loss 325.5003 with MSE metric 44692.4952\n",
      "Epoch 11 batch 160 train Loss 851.2339 test Loss 324.4191 with MSE metric 44690.6223\n",
      "Epoch 11 batch 170 train Loss 848.3575 test Loss 323.3452 with MSE metric 44689.6701\n",
      "Epoch 11 batch 180 train Loss 845.5189 test Loss 322.2788 with MSE metric 44688.6152\n",
      "Epoch 11 batch 190 train Loss 842.7060 test Loss 321.2197 with MSE metric 44688.2442\n",
      "Epoch 11 batch 200 train Loss 839.8990 test Loss 320.1676 with MSE metric 44689.1026\n",
      "Epoch 11 batch 210 train Loss 837.1020 test Loss 319.1224 with MSE metric 44685.8370\n",
      "Epoch 11 batch 220 train Loss 834.3277 test Loss 318.0846 with MSE metric 44685.0183\n",
      "Epoch 11 batch 230 train Loss 831.5774 test Loss 317.0537 with MSE metric 44688.6831\n",
      "Epoch 11 batch 240 train Loss 828.8410 test Loss 316.0297 with MSE metric 44687.6819\n",
      "Time taken for 1 epoch: 27.862637758255005 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 826.1139 test Loss 315.0125 with MSE metric 44686.7473\n",
      "Epoch 12 batch 10 train Loss 823.4161 test Loss 314.0018 with MSE metric 44686.8018\n",
      "Epoch 12 batch 20 train Loss 820.7248 test Loss 312.9979 with MSE metric 44684.6282\n",
      "Epoch 12 batch 30 train Loss 818.0516 test Loss 312.0006 with MSE metric 44682.8346\n",
      "Epoch 12 batch 40 train Loss 815.3965 test Loss 311.0100 with MSE metric 44678.7482\n",
      "Epoch 12 batch 50 train Loss 812.7663 test Loss 310.0258 with MSE metric 44680.0629\n",
      "Epoch 12 batch 60 train Loss 810.1732 test Loss 309.0481 with MSE metric 44682.5152\n",
      "Epoch 12 batch 70 train Loss 807.5722 test Loss 308.0768 with MSE metric 44683.3552\n",
      "Epoch 12 batch 80 train Loss 804.9882 test Loss 307.1117 with MSE metric 44681.0805\n",
      "Epoch 12 batch 90 train Loss 802.4322 test Loss 306.1529 with MSE metric 44679.0264\n",
      "Epoch 12 batch 100 train Loss 799.8885 test Loss 305.2003 with MSE metric 44684.1487\n",
      "Epoch 12 batch 110 train Loss 797.3700 test Loss 304.2537 with MSE metric 44682.2568\n",
      "Epoch 12 batch 120 train Loss 794.8503 test Loss 303.3133 with MSE metric 44679.5334\n",
      "Epoch 12 batch 130 train Loss 792.3456 test Loss 302.3790 with MSE metric 44675.8627\n",
      "Epoch 12 batch 140 train Loss 789.8730 test Loss 301.4506 with MSE metric 44672.1712\n",
      "Epoch 12 batch 150 train Loss 787.4012 test Loss 300.5279 with MSE metric 44672.5554\n",
      "Epoch 12 batch 160 train Loss 784.9648 test Loss 299.6110 with MSE metric 44675.9904\n",
      "Epoch 12 batch 170 train Loss 782.5254 test Loss 298.7000 with MSE metric 44681.6546\n",
      "Epoch 12 batch 180 train Loss 780.1048 test Loss 297.7949 with MSE metric 44680.5267\n",
      "Epoch 12 batch 190 train Loss 777.6986 test Loss 296.8954 with MSE metric 44677.7837\n",
      "Epoch 12 batch 200 train Loss 775.3045 test Loss 296.0015 with MSE metric 44678.0041\n",
      "Epoch 12 batch 210 train Loss 772.9262 test Loss 295.1132 with MSE metric 44675.5806\n",
      "Epoch 12 batch 220 train Loss 770.5724 test Loss 294.2304 with MSE metric 44677.7565\n",
      "Epoch 12 batch 230 train Loss 768.2180 test Loss 293.3529 with MSE metric 44679.8394\n",
      "Epoch 12 batch 240 train Loss 765.8847 test Loss 292.4810 with MSE metric 44678.7118\n",
      "Time taken for 1 epoch: 28.55362367630005 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 763.5676 test Loss 291.6143 with MSE metric 44672.9973\n",
      "Epoch 13 batch 10 train Loss 761.3746 test Loss 290.7530 with MSE metric 44671.4468\n",
      "Epoch 13 batch 20 train Loss 759.0766 test Loss 289.8969 with MSE metric 44671.0782\n",
      "Epoch 13 batch 30 train Loss 756.7954 test Loss 289.0460 with MSE metric 44669.3588\n",
      "Epoch 13 batch 40 train Loss 754.5238 test Loss 288.2005 with MSE metric 44671.2155\n",
      "Epoch 13 batch 50 train Loss 752.2770 test Loss 287.3601 with MSE metric 44674.3512\n",
      "Epoch 13 batch 60 train Loss 750.0392 test Loss 286.5248 with MSE metric 44676.4519\n",
      "Epoch 13 batch 70 train Loss 747.8237 test Loss 285.6944 with MSE metric 44676.9571\n",
      "Epoch 13 batch 80 train Loss 745.6089 test Loss 284.8692 with MSE metric 44676.2174\n",
      "Epoch 13 batch 90 train Loss 743.4107 test Loss 284.0488 with MSE metric 44676.2114\n",
      "Epoch 13 batch 100 train Loss 741.2214 test Loss 283.2334 with MSE metric 44674.6699\n",
      "Epoch 13 batch 110 train Loss 739.0465 test Loss 282.4228 with MSE metric 44671.0623\n",
      "Epoch 13 batch 120 train Loss 736.8860 test Loss 281.6169 with MSE metric 44669.2436\n",
      "Epoch 13 batch 130 train Loss 734.7384 test Loss 280.8158 with MSE metric 44667.1430\n",
      "Epoch 13 batch 140 train Loss 732.6059 test Loss 280.0193 with MSE metric 44662.0977\n",
      "Epoch 13 batch 150 train Loss 730.4834 test Loss 279.2276 with MSE metric 44665.6378\n",
      "Epoch 13 batch 160 train Loss 728.3706 test Loss 278.4405 with MSE metric 44665.4503\n",
      "Epoch 13 batch 170 train Loss 726.3468 test Loss 277.6581 with MSE metric 44665.1478\n",
      "Epoch 13 batch 180 train Loss 724.2588 test Loss 276.8801 with MSE metric 44661.0026\n",
      "Epoch 13 batch 190 train Loss 722.1830 test Loss 276.1065 with MSE metric 44664.2451\n",
      "Epoch 13 batch 200 train Loss 720.1283 test Loss 275.3376 with MSE metric 44665.0625\n",
      "Epoch 13 batch 210 train Loss 718.0773 test Loss 274.5731 with MSE metric 44665.2590\n",
      "Epoch 13 batch 220 train Loss 716.0477 test Loss 273.8132 with MSE metric 44666.0939\n",
      "Epoch 13 batch 230 train Loss 714.0193 test Loss 273.0575 with MSE metric 44669.2406\n",
      "Epoch 13 batch 240 train Loss 712.0108 test Loss 272.3061 with MSE metric 44670.3363\n",
      "Time taken for 1 epoch: 25.893537998199463 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 710.0040 test Loss 271.5590 with MSE metric 44667.5588\n",
      "Epoch 14 batch 10 train Loss 708.0099 test Loss 270.8161 with MSE metric 44664.6253\n",
      "Epoch 14 batch 20 train Loss 706.0275 test Loss 270.0775 with MSE metric 44659.6701\n",
      "Epoch 14 batch 30 train Loss 704.0543 test Loss 269.3430 with MSE metric 44658.5434\n",
      "Epoch 14 batch 40 train Loss 702.0951 test Loss 268.6127 with MSE metric 44659.1883\n",
      "Epoch 14 batch 50 train Loss 700.1510 test Loss 267.8866 with MSE metric 44656.3263\n",
      "Epoch 14 batch 60 train Loss 698.2208 test Loss 267.1644 with MSE metric 44656.3787\n",
      "Epoch 14 batch 70 train Loss 696.2933 test Loss 266.4465 with MSE metric 44655.9660\n",
      "Epoch 14 batch 80 train Loss 694.3830 test Loss 265.7326 with MSE metric 44656.6759\n",
      "Epoch 14 batch 90 train Loss 692.4755 test Loss 265.0223 with MSE metric 44658.0571\n",
      "Epoch 14 batch 100 train Loss 690.5795 test Loss 264.3162 with MSE metric 44656.7533\n",
      "Epoch 14 batch 110 train Loss 688.6938 test Loss 263.6140 with MSE metric 44659.1272\n",
      "Epoch 14 batch 120 train Loss 686.8213 test Loss 262.9157 with MSE metric 44659.9702\n",
      "Epoch 14 batch 130 train Loss 684.9554 test Loss 262.2213 with MSE metric 44656.1318\n",
      "Epoch 14 batch 140 train Loss 683.1003 test Loss 261.5305 with MSE metric 44652.9989\n",
      "Epoch 14 batch 150 train Loss 681.2574 test Loss 260.8438 with MSE metric 44651.5726\n",
      "Epoch 14 batch 160 train Loss 679.4220 test Loss 260.1608 with MSE metric 44650.6180\n",
      "Epoch 14 batch 170 train Loss 677.6114 test Loss 259.4814 with MSE metric 44650.9449\n",
      "Epoch 14 batch 180 train Loss 675.8012 test Loss 258.8057 with MSE metric 44646.6062\n",
      "Epoch 14 batch 190 train Loss 673.9959 test Loss 258.1337 with MSE metric 44644.9843\n",
      "Epoch 14 batch 200 train Loss 672.2022 test Loss 257.4655 with MSE metric 44643.9647\n",
      "Epoch 14 batch 210 train Loss 670.4183 test Loss 256.8008 with MSE metric 44645.5900\n",
      "Epoch 14 batch 220 train Loss 668.6418 test Loss 256.1397 with MSE metric 44644.3464\n",
      "Epoch 14 batch 230 train Loss 666.8784 test Loss 255.4822 with MSE metric 44644.5990\n",
      "Epoch 14 batch 240 train Loss 665.1224 test Loss 254.8280 with MSE metric 44645.2015\n",
      "Time taken for 1 epoch: 28.45941400527954 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 663.3777 test Loss 254.1773 with MSE metric 44646.0120\n",
      "Epoch 15 batch 10 train Loss 661.6410 test Loss 253.5301 with MSE metric 44645.7942\n",
      "Epoch 15 batch 20 train Loss 659.9102 test Loss 252.8864 with MSE metric 44646.8515\n",
      "Epoch 15 batch 30 train Loss 658.2015 test Loss 252.2461 with MSE metric 44648.8572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 batch 40 train Loss 656.4897 test Loss 251.6091 with MSE metric 44649.6013\n",
      "Epoch 15 batch 50 train Loss 654.7873 test Loss 250.9754 with MSE metric 44650.0012\n",
      "Epoch 15 batch 60 train Loss 653.1027 test Loss 250.3452 with MSE metric 44650.0421\n",
      "Epoch 15 batch 70 train Loss 651.4181 test Loss 249.7183 with MSE metric 44649.1121\n",
      "Epoch 15 batch 80 train Loss 649.7408 test Loss 249.0947 with MSE metric 44647.7067\n",
      "Epoch 15 batch 90 train Loss 648.0725 test Loss 248.4744 with MSE metric 44646.1725\n",
      "Epoch 15 batch 100 train Loss 646.4165 test Loss 247.8572 with MSE metric 44645.2851\n",
      "Epoch 15 batch 110 train Loss 644.7653 test Loss 247.2429 with MSE metric 44644.8591\n",
      "Epoch 15 batch 120 train Loss 643.1229 test Loss 246.6321 with MSE metric 44643.1733\n",
      "Epoch 15 batch 130 train Loss 641.4899 test Loss 246.0245 with MSE metric 44641.2036\n",
      "Epoch 15 batch 140 train Loss 639.8646 test Loss 245.4198 with MSE metric 44639.2454\n",
      "Epoch 15 batch 150 train Loss 638.2462 test Loss 244.8183 with MSE metric 44637.1294\n",
      "Epoch 15 batch 160 train Loss 636.6366 test Loss 244.2200 with MSE metric 44636.2056\n",
      "Epoch 15 batch 170 train Loss 635.0384 test Loss 243.6246 with MSE metric 44633.5415\n",
      "Epoch 15 batch 180 train Loss 633.4459 test Loss 243.0323 with MSE metric 44634.9891\n",
      "Epoch 15 batch 190 train Loss 631.8672 test Loss 242.4429 with MSE metric 44632.7174\n",
      "Epoch 15 batch 200 train Loss 630.2952 test Loss 241.8566 with MSE metric 44630.0086\n",
      "Epoch 15 batch 210 train Loss 628.7249 test Loss 241.2731 with MSE metric 44627.8200\n",
      "Epoch 15 batch 220 train Loss 627.1637 test Loss 240.6927 with MSE metric 44626.1303\n",
      "Epoch 15 batch 230 train Loss 625.6177 test Loss 240.1152 with MSE metric 44621.7435\n",
      "Epoch 15 batch 240 train Loss 624.0721 test Loss 239.5405 with MSE metric 44619.6470\n",
      "Time taken for 1 epoch: 28.90988326072693 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 622.5375 test Loss 238.9688 with MSE metric 44619.5518\n",
      "Epoch 16 batch 10 train Loss 621.0085 test Loss 238.3998 with MSE metric 44616.2723\n",
      "Epoch 16 batch 20 train Loss 619.4871 test Loss 237.8339 with MSE metric 44612.1733\n",
      "Epoch 16 batch 30 train Loss 617.9728 test Loss 237.2707 with MSE metric 44613.5428\n",
      "Epoch 16 batch 40 train Loss 616.4645 test Loss 236.7103 with MSE metric 44615.1343\n",
      "Epoch 16 batch 50 train Loss 614.9680 test Loss 236.1526 with MSE metric 44615.7748\n",
      "Epoch 16 batch 60 train Loss 613.4840 test Loss 235.5977 with MSE metric 44617.3193\n",
      "Epoch 16 batch 70 train Loss 611.9978 test Loss 235.0456 with MSE metric 44615.2659\n",
      "Epoch 16 batch 80 train Loss 610.5251 test Loss 234.4961 with MSE metric 44617.2815\n",
      "Epoch 16 batch 90 train Loss 609.0752 test Loss 233.9494 with MSE metric 44619.3489\n",
      "Epoch 16 batch 100 train Loss 607.6105 test Loss 233.4053 with MSE metric 44616.7754\n",
      "Epoch 16 batch 110 train Loss 606.1545 test Loss 232.8640 with MSE metric 44617.7027\n",
      "Epoch 16 batch 120 train Loss 604.7073 test Loss 232.3252 with MSE metric 44618.3692\n",
      "Epoch 16 batch 130 train Loss 603.2639 test Loss 231.7892 with MSE metric 44620.2422\n",
      "Epoch 16 batch 140 train Loss 601.8282 test Loss 231.2557 with MSE metric 44621.7492\n",
      "Epoch 16 batch 150 train Loss 600.3996 test Loss 230.7247 with MSE metric 44622.8676\n",
      "Epoch 16 batch 160 train Loss 598.9808 test Loss 230.1964 with MSE metric 44621.7593\n",
      "Epoch 16 batch 170 train Loss 597.5665 test Loss 229.6704 with MSE metric 44623.5806\n",
      "Epoch 16 batch 180 train Loss 596.1589 test Loss 229.1471 with MSE metric 44622.9014\n",
      "Epoch 16 batch 190 train Loss 594.7965 test Loss 228.6263 with MSE metric 44623.5280\n",
      "Epoch 16 batch 200 train Loss 593.4124 test Loss 228.1079 with MSE metric 44621.6565\n",
      "Epoch 16 batch 210 train Loss 592.0234 test Loss 227.5920 with MSE metric 44622.1929\n",
      "Epoch 16 batch 220 train Loss 590.6417 test Loss 227.0783 with MSE metric 44622.4778\n",
      "Epoch 16 batch 230 train Loss 589.2674 test Loss 226.5673 with MSE metric 44622.3663\n",
      "Epoch 16 batch 240 train Loss 587.9002 test Loss 226.0586 with MSE metric 44621.4502\n",
      "Time taken for 1 epoch: 28.559781074523926 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 586.5374 test Loss 225.5524 with MSE metric 44619.4639\n",
      "Epoch 17 batch 10 train Loss 585.1842 test Loss 225.0485 with MSE metric 44618.4106\n",
      "Epoch 17 batch 20 train Loss 583.8353 test Loss 224.5471 with MSE metric 44619.4588\n",
      "Epoch 17 batch 30 train Loss 582.4959 test Loss 224.0481 with MSE metric 44618.4556\n",
      "Epoch 17 batch 40 train Loss 581.1578 test Loss 223.5512 with MSE metric 44617.4340\n",
      "Epoch 17 batch 50 train Loss 579.8283 test Loss 223.0567 with MSE metric 44616.7769\n",
      "Epoch 17 batch 60 train Loss 578.5057 test Loss 222.5645 with MSE metric 44616.3809\n",
      "Epoch 17 batch 70 train Loss 577.1856 test Loss 222.0746 with MSE metric 44616.3480\n",
      "Epoch 17 batch 80 train Loss 575.8727 test Loss 221.5867 with MSE metric 44614.7490\n",
      "Epoch 17 batch 90 train Loss 574.5689 test Loss 221.1013 with MSE metric 44618.5417\n",
      "Epoch 17 batch 100 train Loss 573.2682 test Loss 220.6182 with MSE metric 44617.8802\n",
      "Epoch 17 batch 110 train Loss 571.9757 test Loss 220.1373 with MSE metric 44617.5010\n",
      "Epoch 17 batch 120 train Loss 570.6888 test Loss 219.6584 with MSE metric 44616.9643\n",
      "Epoch 17 batch 130 train Loss 569.4081 test Loss 219.1819 with MSE metric 44616.1957\n",
      "Epoch 17 batch 140 train Loss 568.1315 test Loss 218.7074 with MSE metric 44614.6499\n",
      "Epoch 17 batch 150 train Loss 566.8612 test Loss 218.2352 with MSE metric 44612.1384\n",
      "Epoch 17 batch 160 train Loss 565.5968 test Loss 217.7651 with MSE metric 44609.1376\n",
      "Epoch 17 batch 170 train Loss 564.3373 test Loss 217.2971 with MSE metric 44609.7966\n",
      "Epoch 17 batch 180 train Loss 563.0825 test Loss 216.8313 with MSE metric 44610.3200\n",
      "Epoch 17 batch 190 train Loss 561.8392 test Loss 216.3677 with MSE metric 44609.1782\n",
      "Epoch 17 batch 200 train Loss 560.5953 test Loss 215.9061 with MSE metric 44609.8612\n",
      "Epoch 17 batch 210 train Loss 559.3602 test Loss 215.4465 with MSE metric 44609.8614\n",
      "Epoch 17 batch 220 train Loss 558.1292 test Loss 214.9890 with MSE metric 44608.8580\n",
      "Epoch 17 batch 230 train Loss 556.9032 test Loss 214.5336 with MSE metric 44610.3394\n",
      "Epoch 17 batch 240 train Loss 555.6828 test Loss 214.0800 with MSE metric 44611.4293\n",
      "Time taken for 1 epoch: 29.99758791923523 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 554.4664 test Loss 213.6286 with MSE metric 44611.7451\n",
      "Epoch 18 batch 10 train Loss 553.2573 test Loss 213.1791 with MSE metric 44610.5758\n",
      "Epoch 18 batch 20 train Loss 552.0523 test Loss 212.7318 with MSE metric 44608.1104\n",
      "Epoch 18 batch 30 train Loss 550.8527 test Loss 212.2865 with MSE metric 44605.2050\n",
      "Epoch 18 batch 40 train Loss 549.6578 test Loss 211.8430 with MSE metric 44601.0230\n",
      "Epoch 18 batch 50 train Loss 548.4685 test Loss 211.4016 with MSE metric 44599.6690\n",
      "Epoch 18 batch 60 train Loss 547.2831 test Loss 210.9620 with MSE metric 44600.2956\n",
      "Epoch 18 batch 70 train Loss 546.1045 test Loss 210.5242 with MSE metric 44598.7428\n",
      "Epoch 18 batch 80 train Loss 544.9328 test Loss 210.0884 with MSE metric 44598.1984\n",
      "Epoch 18 batch 90 train Loss 543.7638 test Loss 209.6547 with MSE metric 44597.9782\n",
      "Epoch 18 batch 100 train Loss 542.5996 test Loss 209.2227 with MSE metric 44596.8771\n",
      "Epoch 18 batch 110 train Loss 541.4404 test Loss 208.7927 with MSE metric 44594.8061\n",
      "Epoch 18 batch 120 train Loss 540.2863 test Loss 208.3646 with MSE metric 44591.9276\n",
      "Epoch 18 batch 130 train Loss 539.1372 test Loss 207.9383 with MSE metric 44591.9650\n",
      "Epoch 18 batch 140 train Loss 537.9936 test Loss 207.5138 with MSE metric 44591.0258\n",
      "Epoch 18 batch 150 train Loss 536.8629 test Loss 207.0911 with MSE metric 44591.6491\n",
      "Epoch 18 batch 160 train Loss 535.7331 test Loss 206.6703 with MSE metric 44590.9638\n",
      "Epoch 18 batch 170 train Loss 534.6034 test Loss 206.2512 with MSE metric 44589.4598\n",
      "Epoch 18 batch 180 train Loss 533.4796 test Loss 205.8339 with MSE metric 44588.8982\n",
      "Epoch 18 batch 190 train Loss 532.3612 test Loss 205.4184 with MSE metric 44590.7852\n",
      "Epoch 18 batch 200 train Loss 531.2461 test Loss 205.0045 with MSE metric 44591.5490\n",
      "Epoch 18 batch 210 train Loss 530.1370 test Loss 204.5927 with MSE metric 44589.9326\n",
      "Epoch 18 batch 220 train Loss 529.0327 test Loss 204.1825 with MSE metric 44587.4440\n",
      "Epoch 18 batch 230 train Loss 527.9343 test Loss 203.7739 with MSE metric 44586.7431\n",
      "Epoch 18 batch 240 train Loss 526.8394 test Loss 203.3671 with MSE metric 44585.4931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 29.84224796295166 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 525.7473 test Loss 202.9620 with MSE metric 44585.1595\n",
      "Epoch 19 batch 10 train Loss 524.6610 test Loss 202.5586 with MSE metric 44583.3717\n",
      "Epoch 19 batch 20 train Loss 523.5810 test Loss 202.1569 with MSE metric 44583.4698\n",
      "Epoch 19 batch 30 train Loss 522.5030 test Loss 201.7569 with MSE metric 44582.7458\n",
      "Epoch 19 batch 40 train Loss 521.4332 test Loss 201.3587 with MSE metric 44581.1580\n",
      "Epoch 19 batch 50 train Loss 520.3638 test Loss 200.9622 with MSE metric 44579.6533\n",
      "Epoch 19 batch 60 train Loss 519.2989 test Loss 200.5672 with MSE metric 44577.5454\n",
      "Epoch 19 batch 70 train Loss 518.2389 test Loss 200.1739 with MSE metric 44574.7417\n",
      "Epoch 19 batch 80 train Loss 517.1837 test Loss 199.7822 with MSE metric 44573.0875\n",
      "Epoch 19 batch 90 train Loss 516.1344 test Loss 199.3921 with MSE metric 44571.1268\n",
      "Epoch 19 batch 100 train Loss 515.0874 test Loss 199.0036 with MSE metric 44570.0729\n",
      "Epoch 19 batch 110 train Loss 514.0465 test Loss 198.6167 with MSE metric 44570.3090\n",
      "Epoch 19 batch 120 train Loss 513.0080 test Loss 198.2315 with MSE metric 44567.8395\n",
      "Epoch 19 batch 130 train Loss 511.9804 test Loss 197.8478 with MSE metric 44565.7494\n",
      "Epoch 19 batch 140 train Loss 510.9505 test Loss 197.4656 with MSE metric 44564.7961\n",
      "Epoch 19 batch 150 train Loss 509.9253 test Loss 197.0852 with MSE metric 44566.4279\n",
      "Epoch 19 batch 160 train Loss 508.9041 test Loss 196.7063 with MSE metric 44564.1533\n",
      "Epoch 19 batch 170 train Loss 507.8868 test Loss 196.3288 with MSE metric 44562.3490\n",
      "Epoch 19 batch 180 train Loss 506.8751 test Loss 195.9528 with MSE metric 44562.4334\n",
      "Epoch 19 batch 190 train Loss 505.8698 test Loss 195.5784 with MSE metric 44564.2516\n",
      "Epoch 19 batch 200 train Loss 504.8644 test Loss 195.2056 with MSE metric 44563.3392\n",
      "Epoch 19 batch 210 train Loss 503.8771 test Loss 194.8343 with MSE metric 44563.3639\n",
      "Epoch 19 batch 220 train Loss 502.8801 test Loss 194.4645 with MSE metric 44562.4938\n",
      "Epoch 19 batch 230 train Loss 501.8873 test Loss 194.0961 with MSE metric 44560.2809\n",
      "Epoch 19 batch 240 train Loss 500.8983 test Loss 193.7293 with MSE metric 44559.0701\n",
      "Time taken for 1 epoch: 30.568185091018677 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 499.9135 test Loss 193.3640 with MSE metric 44559.5486\n",
      "Epoch 20 batch 10 train Loss 498.9328 test Loss 193.0001 with MSE metric 44559.9785\n",
      "Epoch 20 batch 20 train Loss 497.9550 test Loss 192.6377 with MSE metric 44560.1646\n",
      "Epoch 20 batch 30 train Loss 496.9813 test Loss 192.2765 with MSE metric 44557.6269\n",
      "Epoch 20 batch 40 train Loss 496.0113 test Loss 191.9170 with MSE metric 44555.8084\n",
      "Epoch 20 batch 50 train Loss 495.0456 test Loss 191.5588 with MSE metric 44556.9033\n",
      "Epoch 20 batch 60 train Loss 494.0829 test Loss 191.2020 with MSE metric 44556.1448\n",
      "Epoch 20 batch 70 train Loss 493.1243 test Loss 190.8467 with MSE metric 44554.2765\n",
      "Epoch 20 batch 80 train Loss 492.1821 test Loss 190.4927 with MSE metric 44553.2407\n",
      "Epoch 20 batch 90 train Loss 491.2308 test Loss 190.1401 with MSE metric 44552.6936\n",
      "Epoch 20 batch 100 train Loss 490.2838 test Loss 189.7890 with MSE metric 44552.7122\n",
      "Epoch 20 batch 110 train Loss 489.3421 test Loss 189.4391 with MSE metric 44552.6955\n",
      "Epoch 20 batch 120 train Loss 488.4034 test Loss 189.0906 with MSE metric 44551.7263\n",
      "Epoch 20 batch 130 train Loss 487.4682 test Loss 188.7435 with MSE metric 44550.7526\n",
      "Epoch 20 batch 140 train Loss 486.5357 test Loss 188.3978 with MSE metric 44549.9307\n",
      "Epoch 20 batch 150 train Loss 485.6121 test Loss 188.0535 with MSE metric 44547.1567\n",
      "Epoch 20 batch 160 train Loss 484.6869 test Loss 187.7105 with MSE metric 44545.5417\n",
      "Epoch 20 batch 170 train Loss 483.7676 test Loss 187.3688 with MSE metric 44545.3842\n",
      "Epoch 20 batch 180 train Loss 482.8502 test Loss 187.0285 with MSE metric 44544.4560\n",
      "Epoch 20 batch 190 train Loss 481.9357 test Loss 186.6895 with MSE metric 44544.5368\n",
      "Epoch 20 batch 200 train Loss 481.0239 test Loss 186.3518 with MSE metric 44544.0892\n",
      "Epoch 20 batch 210 train Loss 480.1160 test Loss 186.0154 with MSE metric 44543.4862\n",
      "Epoch 20 batch 220 train Loss 479.2114 test Loss 185.6802 with MSE metric 44541.3666\n",
      "Epoch 20 batch 230 train Loss 478.3118 test Loss 185.3463 with MSE metric 44539.3565\n",
      "Epoch 20 batch 240 train Loss 477.4146 test Loss 185.0136 with MSE metric 44538.8495\n",
      "Time taken for 1 epoch: 30.16786217689514 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 476.5206 test Loss 184.6823 with MSE metric 44541.2643\n",
      "Epoch 21 batch 10 train Loss 475.6377 test Loss 184.3524 with MSE metric 44540.3382\n",
      "Epoch 21 batch 20 train Loss 474.7501 test Loss 184.0235 with MSE metric 44540.6327\n",
      "Epoch 21 batch 30 train Loss 473.8670 test Loss 183.6959 with MSE metric 44539.9980\n",
      "Epoch 21 batch 40 train Loss 472.9864 test Loss 183.3695 with MSE metric 44539.7680\n",
      "Epoch 21 batch 50 train Loss 472.1109 test Loss 183.0446 with MSE metric 44538.6627\n",
      "Epoch 21 batch 60 train Loss 471.2366 test Loss 182.7208 with MSE metric 44539.1137\n",
      "Epoch 21 batch 70 train Loss 470.3668 test Loss 182.3981 with MSE metric 44538.3571\n",
      "Epoch 21 batch 80 train Loss 469.5000 test Loss 182.0767 with MSE metric 44539.5117\n",
      "Epoch 21 batch 90 train Loss 468.6358 test Loss 181.7566 with MSE metric 44537.4115\n",
      "Epoch 21 batch 100 train Loss 467.7763 test Loss 181.4376 with MSE metric 44536.6927\n",
      "Epoch 21 batch 110 train Loss 466.9180 test Loss 181.1198 with MSE metric 44533.3931\n",
      "Epoch 21 batch 120 train Loss 466.0633 test Loss 180.8031 with MSE metric 44531.6526\n",
      "Epoch 21 batch 130 train Loss 465.2125 test Loss 180.4877 with MSE metric 44532.1801\n",
      "Epoch 21 batch 140 train Loss 464.3646 test Loss 180.1734 with MSE metric 44532.1076\n",
      "Epoch 21 batch 150 train Loss 463.5209 test Loss 179.8604 with MSE metric 44531.6240\n",
      "Epoch 21 batch 160 train Loss 462.6793 test Loss 179.5484 with MSE metric 44530.1659\n",
      "Epoch 21 batch 170 train Loss 461.8398 test Loss 179.2376 with MSE metric 44531.7636\n",
      "Epoch 21 batch 180 train Loss 461.0042 test Loss 178.9280 with MSE metric 44530.2022\n",
      "Epoch 21 batch 190 train Loss 460.1713 test Loss 178.6195 with MSE metric 44529.8319\n",
      "Epoch 21 batch 200 train Loss 459.3415 test Loss 178.3120 with MSE metric 44528.6794\n",
      "Epoch 21 batch 210 train Loss 458.5150 test Loss 178.0057 with MSE metric 44528.1707\n",
      "Epoch 21 batch 220 train Loss 457.6914 test Loss 177.7007 with MSE metric 44528.0325\n",
      "Epoch 21 batch 230 train Loss 456.8706 test Loss 177.3966 with MSE metric 44526.8091\n",
      "Epoch 21 batch 240 train Loss 456.0535 test Loss 177.0936 with MSE metric 44526.0047\n",
      "Time taken for 1 epoch: 27.78231692314148 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 455.2401 test Loss 176.7919 with MSE metric 44524.9635\n",
      "Epoch 22 batch 10 train Loss 454.4282 test Loss 176.4912 with MSE metric 44524.3901\n",
      "Epoch 22 batch 20 train Loss 453.6195 test Loss 176.1915 with MSE metric 44526.1289\n",
      "Epoch 22 batch 30 train Loss 452.8161 test Loss 175.8930 with MSE metric 44526.2864\n",
      "Epoch 22 batch 40 train Loss 452.0447 test Loss 175.5956 with MSE metric 44527.5907\n",
      "Epoch 22 batch 50 train Loss 451.2441 test Loss 175.2994 with MSE metric 44526.3286\n",
      "Epoch 22 batch 60 train Loss 450.4469 test Loss 175.0039 with MSE metric 44527.5434\n",
      "Epoch 22 batch 70 train Loss 449.6531 test Loss 174.7099 with MSE metric 44526.3519\n",
      "Epoch 22 batch 80 train Loss 448.8614 test Loss 174.4166 with MSE metric 44526.2595\n",
      "Epoch 22 batch 90 train Loss 448.0724 test Loss 174.1245 with MSE metric 44525.1485\n",
      "Epoch 22 batch 100 train Loss 447.2866 test Loss 173.8336 with MSE metric 44523.5136\n",
      "Epoch 22 batch 110 train Loss 446.5031 test Loss 173.5437 with MSE metric 44522.2833\n",
      "Epoch 22 batch 120 train Loss 445.7228 test Loss 173.2547 with MSE metric 44521.4576\n",
      "Epoch 22 batch 130 train Loss 444.9455 test Loss 172.9668 with MSE metric 44519.5376\n",
      "Epoch 22 batch 140 train Loss 444.1720 test Loss 172.6798 with MSE metric 44519.5327\n",
      "Epoch 22 batch 150 train Loss 443.4007 test Loss 172.3939 with MSE metric 44518.4265\n",
      "Epoch 22 batch 160 train Loss 442.6321 test Loss 172.1090 with MSE metric 44518.5569\n",
      "Epoch 22 batch 170 train Loss 441.8662 test Loss 171.8251 with MSE metric 44517.7236\n",
      "Epoch 22 batch 180 train Loss 441.1028 test Loss 171.5422 with MSE metric 44517.9067\n",
      "Epoch 22 batch 190 train Loss 440.3419 test Loss 171.2603 with MSE metric 44516.4745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 batch 200 train Loss 439.5845 test Loss 170.9795 with MSE metric 44517.1503\n",
      "Epoch 22 batch 210 train Loss 438.8287 test Loss 170.6996 with MSE metric 44518.5449\n",
      "Epoch 22 batch 220 train Loss 438.0748 test Loss 170.4206 with MSE metric 44515.3944\n",
      "Epoch 22 batch 230 train Loss 437.3248 test Loss 170.1427 with MSE metric 44514.9178\n",
      "Epoch 22 batch 240 train Loss 436.5766 test Loss 169.8657 with MSE metric 44516.0122\n",
      "Time taken for 1 epoch: 30.260584354400635 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 435.8310 test Loss 169.5898 with MSE metric 44513.3356\n",
      "Epoch 23 batch 10 train Loss 435.0883 test Loss 169.3148 with MSE metric 44512.0308\n",
      "Epoch 23 batch 20 train Loss 434.3486 test Loss 169.0408 with MSE metric 44513.1384\n",
      "Epoch 23 batch 30 train Loss 433.6202 test Loss 168.7675 with MSE metric 44513.4129\n",
      "Epoch 23 batch 40 train Loss 432.8849 test Loss 168.4954 with MSE metric 44511.8111\n",
      "Epoch 23 batch 50 train Loss 432.1534 test Loss 168.2243 with MSE metric 44511.1223\n",
      "Epoch 23 batch 60 train Loss 431.4247 test Loss 167.9540 with MSE metric 44508.3982\n",
      "Epoch 23 batch 70 train Loss 430.6979 test Loss 167.6847 with MSE metric 44507.5394\n",
      "Epoch 23 batch 80 train Loss 429.9726 test Loss 167.4163 with MSE metric 44506.6466\n",
      "Epoch 23 batch 90 train Loss 429.2497 test Loss 167.1489 with MSE metric 44506.2492\n",
      "Epoch 23 batch 100 train Loss 428.5315 test Loss 166.8823 with MSE metric 44506.8506\n",
      "Epoch 23 batch 110 train Loss 427.8139 test Loss 166.6166 with MSE metric 44506.0082\n",
      "Epoch 23 batch 120 train Loss 427.0996 test Loss 166.3517 with MSE metric 44505.4590\n",
      "Epoch 23 batch 130 train Loss 426.3896 test Loss 166.0879 with MSE metric 44505.9764\n",
      "Epoch 23 batch 140 train Loss 425.6798 test Loss 165.8250 with MSE metric 44505.8162\n",
      "Epoch 23 batch 150 train Loss 424.9717 test Loss 165.5628 with MSE metric 44505.3034\n",
      "Epoch 23 batch 160 train Loss 424.2654 test Loss 165.3016 with MSE metric 44504.3922\n",
      "Epoch 23 batch 170 train Loss 423.5618 test Loss 165.0414 with MSE metric 44504.5068\n",
      "Epoch 23 batch 180 train Loss 422.8617 test Loss 164.7820 with MSE metric 44504.6756\n",
      "Epoch 23 batch 190 train Loss 422.1629 test Loss 164.5235 with MSE metric 44502.3542\n",
      "Epoch 23 batch 200 train Loss 421.4668 test Loss 164.2659 with MSE metric 44503.2639\n",
      "Epoch 23 batch 210 train Loss 420.7730 test Loss 164.0091 with MSE metric 44502.4669\n",
      "Epoch 23 batch 220 train Loss 420.0812 test Loss 163.7533 with MSE metric 44503.5011\n",
      "Epoch 23 batch 230 train Loss 419.3917 test Loss 163.4983 with MSE metric 44503.7991\n",
      "Epoch 23 batch 240 train Loss 418.7049 test Loss 163.2442 with MSE metric 44501.7502\n",
      "Time taken for 1 epoch: 29.787312030792236 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 418.0205 test Loss 162.9907 with MSE metric 44502.2103\n",
      "Epoch 24 batch 10 train Loss 417.3378 test Loss 162.7382 with MSE metric 44502.9575\n",
      "Epoch 24 batch 20 train Loss 416.6598 test Loss 162.4864 with MSE metric 44503.3249\n",
      "Epoch 24 batch 30 train Loss 415.9820 test Loss 162.2356 with MSE metric 44503.9373\n",
      "Epoch 24 batch 40 train Loss 415.3063 test Loss 161.9856 with MSE metric 44502.4237\n",
      "Epoch 24 batch 50 train Loss 414.6334 test Loss 161.7364 with MSE metric 44502.3820\n",
      "Epoch 24 batch 60 train Loss 413.9619 test Loss 161.4880 with MSE metric 44501.7777\n",
      "Epoch 24 batch 70 train Loss 413.2925 test Loss 161.2404 with MSE metric 44500.2765\n",
      "Epoch 24 batch 80 train Loss 412.6270 test Loss 160.9937 with MSE metric 44500.1991\n",
      "Epoch 24 batch 90 train Loss 411.9636 test Loss 160.7476 with MSE metric 44500.1292\n",
      "Epoch 24 batch 100 train Loss 411.3011 test Loss 160.5025 with MSE metric 44499.8309\n",
      "Epoch 24 batch 110 train Loss 410.6409 test Loss 160.2583 with MSE metric 44499.1573\n",
      "Epoch 24 batch 120 train Loss 409.9837 test Loss 160.0147 with MSE metric 44498.7194\n",
      "Epoch 24 batch 130 train Loss 409.3280 test Loss 159.7720 with MSE metric 44497.9833\n",
      "Epoch 24 batch 140 train Loss 408.6762 test Loss 159.5301 with MSE metric 44496.5639\n",
      "Epoch 24 batch 150 train Loss 408.0267 test Loss 159.2889 with MSE metric 44495.7882\n",
      "Epoch 24 batch 160 train Loss 407.3770 test Loss 159.0485 with MSE metric 44494.3233\n",
      "Epoch 24 batch 170 train Loss 406.7300 test Loss 158.8089 with MSE metric 44493.5046\n",
      "Epoch 24 batch 180 train Loss 406.0849 test Loss 158.5701 with MSE metric 44491.4073\n",
      "Epoch 24 batch 190 train Loss 405.4413 test Loss 158.3321 with MSE metric 44492.0621\n",
      "Epoch 24 batch 200 train Loss 404.8007 test Loss 158.0948 with MSE metric 44491.4432\n",
      "Epoch 24 batch 210 train Loss 404.1613 test Loss 157.8582 with MSE metric 44489.7749\n",
      "Epoch 24 batch 220 train Loss 403.5257 test Loss 157.6225 with MSE metric 44489.0644\n",
      "Epoch 24 batch 230 train Loss 402.8906 test Loss 157.3874 with MSE metric 44490.8394\n",
      "Epoch 24 batch 240 train Loss 402.2578 test Loss 157.1532 with MSE metric 44490.8048\n",
      "Time taken for 1 epoch: 31.27457094192505 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 401.6268 test Loss 156.9197 with MSE metric 44488.0306\n",
      "Epoch 25 batch 10 train Loss 400.9975 test Loss 156.6870 with MSE metric 44488.0853\n",
      "Epoch 25 batch 20 train Loss 400.3706 test Loss 156.4550 with MSE metric 44485.9134\n",
      "Epoch 25 batch 30 train Loss 399.7454 test Loss 156.2236 with MSE metric 44486.0850\n",
      "Epoch 25 batch 40 train Loss 399.1220 test Loss 155.9931 with MSE metric 44484.2154\n",
      "Epoch 25 batch 50 train Loss 398.5028 test Loss 155.7634 with MSE metric 44482.0640\n",
      "Epoch 25 batch 60 train Loss 397.8842 test Loss 155.5344 with MSE metric 44481.5086\n",
      "Epoch 25 batch 70 train Loss 397.2674 test Loss 155.3061 with MSE metric 44480.6203\n",
      "Epoch 25 batch 80 train Loss 396.6529 test Loss 155.0785 with MSE metric 44481.1450\n",
      "Epoch 25 batch 90 train Loss 396.0489 test Loss 154.8516 with MSE metric 44478.6523\n",
      "Epoch 25 batch 100 train Loss 395.4383 test Loss 154.6256 with MSE metric 44477.8937\n",
      "Epoch 25 batch 110 train Loss 394.8297 test Loss 154.4001 with MSE metric 44478.1594\n",
      "Epoch 25 batch 120 train Loss 394.2217 test Loss 154.1754 with MSE metric 44479.8721\n",
      "Epoch 25 batch 130 train Loss 393.6163 test Loss 153.9514 with MSE metric 44479.5171\n",
      "Epoch 25 batch 140 train Loss 393.0127 test Loss 153.7280 with MSE metric 44478.3759\n",
      "Epoch 25 batch 150 train Loss 392.4116 test Loss 153.5055 with MSE metric 44476.9842\n",
      "Epoch 25 batch 160 train Loss 391.8122 test Loss 153.2835 with MSE metric 44477.0386\n",
      "Epoch 25 batch 170 train Loss 391.2140 test Loss 153.0623 with MSE metric 44476.7864\n",
      "Epoch 25 batch 180 train Loss 390.6300 test Loss 152.8417 with MSE metric 44474.1866\n",
      "Epoch 25 batch 190 train Loss 390.0355 test Loss 152.6218 with MSE metric 44472.6137\n",
      "Epoch 25 batch 200 train Loss 389.4442 test Loss 152.4028 with MSE metric 44473.8797\n",
      "Epoch 25 batch 210 train Loss 388.8541 test Loss 152.1844 with MSE metric 44472.8964\n",
      "Epoch 25 batch 220 train Loss 388.2657 test Loss 151.9665 with MSE metric 44469.5572\n",
      "Epoch 25 batch 230 train Loss 387.6784 test Loss 151.7494 with MSE metric 44467.5549\n",
      "Epoch 25 batch 240 train Loss 387.0934 test Loss 151.5329 with MSE metric 44465.7508\n",
      "Time taken for 1 epoch: 31.710872888565063 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 386.5099 test Loss 151.3171 with MSE metric 44464.5398\n",
      "Epoch 26 batch 10 train Loss 385.9280 test Loss 151.1020 with MSE metric 44464.5045\n",
      "Epoch 26 batch 20 train Loss 385.3493 test Loss 150.8875 with MSE metric 44464.3144\n",
      "Epoch 26 batch 30 train Loss 384.7712 test Loss 150.6738 with MSE metric 44464.6405\n",
      "Epoch 26 batch 40 train Loss 384.1945 test Loss 150.4606 with MSE metric 44461.8838\n",
      "Epoch 26 batch 50 train Loss 383.6199 test Loss 150.2481 with MSE metric 44462.6456\n",
      "Epoch 26 batch 60 train Loss 383.0496 test Loss 150.0362 with MSE metric 44461.7897\n",
      "Epoch 26 batch 70 train Loss 382.4783 test Loss 149.8249 with MSE metric 44461.0442\n",
      "Epoch 26 batch 80 train Loss 381.9085 test Loss 149.6143 with MSE metric 44457.9318\n",
      "Epoch 26 batch 90 train Loss 381.3410 test Loss 149.4044 with MSE metric 44456.8822\n",
      "Epoch 26 batch 100 train Loss 380.7748 test Loss 149.1950 with MSE metric 44456.3727\n",
      "Epoch 26 batch 110 train Loss 380.2101 test Loss 148.9863 with MSE metric 44457.0918\n",
      "Epoch 26 batch 120 train Loss 379.6481 test Loss 148.7783 with MSE metric 44456.4343\n",
      "Epoch 26 batch 130 train Loss 379.0873 test Loss 148.5710 with MSE metric 44454.6530\n",
      "Epoch 26 batch 140 train Loss 378.5311 test Loss 148.3642 with MSE metric 44454.9664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 batch 150 train Loss 377.9733 test Loss 148.1580 with MSE metric 44454.7234\n",
      "Epoch 26 batch 160 train Loss 377.4174 test Loss 147.9525 with MSE metric 44453.3625\n",
      "Epoch 26 batch 170 train Loss 376.8630 test Loss 147.7476 with MSE metric 44453.6740\n",
      "Epoch 26 batch 180 train Loss 376.3102 test Loss 147.5433 with MSE metric 44453.7684\n",
      "Epoch 26 batch 190 train Loss 375.7591 test Loss 147.3397 with MSE metric 44451.9496\n",
      "Epoch 26 batch 200 train Loss 375.2098 test Loss 147.1366 with MSE metric 44450.7005\n",
      "Epoch 26 batch 210 train Loss 374.6619 test Loss 146.9341 with MSE metric 44452.3522\n",
      "Epoch 26 batch 220 train Loss 374.1156 test Loss 146.7323 with MSE metric 44451.1834\n",
      "Epoch 26 batch 230 train Loss 373.5711 test Loss 146.5309 with MSE metric 44450.5299\n",
      "Epoch 26 batch 240 train Loss 373.0280 test Loss 146.3302 with MSE metric 44451.0870\n",
      "Time taken for 1 epoch: 29.952781915664673 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 372.4866 test Loss 146.1301 with MSE metric 44450.3651\n",
      "Epoch 27 batch 10 train Loss 371.9473 test Loss 145.9306 with MSE metric 44449.2412\n",
      "Epoch 27 batch 20 train Loss 371.4093 test Loss 145.7317 with MSE metric 44448.9084\n",
      "Epoch 27 batch 30 train Loss 370.8729 test Loss 145.5333 with MSE metric 44449.6830\n",
      "Epoch 27 batch 40 train Loss 370.3386 test Loss 145.3356 with MSE metric 44448.7742\n",
      "Epoch 27 batch 50 train Loss 369.8053 test Loss 145.1385 with MSE metric 44447.6842\n",
      "Epoch 27 batch 60 train Loss 369.2740 test Loss 144.9418 with MSE metric 44446.6652\n",
      "Epoch 27 batch 70 train Loss 368.7440 test Loss 144.7458 with MSE metric 44447.4786\n",
      "Epoch 27 batch 80 train Loss 368.2153 test Loss 144.5504 with MSE metric 44446.3417\n",
      "Epoch 27 batch 90 train Loss 367.6882 test Loss 144.3555 with MSE metric 44445.6922\n",
      "Epoch 27 batch 100 train Loss 367.1627 test Loss 144.1611 with MSE metric 44444.4540\n",
      "Epoch 27 batch 110 train Loss 366.6407 test Loss 143.9674 with MSE metric 44444.1631\n",
      "Epoch 27 batch 120 train Loss 366.1184 test Loss 143.7743 with MSE metric 44443.1881\n",
      "Epoch 27 batch 130 train Loss 365.5988 test Loss 143.5816 with MSE metric 44443.2500\n",
      "Epoch 27 batch 140 train Loss 365.0795 test Loss 143.3895 with MSE metric 44443.1479\n",
      "Epoch 27 batch 150 train Loss 364.5614 test Loss 143.1980 with MSE metric 44442.3447\n",
      "Epoch 27 batch 160 train Loss 364.0450 test Loss 143.0071 with MSE metric 44442.1274\n",
      "Epoch 27 batch 170 train Loss 363.5299 test Loss 142.8167 with MSE metric 44441.4744\n",
      "Epoch 27 batch 180 train Loss 363.0175 test Loss 142.6269 with MSE metric 44441.3993\n",
      "Epoch 27 batch 190 train Loss 362.5054 test Loss 142.4376 with MSE metric 44440.4337\n",
      "Epoch 27 batch 200 train Loss 361.9948 test Loss 142.2489 with MSE metric 44441.4784\n",
      "Epoch 27 batch 210 train Loss 361.4859 test Loss 142.0607 with MSE metric 44439.3951\n",
      "Epoch 27 batch 220 train Loss 360.9829 test Loss 141.8731 with MSE metric 44438.3368\n",
      "Epoch 27 batch 230 train Loss 360.4770 test Loss 141.6860 with MSE metric 44437.3609\n",
      "Epoch 27 batch 240 train Loss 359.9724 test Loss 141.4994 with MSE metric 44436.7279\n",
      "Time taken for 1 epoch: 30.433131217956543 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 359.4695 test Loss 141.3133 with MSE metric 44438.7372\n",
      "Epoch 28 batch 10 train Loss 358.9675 test Loss 141.1279 with MSE metric 44437.4755\n",
      "Epoch 28 batch 20 train Loss 358.4671 test Loss 140.9429 with MSE metric 44437.5265\n",
      "Epoch 28 batch 30 train Loss 357.9683 test Loss 140.7585 with MSE metric 44436.9023\n",
      "Epoch 28 batch 40 train Loss 357.4705 test Loss 140.5746 with MSE metric 44436.6041\n",
      "Epoch 28 batch 50 train Loss 356.9742 test Loss 140.3913 with MSE metric 44437.2828\n",
      "Epoch 28 batch 60 train Loss 356.4793 test Loss 140.2083 with MSE metric 44437.3487\n",
      "Epoch 28 batch 70 train Loss 355.9859 test Loss 140.0260 with MSE metric 44436.1055\n",
      "Epoch 28 batch 80 train Loss 355.4940 test Loss 139.8441 with MSE metric 44435.6005\n",
      "Epoch 28 batch 90 train Loss 355.0031 test Loss 139.6627 with MSE metric 44433.3865\n",
      "Epoch 28 batch 100 train Loss 354.5138 test Loss 139.4819 with MSE metric 44432.6426\n",
      "Epoch 28 batch 110 train Loss 354.0259 test Loss 139.3016 with MSE metric 44432.1475\n",
      "Epoch 28 batch 120 train Loss 353.5395 test Loss 139.1218 with MSE metric 44432.6995\n",
      "Epoch 28 batch 130 train Loss 353.0546 test Loss 138.9425 with MSE metric 44432.1447\n",
      "Epoch 28 batch 140 train Loss 352.5710 test Loss 138.7637 with MSE metric 44432.0027\n",
      "Epoch 28 batch 150 train Loss 352.0897 test Loss 138.5854 with MSE metric 44432.4883\n",
      "Epoch 28 batch 160 train Loss 351.6086 test Loss 138.4076 with MSE metric 44431.3688\n",
      "Epoch 28 batch 170 train Loss 351.1289 test Loss 138.2303 with MSE metric 44430.2415\n",
      "Epoch 28 batch 180 train Loss 350.6507 test Loss 138.0534 with MSE metric 44429.4506\n",
      "Epoch 28 batch 190 train Loss 350.1737 test Loss 137.8771 with MSE metric 44428.0734\n",
      "Epoch 28 batch 200 train Loss 349.6987 test Loss 137.7013 with MSE metric 44426.6721\n",
      "Epoch 28 batch 210 train Loss 349.2244 test Loss 137.5259 with MSE metric 44425.8820\n",
      "Epoch 28 batch 220 train Loss 348.7517 test Loss 137.3510 with MSE metric 44425.3358\n",
      "Epoch 28 batch 230 train Loss 348.2798 test Loss 137.1767 with MSE metric 44424.6874\n",
      "Epoch 28 batch 240 train Loss 347.8092 test Loss 137.0027 with MSE metric 44425.2455\n",
      "Time taken for 1 epoch: 29.607370853424072 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 347.3403 test Loss 136.8293 with MSE metric 44425.7740\n",
      "Epoch 29 batch 10 train Loss 346.8725 test Loss 136.6563 with MSE metric 44425.5950\n",
      "Epoch 29 batch 20 train Loss 346.4059 test Loss 136.4839 with MSE metric 44424.5724\n",
      "Epoch 29 batch 30 train Loss 345.9405 test Loss 136.3119 with MSE metric 44423.6686\n",
      "Epoch 29 batch 40 train Loss 345.4763 test Loss 136.1404 with MSE metric 44422.7239\n",
      "Epoch 29 batch 50 train Loss 345.0135 test Loss 135.9693 with MSE metric 44422.0632\n",
      "Epoch 29 batch 60 train Loss 344.5517 test Loss 135.7987 with MSE metric 44420.0858\n",
      "Epoch 29 batch 70 train Loss 344.0917 test Loss 135.6285 with MSE metric 44422.2485\n",
      "Epoch 29 batch 80 train Loss 343.6329 test Loss 135.4588 with MSE metric 44421.4307\n",
      "Epoch 29 batch 90 train Loss 343.1751 test Loss 135.2896 with MSE metric 44422.5613\n",
      "Epoch 29 batch 100 train Loss 342.7184 test Loss 135.1208 with MSE metric 44421.2323\n",
      "Epoch 29 batch 110 train Loss 342.2631 test Loss 134.9525 with MSE metric 44421.0077\n",
      "Epoch 29 batch 120 train Loss 341.8093 test Loss 134.7848 with MSE metric 44421.0726\n",
      "Epoch 29 batch 130 train Loss 341.3565 test Loss 134.6173 with MSE metric 44420.3308\n",
      "Epoch 29 batch 140 train Loss 340.9048 test Loss 134.4504 with MSE metric 44421.6821\n",
      "Epoch 29 batch 150 train Loss 340.4570 test Loss 134.2839 with MSE metric 44421.8485\n",
      "Epoch 29 batch 160 train Loss 340.0078 test Loss 134.1179 with MSE metric 44420.3539\n",
      "Epoch 29 batch 170 train Loss 339.5603 test Loss 133.9523 with MSE metric 44418.9651\n",
      "Epoch 29 batch 180 train Loss 339.1134 test Loss 133.7871 with MSE metric 44417.8854\n",
      "Epoch 29 batch 190 train Loss 338.6681 test Loss 133.6224 with MSE metric 44417.0549\n",
      "Epoch 29 batch 200 train Loss 338.2237 test Loss 133.4581 with MSE metric 44414.8476\n",
      "Epoch 29 batch 210 train Loss 337.7805 test Loss 133.2942 with MSE metric 44413.2550\n",
      "Epoch 29 batch 220 train Loss 337.3386 test Loss 133.1308 with MSE metric 44412.6882\n",
      "Epoch 29 batch 230 train Loss 336.8982 test Loss 132.9679 with MSE metric 44411.6667\n",
      "Epoch 29 batch 240 train Loss 336.4586 test Loss 132.8054 with MSE metric 44411.4782\n",
      "Time taken for 1 epoch: 30.93397092819214 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 336.0203 test Loss 132.6433 with MSE metric 44410.5437\n",
      "Epoch 30 batch 10 train Loss 335.5829 test Loss 132.4817 with MSE metric 44410.1456\n",
      "Epoch 30 batch 20 train Loss 335.1466 test Loss 132.3204 with MSE metric 44409.1879\n",
      "Epoch 30 batch 30 train Loss 334.7115 test Loss 132.1596 with MSE metric 44408.9858\n",
      "Epoch 30 batch 40 train Loss 334.2780 test Loss 131.9991 with MSE metric 44407.8582\n",
      "Epoch 30 batch 50 train Loss 333.8476 test Loss 131.8392 with MSE metric 44406.0127\n",
      "Epoch 30 batch 60 train Loss 333.4162 test Loss 131.6797 with MSE metric 44405.6028\n",
      "Epoch 30 batch 70 train Loss 332.9860 test Loss 131.5206 with MSE metric 44404.6121\n",
      "Epoch 30 batch 80 train Loss 332.5567 test Loss 131.3619 with MSE metric 44404.7620\n",
      "Epoch 30 batch 90 train Loss 332.1288 test Loss 131.2037 with MSE metric 44403.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 batch 100 train Loss 331.7017 test Loss 131.0458 with MSE metric 44402.5543\n",
      "Epoch 30 batch 110 train Loss 331.2757 test Loss 130.8884 with MSE metric 44401.7364\n",
      "Epoch 30 batch 120 train Loss 330.8509 test Loss 130.7314 with MSE metric 44401.1948\n",
      "Epoch 30 batch 130 train Loss 330.4272 test Loss 130.5748 with MSE metric 44402.4479\n",
      "Epoch 30 batch 140 train Loss 330.0047 test Loss 130.4186 with MSE metric 44402.8617\n",
      "Epoch 30 batch 150 train Loss 329.5861 test Loss 130.2628 with MSE metric 44401.9661\n",
      "Epoch 30 batch 160 train Loss 329.1658 test Loss 130.1074 with MSE metric 44401.9016\n",
      "Epoch 30 batch 170 train Loss 328.7467 test Loss 129.9523 with MSE metric 44402.1558\n",
      "Epoch 30 batch 180 train Loss 328.3287 test Loss 129.7978 with MSE metric 44402.1161\n",
      "Epoch 30 batch 190 train Loss 327.9115 test Loss 129.6436 with MSE metric 44402.0686\n",
      "Epoch 30 batch 200 train Loss 327.4956 test Loss 129.4898 with MSE metric 44400.9390\n",
      "Epoch 30 batch 210 train Loss 327.0813 test Loss 129.3364 with MSE metric 44400.0264\n",
      "Epoch 30 batch 220 train Loss 326.6677 test Loss 129.1834 with MSE metric 44399.6060\n",
      "Epoch 30 batch 230 train Loss 326.2550 test Loss 129.0307 with MSE metric 44398.4512\n",
      "Epoch 30 batch 240 train Loss 325.8431 test Loss 128.8785 with MSE metric 44397.8178\n",
      "Time taken for 1 epoch: 31.132457971572876 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 325.4323 test Loss 128.7267 with MSE metric 44395.2933\n",
      "Epoch 31 batch 10 train Loss 325.0231 test Loss 128.5753 with MSE metric 44394.3125\n",
      "Epoch 31 batch 20 train Loss 324.6146 test Loss 128.4243 with MSE metric 44392.9813\n",
      "Epoch 31 batch 30 train Loss 324.2071 test Loss 128.2736 with MSE metric 44392.2537\n",
      "Epoch 31 batch 40 train Loss 323.8006 test Loss 128.1233 with MSE metric 44392.2389\n",
      "Epoch 31 batch 50 train Loss 323.3963 test Loss 127.9734 with MSE metric 44391.1102\n",
      "Epoch 31 batch 60 train Loss 322.9918 test Loss 127.8238 with MSE metric 44391.0664\n",
      "Epoch 31 batch 70 train Loss 322.5884 test Loss 127.6747 with MSE metric 44390.2452\n",
      "Epoch 31 batch 80 train Loss 322.1888 test Loss 127.5259 with MSE metric 44389.5432\n",
      "Epoch 31 batch 90 train Loss 321.7874 test Loss 127.3775 with MSE metric 44388.6763\n",
      "Epoch 31 batch 100 train Loss 321.3870 test Loss 127.2295 with MSE metric 44389.5070\n",
      "Epoch 31 batch 110 train Loss 320.9879 test Loss 127.0819 with MSE metric 44388.3588\n",
      "Epoch 31 batch 120 train Loss 320.5909 test Loss 126.9346 with MSE metric 44387.7711\n",
      "Epoch 31 batch 130 train Loss 320.1937 test Loss 126.7877 with MSE metric 44388.2631\n",
      "Epoch 31 batch 140 train Loss 319.7977 test Loss 126.6412 with MSE metric 44387.2531\n",
      "Epoch 31 batch 150 train Loss 319.4026 test Loss 126.4950 with MSE metric 44387.2137\n",
      "Epoch 31 batch 160 train Loss 319.0084 test Loss 126.3492 with MSE metric 44388.2219\n",
      "Epoch 31 batch 170 train Loss 318.6172 test Loss 126.2038 with MSE metric 44387.2905\n",
      "Epoch 31 batch 180 train Loss 318.2248 test Loss 126.0588 with MSE metric 44385.9407\n",
      "Epoch 31 batch 190 train Loss 317.8335 test Loss 125.9141 with MSE metric 44384.2443\n",
      "Epoch 31 batch 200 train Loss 317.4435 test Loss 125.7698 with MSE metric 44383.4457\n",
      "Epoch 31 batch 210 train Loss 317.0541 test Loss 125.6258 with MSE metric 44383.4995\n",
      "Epoch 31 batch 220 train Loss 316.6658 test Loss 125.4822 with MSE metric 44383.6009\n",
      "Epoch 31 batch 230 train Loss 316.2797 test Loss 125.3389 with MSE metric 44384.1267\n",
      "Epoch 31 batch 240 train Loss 315.8934 test Loss 125.1960 with MSE metric 44384.4867\n",
      "Time taken for 1 epoch: 33.38770318031311 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 315.5080 test Loss 125.0535 with MSE metric 44383.2921\n",
      "Epoch 32 batch 10 train Loss 315.1234 test Loss 124.9113 with MSE metric 44382.3624\n",
      "Epoch 32 batch 20 train Loss 314.7398 test Loss 124.7695 with MSE metric 44381.7614\n",
      "Epoch 32 batch 30 train Loss 314.3573 test Loss 124.6280 with MSE metric 44381.2901\n",
      "Epoch 32 batch 40 train Loss 313.9757 test Loss 124.4869 with MSE metric 44380.4358\n",
      "Epoch 32 batch 50 train Loss 313.5950 test Loss 124.3461 with MSE metric 44379.5308\n",
      "Epoch 32 batch 60 train Loss 313.2153 test Loss 124.2056 with MSE metric 44379.0752\n",
      "Epoch 32 batch 70 train Loss 312.8365 test Loss 124.0654 with MSE metric 44379.4068\n",
      "Epoch 32 batch 80 train Loss 312.4586 test Loss 123.9257 with MSE metric 44378.3717\n",
      "Epoch 32 batch 90 train Loss 312.0819 test Loss 123.7863 with MSE metric 44378.1085\n",
      "Epoch 32 batch 100 train Loss 311.7059 test Loss 123.6472 with MSE metric 44378.0782\n",
      "Epoch 32 batch 110 train Loss 311.3366 test Loss 123.5085 with MSE metric 44377.8277\n",
      "Epoch 32 batch 120 train Loss 310.9623 test Loss 123.3701 with MSE metric 44376.7873\n",
      "Epoch 32 batch 130 train Loss 310.5890 test Loss 123.2322 with MSE metric 44375.8646\n",
      "Epoch 32 batch 140 train Loss 310.2166 test Loss 123.0945 with MSE metric 44374.4045\n",
      "Epoch 32 batch 150 train Loss 309.8452 test Loss 122.9572 with MSE metric 44372.9337\n",
      "Epoch 32 batch 160 train Loss 309.4761 test Loss 122.8203 with MSE metric 44372.9908\n",
      "Epoch 32 batch 170 train Loss 309.1064 test Loss 122.6836 with MSE metric 44372.3393\n",
      "Epoch 32 batch 180 train Loss 308.7377 test Loss 122.5472 with MSE metric 44370.9973\n",
      "Epoch 32 batch 190 train Loss 308.3701 test Loss 122.4112 with MSE metric 44370.9990\n",
      "Epoch 32 batch 200 train Loss 308.0032 test Loss 122.2756 with MSE metric 44368.7280\n",
      "Epoch 32 batch 210 train Loss 307.6378 test Loss 122.1403 with MSE metric 44366.9120\n",
      "Epoch 32 batch 220 train Loss 307.2744 test Loss 122.0053 with MSE metric 44365.4667\n",
      "Epoch 32 batch 230 train Loss 306.9102 test Loss 121.8707 with MSE metric 44364.8996\n",
      "Epoch 32 batch 240 train Loss 306.5470 test Loss 121.7363 with MSE metric 44365.4152\n",
      "Time taken for 1 epoch: 26.995386123657227 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 306.1863 test Loss 121.6023 with MSE metric 44364.5159\n",
      "Epoch 33 batch 10 train Loss 305.8247 test Loss 121.4686 with MSE metric 44364.8901\n",
      "Epoch 33 batch 20 train Loss 305.4642 test Loss 121.3353 with MSE metric 44363.4346\n",
      "Epoch 33 batch 30 train Loss 305.1044 test Loss 121.2023 with MSE metric 44364.1777\n",
      "Epoch 33 batch 40 train Loss 304.7454 test Loss 121.0696 with MSE metric 44362.8307\n",
      "Epoch 33 batch 50 train Loss 304.3874 test Loss 120.9372 with MSE metric 44360.8439\n",
      "Epoch 33 batch 60 train Loss 304.0304 test Loss 120.8051 with MSE metric 44359.1693\n",
      "Epoch 33 batch 70 train Loss 303.6739 test Loss 120.6733 with MSE metric 44358.7875\n",
      "Epoch 33 batch 80 train Loss 303.3184 test Loss 120.5418 with MSE metric 44357.1135\n",
      "Epoch 33 batch 90 train Loss 302.9639 test Loss 120.4106 with MSE metric 44355.8522\n",
      "Epoch 33 batch 100 train Loss 302.6099 test Loss 120.2798 with MSE metric 44355.5773\n",
      "Epoch 33 batch 110 train Loss 302.2571 test Loss 120.1493 with MSE metric 44355.6622\n",
      "Epoch 33 batch 120 train Loss 301.9050 test Loss 120.0191 with MSE metric 44355.2984\n",
      "Epoch 33 batch 130 train Loss 301.5537 test Loss 119.8892 with MSE metric 44354.3823\n",
      "Epoch 33 batch 140 train Loss 301.2033 test Loss 119.7596 with MSE metric 44352.2472\n",
      "Epoch 33 batch 150 train Loss 300.8537 test Loss 119.6303 with MSE metric 44351.3199\n",
      "Epoch 33 batch 160 train Loss 300.5051 test Loss 119.5013 with MSE metric 44350.6661\n",
      "Epoch 33 batch 170 train Loss 300.1571 test Loss 119.3726 with MSE metric 44350.3086\n",
      "Epoch 33 batch 180 train Loss 299.8100 test Loss 119.2442 with MSE metric 44349.4143\n",
      "Epoch 33 batch 190 train Loss 299.4637 test Loss 119.1161 with MSE metric 44349.9665\n",
      "Epoch 33 batch 200 train Loss 299.1181 test Loss 118.9882 with MSE metric 44349.2473\n",
      "Epoch 33 batch 210 train Loss 298.7734 test Loss 118.8607 with MSE metric 44348.5530\n",
      "Epoch 33 batch 220 train Loss 298.4296 test Loss 118.7335 with MSE metric 44347.2971\n",
      "Epoch 33 batch 230 train Loss 298.0864 test Loss 118.6066 with MSE metric 44345.7185\n",
      "Epoch 33 batch 240 train Loss 297.7441 test Loss 118.4800 with MSE metric 44345.2235\n",
      "Time taken for 1 epoch: 33.21414113044739 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 297.4025 test Loss 118.3536 with MSE metric 44344.1740\n",
      "Epoch 34 batch 10 train Loss 297.0620 test Loss 118.2276 with MSE metric 44343.7262\n",
      "Epoch 34 batch 20 train Loss 296.7221 test Loss 118.1018 with MSE metric 44343.7238\n",
      "Epoch 34 batch 30 train Loss 296.3831 test Loss 117.9763 with MSE metric 44343.3633\n",
      "Epoch 34 batch 40 train Loss 296.0451 test Loss 117.8511 with MSE metric 44343.0560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 batch 50 train Loss 295.7075 test Loss 117.7262 with MSE metric 44341.9728\n",
      "Epoch 34 batch 60 train Loss 295.3707 test Loss 117.6016 with MSE metric 44341.8949\n",
      "Epoch 34 batch 70 train Loss 295.0348 test Loss 117.4773 with MSE metric 44341.6695\n",
      "Epoch 34 batch 80 train Loss 294.7001 test Loss 117.3532 with MSE metric 44340.4276\n",
      "Epoch 34 batch 90 train Loss 294.3657 test Loss 117.2294 with MSE metric 44340.7906\n",
      "Epoch 34 batch 100 train Loss 294.0321 test Loss 117.1060 with MSE metric 44340.4896\n",
      "Epoch 34 batch 110 train Loss 293.6995 test Loss 116.9828 with MSE metric 44340.1295\n",
      "Epoch 34 batch 120 train Loss 293.3678 test Loss 116.8599 with MSE metric 44338.9561\n",
      "Epoch 34 batch 130 train Loss 293.0365 test Loss 116.7373 with MSE metric 44338.4681\n",
      "Epoch 34 batch 140 train Loss 292.7062 test Loss 116.6149 with MSE metric 44336.6368\n",
      "Epoch 34 batch 150 train Loss 292.3764 test Loss 116.4928 with MSE metric 44335.7128\n",
      "Epoch 34 batch 160 train Loss 292.0485 test Loss 116.3710 with MSE metric 44334.6346\n",
      "Epoch 34 batch 170 train Loss 291.7201 test Loss 116.2495 with MSE metric 44335.5717\n",
      "Epoch 34 batch 180 train Loss 291.3925 test Loss 116.1282 with MSE metric 44334.1412\n",
      "Epoch 34 batch 190 train Loss 291.0657 test Loss 116.0073 with MSE metric 44333.5644\n",
      "Epoch 34 batch 200 train Loss 290.7397 test Loss 115.8866 with MSE metric 44332.8458\n",
      "Epoch 34 batch 210 train Loss 290.4143 test Loss 115.7661 with MSE metric 44331.1580\n",
      "Epoch 34 batch 220 train Loss 290.0901 test Loss 115.6460 with MSE metric 44330.2315\n",
      "Epoch 34 batch 230 train Loss 289.7663 test Loss 115.5260 with MSE metric 44329.7985\n",
      "Epoch 34 batch 240 train Loss 289.4433 test Loss 115.4064 with MSE metric 44329.3568\n",
      "Time taken for 1 epoch: 31.681270122528076 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 289.1209 test Loss 115.2871 with MSE metric 44329.9038\n",
      "Epoch 35 batch 10 train Loss 288.7994 test Loss 115.1681 with MSE metric 44328.5986\n",
      "Epoch 35 batch 20 train Loss 288.4785 test Loss 115.0493 with MSE metric 44329.1525\n",
      "Epoch 35 batch 30 train Loss 288.1585 test Loss 114.9308 with MSE metric 44328.8736\n",
      "Epoch 35 batch 40 train Loss 287.8392 test Loss 114.8125 with MSE metric 44328.0815\n",
      "Epoch 35 batch 50 train Loss 287.5206 test Loss 114.6945 with MSE metric 44327.3490\n",
      "Epoch 35 batch 60 train Loss 287.2027 test Loss 114.5767 with MSE metric 44326.8305\n",
      "Epoch 35 batch 70 train Loss 286.8855 test Loss 114.4593 with MSE metric 44326.1724\n",
      "Epoch 35 batch 80 train Loss 286.5692 test Loss 114.3421 with MSE metric 44325.8708\n",
      "Epoch 35 batch 90 train Loss 286.2534 test Loss 114.2251 with MSE metric 44324.7114\n",
      "Epoch 35 batch 100 train Loss 285.9383 test Loss 114.1085 with MSE metric 44322.8426\n",
      "Epoch 35 batch 110 train Loss 285.6240 test Loss 113.9920 with MSE metric 44321.1611\n",
      "Epoch 35 batch 120 train Loss 285.3104 test Loss 113.8758 with MSE metric 44320.7945\n",
      "Epoch 35 batch 130 train Loss 284.9977 test Loss 113.7599 with MSE metric 44320.5615\n",
      "Epoch 35 batch 140 train Loss 284.6855 test Loss 113.6442 with MSE metric 44319.4807\n",
      "Epoch 35 batch 150 train Loss 284.3743 test Loss 113.5288 with MSE metric 44319.4863\n",
      "Epoch 35 batch 160 train Loss 284.0644 test Loss 113.4137 with MSE metric 44319.6822\n",
      "Epoch 35 batch 170 train Loss 283.7543 test Loss 113.2988 with MSE metric 44318.8634\n",
      "Epoch 35 batch 180 train Loss 283.4447 test Loss 113.1841 with MSE metric 44317.8515\n",
      "Epoch 35 batch 190 train Loss 283.1360 test Loss 113.0698 with MSE metric 44317.8409\n",
      "Epoch 35 batch 200 train Loss 282.8279 test Loss 112.9556 with MSE metric 44317.0318\n",
      "Epoch 35 batch 210 train Loss 282.5206 test Loss 112.8417 with MSE metric 44316.4663\n",
      "Epoch 35 batch 220 train Loss 282.2139 test Loss 112.7280 with MSE metric 44315.8152\n",
      "Epoch 35 batch 230 train Loss 281.9078 test Loss 112.6146 with MSE metric 44315.3655\n",
      "Epoch 35 batch 240 train Loss 281.6023 test Loss 112.5015 with MSE metric 44314.8838\n",
      "Time taken for 1 epoch: 30.756860971450806 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 281.2975 test Loss 112.3886 with MSE metric 44314.3072\n",
      "Epoch 36 batch 10 train Loss 280.9935 test Loss 112.2759 with MSE metric 44314.2527\n",
      "Epoch 36 batch 20 train Loss 280.6903 test Loss 112.1635 with MSE metric 44314.4038\n",
      "Epoch 36 batch 30 train Loss 280.3875 test Loss 112.0513 with MSE metric 44312.7680\n",
      "Epoch 36 batch 40 train Loss 280.0855 test Loss 111.9393 with MSE metric 44312.1401\n",
      "Epoch 36 batch 50 train Loss 279.7842 test Loss 111.8276 with MSE metric 44311.2905\n",
      "Epoch 36 batch 60 train Loss 279.4835 test Loss 111.7161 with MSE metric 44311.2226\n",
      "Epoch 36 batch 70 train Loss 279.1835 test Loss 111.6049 with MSE metric 44311.6194\n",
      "Epoch 36 batch 80 train Loss 278.8843 test Loss 111.4940 with MSE metric 44311.6559\n",
      "Epoch 36 batch 90 train Loss 278.5856 test Loss 111.3832 with MSE metric 44310.8827\n",
      "Epoch 36 batch 100 train Loss 278.2875 test Loss 111.2727 with MSE metric 44309.6556\n",
      "Epoch 36 batch 110 train Loss 277.9902 test Loss 111.1624 with MSE metric 44308.6758\n",
      "Epoch 36 batch 120 train Loss 277.6934 test Loss 111.0524 with MSE metric 44308.9942\n",
      "Epoch 36 batch 130 train Loss 277.3972 test Loss 110.9427 with MSE metric 44309.3514\n",
      "Epoch 36 batch 140 train Loss 277.1018 test Loss 110.8331 with MSE metric 44307.9419\n",
      "Epoch 36 batch 150 train Loss 276.8069 test Loss 110.7237 with MSE metric 44306.9068\n",
      "Epoch 36 batch 160 train Loss 276.5128 test Loss 110.6146 with MSE metric 44307.2725\n",
      "Epoch 36 batch 170 train Loss 276.2192 test Loss 110.5057 with MSE metric 44306.1406\n",
      "Epoch 36 batch 180 train Loss 275.9263 test Loss 110.3971 with MSE metric 44304.8760\n",
      "Epoch 36 batch 190 train Loss 275.6342 test Loss 110.2887 with MSE metric 44303.5625\n",
      "Epoch 36 batch 200 train Loss 275.3426 test Loss 110.1806 with MSE metric 44303.4807\n",
      "Epoch 36 batch 210 train Loss 275.0516 test Loss 110.0726 with MSE metric 44301.9333\n",
      "Epoch 36 batch 220 train Loss 274.7613 test Loss 109.9649 with MSE metric 44300.6103\n",
      "Epoch 36 batch 230 train Loss 274.4718 test Loss 109.8574 with MSE metric 44300.1058\n",
      "Epoch 36 batch 240 train Loss 274.1826 test Loss 109.7502 with MSE metric 44299.4267\n",
      "Time taken for 1 epoch: 29.93765091896057 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 273.8941 test Loss 109.6432 with MSE metric 44297.5856\n",
      "Epoch 37 batch 10 train Loss 273.6063 test Loss 109.5363 with MSE metric 44296.6035\n",
      "Epoch 37 batch 20 train Loss 273.3199 test Loss 109.4298 with MSE metric 44294.8407\n",
      "Epoch 37 batch 30 train Loss 273.0332 test Loss 109.3234 with MSE metric 44294.3430\n",
      "Epoch 37 batch 40 train Loss 272.7472 test Loss 109.2173 with MSE metric 44293.7683\n",
      "Epoch 37 batch 50 train Loss 272.4619 test Loss 109.1114 with MSE metric 44292.9573\n",
      "Epoch 37 batch 60 train Loss 272.1771 test Loss 109.0057 with MSE metric 44292.9156\n",
      "Epoch 37 batch 70 train Loss 271.8928 test Loss 108.9002 with MSE metric 44292.3996\n",
      "Epoch 37 batch 80 train Loss 271.6093 test Loss 108.7950 with MSE metric 44290.9496\n",
      "Epoch 37 batch 90 train Loss 271.3262 test Loss 108.6900 with MSE metric 44289.6007\n",
      "Epoch 37 batch 100 train Loss 271.0440 test Loss 108.5852 with MSE metric 44290.1560\n",
      "Epoch 37 batch 110 train Loss 270.7623 test Loss 108.4806 with MSE metric 44290.9327\n",
      "Epoch 37 batch 120 train Loss 270.4812 test Loss 108.3763 with MSE metric 44289.6274\n",
      "Epoch 37 batch 130 train Loss 270.2006 test Loss 108.2721 with MSE metric 44289.2790\n",
      "Epoch 37 batch 140 train Loss 269.9206 test Loss 108.1682 with MSE metric 44290.4066\n",
      "Epoch 37 batch 150 train Loss 269.6412 test Loss 108.0645 with MSE metric 44289.1958\n",
      "Epoch 37 batch 160 train Loss 269.3623 test Loss 107.9610 with MSE metric 44289.8178\n",
      "Epoch 37 batch 170 train Loss 269.0843 test Loss 107.8577 with MSE metric 44288.8699\n",
      "Epoch 37 batch 180 train Loss 268.8067 test Loss 107.7546 with MSE metric 44287.8802\n",
      "Epoch 37 batch 190 train Loss 268.5297 test Loss 107.6517 with MSE metric 44287.3426\n",
      "Epoch 37 batch 200 train Loss 268.2534 test Loss 107.5490 with MSE metric 44286.5428\n",
      "Epoch 37 batch 210 train Loss 267.9776 test Loss 107.4466 with MSE metric 44285.8722\n",
      "Epoch 37 batch 220 train Loss 267.7023 test Loss 107.3444 with MSE metric 44285.2446\n",
      "Epoch 37 batch 230 train Loss 267.4276 test Loss 107.2424 with MSE metric 44283.8042\n",
      "Epoch 37 batch 240 train Loss 267.1535 test Loss 107.1406 with MSE metric 44283.8780\n",
      "Time taken for 1 epoch: 32.63907194137573 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 266.8799 test Loss 107.0390 with MSE metric 44282.0343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 batch 10 train Loss 266.6070 test Loss 106.9376 with MSE metric 44281.3151\n",
      "Epoch 38 batch 20 train Loss 266.3348 test Loss 106.8364 with MSE metric 44281.0579\n",
      "Epoch 38 batch 30 train Loss 266.0629 test Loss 106.7355 with MSE metric 44280.2183\n",
      "Epoch 38 batch 40 train Loss 265.7916 test Loss 106.6347 with MSE metric 44279.0542\n",
      "Epoch 38 batch 50 train Loss 265.5209 test Loss 106.5341 with MSE metric 44279.6080\n",
      "Epoch 38 batch 60 train Loss 265.2507 test Loss 106.4338 with MSE metric 44278.0860\n",
      "Epoch 38 batch 70 train Loss 264.9811 test Loss 106.3336 with MSE metric 44276.4217\n",
      "Epoch 38 batch 80 train Loss 264.7121 test Loss 106.2337 with MSE metric 44275.2926\n",
      "Epoch 38 batch 90 train Loss 264.4436 test Loss 106.1339 with MSE metric 44274.6828\n",
      "Epoch 38 batch 100 train Loss 264.1757 test Loss 106.0344 with MSE metric 44273.8267\n",
      "Epoch 38 batch 110 train Loss 263.9085 test Loss 105.9350 with MSE metric 44273.1122\n",
      "Epoch 38 batch 120 train Loss 263.6417 test Loss 105.8359 with MSE metric 44271.9023\n",
      "Epoch 38 batch 130 train Loss 263.3755 test Loss 105.7369 with MSE metric 44271.1053\n",
      "Epoch 38 batch 140 train Loss 263.1103 test Loss 105.6382 with MSE metric 44270.4151\n",
      "Epoch 38 batch 150 train Loss 262.8453 test Loss 105.5396 with MSE metric 44269.8211\n",
      "Epoch 38 batch 160 train Loss 262.5807 test Loss 105.4413 with MSE metric 44268.6953\n",
      "Epoch 38 batch 170 train Loss 262.3167 test Loss 105.3431 with MSE metric 44269.0247\n",
      "Epoch 38 batch 180 train Loss 262.0532 test Loss 105.2452 with MSE metric 44268.7948\n",
      "Epoch 38 batch 190 train Loss 261.7902 test Loss 105.1474 with MSE metric 44266.9701\n",
      "Epoch 38 batch 200 train Loss 261.5278 test Loss 105.0498 with MSE metric 44266.0274\n",
      "Epoch 38 batch 210 train Loss 261.2660 test Loss 104.9525 with MSE metric 44265.5728\n",
      "Epoch 38 batch 220 train Loss 261.0047 test Loss 104.8553 with MSE metric 44264.4439\n",
      "Epoch 38 batch 230 train Loss 260.7439 test Loss 104.7583 with MSE metric 44263.9222\n",
      "Epoch 38 batch 240 train Loss 260.4837 test Loss 104.6616 with MSE metric 44264.8702\n",
      "Time taken for 1 epoch: 33.288737058639526 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 260.2241 test Loss 104.5650 with MSE metric 44264.4590\n",
      "Epoch 39 batch 10 train Loss 259.9653 test Loss 104.4686 with MSE metric 44265.1907\n",
      "Epoch 39 batch 20 train Loss 259.7066 test Loss 104.3724 with MSE metric 44264.0552\n",
      "Epoch 39 batch 30 train Loss 259.4485 test Loss 104.2764 with MSE metric 44263.9913\n",
      "Epoch 39 batch 40 train Loss 259.1912 test Loss 104.1806 with MSE metric 44262.8369\n",
      "Epoch 39 batch 50 train Loss 258.9342 test Loss 104.0850 with MSE metric 44261.9097\n",
      "Epoch 39 batch 60 train Loss 258.6777 test Loss 103.9896 with MSE metric 44261.1320\n",
      "Epoch 39 batch 70 train Loss 258.4216 test Loss 103.8944 with MSE metric 44260.6871\n",
      "Epoch 39 batch 80 train Loss 258.1661 test Loss 103.7993 with MSE metric 44259.5090\n",
      "Epoch 39 batch 90 train Loss 257.9115 test Loss 103.7045 with MSE metric 44258.0243\n",
      "Epoch 39 batch 100 train Loss 257.6570 test Loss 103.6098 with MSE metric 44256.1147\n",
      "Epoch 39 batch 110 train Loss 257.4031 test Loss 103.5153 with MSE metric 44255.8874\n",
      "Epoch 39 batch 120 train Loss 257.1496 test Loss 103.4211 with MSE metric 44254.0409\n",
      "Epoch 39 batch 130 train Loss 256.8967 test Loss 103.3270 with MSE metric 44252.3346\n",
      "Epoch 39 batch 140 train Loss 256.6443 test Loss 103.2331 with MSE metric 44250.7497\n",
      "Epoch 39 batch 150 train Loss 256.3923 test Loss 103.1394 with MSE metric 44249.3169\n",
      "Epoch 39 batch 160 train Loss 256.1410 test Loss 103.0458 with MSE metric 44247.8565\n",
      "Epoch 39 batch 170 train Loss 255.8900 test Loss 102.9524 with MSE metric 44247.7978\n",
      "Epoch 39 batch 180 train Loss 255.6396 test Loss 102.8593 with MSE metric 44246.6254\n",
      "Epoch 39 batch 190 train Loss 255.3904 test Loss 102.7663 with MSE metric 44246.0773\n",
      "Epoch 39 batch 200 train Loss 255.1415 test Loss 102.6735 with MSE metric 44244.7466\n",
      "Epoch 39 batch 210 train Loss 254.8927 test Loss 102.5809 with MSE metric 44244.7289\n",
      "Epoch 39 batch 220 train Loss 254.6442 test Loss 102.4885 with MSE metric 44244.2515\n",
      "Epoch 39 batch 230 train Loss 254.3963 test Loss 102.3962 with MSE metric 44243.2014\n",
      "Epoch 39 batch 240 train Loss 254.1489 test Loss 102.3042 with MSE metric 44242.6197\n",
      "Time taken for 1 epoch: 32.42719101905823 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 253.9020 test Loss 102.2123 with MSE metric 44241.3190\n",
      "Epoch 40 batch 10 train Loss 253.6557 test Loss 102.1206 with MSE metric 44239.6672\n",
      "Epoch 40 batch 20 train Loss 253.4099 test Loss 102.0291 with MSE metric 44238.6448\n",
      "Epoch 40 batch 30 train Loss 253.1644 test Loss 101.9377 with MSE metric 44237.8432\n",
      "Epoch 40 batch 40 train Loss 252.9196 test Loss 101.8466 with MSE metric 44236.8877\n",
      "Epoch 40 batch 50 train Loss 252.6751 test Loss 101.7556 with MSE metric 44236.7331\n",
      "Epoch 40 batch 60 train Loss 252.4312 test Loss 101.6648 with MSE metric 44236.9618\n",
      "Epoch 40 batch 70 train Loss 252.1877 test Loss 101.5742 with MSE metric 44235.4930\n",
      "Epoch 40 batch 80 train Loss 251.9448 test Loss 101.4837 with MSE metric 44234.6759\n",
      "Epoch 40 batch 90 train Loss 251.7022 test Loss 101.3934 with MSE metric 44232.7219\n",
      "Epoch 40 batch 100 train Loss 251.4601 test Loss 101.3033 with MSE metric 44232.2949\n",
      "Epoch 40 batch 110 train Loss 251.2185 test Loss 101.2134 with MSE metric 44231.5173\n",
      "Epoch 40 batch 120 train Loss 250.9774 test Loss 101.1236 with MSE metric 44230.1837\n",
      "Epoch 40 batch 130 train Loss 250.7368 test Loss 101.0340 with MSE metric 44230.0426\n",
      "Epoch 40 batch 140 train Loss 250.4966 test Loss 100.9446 with MSE metric 44228.8666\n",
      "Epoch 40 batch 150 train Loss 250.2569 test Loss 100.8554 with MSE metric 44228.0661\n",
      "Epoch 40 batch 160 train Loss 250.0178 test Loss 100.7663 with MSE metric 44228.3773\n",
      "Epoch 40 batch 170 train Loss 249.7791 test Loss 100.6774 with MSE metric 44228.0309\n",
      "Epoch 40 batch 180 train Loss 249.5409 test Loss 100.5887 with MSE metric 44227.2670\n",
      "Epoch 40 batch 190 train Loss 249.3030 test Loss 100.5001 with MSE metric 44225.0658\n",
      "Epoch 40 batch 200 train Loss 249.0657 test Loss 100.4117 with MSE metric 44224.5324\n",
      "Epoch 40 batch 210 train Loss 248.8288 test Loss 100.3235 with MSE metric 44223.6498\n",
      "Epoch 40 batch 220 train Loss 248.5924 test Loss 100.2354 with MSE metric 44222.9472\n",
      "Epoch 40 batch 230 train Loss 248.3564 test Loss 100.1475 with MSE metric 44222.1505\n",
      "Epoch 40 batch 240 train Loss 248.1209 test Loss 100.0598 with MSE metric 44221.2615\n",
      "Time taken for 1 epoch: 30.55947208404541 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 247.8859 test Loss 99.9723 with MSE metric 44220.8223\n",
      "Epoch 41 batch 10 train Loss 247.6513 test Loss 99.8849 with MSE metric 44219.3389\n",
      "Epoch 41 batch 20 train Loss 247.4171 test Loss 99.7976 with MSE metric 44218.4671\n",
      "Epoch 41 batch 30 train Loss 247.1835 test Loss 99.7106 with MSE metric 44217.6524\n",
      "Epoch 41 batch 40 train Loss 246.9503 test Loss 99.6237 with MSE metric 44216.3274\n",
      "Epoch 41 batch 50 train Loss 246.7175 test Loss 99.5369 with MSE metric 44216.1125\n",
      "Epoch 41 batch 60 train Loss 246.4854 test Loss 99.4504 with MSE metric 44215.7634\n",
      "Epoch 41 batch 70 train Loss 246.2536 test Loss 99.3640 with MSE metric 44215.1512\n",
      "Epoch 41 batch 80 train Loss 246.0220 test Loss 99.2777 with MSE metric 44213.7824\n",
      "Epoch 41 batch 90 train Loss 245.7911 test Loss 99.1916 with MSE metric 44212.5612\n",
      "Epoch 41 batch 100 train Loss 245.5606 test Loss 99.1057 with MSE metric 44210.3916\n",
      "Epoch 41 batch 110 train Loss 245.3305 test Loss 99.0199 with MSE metric 44209.9257\n",
      "Epoch 41 batch 120 train Loss 245.1010 test Loss 98.9343 with MSE metric 44210.2498\n",
      "Epoch 41 batch 130 train Loss 244.8718 test Loss 98.8489 with MSE metric 44209.8434\n",
      "Epoch 41 batch 140 train Loss 244.6429 test Loss 98.7636 with MSE metric 44208.3411\n",
      "Epoch 41 batch 150 train Loss 244.4146 test Loss 98.6785 with MSE metric 44206.9444\n",
      "Epoch 41 batch 160 train Loss 244.1867 test Loss 98.5935 with MSE metric 44206.3844\n",
      "Epoch 41 batch 170 train Loss 243.9591 test Loss 98.5087 with MSE metric 44205.4863\n",
      "Epoch 41 batch 180 train Loss 243.7321 test Loss 98.4241 with MSE metric 44204.6084\n",
      "Epoch 41 batch 190 train Loss 243.5056 test Loss 98.3396 with MSE metric 44202.9626\n",
      "Epoch 41 batch 200 train Loss 243.2794 test Loss 98.2553 with MSE metric 44201.6485\n",
      "Epoch 41 batch 210 train Loss 243.0537 test Loss 98.1711 with MSE metric 44201.2445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 batch 220 train Loss 242.8285 test Loss 98.0871 with MSE metric 44200.8859\n",
      "Epoch 41 batch 230 train Loss 242.6036 test Loss 98.0032 with MSE metric 44200.7658\n",
      "Epoch 41 batch 240 train Loss 242.3791 test Loss 97.9195 with MSE metric 44200.1878\n",
      "Time taken for 1 epoch: 33.621551752090454 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 242.1551 test Loss 97.8359 with MSE metric 44200.0555\n",
      "Epoch 42 batch 10 train Loss 241.9316 test Loss 97.7525 with MSE metric 44199.4589\n",
      "Epoch 42 batch 20 train Loss 241.7084 test Loss 97.6693 with MSE metric 44199.1145\n",
      "Epoch 42 batch 30 train Loss 241.4857 test Loss 97.5862 with MSE metric 44198.2599\n",
      "Epoch 42 batch 40 train Loss 241.2634 test Loss 97.5032 with MSE metric 44196.2663\n",
      "Epoch 42 batch 50 train Loss 241.0415 test Loss 97.4205 with MSE metric 44195.1520\n",
      "Epoch 42 batch 60 train Loss 240.8200 test Loss 97.3378 with MSE metric 44194.8486\n",
      "Epoch 42 batch 70 train Loss 240.5990 test Loss 97.2553 with MSE metric 44194.6064\n",
      "Epoch 42 batch 80 train Loss 240.3788 test Loss 97.1730 with MSE metric 44193.1305\n",
      "Epoch 42 batch 90 train Loss 240.1586 test Loss 97.0909 with MSE metric 44191.9936\n",
      "Epoch 42 batch 100 train Loss 239.9388 test Loss 97.0089 with MSE metric 44192.2105\n",
      "Epoch 42 batch 110 train Loss 239.7194 test Loss 96.9270 with MSE metric 44191.5003\n",
      "Epoch 42 batch 120 train Loss 239.5004 test Loss 96.8453 with MSE metric 44190.5800\n",
      "Epoch 42 batch 130 train Loss 239.2818 test Loss 96.7638 with MSE metric 44188.9190\n",
      "Epoch 42 batch 140 train Loss 239.0636 test Loss 96.6824 with MSE metric 44188.6206\n",
      "Epoch 42 batch 150 train Loss 238.8460 test Loss 96.6011 with MSE metric 44187.4135\n",
      "Epoch 42 batch 160 train Loss 238.6287 test Loss 96.5200 with MSE metric 44187.4582\n",
      "Epoch 42 batch 170 train Loss 238.4116 test Loss 96.4390 with MSE metric 44186.8488\n",
      "Epoch 42 batch 180 train Loss 238.1951 test Loss 96.3582 with MSE metric 44185.8520\n",
      "Epoch 42 batch 190 train Loss 237.9789 test Loss 96.2776 with MSE metric 44185.9766\n",
      "Epoch 42 batch 200 train Loss 237.7631 test Loss 96.1970 with MSE metric 44184.6833\n",
      "Epoch 42 batch 210 train Loss 237.5477 test Loss 96.1166 with MSE metric 44184.1209\n",
      "Epoch 42 batch 220 train Loss 237.3328 test Loss 96.0364 with MSE metric 44183.3255\n",
      "Epoch 42 batch 230 train Loss 237.1182 test Loss 95.9563 with MSE metric 44182.0264\n",
      "Epoch 42 batch 240 train Loss 236.9041 test Loss 95.8763 with MSE metric 44180.0684\n",
      "Time taken for 1 epoch: 33.12707304954529 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 236.6902 test Loss 95.7965 with MSE metric 44178.6335\n",
      "Epoch 43 batch 10 train Loss 236.4769 test Loss 95.7168 with MSE metric 44177.4926\n",
      "Epoch 43 batch 20 train Loss 236.2639 test Loss 95.6373 with MSE metric 44176.4451\n",
      "Epoch 43 batch 30 train Loss 236.0514 test Loss 95.5579 with MSE metric 44175.7181\n",
      "Epoch 43 batch 40 train Loss 235.8392 test Loss 95.4786 with MSE metric 44174.7513\n",
      "Epoch 43 batch 50 train Loss 235.6274 test Loss 95.3995 with MSE metric 44173.3290\n",
      "Epoch 43 batch 60 train Loss 235.4160 test Loss 95.3206 with MSE metric 44171.4713\n",
      "Epoch 43 batch 70 train Loss 235.2050 test Loss 95.2418 with MSE metric 44170.6628\n",
      "Epoch 43 batch 80 train Loss 234.9943 test Loss 95.1631 with MSE metric 44169.3941\n",
      "Epoch 43 batch 90 train Loss 234.7841 test Loss 95.0845 with MSE metric 44169.0801\n",
      "Epoch 43 batch 100 train Loss 234.5742 test Loss 95.0061 with MSE metric 44167.6645\n",
      "Epoch 43 batch 110 train Loss 234.3647 test Loss 94.9278 with MSE metric 44166.2373\n",
      "Epoch 43 batch 120 train Loss 234.1556 test Loss 94.8497 with MSE metric 44165.0383\n",
      "Epoch 43 batch 130 train Loss 233.9468 test Loss 94.7717 with MSE metric 44163.2692\n",
      "Epoch 43 batch 140 train Loss 233.7385 test Loss 94.6938 with MSE metric 44161.9758\n",
      "Epoch 43 batch 150 train Loss 233.5305 test Loss 94.6161 with MSE metric 44160.9732\n",
      "Epoch 43 batch 160 train Loss 233.3229 test Loss 94.5385 with MSE metric 44159.0930\n",
      "Epoch 43 batch 170 train Loss 233.1158 test Loss 94.4610 with MSE metric 44158.6332\n",
      "Epoch 43 batch 180 train Loss 232.9090 test Loss 94.3837 with MSE metric 44157.0270\n",
      "Epoch 43 batch 190 train Loss 232.7026 test Loss 94.3066 with MSE metric 44155.6314\n",
      "Epoch 43 batch 200 train Loss 232.4965 test Loss 94.2295 with MSE metric 44154.2341\n",
      "Epoch 43 batch 210 train Loss 232.2908 test Loss 94.1526 with MSE metric 44154.2762\n",
      "Epoch 43 batch 220 train Loss 232.0855 test Loss 94.0759 with MSE metric 44152.8231\n",
      "Epoch 43 batch 230 train Loss 231.8806 test Loss 93.9992 with MSE metric 44151.3461\n",
      "Epoch 43 batch 240 train Loss 231.6760 test Loss 93.9227 with MSE metric 44149.8299\n",
      "Time taken for 1 epoch: 31.869013786315918 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 231.4718 test Loss 93.8464 with MSE metric 44149.7274\n",
      "Epoch 44 batch 10 train Loss 231.2680 test Loss 93.7701 with MSE metric 44148.9147\n",
      "Epoch 44 batch 20 train Loss 231.0645 test Loss 93.6941 with MSE metric 44147.2564\n",
      "Epoch 44 batch 30 train Loss 230.8614 test Loss 93.6181 with MSE metric 44146.7435\n",
      "Epoch 44 batch 40 train Loss 230.6587 test Loss 93.5423 with MSE metric 44146.6256\n",
      "Epoch 44 batch 50 train Loss 230.4563 test Loss 93.4666 with MSE metric 44145.7459\n",
      "Epoch 44 batch 60 train Loss 230.2545 test Loss 93.3910 with MSE metric 44145.0781\n",
      "Epoch 44 batch 70 train Loss 230.0528 test Loss 93.3156 with MSE metric 44145.2275\n",
      "Epoch 44 batch 80 train Loss 229.8516 test Loss 93.2403 with MSE metric 44142.7437\n",
      "Epoch 44 batch 90 train Loss 229.6507 test Loss 93.1651 with MSE metric 44141.6895\n",
      "Epoch 44 batch 100 train Loss 229.4502 test Loss 93.0900 with MSE metric 44141.1169\n",
      "Epoch 44 batch 110 train Loss 229.2500 test Loss 93.0151 with MSE metric 44141.3896\n",
      "Epoch 44 batch 120 train Loss 229.0501 test Loss 92.9404 with MSE metric 44140.2872\n",
      "Epoch 44 batch 130 train Loss 228.8506 test Loss 92.8657 with MSE metric 44138.6801\n",
      "Epoch 44 batch 140 train Loss 228.6515 test Loss 92.7912 with MSE metric 44138.0042\n",
      "Epoch 44 batch 150 train Loss 228.4528 test Loss 92.7168 with MSE metric 44135.1291\n",
      "Epoch 44 batch 160 train Loss 228.2544 test Loss 92.6426 with MSE metric 44134.1952\n",
      "Epoch 44 batch 170 train Loss 228.0563 test Loss 92.5685 with MSE metric 44132.9806\n",
      "Epoch 44 batch 180 train Loss 227.8585 test Loss 92.4945 with MSE metric 44129.9665\n",
      "Epoch 44 batch 190 train Loss 227.6612 test Loss 92.4206 with MSE metric 44128.8531\n",
      "Epoch 44 batch 200 train Loss 227.4642 test Loss 92.3468 with MSE metric 44127.5864\n",
      "Epoch 44 batch 210 train Loss 227.2675 test Loss 92.2732 with MSE metric 44126.5566\n",
      "Epoch 44 batch 220 train Loss 227.0712 test Loss 92.1997 with MSE metric 44125.5749\n",
      "Epoch 44 batch 230 train Loss 226.8752 test Loss 92.1263 with MSE metric 44124.5601\n",
      "Epoch 44 batch 240 train Loss 226.6797 test Loss 92.0531 with MSE metric 44125.0060\n",
      "Time taken for 1 epoch: 31.517754793167114 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 226.4845 test Loss 91.9800 with MSE metric 44123.4724\n",
      "Epoch 45 batch 10 train Loss 226.2896 test Loss 91.9070 with MSE metric 44122.6808\n",
      "Epoch 45 batch 20 train Loss 226.0952 test Loss 91.8342 with MSE metric 44121.4780\n",
      "Epoch 45 batch 30 train Loss 225.9010 test Loss 91.7615 with MSE metric 44120.8613\n",
      "Epoch 45 batch 40 train Loss 225.7071 test Loss 91.6889 with MSE metric 44119.1131\n",
      "Epoch 45 batch 50 train Loss 225.5136 test Loss 91.6164 with MSE metric 44118.8455\n",
      "Epoch 45 batch 60 train Loss 225.3203 test Loss 91.5441 with MSE metric 44116.8436\n",
      "Epoch 45 batch 70 train Loss 225.1274 test Loss 91.4718 with MSE metric 44114.5110\n",
      "Epoch 45 batch 80 train Loss 224.9349 test Loss 91.3997 with MSE metric 44113.7890\n",
      "Epoch 45 batch 90 train Loss 224.7427 test Loss 91.3277 with MSE metric 44111.3984\n",
      "Epoch 45 batch 100 train Loss 224.5508 test Loss 91.2559 with MSE metric 44109.8675\n",
      "Epoch 45 batch 110 train Loss 224.3593 test Loss 91.1841 with MSE metric 44109.5829\n",
      "Epoch 45 batch 120 train Loss 224.1682 test Loss 91.1125 with MSE metric 44108.1699\n",
      "Epoch 45 batch 130 train Loss 223.9773 test Loss 91.0410 with MSE metric 44107.7133\n",
      "Epoch 45 batch 140 train Loss 223.7868 test Loss 90.9696 with MSE metric 44107.0000\n",
      "Epoch 45 batch 150 train Loss 223.5966 test Loss 90.8984 with MSE metric 44105.2737\n",
      "Epoch 45 batch 160 train Loss 223.4069 test Loss 90.8272 with MSE metric 44104.0978\n",
      "Epoch 45 batch 170 train Loss 223.2174 test Loss 90.7562 with MSE metric 44102.9904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 batch 180 train Loss 223.0282 test Loss 90.6853 with MSE metric 44101.5011\n",
      "Epoch 45 batch 190 train Loss 222.8394 test Loss 90.6146 with MSE metric 44100.6974\n",
      "Epoch 45 batch 200 train Loss 222.6510 test Loss 90.5439 with MSE metric 44099.5101\n",
      "Epoch 45 batch 210 train Loss 222.4628 test Loss 90.4734 with MSE metric 44098.1245\n",
      "Epoch 45 batch 220 train Loss 222.2749 test Loss 90.4029 with MSE metric 44097.0320\n",
      "Epoch 45 batch 230 train Loss 222.0874 test Loss 90.3326 with MSE metric 44096.3366\n",
      "Epoch 45 batch 240 train Loss 221.9002 test Loss 90.2625 with MSE metric 44094.5095\n",
      "Time taken for 1 epoch: 33.040276288986206 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 221.7133 test Loss 90.1924 with MSE metric 44093.0937\n",
      "Epoch 46 batch 10 train Loss 221.5267 test Loss 90.1225 with MSE metric 44091.4200\n",
      "Epoch 46 batch 20 train Loss 221.3404 test Loss 90.0526 with MSE metric 44089.4272\n",
      "Epoch 46 batch 30 train Loss 221.1545 test Loss 89.9829 with MSE metric 44087.6704\n",
      "Epoch 46 batch 40 train Loss 220.9689 test Loss 89.9133 with MSE metric 44085.8873\n",
      "Epoch 46 batch 50 train Loss 220.7836 test Loss 89.8438 with MSE metric 44084.5413\n",
      "Epoch 46 batch 60 train Loss 220.5986 test Loss 89.7745 with MSE metric 44084.2292\n",
      "Epoch 46 batch 70 train Loss 220.4140 test Loss 89.7052 with MSE metric 44083.1551\n",
      "Epoch 46 batch 80 train Loss 220.2296 test Loss 89.6361 with MSE metric 44081.7022\n",
      "Epoch 46 batch 90 train Loss 220.0457 test Loss 89.5670 with MSE metric 44081.0737\n",
      "Epoch 46 batch 100 train Loss 219.8620 test Loss 89.4981 with MSE metric 44079.8102\n",
      "Epoch 46 batch 110 train Loss 219.6786 test Loss 89.4293 with MSE metric 44077.8945\n",
      "Epoch 46 batch 120 train Loss 219.4955 test Loss 89.3606 with MSE metric 44076.7422\n",
      "Epoch 46 batch 130 train Loss 219.3129 test Loss 89.2921 with MSE metric 44075.3843\n",
      "Epoch 46 batch 140 train Loss 219.1304 test Loss 89.2236 with MSE metric 44073.9651\n",
      "Epoch 46 batch 150 train Loss 218.9483 test Loss 89.1553 with MSE metric 44072.9436\n",
      "Epoch 46 batch 160 train Loss 218.7664 test Loss 89.0871 with MSE metric 44071.4618\n",
      "Epoch 46 batch 170 train Loss 218.5850 test Loss 89.0189 with MSE metric 44070.8005\n",
      "Epoch 46 batch 180 train Loss 218.4038 test Loss 88.9509 with MSE metric 44069.9641\n",
      "Epoch 46 batch 190 train Loss 218.2229 test Loss 88.8830 with MSE metric 44068.8582\n",
      "Epoch 46 batch 200 train Loss 218.0424 test Loss 88.8152 with MSE metric 44067.6283\n",
      "Epoch 46 batch 210 train Loss 217.8623 test Loss 88.7476 with MSE metric 44065.9127\n",
      "Epoch 46 batch 220 train Loss 217.6824 test Loss 88.6801 with MSE metric 44064.1582\n",
      "Epoch 46 batch 230 train Loss 217.5027 test Loss 88.6126 with MSE metric 44062.9305\n",
      "Epoch 46 batch 240 train Loss 217.3233 test Loss 88.5453 with MSE metric 44062.9746\n",
      "Time taken for 1 epoch: 31.011949062347412 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 217.1442 test Loss 88.4781 with MSE metric 44060.7639\n",
      "Epoch 47 batch 10 train Loss 216.9654 test Loss 88.4110 with MSE metric 44058.9695\n",
      "Epoch 47 batch 20 train Loss 216.7870 test Loss 88.3440 with MSE metric 44057.8283\n",
      "Epoch 47 batch 30 train Loss 216.6088 test Loss 88.2771 with MSE metric 44056.5806\n",
      "Epoch 47 batch 40 train Loss 216.4310 test Loss 88.2103 with MSE metric 44054.7489\n",
      "Epoch 47 batch 50 train Loss 216.2535 test Loss 88.1437 with MSE metric 44053.6128\n",
      "Epoch 47 batch 60 train Loss 216.0762 test Loss 88.0771 with MSE metric 44051.8764\n",
      "Epoch 47 batch 70 train Loss 215.8992 test Loss 88.0107 with MSE metric 44049.7983\n",
      "Epoch 47 batch 80 train Loss 215.7226 test Loss 87.9443 with MSE metric 44048.1402\n",
      "Epoch 47 batch 90 train Loss 215.5463 test Loss 87.8781 with MSE metric 44046.5615\n",
      "Epoch 47 batch 100 train Loss 215.3702 test Loss 87.8120 with MSE metric 44044.8843\n",
      "Epoch 47 batch 110 train Loss 215.1944 test Loss 87.7460 with MSE metric 44044.2802\n",
      "Epoch 47 batch 120 train Loss 215.0191 test Loss 87.6801 with MSE metric 44042.8478\n",
      "Epoch 47 batch 130 train Loss 214.8440 test Loss 87.6143 with MSE metric 44042.1005\n",
      "Epoch 47 batch 140 train Loss 214.6691 test Loss 87.5486 with MSE metric 44040.8183\n",
      "Epoch 47 batch 150 train Loss 214.4945 test Loss 87.4830 with MSE metric 44039.8121\n",
      "Epoch 47 batch 160 train Loss 214.3202 test Loss 87.4176 with MSE metric 44038.9089\n",
      "Epoch 47 batch 170 train Loss 214.1462 test Loss 87.3522 with MSE metric 44036.8354\n",
      "Epoch 47 batch 180 train Loss 213.9724 test Loss 87.2869 with MSE metric 44035.7625\n",
      "Epoch 47 batch 190 train Loss 213.7990 test Loss 87.2218 with MSE metric 44034.6874\n",
      "Epoch 47 batch 200 train Loss 213.6258 test Loss 87.1567 with MSE metric 44033.2088\n",
      "Epoch 47 batch 210 train Loss 213.4529 test Loss 87.0917 with MSE metric 44031.2126\n",
      "Epoch 47 batch 220 train Loss 213.2804 test Loss 87.0268 with MSE metric 44030.2011\n",
      "Epoch 47 batch 230 train Loss 213.1081 test Loss 86.9621 with MSE metric 44028.6736\n",
      "Epoch 47 batch 240 train Loss 212.9361 test Loss 86.8974 with MSE metric 44026.6710\n",
      "Time taken for 1 epoch: 31.84079623222351 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 212.7644 test Loss 86.8329 with MSE metric 44024.8405\n",
      "Epoch 48 batch 10 train Loss 212.5929 test Loss 86.7684 with MSE metric 44023.5235\n",
      "Epoch 48 batch 20 train Loss 212.4218 test Loss 86.7040 with MSE metric 44022.8148\n",
      "Epoch 48 batch 30 train Loss 212.2510 test Loss 86.6398 with MSE metric 44021.7463\n",
      "Epoch 48 batch 40 train Loss 212.0804 test Loss 86.5757 with MSE metric 44020.3698\n",
      "Epoch 48 batch 50 train Loss 211.9101 test Loss 86.5116 with MSE metric 44018.4045\n",
      "Epoch 48 batch 60 train Loss 211.7401 test Loss 86.4477 with MSE metric 44017.6237\n",
      "Epoch 48 batch 70 train Loss 211.5703 test Loss 86.3838 with MSE metric 44016.4899\n",
      "Epoch 48 batch 80 train Loss 211.4009 test Loss 86.3201 with MSE metric 44015.0179\n",
      "Epoch 48 batch 90 train Loss 211.2317 test Loss 86.2565 with MSE metric 44013.5394\n",
      "Epoch 48 batch 100 train Loss 211.0628 test Loss 86.1929 with MSE metric 44011.9281\n",
      "Epoch 48 batch 110 train Loss 210.8943 test Loss 86.1295 with MSE metric 44009.9233\n",
      "Epoch 48 batch 120 train Loss 210.7260 test Loss 86.0662 with MSE metric 44008.4332\n",
      "Epoch 48 batch 130 train Loss 210.5579 test Loss 86.0029 with MSE metric 44007.3379\n",
      "Epoch 48 batch 140 train Loss 210.3901 test Loss 85.9398 with MSE metric 44005.7455\n",
      "Epoch 48 batch 150 train Loss 210.2226 test Loss 85.8768 with MSE metric 44004.6245\n",
      "Epoch 48 batch 160 train Loss 210.0554 test Loss 85.8139 with MSE metric 44002.9767\n",
      "Epoch 48 batch 170 train Loss 209.8884 test Loss 85.7510 with MSE metric 44001.0329\n",
      "Epoch 48 batch 180 train Loss 209.7217 test Loss 85.6883 with MSE metric 44000.1619\n",
      "Epoch 48 batch 190 train Loss 209.5552 test Loss 85.6257 with MSE metric 43999.0776\n",
      "Epoch 48 batch 200 train Loss 209.3891 test Loss 85.5631 with MSE metric 43996.8495\n",
      "Epoch 48 batch 210 train Loss 209.2232 test Loss 85.5007 with MSE metric 43995.3758\n",
      "Epoch 48 batch 220 train Loss 209.0577 test Loss 85.4384 with MSE metric 43993.8374\n",
      "Epoch 48 batch 230 train Loss 208.8924 test Loss 85.3761 with MSE metric 43992.5598\n",
      "Epoch 48 batch 240 train Loss 208.7274 test Loss 85.3140 with MSE metric 43991.0288\n",
      "Time taken for 1 epoch: 31.3562970161438 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 208.5626 test Loss 85.2519 with MSE metric 43989.2086\n",
      "Epoch 49 batch 10 train Loss 208.3980 test Loss 85.1900 with MSE metric 43987.9699\n",
      "Epoch 49 batch 20 train Loss 208.2337 test Loss 85.1282 with MSE metric 43986.1418\n",
      "Epoch 49 batch 30 train Loss 208.0697 test Loss 85.0664 with MSE metric 43984.4799\n",
      "Epoch 49 batch 40 train Loss 207.9060 test Loss 85.0047 with MSE metric 43982.0884\n",
      "Epoch 49 batch 50 train Loss 207.7425 test Loss 84.9432 with MSE metric 43980.9463\n",
      "Epoch 49 batch 60 train Loss 207.5793 test Loss 84.8817 with MSE metric 43979.2208\n",
      "Epoch 49 batch 70 train Loss 207.4163 test Loss 84.8203 with MSE metric 43977.8558\n",
      "Epoch 49 batch 80 train Loss 207.2537 test Loss 84.7591 with MSE metric 43976.3805\n",
      "Epoch 49 batch 90 train Loss 207.0912 test Loss 84.6979 with MSE metric 43976.0231\n",
      "Epoch 49 batch 100 train Loss 206.9290 test Loss 84.6368 with MSE metric 43974.8148\n",
      "Epoch 49 batch 110 train Loss 206.7672 test Loss 84.5758 with MSE metric 43973.9336\n",
      "Epoch 49 batch 120 train Loss 206.6055 test Loss 84.5149 with MSE metric 43971.6465\n",
      "Epoch 49 batch 130 train Loss 206.4442 test Loss 84.4541 with MSE metric 43969.3327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 batch 140 train Loss 206.2832 test Loss 84.3934 with MSE metric 43967.4418\n",
      "Epoch 49 batch 150 train Loss 206.1224 test Loss 84.3328 with MSE metric 43966.2756\n",
      "Epoch 49 batch 160 train Loss 205.9618 test Loss 84.2723 with MSE metric 43964.9097\n",
      "Epoch 49 batch 170 train Loss 205.8014 test Loss 84.2119 with MSE metric 43963.0417\n",
      "Epoch 49 batch 180 train Loss 205.6413 test Loss 84.1516 with MSE metric 43961.4574\n",
      "Epoch 49 batch 190 train Loss 205.4816 test Loss 84.0914 with MSE metric 43960.0805\n",
      "Epoch 49 batch 200 train Loss 205.3220 test Loss 84.0313 with MSE metric 43957.7946\n",
      "Epoch 49 batch 210 train Loss 205.1627 test Loss 83.9712 with MSE metric 43955.5272\n",
      "Epoch 49 batch 220 train Loss 205.0035 test Loss 83.9113 with MSE metric 43954.2889\n",
      "Epoch 49 batch 230 train Loss 204.8447 test Loss 83.8514 with MSE metric 43952.7308\n",
      "Epoch 49 batch 240 train Loss 204.6862 test Loss 83.7916 with MSE metric 43951.1960\n",
      "Time taken for 1 epoch: 33.75829815864563 secs\n",
      "\n",
      "Epoch 50 batch 0 train Loss 204.5279 test Loss 83.7320 with MSE metric 43949.3619\n",
      "Epoch 50 batch 10 train Loss 204.3699 test Loss 83.6724 with MSE metric 43947.5338\n",
      "Epoch 50 batch 20 train Loss 204.2121 test Loss 83.6129 with MSE metric 43945.6055\n",
      "Epoch 50 batch 30 train Loss 204.0545 test Loss 83.5535 with MSE metric 43944.0121\n",
      "Epoch 50 batch 40 train Loss 203.8972 test Loss 83.4942 with MSE metric 43941.6410\n",
      "Epoch 50 batch 50 train Loss 203.7401 test Loss 83.4349 with MSE metric 43940.1793\n",
      "Epoch 50 batch 60 train Loss 203.5832 test Loss 83.3758 with MSE metric 43938.5313\n",
      "Epoch 50 batch 70 train Loss 203.4266 test Loss 83.3167 with MSE metric 43936.6647\n",
      "Epoch 50 batch 80 train Loss 203.2703 test Loss 83.2577 with MSE metric 43934.5733\n",
      "Epoch 50 batch 90 train Loss 203.1142 test Loss 83.1988 with MSE metric 43932.3335\n",
      "Epoch 50 batch 100 train Loss 202.9584 test Loss 83.1400 with MSE metric 43930.1688\n",
      "Epoch 50 batch 110 train Loss 202.8028 test Loss 83.0813 with MSE metric 43928.2311\n",
      "Epoch 50 batch 120 train Loss 202.6474 test Loss 83.0227 with MSE metric 43926.2042\n",
      "Epoch 50 batch 130 train Loss 202.4923 test Loss 82.9642 with MSE metric 43924.4945\n",
      "Epoch 50 batch 140 train Loss 202.3375 test Loss 82.9057 with MSE metric 43922.3981\n",
      "Epoch 50 batch 150 train Loss 202.1829 test Loss 82.8474 with MSE metric 43920.8013\n",
      "Epoch 50 batch 160 train Loss 202.0285 test Loss 82.7891 with MSE metric 43918.9594\n",
      "Epoch 50 batch 170 train Loss 201.8744 test Loss 82.7309 with MSE metric 43917.5746\n",
      "Epoch 50 batch 180 train Loss 201.7205 test Loss 82.6728 with MSE metric 43916.1582\n",
      "Epoch 50 batch 190 train Loss 201.5668 test Loss 82.6148 with MSE metric 43915.0323\n",
      "Epoch 50 batch 200 train Loss 201.4134 test Loss 82.5569 with MSE metric 43913.3935\n",
      "Epoch 50 batch 210 train Loss 201.2603 test Loss 82.4990 with MSE metric 43912.4971\n",
      "Epoch 50 batch 220 train Loss 201.1073 test Loss 82.4413 with MSE metric 43909.9305\n",
      "Epoch 50 batch 230 train Loss 200.9547 test Loss 82.3836 with MSE metric 43908.3070\n",
      "Epoch 50 batch 240 train Loss 200.8022 test Loss 82.3261 with MSE metric 43906.6740\n",
      "Time taken for 1 epoch: 30.503470182418823 secs\n",
      "\n",
      "Epoch 51 batch 0 train Loss 200.6500 test Loss 82.2686 with MSE metric 43904.8650\n",
      "Epoch 51 batch 10 train Loss 200.4981 test Loss 82.2112 with MSE metric 43903.2019\n",
      "Epoch 51 batch 20 train Loss 200.3463 test Loss 82.1539 with MSE metric 43901.1875\n",
      "Epoch 51 batch 30 train Loss 200.1952 test Loss 82.0967 with MSE metric 43899.3041\n",
      "Epoch 51 batch 40 train Loss 200.0440 test Loss 82.0396 with MSE metric 43897.0319\n",
      "Epoch 51 batch 50 train Loss 199.8929 test Loss 81.9825 with MSE metric 43895.7978\n",
      "Epoch 51 batch 60 train Loss 199.7422 test Loss 81.9256 with MSE metric 43894.3778\n",
      "Epoch 51 batch 70 train Loss 199.5918 test Loss 81.8687 with MSE metric 43892.6570\n",
      "Epoch 51 batch 80 train Loss 199.4414 test Loss 81.8120 with MSE metric 43890.5286\n",
      "Epoch 51 batch 90 train Loss 199.2914 test Loss 81.7553 with MSE metric 43888.8149\n",
      "Epoch 51 batch 100 train Loss 199.1415 test Loss 81.6987 with MSE metric 43887.5705\n",
      "Epoch 51 batch 110 train Loss 198.9919 test Loss 81.6422 with MSE metric 43885.9152\n",
      "Epoch 51 batch 120 train Loss 198.8425 test Loss 81.5858 with MSE metric 43883.9172\n",
      "Epoch 51 batch 130 train Loss 198.6933 test Loss 81.5294 with MSE metric 43881.8234\n",
      "Epoch 51 batch 140 train Loss 198.5443 test Loss 81.4732 with MSE metric 43879.2033\n",
      "Epoch 51 batch 150 train Loss 198.3956 test Loss 81.4170 with MSE metric 43877.9714\n",
      "Epoch 51 batch 160 train Loss 198.2472 test Loss 81.3609 with MSE metric 43875.7586\n",
      "Epoch 51 batch 170 train Loss 198.0989 test Loss 81.3049 with MSE metric 43873.5673\n",
      "Epoch 51 batch 180 train Loss 197.9509 test Loss 81.2489 with MSE metric 43871.8523\n",
      "Epoch 51 batch 190 train Loss 197.8032 test Loss 81.1931 with MSE metric 43869.4629\n",
      "Epoch 51 batch 200 train Loss 197.6557 test Loss 81.1373 with MSE metric 43868.0414\n",
      "Epoch 51 batch 210 train Loss 197.5084 test Loss 81.0817 with MSE metric 43865.8812\n",
      "Epoch 51 batch 220 train Loss 197.3613 test Loss 81.0260 with MSE metric 43863.6250\n",
      "Epoch 51 batch 230 train Loss 197.2143 test Loss 80.9705 with MSE metric 43861.2009\n",
      "Epoch 51 batch 240 train Loss 197.0677 test Loss 80.9151 with MSE metric 43859.8146\n",
      "Time taken for 1 epoch: 28.349923849105835 secs\n",
      "\n",
      "Epoch 52 batch 0 train Loss 196.9212 test Loss 80.8597 with MSE metric 43858.4506\n",
      "Epoch 52 batch 10 train Loss 196.7751 test Loss 80.8044 with MSE metric 43856.2242\n",
      "Epoch 52 batch 20 train Loss 196.6291 test Loss 80.7492 with MSE metric 43854.3221\n",
      "Epoch 52 batch 30 train Loss 196.4833 test Loss 80.6941 with MSE metric 43852.9784\n",
      "Epoch 52 batch 40 train Loss 196.3378 test Loss 80.6391 with MSE metric 43852.0118\n",
      "Epoch 52 batch 50 train Loss 196.1924 test Loss 80.5841 with MSE metric 43849.3785\n",
      "Epoch 52 batch 60 train Loss 196.0474 test Loss 80.5293 with MSE metric 43847.6157\n",
      "Epoch 52 batch 70 train Loss 195.9025 test Loss 80.4745 with MSE metric 43844.7767\n",
      "Epoch 52 batch 80 train Loss 195.7578 test Loss 80.4197 with MSE metric 43842.9796\n",
      "Epoch 52 batch 90 train Loss 195.6133 test Loss 80.3651 with MSE metric 43841.0441\n",
      "Epoch 52 batch 100 train Loss 195.4691 test Loss 80.3105 with MSE metric 43838.8139\n",
      "Epoch 52 batch 110 train Loss 195.3251 test Loss 80.2561 with MSE metric 43836.8640\n",
      "Epoch 52 batch 120 train Loss 195.1813 test Loss 80.2017 with MSE metric 43835.3043\n",
      "Epoch 52 batch 130 train Loss 195.0378 test Loss 80.1473 with MSE metric 43832.9281\n",
      "Epoch 52 batch 140 train Loss 194.8944 test Loss 80.0931 with MSE metric 43830.8829\n",
      "Epoch 52 batch 150 train Loss 194.7513 test Loss 80.0389 with MSE metric 43827.9559\n",
      "Epoch 52 batch 160 train Loss 194.6083 test Loss 79.9848 with MSE metric 43825.3926\n",
      "Epoch 52 batch 170 train Loss 194.4656 test Loss 79.9308 with MSE metric 43822.9552\n",
      "Epoch 52 batch 180 train Loss 194.3231 test Loss 79.8768 with MSE metric 43820.3946\n",
      "Epoch 52 batch 190 train Loss 194.1808 test Loss 79.8230 with MSE metric 43818.4430\n",
      "Epoch 52 batch 200 train Loss 194.0389 test Loss 79.7692 with MSE metric 43816.9433\n",
      "Epoch 52 batch 210 train Loss 193.8970 test Loss 79.7155 with MSE metric 43814.9046\n",
      "Epoch 52 batch 220 train Loss 193.7554 test Loss 79.6619 with MSE metric 43813.0890\n",
      "Epoch 52 batch 230 train Loss 193.6140 test Loss 79.6084 with MSE metric 43810.8684\n",
      "Epoch 52 batch 240 train Loss 193.4728 test Loss 79.5549 with MSE metric 43808.5364\n",
      "Time taken for 1 epoch: 29.446740865707397 secs\n",
      "\n",
      "Epoch 53 batch 0 train Loss 193.3318 test Loss 79.5016 with MSE metric 43806.4766\n",
      "Epoch 53 batch 10 train Loss 193.1910 test Loss 79.4482 with MSE metric 43803.9503\n",
      "Epoch 53 batch 20 train Loss 193.0504 test Loss 79.3950 with MSE metric 43802.6426\n",
      "Epoch 53 batch 30 train Loss 192.9101 test Loss 79.3419 with MSE metric 43800.6102\n",
      "Epoch 53 batch 40 train Loss 192.7699 test Loss 79.2888 with MSE metric 43798.6160\n",
      "Epoch 53 batch 50 train Loss 192.6300 test Loss 79.2358 with MSE metric 43796.6825\n",
      "Epoch 53 batch 60 train Loss 192.4902 test Loss 79.1829 with MSE metric 43795.0944\n",
      "Epoch 53 batch 70 train Loss 192.3507 test Loss 79.1300 with MSE metric 43792.9440\n",
      "Epoch 53 batch 80 train Loss 192.2114 test Loss 79.0772 with MSE metric 43791.0422\n",
      "Epoch 53 batch 90 train Loss 192.0723 test Loss 79.0245 with MSE metric 43788.8212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 batch 100 train Loss 191.9334 test Loss 78.9719 with MSE metric 43786.0568\n",
      "Epoch 53 batch 110 train Loss 191.7946 test Loss 78.9194 with MSE metric 43783.1163\n",
      "Epoch 53 batch 120 train Loss 191.6561 test Loss 78.8669 with MSE metric 43780.4839\n",
      "Epoch 53 batch 130 train Loss 191.5178 test Loss 78.8145 with MSE metric 43778.7929\n",
      "Epoch 53 batch 140 train Loss 191.3798 test Loss 78.7621 with MSE metric 43776.6475\n",
      "Epoch 53 batch 150 train Loss 191.2419 test Loss 78.7098 with MSE metric 43774.1866\n",
      "Epoch 53 batch 160 train Loss 191.1042 test Loss 78.6577 with MSE metric 43770.7261\n",
      "Epoch 53 batch 170 train Loss 190.9667 test Loss 78.6055 with MSE metric 43768.6448\n",
      "Epoch 53 batch 180 train Loss 190.8294 test Loss 78.5535 with MSE metric 43766.5369\n",
      "Epoch 53 batch 190 train Loss 190.6924 test Loss 78.5015 with MSE metric 43763.5039\n",
      "Epoch 53 batch 200 train Loss 190.5555 test Loss 78.4496 with MSE metric 43760.7491\n",
      "Epoch 53 batch 210 train Loss 190.4189 test Loss 78.3978 with MSE metric 43759.1940\n",
      "Epoch 53 batch 220 train Loss 190.2824 test Loss 78.3461 with MSE metric 43756.6820\n",
      "Epoch 53 batch 230 train Loss 190.1462 test Loss 78.2944 with MSE metric 43754.7606\n",
      "Epoch 53 batch 240 train Loss 190.0101 test Loss 78.2428 with MSE metric 43751.9831\n",
      "Time taken for 1 epoch: 28.73857092857361 secs\n",
      "\n",
      "Epoch 54 batch 0 train Loss 189.8743 test Loss 78.1913 with MSE metric 43749.5266\n",
      "Epoch 54 batch 10 train Loss 189.7387 test Loss 78.1399 with MSE metric 43746.3092\n",
      "Epoch 54 batch 20 train Loss 189.6032 test Loss 78.0885 with MSE metric 43743.4048\n",
      "Epoch 54 batch 30 train Loss 189.4680 test Loss 78.0372 with MSE metric 43740.4624\n",
      "Epoch 54 batch 40 train Loss 189.3329 test Loss 77.9860 with MSE metric 43737.4065\n",
      "Epoch 54 batch 50 train Loss 189.1981 test Loss 77.9348 with MSE metric 43735.2705\n",
      "Epoch 54 batch 60 train Loss 189.0634 test Loss 77.8837 with MSE metric 43733.1054\n",
      "Epoch 54 batch 70 train Loss 188.9290 test Loss 77.8327 with MSE metric 43730.5383\n",
      "Epoch 54 batch 80 train Loss 188.7947 test Loss 77.7818 with MSE metric 43728.0665\n",
      "Epoch 54 batch 90 train Loss 188.6606 test Loss 77.7309 with MSE metric 43725.7135\n",
      "Epoch 54 batch 100 train Loss 188.5268 test Loss 77.6801 with MSE metric 43723.0022\n",
      "Epoch 54 batch 110 train Loss 188.3931 test Loss 77.6294 with MSE metric 43720.4951\n",
      "Epoch 54 batch 120 train Loss 188.2596 test Loss 77.5788 with MSE metric 43718.0521\n",
      "Epoch 54 batch 130 train Loss 188.1263 test Loss 77.5282 with MSE metric 43715.6649\n",
      "Epoch 54 batch 140 train Loss 187.9933 test Loss 77.4777 with MSE metric 43713.0730\n",
      "Epoch 54 batch 150 train Loss 187.8604 test Loss 77.4272 with MSE metric 43710.4196\n",
      "Epoch 54 batch 160 train Loss 187.7277 test Loss 77.3769 with MSE metric 43707.2357\n",
      "Epoch 54 batch 170 train Loss 187.5951 test Loss 77.3266 with MSE metric 43704.5649\n",
      "Epoch 54 batch 180 train Loss 187.4628 test Loss 77.2764 with MSE metric 43702.8702\n",
      "Epoch 54 batch 190 train Loss 187.3308 test Loss 77.2262 with MSE metric 43700.7892\n",
      "Epoch 54 batch 200 train Loss 187.1988 test Loss 77.1761 with MSE metric 43698.9194\n",
      "Epoch 54 batch 210 train Loss 187.0671 test Loss 77.1261 with MSE metric 43696.0193\n",
      "Epoch 54 batch 220 train Loss 186.9356 test Loss 77.0762 with MSE metric 43694.1533\n",
      "Epoch 54 batch 230 train Loss 186.8042 test Loss 77.0263 with MSE metric 43691.0829\n",
      "Epoch 54 batch 240 train Loss 186.6730 test Loss 76.9765 with MSE metric 43687.9095\n",
      "Time taken for 1 epoch: 29.11249089241028 secs\n",
      "\n",
      "Epoch 55 batch 0 train Loss 186.5420 test Loss 76.9268 with MSE metric 43685.2027\n",
      "Epoch 55 batch 10 train Loss 186.4112 test Loss 76.8771 with MSE metric 43682.4188\n",
      "Epoch 55 batch 20 train Loss 186.2806 test Loss 76.8275 with MSE metric 43679.7524\n",
      "Epoch 55 batch 30 train Loss 186.1502 test Loss 76.7780 with MSE metric 43676.8364\n",
      "Epoch 55 batch 40 train Loss 186.0200 test Loss 76.7285 with MSE metric 43673.8439\n",
      "Epoch 55 batch 50 train Loss 185.8899 test Loss 76.6791 with MSE metric 43672.0184\n",
      "Epoch 55 batch 60 train Loss 185.7601 test Loss 76.6298 with MSE metric 43668.8093\n",
      "Epoch 55 batch 70 train Loss 185.6304 test Loss 76.5805 with MSE metric 43666.4294\n",
      "Epoch 55 batch 80 train Loss 185.5009 test Loss 76.5313 with MSE metric 43664.0074\n",
      "Epoch 55 batch 90 train Loss 185.3716 test Loss 76.4822 with MSE metric 43661.1757\n",
      "Epoch 55 batch 100 train Loss 185.2425 test Loss 76.4331 with MSE metric 43657.7570\n",
      "Epoch 55 batch 110 train Loss 185.1135 test Loss 76.3842 with MSE metric 43654.7223\n",
      "Epoch 55 batch 120 train Loss 184.9850 test Loss 76.3352 with MSE metric 43651.5493\n",
      "Epoch 55 batch 130 train Loss 184.8565 test Loss 76.2864 with MSE metric 43649.0913\n",
      "Epoch 55 batch 140 train Loss 184.7281 test Loss 76.2377 with MSE metric 43646.1840\n",
      "Epoch 55 batch 150 train Loss 184.5999 test Loss 76.1890 with MSE metric 43643.3045\n",
      "Epoch 55 batch 160 train Loss 184.4719 test Loss 76.1404 with MSE metric 43640.0280\n",
      "Epoch 55 batch 170 train Loss 184.3441 test Loss 76.0918 with MSE metric 43637.6280\n",
      "Epoch 55 batch 180 train Loss 184.2164 test Loss 76.0433 with MSE metric 43634.7114\n",
      "Epoch 55 batch 190 train Loss 184.0890 test Loss 75.9949 with MSE metric 43632.0788\n",
      "Epoch 55 batch 200 train Loss 183.9617 test Loss 75.9465 with MSE metric 43628.8685\n",
      "Epoch 55 batch 210 train Loss 183.8346 test Loss 75.8982 with MSE metric 43625.7771\n",
      "Epoch 55 batch 220 train Loss 183.7077 test Loss 75.8500 with MSE metric 43623.3212\n",
      "Epoch 55 batch 230 train Loss 183.5810 test Loss 75.8018 with MSE metric 43621.0151\n",
      "Epoch 55 batch 240 train Loss 183.4544 test Loss 75.7537 with MSE metric 43618.1138\n",
      "Time taken for 1 epoch: 26.767453908920288 secs\n",
      "\n",
      "Epoch 56 batch 0 train Loss 183.3281 test Loss 75.7057 with MSE metric 43615.2894\n",
      "Epoch 56 batch 10 train Loss 183.2018 test Loss 75.6577 with MSE metric 43611.5622\n",
      "Epoch 56 batch 20 train Loss 183.0758 test Loss 75.6098 with MSE metric 43608.4411\n",
      "Epoch 56 batch 30 train Loss 182.9500 test Loss 75.5619 with MSE metric 43605.1964\n",
      "Epoch 56 batch 40 train Loss 182.8243 test Loss 75.5141 with MSE metric 43602.6448\n",
      "Epoch 56 batch 50 train Loss 182.6988 test Loss 75.4664 with MSE metric 43599.2040\n",
      "Epoch 56 batch 60 train Loss 182.5735 test Loss 75.4187 with MSE metric 43596.6588\n",
      "Epoch 56 batch 70 train Loss 182.4484 test Loss 75.3712 with MSE metric 43594.3468\n",
      "Epoch 56 batch 80 train Loss 182.3235 test Loss 75.3237 with MSE metric 43591.6102\n",
      "Epoch 56 batch 90 train Loss 182.1987 test Loss 75.2762 with MSE metric 43588.8538\n",
      "Epoch 56 batch 100 train Loss 182.0740 test Loss 75.2288 with MSE metric 43585.4673\n",
      "Epoch 56 batch 110 train Loss 181.9496 test Loss 75.1815 with MSE metric 43582.3993\n",
      "Epoch 56 batch 120 train Loss 181.8254 test Loss 75.1342 with MSE metric 43579.1044\n",
      "Epoch 56 batch 130 train Loss 181.7013 test Loss 75.0870 with MSE metric 43575.9364\n",
      "Epoch 56 batch 140 train Loss 181.5774 test Loss 75.0398 with MSE metric 43572.9714\n",
      "Epoch 56 batch 150 train Loss 181.4536 test Loss 74.9928 with MSE metric 43569.8576\n",
      "Epoch 56 batch 160 train Loss 181.3301 test Loss 74.9457 with MSE metric 43567.0044\n",
      "Epoch 56 batch 170 train Loss 181.2067 test Loss 74.8988 with MSE metric 43564.2145\n",
      "Epoch 56 batch 180 train Loss 181.0835 test Loss 74.8519 with MSE metric 43560.2380\n",
      "Epoch 56 batch 190 train Loss 180.9604 test Loss 74.8050 with MSE metric 43557.0544\n",
      "Epoch 56 batch 200 train Loss 180.8376 test Loss 74.7583 with MSE metric 43553.9420\n",
      "Epoch 56 batch 210 train Loss 180.7149 test Loss 74.7116 with MSE metric 43550.8250\n",
      "Epoch 56 batch 220 train Loss 180.5923 test Loss 74.6649 with MSE metric 43547.0558\n",
      "Epoch 56 batch 230 train Loss 180.4700 test Loss 74.6183 with MSE metric 43543.3499\n",
      "Epoch 56 batch 240 train Loss 180.3478 test Loss 74.5718 with MSE metric 43540.3540\n",
      "Time taken for 1 epoch: 31.742398977279663 secs\n",
      "\n",
      "Epoch 57 batch 0 train Loss 180.2258 test Loss 74.5253 with MSE metric 43536.8852\n",
      "Epoch 57 batch 10 train Loss 180.1039 test Loss 74.4789 with MSE metric 43533.0149\n",
      "Epoch 57 batch 20 train Loss 179.9823 test Loss 74.4326 with MSE metric 43530.0612\n",
      "Epoch 57 batch 30 train Loss 179.8607 test Loss 74.3863 with MSE metric 43526.5547\n",
      "Epoch 57 batch 40 train Loss 179.7394 test Loss 74.3401 with MSE metric 43522.9639\n",
      "Epoch 57 batch 50 train Loss 179.6182 test Loss 74.2940 with MSE metric 43519.1685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 batch 60 train Loss 179.4972 test Loss 74.2479 with MSE metric 43516.1597\n",
      "Epoch 57 batch 70 train Loss 179.3764 test Loss 74.2018 with MSE metric 43512.3787\n",
      "Epoch 57 batch 80 train Loss 179.2557 test Loss 74.1558 with MSE metric 43508.0618\n",
      "Epoch 57 batch 90 train Loss 179.1352 test Loss 74.1099 with MSE metric 43504.2698\n",
      "Epoch 57 batch 100 train Loss 179.0149 test Loss 74.0641 with MSE metric 43500.6652\n",
      "Epoch 57 batch 110 train Loss 178.8947 test Loss 74.0183 with MSE metric 43497.0397\n",
      "Epoch 57 batch 120 train Loss 178.7747 test Loss 73.9725 with MSE metric 43493.8067\n",
      "Epoch 57 batch 130 train Loss 178.6549 test Loss 73.9269 with MSE metric 43490.1209\n",
      "Epoch 57 batch 140 train Loss 178.5352 test Loss 73.8812 with MSE metric 43486.7418\n",
      "Epoch 57 batch 150 train Loss 178.4157 test Loss 73.8357 with MSE metric 43482.7810\n",
      "Epoch 57 batch 160 train Loss 178.2964 test Loss 73.7902 with MSE metric 43479.2398\n",
      "Epoch 57 batch 170 train Loss 178.1772 test Loss 73.7448 with MSE metric 43475.4671\n",
      "Epoch 57 batch 180 train Loss 178.0582 test Loss 73.6994 with MSE metric 43472.4251\n",
      "Epoch 57 batch 190 train Loss 177.9394 test Loss 73.6541 with MSE metric 43468.8613\n",
      "Epoch 57 batch 200 train Loss 177.8207 test Loss 73.6089 with MSE metric 43464.9967\n",
      "Epoch 57 batch 210 train Loss 177.7022 test Loss 73.5637 with MSE metric 43461.8264\n",
      "Epoch 57 batch 220 train Loss 177.5838 test Loss 73.5186 with MSE metric 43458.0320\n",
      "Epoch 57 batch 230 train Loss 177.4656 test Loss 73.4736 with MSE metric 43454.9173\n",
      "Epoch 57 batch 240 train Loss 177.3476 test Loss 73.4286 with MSE metric 43451.3658\n",
      "Time taken for 1 epoch: 29.905468940734863 secs\n",
      "\n",
      "Epoch 58 batch 0 train Loss 177.2297 test Loss 73.3836 with MSE metric 43448.2033\n",
      "Epoch 58 batch 10 train Loss 177.1120 test Loss 73.3387 with MSE metric 43444.6326\n",
      "Epoch 58 batch 20 train Loss 176.9945 test Loss 73.2939 with MSE metric 43441.6868\n",
      "Epoch 58 batch 30 train Loss 176.8771 test Loss 73.2492 with MSE metric 43438.0696\n",
      "Epoch 58 batch 40 train Loss 176.7599 test Loss 73.2045 with MSE metric 43434.4981\n",
      "Epoch 58 batch 50 train Loss 176.6429 test Loss 73.1598 with MSE metric 43430.8217\n",
      "Epoch 58 batch 60 train Loss 176.5259 test Loss 73.1152 with MSE metric 43427.1369\n",
      "Epoch 58 batch 70 train Loss 176.4092 test Loss 73.0707 with MSE metric 43423.1432\n",
      "Epoch 58 batch 80 train Loss 176.2926 test Loss 73.0262 with MSE metric 43419.4013\n",
      "Epoch 58 batch 90 train Loss 176.1762 test Loss 72.9818 with MSE metric 43415.6381\n",
      "Epoch 58 batch 100 train Loss 176.0599 test Loss 72.9374 with MSE metric 43411.6572\n",
      "Epoch 58 batch 110 train Loss 175.9438 test Loss 72.8931 with MSE metric 43407.5564\n",
      "Epoch 58 batch 120 train Loss 175.8279 test Loss 72.8489 with MSE metric 43403.4464\n",
      "Epoch 58 batch 130 train Loss 175.7121 test Loss 72.8047 with MSE metric 43399.3165\n",
      "Epoch 58 batch 140 train Loss 175.5964 test Loss 72.7606 with MSE metric 43395.0380\n",
      "Epoch 58 batch 150 train Loss 175.4809 test Loss 72.7165 with MSE metric 43391.5018\n",
      "Epoch 58 batch 160 train Loss 175.3656 test Loss 72.6725 with MSE metric 43387.9000\n",
      "Epoch 58 batch 170 train Loss 175.2505 test Loss 72.6285 with MSE metric 43384.2617\n",
      "Epoch 58 batch 180 train Loss 175.1355 test Loss 72.5847 with MSE metric 43380.1016\n",
      "Epoch 58 batch 190 train Loss 175.0207 test Loss 72.5408 with MSE metric 43376.5708\n",
      "Epoch 58 batch 200 train Loss 174.9060 test Loss 72.4971 with MSE metric 43372.5858\n",
      "Epoch 58 batch 210 train Loss 174.7914 test Loss 72.4534 with MSE metric 43368.7030\n",
      "Epoch 58 batch 220 train Loss 174.6770 test Loss 72.4097 with MSE metric 43364.7422\n",
      "Epoch 58 batch 230 train Loss 174.5628 test Loss 72.3661 with MSE metric 43360.3851\n",
      "Epoch 58 batch 240 train Loss 174.4487 test Loss 72.3225 with MSE metric 43356.1732\n",
      "Time taken for 1 epoch: 28.315624952316284 secs\n",
      "\n",
      "Epoch 59 batch 0 train Loss 174.3347 test Loss 72.2790 with MSE metric 43352.0859\n",
      "Epoch 59 batch 10 train Loss 174.2210 test Loss 72.2355 with MSE metric 43348.5863\n",
      "Epoch 59 batch 20 train Loss 174.1073 test Loss 72.1922 with MSE metric 43344.8289\n",
      "Epoch 59 batch 30 train Loss 173.9939 test Loss 72.1488 with MSE metric 43340.1343\n",
      "Epoch 59 batch 40 train Loss 173.8805 test Loss 72.1055 with MSE metric 43335.9208\n",
      "Epoch 59 batch 50 train Loss 173.7674 test Loss 72.0623 with MSE metric 43331.4245\n",
      "Epoch 59 batch 60 train Loss 173.6543 test Loss 72.0191 with MSE metric 43327.1411\n",
      "Epoch 59 batch 70 train Loss 173.5415 test Loss 71.9760 with MSE metric 43322.9119\n",
      "Epoch 59 batch 80 train Loss 173.4288 test Loss 71.9330 with MSE metric 43318.9039\n",
      "Epoch 59 batch 90 train Loss 173.3162 test Loss 71.8900 with MSE metric 43314.7062\n",
      "Epoch 59 batch 100 train Loss 173.2038 test Loss 71.8470 with MSE metric 43310.7232\n",
      "Epoch 59 batch 110 train Loss 173.0916 test Loss 71.8041 with MSE metric 43306.6079\n",
      "Epoch 59 batch 120 train Loss 172.9794 test Loss 71.7613 with MSE metric 43301.9426\n",
      "Epoch 59 batch 130 train Loss 172.8675 test Loss 71.7185 with MSE metric 43297.9795\n",
      "Epoch 59 batch 140 train Loss 172.7557 test Loss 71.6758 with MSE metric 43293.4728\n",
      "Epoch 59 batch 150 train Loss 172.6440 test Loss 71.6331 with MSE metric 43289.2798\n",
      "Epoch 59 batch 160 train Loss 172.5325 test Loss 71.5905 with MSE metric 43285.0726\n",
      "Epoch 59 batch 170 train Loss 172.4212 test Loss 71.5479 with MSE metric 43280.6683\n",
      "Epoch 59 batch 180 train Loss 172.3099 test Loss 71.5054 with MSE metric 43276.4463\n",
      "Epoch 59 batch 190 train Loss 172.1989 test Loss 71.4630 with MSE metric 43271.5710\n",
      "Epoch 59 batch 200 train Loss 172.0880 test Loss 71.4206 with MSE metric 43267.2065\n",
      "Epoch 59 batch 210 train Loss 171.9772 test Loss 71.3782 with MSE metric 43262.7861\n",
      "Epoch 59 batch 220 train Loss 171.8666 test Loss 71.3359 with MSE metric 43258.6658\n",
      "Epoch 59 batch 230 train Loss 171.7561 test Loss 71.2937 with MSE metric 43253.9281\n",
      "Epoch 59 batch 240 train Loss 171.6458 test Loss 71.2515 with MSE metric 43249.7412\n",
      "Time taken for 1 epoch: 26.753772974014282 secs\n",
      "\n",
      "Epoch 60 batch 0 train Loss 171.5356 test Loss 71.2094 with MSE metric 43244.8727\n",
      "Epoch 60 batch 10 train Loss 171.4255 test Loss 71.1673 with MSE metric 43240.0927\n",
      "Epoch 60 batch 20 train Loss 171.3156 test Loss 71.1253 with MSE metric 43235.8022\n",
      "Epoch 60 batch 30 train Loss 171.2059 test Loss 71.0833 with MSE metric 43231.6479\n",
      "Epoch 60 batch 40 train Loss 171.0963 test Loss 71.0414 with MSE metric 43226.9156\n",
      "Epoch 60 batch 50 train Loss 170.9868 test Loss 70.9996 with MSE metric 43222.1819\n",
      "Epoch 60 batch 60 train Loss 170.8775 test Loss 70.9577 with MSE metric 43217.6738\n",
      "Epoch 60 batch 70 train Loss 170.7683 test Loss 70.9160 with MSE metric 43213.1987\n",
      "Epoch 60 batch 80 train Loss 170.6593 test Loss 70.8743 with MSE metric 43208.1839\n",
      "Epoch 60 batch 90 train Loss 170.5504 test Loss 70.8326 with MSE metric 43203.8348\n",
      "Epoch 60 batch 100 train Loss 170.4417 test Loss 70.7910 with MSE metric 43199.4644\n",
      "Epoch 60 batch 110 train Loss 170.3330 test Loss 70.7495 with MSE metric 43194.8968\n",
      "Epoch 60 batch 120 train Loss 170.2246 test Loss 70.7080 with MSE metric 43189.9520\n",
      "Epoch 60 batch 130 train Loss 170.1163 test Loss 70.6665 with MSE metric 43184.7971\n",
      "Epoch 60 batch 140 train Loss 170.0081 test Loss 70.6252 with MSE metric 43179.9653\n",
      "Epoch 60 batch 150 train Loss 169.9001 test Loss 70.5838 with MSE metric 43174.9743\n",
      "Epoch 60 batch 160 train Loss 169.7922 test Loss 70.5425 with MSE metric 43170.0820\n",
      "Epoch 60 batch 170 train Loss 169.6844 test Loss 70.5013 with MSE metric 43164.9952\n",
      "Epoch 60 batch 180 train Loss 169.5768 test Loss 70.4601 with MSE metric 43160.2009\n",
      "Epoch 60 batch 190 train Loss 169.4694 test Loss 70.4190 with MSE metric 43154.9692\n",
      "Epoch 60 batch 200 train Loss 169.3620 test Loss 70.3779 with MSE metric 43149.8042\n",
      "Epoch 60 batch 210 train Loss 169.2548 test Loss 70.3369 with MSE metric 43144.7235\n",
      "Epoch 60 batch 220 train Loss 169.1478 test Loss 70.2959 with MSE metric 43139.5101\n",
      "Epoch 60 batch 230 train Loss 169.0409 test Loss 70.2549 with MSE metric 43134.4401\n",
      "Epoch 60 batch 240 train Loss 168.9341 test Loss 70.2141 with MSE metric 43129.3470\n",
      "Time taken for 1 epoch: 28.824903964996338 secs\n",
      "\n",
      "Epoch 61 batch 0 train Loss 168.8275 test Loss 70.1732 with MSE metric 43124.0327\n",
      "Epoch 61 batch 10 train Loss 168.7210 test Loss 70.1325 with MSE metric 43119.0453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 batch 20 train Loss 168.6146 test Loss 70.0917 with MSE metric 43113.8015\n",
      "Epoch 61 batch 30 train Loss 168.5084 test Loss 70.0511 with MSE metric 43108.8442\n",
      "Epoch 61 batch 40 train Loss 168.4024 test Loss 70.0104 with MSE metric 43103.4229\n",
      "Epoch 61 batch 50 train Loss 168.2964 test Loss 69.9699 with MSE metric 43098.3316\n",
      "Epoch 61 batch 60 train Loss 168.1906 test Loss 69.9294 with MSE metric 43092.8549\n",
      "Epoch 61 batch 70 train Loss 168.0850 test Loss 69.8889 with MSE metric 43087.4941\n",
      "Epoch 61 batch 80 train Loss 167.9794 test Loss 69.8485 with MSE metric 43082.2711\n",
      "Epoch 61 batch 90 train Loss 167.8740 test Loss 69.8081 with MSE metric 43076.8959\n",
      "Epoch 61 batch 100 train Loss 167.7688 test Loss 69.7678 with MSE metric 43071.2955\n",
      "Epoch 61 batch 110 train Loss 167.6636 test Loss 69.7275 with MSE metric 43065.8990\n",
      "Epoch 61 batch 120 train Loss 167.5587 test Loss 69.6873 with MSE metric 43060.4506\n",
      "Epoch 61 batch 130 train Loss 167.4538 test Loss 69.6471 with MSE metric 43055.0496\n",
      "Epoch 61 batch 140 train Loss 167.3491 test Loss 69.6070 with MSE metric 43049.7267\n",
      "Epoch 61 batch 150 train Loss 167.2445 test Loss 69.5669 with MSE metric 43044.6137\n",
      "Epoch 61 batch 160 train Loss 167.1401 test Loss 69.5269 with MSE metric 43038.9544\n",
      "Epoch 61 batch 170 train Loss 167.0358 test Loss 69.4869 with MSE metric 43033.3742\n",
      "Epoch 61 batch 180 train Loss 166.9316 test Loss 69.4470 with MSE metric 43028.0918\n",
      "Epoch 61 batch 190 train Loss 166.8275 test Loss 69.4071 with MSE metric 43022.6740\n",
      "Epoch 61 batch 200 train Loss 166.7236 test Loss 69.3673 with MSE metric 43016.9091\n",
      "Epoch 61 batch 210 train Loss 166.6199 test Loss 69.3275 with MSE metric 43011.4119\n",
      "Epoch 61 batch 220 train Loss 166.5162 test Loss 69.2878 with MSE metric 43005.9976\n",
      "Epoch 61 batch 230 train Loss 166.4127 test Loss 69.2481 with MSE metric 43000.5982\n",
      "Epoch 61 batch 240 train Loss 166.3093 test Loss 69.2085 with MSE metric 42995.1483\n",
      "Time taken for 1 epoch: 28.834238290786743 secs\n",
      "\n",
      "Epoch 62 batch 0 train Loss 166.2061 test Loss 69.1689 with MSE metric 42989.7156\n",
      "Epoch 62 batch 10 train Loss 166.1030 test Loss 69.1294 with MSE metric 42984.0361\n",
      "Epoch 62 batch 20 train Loss 166.0000 test Loss 69.0899 with MSE metric 42978.4185\n",
      "Epoch 62 batch 30 train Loss 165.8972 test Loss 69.0505 with MSE metric 42972.7200\n",
      "Epoch 62 batch 40 train Loss 165.7945 test Loss 69.0111 with MSE metric 42967.1405\n",
      "Epoch 62 batch 50 train Loss 165.6919 test Loss 68.9717 with MSE metric 42961.6433\n",
      "Epoch 62 batch 60 train Loss 165.5894 test Loss 68.9324 with MSE metric 42955.9225\n",
      "Epoch 62 batch 70 train Loss 165.4871 test Loss 68.8932 with MSE metric 42949.9089\n",
      "Epoch 62 batch 80 train Loss 165.3849 test Loss 68.8540 with MSE metric 42944.0122\n",
      "Epoch 62 batch 90 train Loss 165.2828 test Loss 68.8148 with MSE metric 42938.6614\n",
      "Epoch 62 batch 100 train Loss 165.1809 test Loss 68.7757 with MSE metric 42932.7897\n",
      "Epoch 62 batch 110 train Loss 165.0791 test Loss 68.7367 with MSE metric 42926.6772\n",
      "Epoch 62 batch 120 train Loss 164.9774 test Loss 68.6976 with MSE metric 42921.0952\n",
      "Epoch 62 batch 130 train Loss 164.8759 test Loss 68.6587 with MSE metric 42915.4066\n",
      "Epoch 62 batch 140 train Loss 164.7744 test Loss 68.6198 with MSE metric 42909.7451\n",
      "Epoch 62 batch 150 train Loss 164.6732 test Loss 68.5809 with MSE metric 42903.6422\n",
      "Epoch 62 batch 160 train Loss 164.5720 test Loss 68.5421 with MSE metric 42897.2911\n",
      "Epoch 62 batch 170 train Loss 164.4710 test Loss 68.5033 with MSE metric 42891.7323\n",
      "Epoch 62 batch 180 train Loss 164.3700 test Loss 68.4646 with MSE metric 42885.2592\n",
      "Epoch 62 batch 190 train Loss 164.2693 test Loss 68.4259 with MSE metric 42879.2374\n",
      "Epoch 62 batch 200 train Loss 164.1686 test Loss 68.3873 with MSE metric 42873.1244\n",
      "Epoch 62 batch 210 train Loss 164.0681 test Loss 68.3487 with MSE metric 42866.5745\n",
      "Epoch 62 batch 220 train Loss 163.9677 test Loss 68.3101 with MSE metric 42860.1648\n",
      "Epoch 62 batch 230 train Loss 163.8674 test Loss 68.2716 with MSE metric 42853.6989\n",
      "Epoch 62 batch 240 train Loss 163.7673 test Loss 68.2332 with MSE metric 42847.3848\n",
      "Time taken for 1 epoch: 28.10753607749939 secs\n",
      "\n",
      "Epoch 63 batch 0 train Loss 163.6672 test Loss 68.1948 with MSE metric 42840.9608\n",
      "Epoch 63 batch 10 train Loss 163.5673 test Loss 68.1564 with MSE metric 42834.5325\n",
      "Epoch 63 batch 20 train Loss 163.4676 test Loss 68.1181 with MSE metric 42828.0451\n",
      "Epoch 63 batch 30 train Loss 163.3679 test Loss 68.0798 with MSE metric 42821.6197\n",
      "Epoch 63 batch 40 train Loss 163.2684 test Loss 68.0416 with MSE metric 42815.6070\n",
      "Epoch 63 batch 50 train Loss 163.1690 test Loss 68.0034 with MSE metric 42809.5196\n",
      "Epoch 63 batch 60 train Loss 163.0698 test Loss 67.9653 with MSE metric 42803.4461\n",
      "Epoch 63 batch 70 train Loss 162.9706 test Loss 67.9272 with MSE metric 42797.0054\n",
      "Epoch 63 batch 80 train Loss 162.8716 test Loss 67.8892 with MSE metric 42790.3928\n",
      "Epoch 63 batch 90 train Loss 162.7727 test Loss 67.8512 with MSE metric 42783.8533\n",
      "Epoch 63 batch 100 train Loss 162.6740 test Loss 67.8133 with MSE metric 42777.3968\n",
      "Epoch 63 batch 110 train Loss 162.5753 test Loss 67.7754 with MSE metric 42770.7631\n",
      "Epoch 63 batch 120 train Loss 162.4768 test Loss 67.7375 with MSE metric 42764.2400\n",
      "Epoch 63 batch 130 train Loss 162.3784 test Loss 67.6997 with MSE metric 42757.6464\n",
      "Epoch 63 batch 140 train Loss 162.2801 test Loss 67.6619 with MSE metric 42750.9514\n",
      "Epoch 63 batch 150 train Loss 162.1819 test Loss 67.6242 with MSE metric 42744.1505\n",
      "Epoch 63 batch 160 train Loss 162.0839 test Loss 67.5865 with MSE metric 42737.6017\n",
      "Epoch 63 batch 170 train Loss 161.9860 test Loss 67.5489 with MSE metric 42730.8763\n",
      "Epoch 63 batch 180 train Loss 161.8882 test Loss 67.5113 with MSE metric 42724.1452\n",
      "Epoch 63 batch 190 train Loss 161.7905 test Loss 67.4737 with MSE metric 42717.5548\n",
      "Epoch 63 batch 200 train Loss 161.6930 test Loss 67.4362 with MSE metric 42710.5049\n",
      "Epoch 63 batch 210 train Loss 161.5956 test Loss 67.3988 with MSE metric 42703.4320\n",
      "Epoch 63 batch 220 train Loss 161.4982 test Loss 67.3613 with MSE metric 42696.4244\n",
      "Epoch 63 batch 230 train Loss 161.4011 test Loss 67.3240 with MSE metric 42689.8492\n",
      "Epoch 63 batch 240 train Loss 161.3040 test Loss 67.2866 with MSE metric 42683.0080\n",
      "Time taken for 1 epoch: 27.69277310371399 secs\n",
      "\n",
      "Epoch 64 batch 0 train Loss 161.2071 test Loss 67.2494 with MSE metric 42676.1277\n",
      "Epoch 64 batch 10 train Loss 161.1103 test Loss 67.2121 with MSE metric 42669.2647\n",
      "Epoch 64 batch 20 train Loss 161.0136 test Loss 67.1750 with MSE metric 42662.1516\n",
      "Epoch 64 batch 30 train Loss 160.9170 test Loss 67.1378 with MSE metric 42654.8948\n",
      "Epoch 64 batch 40 train Loss 160.8205 test Loss 67.1007 with MSE metric 42647.5283\n",
      "Epoch 64 batch 50 train Loss 160.7241 test Loss 67.0636 with MSE metric 42640.3071\n",
      "Epoch 64 batch 60 train Loss 160.6279 test Loss 67.0266 with MSE metric 42633.2090\n",
      "Epoch 64 batch 70 train Loss 160.5318 test Loss 66.9896 with MSE metric 42626.0112\n",
      "Epoch 64 batch 80 train Loss 160.4358 test Loss 66.9527 with MSE metric 42619.0219\n",
      "Epoch 64 batch 90 train Loss 160.3400 test Loss 66.9158 with MSE metric 42612.0332\n",
      "Epoch 64 batch 100 train Loss 160.2442 test Loss 66.8790 with MSE metric 42605.0939\n",
      "Epoch 64 batch 110 train Loss 160.1486 test Loss 66.8422 with MSE metric 42597.7137\n",
      "Epoch 64 batch 120 train Loss 160.0531 test Loss 66.8055 with MSE metric 42590.5262\n",
      "Epoch 64 batch 130 train Loss 159.9577 test Loss 66.7687 with MSE metric 42583.0483\n",
      "Epoch 64 batch 140 train Loss 159.8624 test Loss 66.7321 with MSE metric 42575.6664\n",
      "Epoch 64 batch 150 train Loss 159.7672 test Loss 66.6954 with MSE metric 42568.4315\n",
      "Epoch 64 batch 160 train Loss 159.6722 test Loss 66.6589 with MSE metric 42560.9661\n",
      "Epoch 64 batch 170 train Loss 159.5772 test Loss 66.6223 with MSE metric 42553.5855\n",
      "Epoch 64 batch 180 train Loss 159.4824 test Loss 66.5858 with MSE metric 42546.2355\n",
      "Epoch 64 batch 190 train Loss 159.3877 test Loss 66.5494 with MSE metric 42538.5223\n",
      "Epoch 64 batch 200 train Loss 159.2932 test Loss 66.5130 with MSE metric 42531.0735\n",
      "Epoch 64 batch 210 train Loss 159.1987 test Loss 66.4766 with MSE metric 42523.5069\n",
      "Epoch 64 batch 220 train Loss 159.1043 test Loss 66.4403 with MSE metric 42515.9933\n",
      "Epoch 64 batch 230 train Loss 159.0101 test Loss 66.4040 with MSE metric 42508.4692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 batch 240 train Loss 158.9160 test Loss 66.3678 with MSE metric 42500.7959\n",
      "Time taken for 1 epoch: 33.85796880722046 secs\n",
      "\n",
      "Epoch 65 batch 0 train Loss 158.8220 test Loss 66.3316 with MSE metric 42493.1513\n",
      "Epoch 65 batch 10 train Loss 158.7281 test Loss 66.2954 with MSE metric 42485.4820\n",
      "Epoch 65 batch 20 train Loss 158.6344 test Loss 66.2593 with MSE metric 42477.7437\n",
      "Epoch 65 batch 30 train Loss 158.5407 test Loss 66.2233 with MSE metric 42469.8829\n",
      "Epoch 65 batch 40 train Loss 158.4471 test Loss 66.1873 with MSE metric 42462.1801\n",
      "Epoch 65 batch 50 train Loss 158.3537 test Loss 66.1513 with MSE metric 42454.3806\n",
      "Epoch 65 batch 60 train Loss 158.2604 test Loss 66.1154 with MSE metric 42446.5080\n",
      "Epoch 65 batch 70 train Loss 158.1672 test Loss 66.0795 with MSE metric 42438.6307\n",
      "Epoch 65 batch 80 train Loss 158.0741 test Loss 66.0436 with MSE metric 42430.9167\n",
      "Epoch 65 batch 90 train Loss 157.9811 test Loss 66.0078 with MSE metric 42422.9520\n",
      "Epoch 65 batch 100 train Loss 157.8882 test Loss 65.9720 with MSE metric 42414.8458\n",
      "Epoch 65 batch 110 train Loss 157.7955 test Loss 65.9363 with MSE metric 42406.8340\n",
      "Epoch 65 batch 120 train Loss 157.7028 test Loss 65.9006 with MSE metric 42398.8270\n",
      "Epoch 65 batch 130 train Loss 157.6103 test Loss 65.8650 with MSE metric 42390.8602\n",
      "Epoch 65 batch 140 train Loss 157.5179 test Loss 65.8294 with MSE metric 42382.7277\n",
      "Epoch 65 batch 150 train Loss 157.4256 test Loss 65.7938 with MSE metric 42374.4895\n",
      "Epoch 65 batch 160 train Loss 157.3334 test Loss 65.7583 with MSE metric 42366.3895\n",
      "Epoch 65 batch 170 train Loss 157.2413 test Loss 65.7228 with MSE metric 42358.2427\n",
      "Epoch 65 batch 180 train Loss 157.1493 test Loss 65.6874 with MSE metric 42350.0857\n",
      "Epoch 65 batch 190 train Loss 157.0574 test Loss 65.6520 with MSE metric 42341.9305\n",
      "Epoch 65 batch 200 train Loss 156.9657 test Loss 65.6166 with MSE metric 42333.6246\n",
      "Epoch 65 batch 210 train Loss 156.8740 test Loss 65.5813 with MSE metric 42325.3072\n",
      "Epoch 65 batch 220 train Loss 156.7825 test Loss 65.5460 with MSE metric 42317.0713\n",
      "Epoch 65 batch 230 train Loss 156.6911 test Loss 65.5108 with MSE metric 42308.8391\n",
      "Epoch 65 batch 240 train Loss 156.5998 test Loss 65.4756 with MSE metric 42300.5058\n",
      "Time taken for 1 epoch: 28.668142080307007 secs\n",
      "\n",
      "Epoch 66 batch 0 train Loss 156.5086 test Loss 65.4404 with MSE metric 42292.1275\n",
      "Epoch 66 batch 10 train Loss 156.4175 test Loss 65.4053 with MSE metric 42283.6773\n",
      "Epoch 66 batch 20 train Loss 156.3265 test Loss 65.3702 with MSE metric 42275.2465\n",
      "Epoch 66 batch 30 train Loss 156.2356 test Loss 65.3352 with MSE metric 42266.7583\n",
      "Epoch 66 batch 40 train Loss 156.1449 test Loss 65.3002 with MSE metric 42258.2951\n",
      "Epoch 66 batch 50 train Loss 156.0542 test Loss 65.2652 with MSE metric 42249.6271\n",
      "Epoch 66 batch 60 train Loss 155.9636 test Loss 65.2303 with MSE metric 42241.0425\n",
      "Epoch 66 batch 70 train Loss 155.8732 test Loss 65.1954 with MSE metric 42232.4509\n",
      "Epoch 66 batch 80 train Loss 155.7828 test Loss 65.1606 with MSE metric 42223.7382\n",
      "Epoch 66 batch 90 train Loss 155.6926 test Loss 65.1258 with MSE metric 42215.0234\n",
      "Epoch 66 batch 100 train Loss 155.6025 test Loss 65.0910 with MSE metric 42206.4154\n",
      "Epoch 66 batch 110 train Loss 155.5125 test Loss 65.0563 with MSE metric 42197.6414\n",
      "Epoch 66 batch 120 train Loss 155.4226 test Loss 65.0216 with MSE metric 42188.9902\n",
      "Epoch 66 batch 130 train Loss 155.3328 test Loss 64.9870 with MSE metric 42180.1554\n",
      "Epoch 66 batch 140 train Loss 155.2431 test Loss 64.9524 with MSE metric 42171.4424\n",
      "Epoch 66 batch 150 train Loss 155.1535 test Loss 64.9178 with MSE metric 42162.6111\n",
      "Epoch 66 batch 160 train Loss 155.0640 test Loss 64.8833 with MSE metric 42153.7066\n",
      "Epoch 66 batch 170 train Loss 154.9747 test Loss 64.8488 with MSE metric 42144.7720\n",
      "Epoch 66 batch 180 train Loss 154.8854 test Loss 64.8143 with MSE metric 42135.8302\n",
      "Epoch 66 batch 190 train Loss 154.7962 test Loss 64.7799 with MSE metric 42126.8900\n",
      "Epoch 66 batch 200 train Loss 154.7072 test Loss 64.7456 with MSE metric 42117.9413\n",
      "Epoch 66 batch 210 train Loss 154.6182 test Loss 64.7112 with MSE metric 42108.9688\n",
      "Epoch 66 batch 220 train Loss 154.5294 test Loss 64.6770 with MSE metric 42099.9596\n",
      "Epoch 66 batch 230 train Loss 154.4407 test Loss 64.6427 with MSE metric 42090.8101\n",
      "Epoch 66 batch 240 train Loss 154.3520 test Loss 64.6085 with MSE metric 42081.7809\n",
      "Time taken for 1 epoch: 27.26377010345459 secs\n",
      "\n",
      "Epoch 67 batch 0 train Loss 154.2635 test Loss 64.5743 with MSE metric 42072.6101\n",
      "Epoch 67 batch 10 train Loss 154.1751 test Loss 64.5402 with MSE metric 42063.5422\n",
      "Epoch 67 batch 20 train Loss 154.0868 test Loss 64.5061 with MSE metric 42054.4073\n",
      "Epoch 67 batch 30 train Loss 153.9986 test Loss 64.4720 with MSE metric 42045.1876\n",
      "Epoch 67 batch 40 train Loss 153.9104 test Loss 64.4380 with MSE metric 42035.9566\n",
      "Epoch 67 batch 50 train Loss 153.8224 test Loss 64.4040 with MSE metric 42026.7144\n",
      "Epoch 67 batch 60 train Loss 153.7345 test Loss 64.3701 with MSE metric 42017.4195\n",
      "Epoch 67 batch 70 train Loss 153.6467 test Loss 64.3362 with MSE metric 42008.0999\n",
      "Epoch 67 batch 80 train Loss 153.5590 test Loss 64.3023 with MSE metric 41998.7790\n",
      "Epoch 67 batch 90 train Loss 153.4715 test Loss 64.2685 with MSE metric 41989.4606\n",
      "Epoch 67 batch 100 train Loss 153.3840 test Loss 64.2347 with MSE metric 41980.1195\n",
      "Epoch 67 batch 110 train Loss 153.2966 test Loss 64.2009 with MSE metric 41970.7079\n",
      "Epoch 67 batch 120 train Loss 153.2093 test Loss 64.1672 with MSE metric 41961.2605\n",
      "Epoch 67 batch 130 train Loss 153.1221 test Loss 64.1335 with MSE metric 41951.8243\n",
      "Epoch 67 batch 140 train Loss 153.0350 test Loss 64.0999 with MSE metric 41942.3736\n",
      "Epoch 67 batch 150 train Loss 152.9481 test Loss 64.0663 with MSE metric 41932.9023\n",
      "Epoch 67 batch 160 train Loss 152.8612 test Loss 64.0327 with MSE metric 41923.3700\n",
      "Epoch 67 batch 170 train Loss 152.7744 test Loss 63.9992 with MSE metric 41913.7935\n",
      "Epoch 67 batch 180 train Loss 152.6877 test Loss 63.9657 with MSE metric 41904.2384\n",
      "Epoch 67 batch 190 train Loss 152.6012 test Loss 63.9322 with MSE metric 41894.6322\n",
      "Epoch 67 batch 200 train Loss 152.5147 test Loss 63.8988 with MSE metric 41885.0183\n",
      "Epoch 67 batch 210 train Loss 152.4283 test Loss 63.8654 with MSE metric 41875.3895\n",
      "Epoch 67 batch 220 train Loss 152.3421 test Loss 63.8321 with MSE metric 41865.7247\n",
      "Epoch 67 batch 230 train Loss 152.2559 test Loss 63.7988 with MSE metric 41855.9598\n",
      "Epoch 67 batch 240 train Loss 152.1698 test Loss 63.7655 with MSE metric 41846.2157\n",
      "Time taken for 1 epoch: 29.05475616455078 secs\n",
      "\n",
      "Epoch 68 batch 0 train Loss 152.0839 test Loss 63.7323 with MSE metric 41836.4717\n",
      "Epoch 68 batch 10 train Loss 151.9980 test Loss 63.6991 with MSE metric 41826.6757\n",
      "Epoch 68 batch 20 train Loss 151.9122 test Loss 63.6659 with MSE metric 41816.8493\n",
      "Epoch 68 batch 30 train Loss 151.8266 test Loss 63.6328 with MSE metric 41806.9834\n",
      "Epoch 68 batch 40 train Loss 151.7410 test Loss 63.5997 with MSE metric 41797.0685\n",
      "Epoch 68 batch 50 train Loss 151.6555 test Loss 63.5667 with MSE metric 41787.2046\n",
      "Epoch 68 batch 60 train Loss 151.5702 test Loss 63.5337 with MSE metric 41777.3605\n",
      "Epoch 68 batch 70 train Loss 151.4849 test Loss 63.5007 with MSE metric 41767.4110\n",
      "Epoch 68 batch 80 train Loss 151.3997 test Loss 63.4678 with MSE metric 41757.4665\n",
      "Epoch 68 batch 90 train Loss 151.3146 test Loss 63.4349 with MSE metric 41747.5424\n",
      "Epoch 68 batch 100 train Loss 151.2297 test Loss 63.4020 with MSE metric 41737.5320\n",
      "Epoch 68 batch 110 train Loss 151.1448 test Loss 63.3692 with MSE metric 41727.5032\n",
      "Epoch 68 batch 120 train Loss 151.0600 test Loss 63.3364 with MSE metric 41717.5139\n",
      "Epoch 68 batch 130 train Loss 150.9753 test Loss 63.3036 with MSE metric 41707.4381\n",
      "Epoch 68 batch 140 train Loss 150.8908 test Loss 63.2709 with MSE metric 41697.3176\n",
      "Epoch 68 batch 150 train Loss 150.8063 test Loss 63.2382 with MSE metric 41687.1536\n",
      "Epoch 68 batch 160 train Loss 150.7219 test Loss 63.2056 with MSE metric 41676.9816\n",
      "Epoch 68 batch 170 train Loss 150.6376 test Loss 63.1729 with MSE metric 41666.8268\n",
      "Epoch 68 batch 180 train Loss 150.5534 test Loss 63.1404 with MSE metric 41656.6228\n",
      "Epoch 68 batch 190 train Loss 150.4693 test Loss 63.1078 with MSE metric 41646.3712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 batch 200 train Loss 150.3853 test Loss 63.0753 with MSE metric 41636.1150\n",
      "Epoch 68 batch 210 train Loss 150.3014 test Loss 63.0428 with MSE metric 41625.8660\n",
      "Epoch 68 batch 220 train Loss 150.2176 test Loss 63.0104 with MSE metric 41615.6253\n",
      "Epoch 68 batch 230 train Loss 150.1339 test Loss 62.9780 with MSE metric 41605.3965\n",
      "Epoch 68 batch 240 train Loss 150.0503 test Loss 62.9456 with MSE metric 41595.0589\n",
      "Time taken for 1 epoch: 30.368643045425415 secs\n",
      "\n",
      "Epoch 69 batch 0 train Loss 149.9668 test Loss 62.9133 with MSE metric 41584.6989\n",
      "Epoch 69 batch 10 train Loss 149.8834 test Loss 62.8810 with MSE metric 41574.3208\n",
      "Epoch 69 batch 20 train Loss 149.8000 test Loss 62.8488 with MSE metric 41563.9285\n",
      "Epoch 69 batch 30 train Loss 149.7168 test Loss 62.8166 with MSE metric 41553.4348\n",
      "Epoch 69 batch 40 train Loss 149.6337 test Loss 62.7844 with MSE metric 41543.0765\n",
      "Epoch 69 batch 50 train Loss 149.5506 test Loss 62.7522 with MSE metric 41532.6208\n",
      "Epoch 69 batch 60 train Loss 149.4677 test Loss 62.7201 with MSE metric 41522.2116\n",
      "Epoch 69 batch 70 train Loss 149.3849 test Loss 62.6881 with MSE metric 41511.7367\n",
      "Epoch 69 batch 80 train Loss 149.3021 test Loss 62.6560 with MSE metric 41501.3991\n",
      "Epoch 69 batch 90 train Loss 149.2195 test Loss 62.6240 with MSE metric 41490.8022\n",
      "Epoch 69 batch 100 train Loss 149.1369 test Loss 62.5920 with MSE metric 41480.3564\n",
      "Epoch 69 batch 110 train Loss 149.0544 test Loss 62.5601 with MSE metric 41469.7668\n",
      "Epoch 69 batch 120 train Loss 148.9720 test Loss 62.5282 with MSE metric 41459.1455\n",
      "Epoch 69 batch 130 train Loss 148.8898 test Loss 62.4963 with MSE metric 41448.5424\n",
      "Epoch 69 batch 140 train Loss 148.8076 test Loss 62.4645 with MSE metric 41438.0274\n",
      "Epoch 69 batch 150 train Loss 148.7255 test Loss 62.4327 with MSE metric 41427.4789\n",
      "Epoch 69 batch 160 train Loss 148.6435 test Loss 62.4009 with MSE metric 41416.6827\n",
      "Epoch 69 batch 170 train Loss 148.5616 test Loss 62.3692 with MSE metric 41406.2326\n",
      "Epoch 69 batch 180 train Loss 148.4798 test Loss 62.3375 with MSE metric 41395.5808\n",
      "Epoch 69 batch 190 train Loss 148.3980 test Loss 62.3058 with MSE metric 41384.9308\n",
      "Epoch 69 batch 200 train Loss 148.3164 test Loss 62.2742 with MSE metric 41374.2180\n",
      "Epoch 69 batch 210 train Loss 148.2349 test Loss 62.2426 with MSE metric 41363.3264\n",
      "Epoch 69 batch 220 train Loss 148.1534 test Loss 62.2110 with MSE metric 41352.5513\n",
      "Epoch 69 batch 230 train Loss 148.0721 test Loss 62.1795 with MSE metric 41341.7810\n",
      "Epoch 69 batch 240 train Loss 147.9908 test Loss 62.1480 with MSE metric 41330.9080\n",
      "Time taken for 1 epoch: 30.47159695625305 secs\n",
      "\n",
      "Epoch 70 batch 0 train Loss 147.9097 test Loss 62.1165 with MSE metric 41320.3583\n",
      "Epoch 70 batch 10 train Loss 147.8286 test Loss 62.0851 with MSE metric 41309.5206\n",
      "Epoch 70 batch 20 train Loss 147.7476 test Loss 62.0537 with MSE metric 41298.7273\n",
      "Epoch 70 batch 30 train Loss 147.6667 test Loss 62.0224 with MSE metric 41287.8478\n",
      "Epoch 70 batch 40 train Loss 147.5859 test Loss 61.9911 with MSE metric 41277.0249\n",
      "Epoch 70 batch 50 train Loss 147.5052 test Loss 61.9598 with MSE metric 41266.2027\n",
      "Epoch 70 batch 60 train Loss 147.4246 test Loss 61.9285 with MSE metric 41255.3228\n",
      "Epoch 70 batch 70 train Loss 147.3441 test Loss 61.8973 with MSE metric 41244.3834\n",
      "Epoch 70 batch 80 train Loss 147.2636 test Loss 61.8661 with MSE metric 41233.5566\n",
      "Epoch 70 batch 90 train Loss 147.1833 test Loss 61.8350 with MSE metric 41222.6930\n",
      "Epoch 70 batch 100 train Loss 147.1030 test Loss 61.8039 with MSE metric 41211.8326\n",
      "Epoch 70 batch 110 train Loss 147.0229 test Loss 61.7728 with MSE metric 41200.7581\n",
      "Epoch 70 batch 120 train Loss 146.9428 test Loss 61.7417 with MSE metric 41189.6718\n",
      "Epoch 70 batch 130 train Loss 146.8628 test Loss 61.7107 with MSE metric 41178.7499\n",
      "Epoch 70 batch 140 train Loss 146.7829 test Loss 61.6797 with MSE metric 41167.4896\n",
      "Epoch 70 batch 150 train Loss 146.7031 test Loss 61.6488 with MSE metric 41156.4000\n",
      "Epoch 70 batch 160 train Loss 146.6234 test Loss 61.6179 with MSE metric 41145.3502\n",
      "Epoch 70 batch 170 train Loss 146.5438 test Loss 61.5870 with MSE metric 41134.5399\n",
      "Epoch 70 batch 180 train Loss 146.4642 test Loss 61.5561 with MSE metric 41123.5992\n",
      "Epoch 70 batch 190 train Loss 146.3848 test Loss 61.5253 with MSE metric 41112.6043\n",
      "Epoch 70 batch 200 train Loss 146.3054 test Loss 61.4946 with MSE metric 41101.6976\n",
      "Epoch 70 batch 210 train Loss 146.2262 test Loss 61.4638 with MSE metric 41090.5710\n",
      "Epoch 70 batch 220 train Loss 146.1470 test Loss 61.4331 with MSE metric 41079.5631\n",
      "Epoch 70 batch 230 train Loss 146.0679 test Loss 61.4024 with MSE metric 41068.4162\n",
      "Epoch 70 batch 240 train Loss 145.9889 test Loss 61.3717 with MSE metric 41057.3240\n",
      "Time taken for 1 epoch: 27.16862177848816 secs\n",
      "\n",
      "Epoch 71 batch 0 train Loss 145.9100 test Loss 61.3411 with MSE metric 41045.9344\n",
      "Epoch 71 batch 10 train Loss 145.8312 test Loss 61.3105 with MSE metric 41034.8285\n",
      "Epoch 71 batch 20 train Loss 145.7524 test Loss 61.2799 with MSE metric 41023.6497\n",
      "Epoch 71 batch 30 train Loss 145.6738 test Loss 61.2494 with MSE metric 41012.4526\n",
      "Epoch 71 batch 40 train Loss 145.5952 test Loss 61.2189 with MSE metric 41001.6148\n",
      "Epoch 71 batch 50 train Loss 145.5167 test Loss 61.1885 with MSE metric 40990.4045\n",
      "Epoch 71 batch 60 train Loss 145.4384 test Loss 61.1580 with MSE metric 40979.0357\n",
      "Epoch 71 batch 70 train Loss 145.3601 test Loss 61.1276 with MSE metric 40967.6509\n",
      "Epoch 71 batch 80 train Loss 145.2818 test Loss 61.0973 with MSE metric 40956.6075\n",
      "Epoch 71 batch 90 train Loss 145.2037 test Loss 61.0670 with MSE metric 40945.4023\n",
      "Epoch 71 batch 100 train Loss 145.1257 test Loss 61.0367 with MSE metric 40934.0568\n",
      "Epoch 71 batch 110 train Loss 145.0477 test Loss 61.0064 with MSE metric 40922.7122\n",
      "Epoch 71 batch 120 train Loss 144.9699 test Loss 60.9762 with MSE metric 40911.3208\n",
      "Epoch 71 batch 130 train Loss 144.8921 test Loss 60.9459 with MSE metric 40900.2494\n",
      "Epoch 71 batch 140 train Loss 144.8144 test Loss 60.9158 with MSE metric 40889.1019\n",
      "Epoch 71 batch 150 train Loss 144.7368 test Loss 60.8856 with MSE metric 40877.8842\n",
      "Epoch 71 batch 160 train Loss 144.6592 test Loss 60.8555 with MSE metric 40866.5019\n",
      "Epoch 71 batch 170 train Loss 144.5818 test Loss 60.8254 with MSE metric 40855.1920\n",
      "Epoch 71 batch 180 train Loss 144.5045 test Loss 60.7954 with MSE metric 40843.9633\n",
      "Epoch 71 batch 190 train Loss 144.4272 test Loss 60.7654 with MSE metric 40832.3917\n",
      "Epoch 71 batch 200 train Loss 144.3500 test Loss 60.7354 with MSE metric 40821.0116\n",
      "Epoch 71 batch 210 train Loss 144.2729 test Loss 60.7054 with MSE metric 40809.4363\n",
      "Epoch 71 batch 220 train Loss 144.1959 test Loss 60.6755 with MSE metric 40797.7273\n",
      "Epoch 71 batch 230 train Loss 144.1190 test Loss 60.6456 with MSE metric 40785.9982\n",
      "Epoch 71 batch 240 train Loss 144.0421 test Loss 60.6157 with MSE metric 40774.4469\n",
      "Time taken for 1 epoch: 32.16075277328491 secs\n",
      "\n",
      "Epoch 72 batch 0 train Loss 143.9654 test Loss 60.5859 with MSE metric 40762.5107\n",
      "Epoch 72 batch 10 train Loss 143.8887 test Loss 60.5561 with MSE metric 40750.7718\n",
      "Epoch 72 batch 20 train Loss 143.8121 test Loss 60.5263 with MSE metric 40739.1344\n",
      "Epoch 72 batch 30 train Loss 143.7356 test Loss 60.4966 with MSE metric 40727.5488\n",
      "Epoch 72 batch 40 train Loss 143.6592 test Loss 60.4669 with MSE metric 40715.9193\n",
      "Epoch 72 batch 50 train Loss 143.5828 test Loss 60.4372 with MSE metric 40704.1958\n",
      "Epoch 72 batch 60 train Loss 143.5066 test Loss 60.4076 with MSE metric 40692.4861\n",
      "Epoch 72 batch 70 train Loss 143.4304 test Loss 60.3780 with MSE metric 40680.7769\n",
      "Epoch 72 batch 80 train Loss 143.3543 test Loss 60.3484 with MSE metric 40668.8122\n",
      "Epoch 72 batch 90 train Loss 143.2783 test Loss 60.3188 with MSE metric 40657.0702\n",
      "Epoch 72 batch 100 train Loss 143.2024 test Loss 60.2893 with MSE metric 40645.1719\n",
      "Epoch 72 batch 110 train Loss 143.1265 test Loss 60.2598 with MSE metric 40633.0413\n",
      "Epoch 72 batch 120 train Loss 143.0508 test Loss 60.2304 with MSE metric 40621.0644\n",
      "Epoch 72 batch 130 train Loss 142.9751 test Loss 60.2009 with MSE metric 40608.9495\n",
      "Epoch 72 batch 140 train Loss 142.8995 test Loss 60.1715 with MSE metric 40596.8507\n",
      "Epoch 72 batch 150 train Loss 142.8240 test Loss 60.1422 with MSE metric 40584.8480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 batch 160 train Loss 142.7486 test Loss 60.1128 with MSE metric 40572.6756\n",
      "Epoch 72 batch 170 train Loss 142.6732 test Loss 60.0835 with MSE metric 40560.4983\n",
      "Epoch 72 batch 180 train Loss 142.5979 test Loss 60.0542 with MSE metric 40548.1898\n",
      "Epoch 72 batch 190 train Loss 142.5227 test Loss 60.0250 with MSE metric 40535.7614\n",
      "Epoch 72 batch 200 train Loss 142.4476 test Loss 59.9958 with MSE metric 40523.3154\n",
      "Epoch 72 batch 210 train Loss 142.3726 test Loss 59.9666 with MSE metric 40510.8252\n",
      "Epoch 72 batch 220 train Loss 142.2977 test Loss 59.9374 with MSE metric 40498.3458\n",
      "Epoch 72 batch 230 train Loss 142.2228 test Loss 59.9083 with MSE metric 40485.6728\n",
      "Epoch 72 batch 240 train Loss 142.1480 test Loss 59.8792 with MSE metric 40473.2145\n",
      "Time taken for 1 epoch: 30.630433082580566 secs\n",
      "\n",
      "Epoch 73 batch 0 train Loss 142.0733 test Loss 59.8502 with MSE metric 40460.6855\n",
      "Epoch 73 batch 10 train Loss 141.9987 test Loss 59.8211 with MSE metric 40447.9178\n",
      "Epoch 73 batch 20 train Loss 141.9241 test Loss 59.7921 with MSE metric 40435.1300\n",
      "Epoch 73 batch 30 train Loss 141.8497 test Loss 59.7632 with MSE metric 40422.4521\n",
      "Epoch 73 batch 40 train Loss 141.7753 test Loss 59.7342 with MSE metric 40409.6150\n",
      "Epoch 73 batch 50 train Loss 141.7010 test Loss 59.7053 with MSE metric 40396.6692\n",
      "Epoch 73 batch 60 train Loss 141.6268 test Loss 59.6764 with MSE metric 40383.7299\n",
      "Epoch 73 batch 70 train Loss 141.5526 test Loss 59.6475 with MSE metric 40370.5949\n",
      "Epoch 73 batch 80 train Loss 141.4785 test Loss 59.6187 with MSE metric 40357.6134\n",
      "Epoch 73 batch 90 train Loss 141.4045 test Loss 59.5899 with MSE metric 40344.5376\n",
      "Epoch 73 batch 100 train Loss 141.3306 test Loss 59.5611 with MSE metric 40331.5678\n",
      "Epoch 73 batch 110 train Loss 141.2568 test Loss 59.5323 with MSE metric 40318.5728\n",
      "Epoch 73 batch 120 train Loss 141.1830 test Loss 59.5036 with MSE metric 40305.3252\n",
      "Epoch 73 batch 130 train Loss 141.1094 test Loss 59.4749 with MSE metric 40292.1207\n",
      "Epoch 73 batch 140 train Loss 141.0358 test Loss 59.4463 with MSE metric 40278.9925\n",
      "Epoch 73 batch 150 train Loss 140.9622 test Loss 59.4176 with MSE metric 40265.5833\n",
      "Epoch 73 batch 160 train Loss 140.8888 test Loss 59.3890 with MSE metric 40252.0902\n",
      "Epoch 73 batch 170 train Loss 140.8154 test Loss 59.3604 with MSE metric 40238.6254\n",
      "Epoch 73 batch 180 train Loss 140.7421 test Loss 59.3319 with MSE metric 40224.9918\n",
      "Epoch 73 batch 190 train Loss 140.6689 test Loss 59.3033 with MSE metric 40211.4678\n",
      "Epoch 73 batch 200 train Loss 140.5958 test Loss 59.2748 with MSE metric 40197.8195\n",
      "Epoch 73 batch 210 train Loss 140.5227 test Loss 59.2463 with MSE metric 40184.0814\n",
      "Epoch 73 batch 220 train Loss 140.4497 test Loss 59.2179 with MSE metric 40170.3875\n",
      "Epoch 73 batch 230 train Loss 140.3768 test Loss 59.1894 with MSE metric 40156.4917\n",
      "Epoch 73 batch 240 train Loss 140.3040 test Loss 59.1610 with MSE metric 40142.7588\n",
      "Time taken for 1 epoch: 30.322659015655518 secs\n",
      "\n",
      "Epoch 74 batch 0 train Loss 140.2312 test Loss 59.1326 with MSE metric 40128.9351\n",
      "Epoch 74 batch 10 train Loss 140.1585 test Loss 59.1043 with MSE metric 40114.9478\n",
      "Epoch 74 batch 20 train Loss 140.0860 test Loss 59.0760 with MSE metric 40100.8185\n",
      "Epoch 74 batch 30 train Loss 140.0134 test Loss 59.0477 with MSE metric 40086.9668\n",
      "Epoch 74 batch 40 train Loss 139.9410 test Loss 59.0194 with MSE metric 40072.8706\n",
      "Epoch 74 batch 50 train Loss 139.8686 test Loss 58.9911 with MSE metric 40058.3927\n",
      "Epoch 74 batch 60 train Loss 139.7963 test Loss 58.9629 with MSE metric 40044.1670\n",
      "Epoch 74 batch 70 train Loss 139.7241 test Loss 58.9347 with MSE metric 40029.9479\n",
      "Epoch 74 batch 80 train Loss 139.6520 test Loss 58.9065 with MSE metric 40015.7301\n",
      "Epoch 74 batch 90 train Loss 139.5799 test Loss 58.8784 with MSE metric 40001.2639\n",
      "Epoch 74 batch 100 train Loss 139.5079 test Loss 58.8503 with MSE metric 39986.7239\n",
      "Epoch 74 batch 110 train Loss 139.4360 test Loss 58.8222 with MSE metric 39972.3729\n",
      "Epoch 74 batch 120 train Loss 139.3641 test Loss 58.7941 with MSE metric 39957.7098\n",
      "Epoch 74 batch 130 train Loss 139.2923 test Loss 58.7661 with MSE metric 39943.3174\n",
      "Epoch 74 batch 140 train Loss 139.2207 test Loss 58.7380 with MSE metric 39928.8487\n",
      "Epoch 74 batch 150 train Loss 139.1490 test Loss 58.7101 with MSE metric 39914.5044\n",
      "Epoch 74 batch 160 train Loss 139.0775 test Loss 58.6821 with MSE metric 39900.1642\n",
      "Epoch 74 batch 170 train Loss 139.0060 test Loss 58.6541 with MSE metric 39885.2876\n",
      "Epoch 74 batch 180 train Loss 138.9346 test Loss 58.6262 with MSE metric 39870.5266\n",
      "Epoch 74 batch 190 train Loss 138.8633 test Loss 58.5983 with MSE metric 39855.5212\n",
      "Epoch 74 batch 200 train Loss 138.7920 test Loss 58.5705 with MSE metric 39840.7704\n",
      "Epoch 74 batch 210 train Loss 138.7209 test Loss 58.5426 with MSE metric 39825.8147\n",
      "Epoch 74 batch 220 train Loss 138.6498 test Loss 58.5148 with MSE metric 39811.0064\n",
      "Epoch 74 batch 230 train Loss 138.5787 test Loss 58.4870 with MSE metric 39796.1255\n",
      "Epoch 74 batch 240 train Loss 138.5078 test Loss 58.4593 with MSE metric 39781.4510\n",
      "Time taken for 1 epoch: 28.702443838119507 secs\n",
      "\n",
      "Epoch 75 batch 0 train Loss 138.4369 test Loss 58.4315 with MSE metric 39766.9989\n",
      "Epoch 75 batch 10 train Loss 138.3661 test Loss 58.4038 with MSE metric 39752.1950\n",
      "Epoch 75 batch 20 train Loss 138.2954 test Loss 58.3761 with MSE metric 39737.3770\n",
      "Epoch 75 batch 30 train Loss 138.2247 test Loss 58.3485 with MSE metric 39722.3972\n",
      "Epoch 75 batch 40 train Loss 138.1541 test Loss 58.3208 with MSE metric 39707.3188\n",
      "Epoch 75 batch 50 train Loss 138.0836 test Loss 58.2932 with MSE metric 39692.3229\n",
      "Epoch 75 batch 60 train Loss 138.0132 test Loss 58.2656 with MSE metric 39677.4046\n",
      "Epoch 75 batch 70 train Loss 137.9428 test Loss 58.2381 with MSE metric 39662.5257\n",
      "Epoch 75 batch 80 train Loss 137.8725 test Loss 58.2105 with MSE metric 39647.3674\n",
      "Epoch 75 batch 90 train Loss 137.8023 test Loss 58.1830 with MSE metric 39632.1705\n",
      "Epoch 75 batch 100 train Loss 137.7321 test Loss 58.1555 with MSE metric 39617.4476\n",
      "Epoch 75 batch 110 train Loss 137.6621 test Loss 58.1281 with MSE metric 39602.4476\n",
      "Epoch 75 batch 120 train Loss 137.5921 test Loss 58.1007 with MSE metric 39587.6013\n",
      "Epoch 75 batch 130 train Loss 137.5221 test Loss 58.0733 with MSE metric 39572.5775\n",
      "Epoch 75 batch 140 train Loss 137.4523 test Loss 58.0459 with MSE metric 39557.7535\n",
      "Epoch 75 batch 150 train Loss 137.3825 test Loss 58.0185 with MSE metric 39542.6547\n",
      "Epoch 75 batch 160 train Loss 137.3128 test Loss 57.9913 with MSE metric 39527.4826\n",
      "Epoch 75 batch 170 train Loss 137.2432 test Loss 57.9639 with MSE metric 39512.4983\n",
      "Epoch 75 batch 180 train Loss 137.1736 test Loss 57.9367 with MSE metric 39497.4066\n",
      "Epoch 75 batch 190 train Loss 137.1041 test Loss 57.9094 with MSE metric 39482.2844\n",
      "Epoch 75 batch 200 train Loss 137.0347 test Loss 57.8822 with MSE metric 39467.1012\n",
      "Epoch 75 batch 210 train Loss 136.9653 test Loss 57.8550 with MSE metric 39451.7784\n",
      "Epoch 75 batch 220 train Loss 136.8961 test Loss 57.8278 with MSE metric 39436.4256\n",
      "Epoch 75 batch 230 train Loss 136.8269 test Loss 57.8007 with MSE metric 39421.3102\n",
      "Epoch 75 batch 240 train Loss 136.7577 test Loss 57.7736 with MSE metric 39406.3997\n",
      "Time taken for 1 epoch: 30.58616614341736 secs\n",
      "\n",
      "Epoch 76 batch 0 train Loss 136.6887 test Loss 57.7465 with MSE metric 39391.6844\n",
      "Epoch 76 batch 10 train Loss 136.6197 test Loss 57.7195 with MSE metric 39376.5777\n",
      "Epoch 76 batch 20 train Loss 136.5508 test Loss 57.6924 with MSE metric 39361.5618\n",
      "Epoch 76 batch 30 train Loss 136.4819 test Loss 57.6654 with MSE metric 39346.3575\n",
      "Epoch 76 batch 40 train Loss 136.4132 test Loss 57.6384 with MSE metric 39330.7564\n",
      "Epoch 76 batch 50 train Loss 136.3444 test Loss 57.6115 with MSE metric 39315.4568\n",
      "Epoch 76 batch 60 train Loss 136.2758 test Loss 57.5846 with MSE metric 39300.2733\n",
      "Epoch 76 batch 70 train Loss 136.2073 test Loss 57.5577 with MSE metric 39284.9475\n",
      "Epoch 76 batch 80 train Loss 136.1388 test Loss 57.5308 with MSE metric 39269.9365\n",
      "Epoch 76 batch 90 train Loss 136.0704 test Loss 57.5039 with MSE metric 39255.0298\n",
      "Epoch 76 batch 100 train Loss 136.0020 test Loss 57.4771 with MSE metric 39240.0044\n",
      "Epoch 76 batch 110 train Loss 135.9337 test Loss 57.4503 with MSE metric 39225.0446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 batch 120 train Loss 135.8655 test Loss 57.4235 with MSE metric 39210.0506\n",
      "Epoch 76 batch 130 train Loss 135.7974 test Loss 57.3968 with MSE metric 39194.9978\n",
      "Epoch 76 batch 140 train Loss 135.7293 test Loss 57.3700 with MSE metric 39179.8237\n",
      "Epoch 76 batch 150 train Loss 135.6613 test Loss 57.3434 with MSE metric 39164.6366\n",
      "Epoch 76 batch 160 train Loss 135.5934 test Loss 57.3167 with MSE metric 39149.2184\n",
      "Epoch 76 batch 170 train Loss 135.5255 test Loss 57.2901 with MSE metric 39134.1760\n",
      "Epoch 76 batch 180 train Loss 135.4578 test Loss 57.2635 with MSE metric 39119.2946\n",
      "Epoch 76 batch 190 train Loss 135.3901 test Loss 57.2368 with MSE metric 39104.2474\n",
      "Epoch 76 batch 200 train Loss 135.3224 test Loss 57.2103 with MSE metric 39089.0369\n",
      "Epoch 76 batch 210 train Loss 135.2548 test Loss 57.1837 with MSE metric 39074.3843\n",
      "Epoch 76 batch 220 train Loss 135.1873 test Loss 57.1572 with MSE metric 39059.4687\n",
      "Epoch 76 batch 230 train Loss 135.1199 test Loss 57.1307 with MSE metric 39044.6261\n",
      "Epoch 76 batch 240 train Loss 135.0525 test Loss 57.1043 with MSE metric 39029.3246\n",
      "Time taken for 1 epoch: 28.293371200561523 secs\n",
      "\n",
      "Epoch 77 batch 0 train Loss 134.9852 test Loss 57.0778 with MSE metric 39014.0633\n",
      "Epoch 77 batch 10 train Loss 134.9180 test Loss 57.0514 with MSE metric 38998.9089\n",
      "Epoch 77 batch 20 train Loss 134.8508 test Loss 57.0250 with MSE metric 38983.8725\n",
      "Epoch 77 batch 30 train Loss 134.7837 test Loss 56.9987 with MSE metric 38968.8196\n",
      "Epoch 77 batch 40 train Loss 134.7167 test Loss 56.9723 with MSE metric 38954.1289\n",
      "Epoch 77 batch 50 train Loss 134.6497 test Loss 56.9460 with MSE metric 38939.0289\n",
      "Epoch 77 batch 60 train Loss 134.5829 test Loss 56.9197 with MSE metric 38923.7256\n",
      "Epoch 77 batch 70 train Loss 134.5161 test Loss 56.8935 with MSE metric 38908.7992\n",
      "Epoch 77 batch 80 train Loss 134.4493 test Loss 56.8673 with MSE metric 38893.8241\n",
      "Epoch 77 batch 90 train Loss 134.3826 test Loss 56.8411 with MSE metric 38878.5880\n",
      "Epoch 77 batch 100 train Loss 134.3160 test Loss 56.8149 with MSE metric 38863.6772\n",
      "Epoch 77 batch 110 train Loss 134.2495 test Loss 56.7887 with MSE metric 38848.7453\n",
      "Epoch 77 batch 120 train Loss 134.1830 test Loss 56.7626 with MSE metric 38833.8226\n",
      "Epoch 77 batch 130 train Loss 134.1166 test Loss 56.7365 with MSE metric 38818.8083\n",
      "Epoch 77 batch 140 train Loss 134.0503 test Loss 56.7105 with MSE metric 38803.9108\n",
      "Epoch 77 batch 150 train Loss 133.9840 test Loss 56.6844 with MSE metric 38789.1250\n",
      "Epoch 77 batch 160 train Loss 133.9178 test Loss 56.6584 with MSE metric 38774.3016\n",
      "Epoch 77 batch 170 train Loss 133.8517 test Loss 56.6324 with MSE metric 38759.9262\n",
      "Epoch 77 batch 180 train Loss 133.7856 test Loss 56.6064 with MSE metric 38744.8547\n",
      "Epoch 77 batch 190 train Loss 133.7196 test Loss 56.5805 with MSE metric 38729.6934\n",
      "Epoch 77 batch 200 train Loss 133.6537 test Loss 56.5546 with MSE metric 38714.8364\n",
      "Epoch 77 batch 210 train Loss 133.5878 test Loss 56.5287 with MSE metric 38699.7127\n",
      "Epoch 77 batch 220 train Loss 133.5221 test Loss 56.5028 with MSE metric 38684.6278\n",
      "Epoch 77 batch 230 train Loss 133.4563 test Loss 56.4770 with MSE metric 38670.0957\n",
      "Epoch 77 batch 240 train Loss 133.3907 test Loss 56.4512 with MSE metric 38655.0636\n",
      "Time taken for 1 epoch: 28.490095853805542 secs\n",
      "\n",
      "Epoch 78 batch 0 train Loss 133.3251 test Loss 56.4254 with MSE metric 38640.0711\n",
      "Epoch 78 batch 10 train Loss 133.2595 test Loss 56.3995 with MSE metric 38625.1871\n",
      "Epoch 78 batch 20 train Loss 133.1941 test Loss 56.3738 with MSE metric 38610.4693\n",
      "Epoch 78 batch 30 train Loss 133.1287 test Loss 56.3481 with MSE metric 38595.6391\n",
      "Epoch 78 batch 40 train Loss 133.0634 test Loss 56.3224 with MSE metric 38581.1735\n",
      "Epoch 78 batch 50 train Loss 132.9981 test Loss 56.2967 with MSE metric 38566.2812\n",
      "Epoch 78 batch 60 train Loss 132.9329 test Loss 56.2711 with MSE metric 38551.3894\n",
      "Epoch 78 batch 70 train Loss 132.8678 test Loss 56.2454 with MSE metric 38536.4769\n",
      "Epoch 78 batch 80 train Loss 132.8027 test Loss 56.2198 with MSE metric 38521.3645\n",
      "Epoch 78 batch 90 train Loss 132.7377 test Loss 56.1942 with MSE metric 38506.9022\n",
      "Epoch 78 batch 100 train Loss 132.6728 test Loss 56.1687 with MSE metric 38491.9813\n",
      "Epoch 78 batch 110 train Loss 132.6079 test Loss 56.1432 with MSE metric 38476.9965\n",
      "Epoch 78 batch 120 train Loss 132.5431 test Loss 56.1177 with MSE metric 38462.4756\n",
      "Epoch 78 batch 130 train Loss 132.4784 test Loss 56.0922 with MSE metric 38447.4838\n",
      "Epoch 78 batch 140 train Loss 132.4137 test Loss 56.0668 with MSE metric 38432.4455\n",
      "Epoch 78 batch 150 train Loss 132.3491 test Loss 56.0413 with MSE metric 38417.8679\n",
      "Epoch 78 batch 160 train Loss 132.2846 test Loss 56.0159 with MSE metric 38403.3239\n",
      "Epoch 78 batch 170 train Loss 132.2201 test Loss 55.9905 with MSE metric 38388.5080\n",
      "Epoch 78 batch 180 train Loss 132.1557 test Loss 55.9652 with MSE metric 38373.6801\n",
      "Epoch 78 batch 190 train Loss 132.0913 test Loss 55.9399 with MSE metric 38358.9644\n",
      "Epoch 78 batch 200 train Loss 132.0270 test Loss 55.9146 with MSE metric 38344.4728\n",
      "Epoch 78 batch 210 train Loss 131.9628 test Loss 55.8893 with MSE metric 38329.4662\n",
      "Epoch 78 batch 220 train Loss 131.8987 test Loss 55.8641 with MSE metric 38315.1627\n",
      "Epoch 78 batch 230 train Loss 131.8346 test Loss 55.8388 with MSE metric 38300.3013\n",
      "Epoch 78 batch 240 train Loss 131.7706 test Loss 55.8136 with MSE metric 38285.5020\n",
      "Time taken for 1 epoch: 26.25069499015808 secs\n",
      "\n",
      "Epoch 79 batch 0 train Loss 131.7066 test Loss 55.7884 with MSE metric 38270.9527\n",
      "Epoch 79 batch 10 train Loss 131.6427 test Loss 55.7632 with MSE metric 38256.2198\n",
      "Epoch 79 batch 20 train Loss 131.5789 test Loss 55.7381 with MSE metric 38241.6214\n",
      "Epoch 79 batch 30 train Loss 131.5151 test Loss 55.7130 with MSE metric 38227.0034\n",
      "Epoch 79 batch 40 train Loss 131.4515 test Loss 55.6879 with MSE metric 38212.1441\n",
      "Epoch 79 batch 50 train Loss 131.3878 test Loss 55.6629 with MSE metric 38197.4192\n",
      "Epoch 79 batch 60 train Loss 131.3243 test Loss 55.6378 with MSE metric 38182.6350\n",
      "Epoch 79 batch 70 train Loss 131.2608 test Loss 55.6128 with MSE metric 38168.1558\n",
      "Epoch 79 batch 80 train Loss 131.1973 test Loss 55.5878 with MSE metric 38153.9717\n",
      "Epoch 79 batch 90 train Loss 131.1339 test Loss 55.5628 with MSE metric 38139.5295\n",
      "Epoch 79 batch 100 train Loss 131.0706 test Loss 55.5379 with MSE metric 38125.0231\n",
      "Epoch 79 batch 110 train Loss 131.0074 test Loss 55.5130 with MSE metric 38110.3974\n",
      "Epoch 79 batch 120 train Loss 130.9442 test Loss 55.4881 with MSE metric 38096.2828\n",
      "Epoch 79 batch 130 train Loss 130.8811 test Loss 55.4633 with MSE metric 38081.8592\n",
      "Epoch 79 batch 140 train Loss 130.8180 test Loss 55.4384 with MSE metric 38067.2644\n",
      "Epoch 79 batch 150 train Loss 130.7550 test Loss 55.4136 with MSE metric 38052.9136\n",
      "Epoch 79 batch 160 train Loss 130.6921 test Loss 55.3888 with MSE metric 38038.2927\n",
      "Epoch 79 batch 170 train Loss 130.6292 test Loss 55.3640 with MSE metric 38023.8726\n",
      "Epoch 79 batch 180 train Loss 130.5664 test Loss 55.3393 with MSE metric 38009.6347\n",
      "Epoch 79 batch 190 train Loss 130.5037 test Loss 55.3146 with MSE metric 37995.4391\n",
      "Epoch 79 batch 200 train Loss 130.4410 test Loss 55.2899 with MSE metric 37981.0020\n",
      "Epoch 79 batch 210 train Loss 130.3784 test Loss 55.2652 with MSE metric 37966.5081\n",
      "Epoch 79 batch 220 train Loss 130.3158 test Loss 55.2406 with MSE metric 37952.0925\n",
      "Epoch 79 batch 230 train Loss 130.2533 test Loss 55.2159 with MSE metric 37937.7177\n",
      "Epoch 79 batch 240 train Loss 130.1909 test Loss 55.1913 with MSE metric 37923.3867\n",
      "Time taken for 1 epoch: 28.670654296875 secs\n",
      "\n",
      "Epoch 80 batch 0 train Loss 130.1285 test Loss 55.1667 with MSE metric 37909.2052\n",
      "Epoch 80 batch 10 train Loss 130.0662 test Loss 55.1422 with MSE metric 37895.2866\n",
      "Epoch 80 batch 20 train Loss 130.0040 test Loss 55.1177 with MSE metric 37881.1669\n",
      "Epoch 80 batch 30 train Loss 129.9418 test Loss 55.0931 with MSE metric 37866.9995\n",
      "Epoch 80 batch 40 train Loss 129.8797 test Loss 55.0686 with MSE metric 37852.7332\n",
      "Epoch 80 batch 50 train Loss 129.8176 test Loss 55.0441 with MSE metric 37838.4936\n",
      "Epoch 80 batch 60 train Loss 129.7556 test Loss 55.0197 with MSE metric 37824.1689\n",
      "Epoch 80 batch 70 train Loss 129.6937 test Loss 54.9953 with MSE metric 37809.7514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 batch 80 train Loss 129.6318 test Loss 54.9709 with MSE metric 37795.2310\n",
      "Epoch 80 batch 90 train Loss 129.5700 test Loss 54.9465 with MSE metric 37780.9588\n",
      "Epoch 80 batch 100 train Loss 129.5082 test Loss 54.9222 with MSE metric 37766.7114\n",
      "Epoch 80 batch 110 train Loss 129.4465 test Loss 54.8979 with MSE metric 37752.4802\n",
      "Epoch 80 batch 120 train Loss 129.3849 test Loss 54.8736 with MSE metric 37738.4823\n",
      "Epoch 80 batch 130 train Loss 129.3233 test Loss 54.8493 with MSE metric 37724.2958\n",
      "Epoch 80 batch 140 train Loss 129.2618 test Loss 54.8250 with MSE metric 37710.2895\n",
      "Epoch 80 batch 150 train Loss 129.2004 test Loss 54.8008 with MSE metric 37696.2608\n",
      "Epoch 80 batch 160 train Loss 129.1389 test Loss 54.7765 with MSE metric 37681.8415\n",
      "Epoch 80 batch 170 train Loss 129.0776 test Loss 54.7523 with MSE metric 37667.8520\n",
      "Epoch 80 batch 180 train Loss 129.0164 test Loss 54.7281 with MSE metric 37653.4626\n",
      "Epoch 80 batch 190 train Loss 128.9551 test Loss 54.7040 with MSE metric 37639.1241\n",
      "Epoch 80 batch 200 train Loss 128.8940 test Loss 54.6799 with MSE metric 37624.8232\n",
      "Epoch 80 batch 210 train Loss 128.8329 test Loss 54.6558 with MSE metric 37610.6792\n",
      "Epoch 80 batch 220 train Loss 128.7719 test Loss 54.6317 with MSE metric 37596.4860\n",
      "Epoch 80 batch 230 train Loss 128.7109 test Loss 54.6076 with MSE metric 37582.2471\n",
      "Epoch 80 batch 240 train Loss 128.6501 test Loss 54.5836 with MSE metric 37568.2681\n",
      "Time taken for 1 epoch: 30.585593223571777 secs\n",
      "\n",
      "Epoch 81 batch 0 train Loss 128.5892 test Loss 54.5596 with MSE metric 37554.1452\n",
      "Epoch 81 batch 10 train Loss 128.5284 test Loss 54.5356 with MSE metric 37540.0550\n",
      "Epoch 81 batch 20 train Loss 128.4677 test Loss 54.5117 with MSE metric 37525.9173\n",
      "Epoch 81 batch 30 train Loss 128.4071 test Loss 54.4877 with MSE metric 37512.1257\n",
      "Epoch 81 batch 40 train Loss 128.3465 test Loss 54.4638 with MSE metric 37497.9043\n",
      "Epoch 81 batch 50 train Loss 128.2859 test Loss 54.4399 with MSE metric 37483.8179\n",
      "Epoch 81 batch 60 train Loss 128.2254 test Loss 54.4160 with MSE metric 37470.1824\n",
      "Epoch 81 batch 70 train Loss 128.1650 test Loss 54.3922 with MSE metric 37456.2096\n",
      "Epoch 81 batch 80 train Loss 128.1047 test Loss 54.3684 with MSE metric 37441.9643\n",
      "Epoch 81 batch 90 train Loss 128.0443 test Loss 54.3446 with MSE metric 37427.9230\n",
      "Epoch 81 batch 100 train Loss 127.9841 test Loss 54.3208 with MSE metric 37413.9359\n",
      "Epoch 81 batch 110 train Loss 127.9239 test Loss 54.2970 with MSE metric 37400.2938\n",
      "Epoch 81 batch 120 train Loss 127.8638 test Loss 54.2733 with MSE metric 37386.3343\n",
      "Epoch 81 batch 130 train Loss 127.8037 test Loss 54.2496 with MSE metric 37372.6961\n",
      "Epoch 81 batch 140 train Loss 127.7437 test Loss 54.2259 with MSE metric 37358.7445\n",
      "Epoch 81 batch 150 train Loss 127.6837 test Loss 54.2022 with MSE metric 37344.7822\n",
      "Epoch 81 batch 160 train Loss 127.6238 test Loss 54.1785 with MSE metric 37331.2113\n",
      "Epoch 81 batch 170 train Loss 127.5640 test Loss 54.1549 with MSE metric 37317.5284\n",
      "Epoch 81 batch 180 train Loss 127.5042 test Loss 54.1313 with MSE metric 37303.6246\n",
      "Epoch 81 batch 190 train Loss 127.4445 test Loss 54.1077 with MSE metric 37289.6973\n",
      "Epoch 81 batch 200 train Loss 127.3848 test Loss 54.0841 with MSE metric 37275.7505\n",
      "Epoch 81 batch 210 train Loss 127.3252 test Loss 54.0606 with MSE metric 37262.0182\n",
      "Epoch 81 batch 220 train Loss 127.2657 test Loss 54.0371 with MSE metric 37248.1477\n",
      "Epoch 81 batch 230 train Loss 127.2062 test Loss 54.0136 with MSE metric 37233.9302\n",
      "Epoch 81 batch 240 train Loss 127.1467 test Loss 53.9901 with MSE metric 37220.1932\n",
      "Time taken for 1 epoch: 29.99742603302002 secs\n",
      "\n",
      "Epoch 82 batch 0 train Loss 127.0874 test Loss 53.9666 with MSE metric 37206.2590\n",
      "Epoch 82 batch 10 train Loss 127.0281 test Loss 53.9432 with MSE metric 37192.4407\n",
      "Epoch 82 batch 20 train Loss 126.9688 test Loss 53.9198 with MSE metric 37178.6591\n",
      "Epoch 82 batch 30 train Loss 126.9096 test Loss 53.8964 with MSE metric 37165.0671\n",
      "Epoch 82 batch 40 train Loss 126.8505 test Loss 53.8730 with MSE metric 37151.4464\n",
      "Epoch 82 batch 50 train Loss 126.7914 test Loss 53.8498 with MSE metric 37137.8901\n",
      "Epoch 82 batch 60 train Loss 126.7324 test Loss 53.8264 with MSE metric 37124.0000\n",
      "Epoch 82 batch 70 train Loss 126.6734 test Loss 53.8031 with MSE metric 37110.3543\n",
      "Epoch 82 batch 80 train Loss 126.6145 test Loss 53.7798 with MSE metric 37096.5516\n",
      "Epoch 82 batch 90 train Loss 126.5556 test Loss 53.7566 with MSE metric 37082.6849\n",
      "Epoch 82 batch 100 train Loss 126.4968 test Loss 53.7334 with MSE metric 37069.2253\n",
      "Epoch 82 batch 110 train Loss 126.4381 test Loss 53.7101 with MSE metric 37055.8611\n",
      "Epoch 82 batch 120 train Loss 126.3794 test Loss 53.6870 with MSE metric 37042.5181\n",
      "Epoch 82 batch 130 train Loss 126.3208 test Loss 53.6638 with MSE metric 37028.7034\n",
      "Epoch 82 batch 140 train Loss 126.2622 test Loss 53.6406 with MSE metric 37015.2007\n",
      "Epoch 82 batch 150 train Loss 126.2037 test Loss 53.6175 with MSE metric 37001.6121\n",
      "Epoch 82 batch 160 train Loss 126.1452 test Loss 53.5944 with MSE metric 36987.8973\n",
      "Epoch 82 batch 170 train Loss 126.0868 test Loss 53.5713 with MSE metric 36973.8281\n",
      "Epoch 82 batch 180 train Loss 126.0285 test Loss 53.5482 with MSE metric 36960.3531\n",
      "Epoch 82 batch 190 train Loss 125.9702 test Loss 53.5252 with MSE metric 36946.6795\n",
      "Epoch 82 batch 200 train Loss 125.9119 test Loss 53.5021 with MSE metric 36932.9769\n",
      "Epoch 82 batch 210 train Loss 125.8537 test Loss 53.4792 with MSE metric 36919.2049\n",
      "Epoch 82 batch 220 train Loss 125.7956 test Loss 53.4561 with MSE metric 36905.7147\n",
      "Epoch 82 batch 230 train Loss 125.7375 test Loss 53.4332 with MSE metric 36892.1182\n",
      "Epoch 82 batch 240 train Loss 125.6795 test Loss 53.4103 with MSE metric 36878.5675\n",
      "Time taken for 1 epoch: 26.556370973587036 secs\n",
      "\n",
      "Epoch 83 batch 0 train Loss 125.6216 test Loss 53.3873 with MSE metric 36865.3364\n",
      "Epoch 83 batch 10 train Loss 125.5637 test Loss 53.3644 with MSE metric 36851.6711\n",
      "Epoch 83 batch 20 train Loss 125.5058 test Loss 53.3416 with MSE metric 36838.3566\n",
      "Epoch 83 batch 30 train Loss 125.4480 test Loss 53.3187 with MSE metric 36824.6944\n",
      "Epoch 83 batch 40 train Loss 125.3903 test Loss 53.2959 with MSE metric 36811.2905\n",
      "Epoch 83 batch 50 train Loss 125.3326 test Loss 53.2731 with MSE metric 36797.8951\n",
      "Epoch 83 batch 60 train Loss 125.2750 test Loss 53.2503 with MSE metric 36784.4124\n",
      "Epoch 83 batch 70 train Loss 125.2174 test Loss 53.2275 with MSE metric 36770.9667\n",
      "Epoch 83 batch 80 train Loss 125.1599 test Loss 53.2048 with MSE metric 36757.4183\n",
      "Epoch 83 batch 90 train Loss 125.1025 test Loss 53.1821 with MSE metric 36744.0589\n",
      "Epoch 83 batch 100 train Loss 125.0450 test Loss 53.1593 with MSE metric 36730.5977\n",
      "Epoch 83 batch 110 train Loss 124.9877 test Loss 53.1366 with MSE metric 36717.0665\n",
      "Epoch 83 batch 120 train Loss 124.9304 test Loss 53.1140 with MSE metric 36703.7982\n",
      "Epoch 83 batch 130 train Loss 124.8732 test Loss 53.0913 with MSE metric 36690.6089\n",
      "Epoch 83 batch 140 train Loss 124.8160 test Loss 53.0687 with MSE metric 36677.1981\n",
      "Epoch 83 batch 150 train Loss 124.7589 test Loss 53.0461 with MSE metric 36663.7738\n",
      "Epoch 83 batch 160 train Loss 124.7018 test Loss 53.0235 with MSE metric 36650.1199\n",
      "Epoch 83 batch 170 train Loss 124.6448 test Loss 53.0010 with MSE metric 36636.7852\n",
      "Epoch 83 batch 180 train Loss 124.5878 test Loss 52.9784 with MSE metric 36623.4023\n",
      "Epoch 83 batch 190 train Loss 124.5309 test Loss 52.9559 with MSE metric 36609.9750\n",
      "Epoch 83 batch 200 train Loss 124.4740 test Loss 52.9334 with MSE metric 36596.7163\n",
      "Epoch 83 batch 210 train Loss 124.4173 test Loss 52.9110 with MSE metric 36583.3592\n",
      "Epoch 83 batch 220 train Loss 124.3605 test Loss 52.8885 with MSE metric 36570.2660\n",
      "Epoch 83 batch 230 train Loss 124.3038 test Loss 52.8661 with MSE metric 36557.3109\n",
      "Epoch 83 batch 240 train Loss 124.2472 test Loss 52.8436 with MSE metric 36544.0717\n",
      "Time taken for 1 epoch: 27.831175327301025 secs\n",
      "\n",
      "Epoch 84 batch 0 train Loss 124.1906 test Loss 52.8212 with MSE metric 36530.7556\n",
      "Epoch 84 batch 10 train Loss 124.1341 test Loss 52.7989 with MSE metric 36517.5496\n",
      "Epoch 84 batch 20 train Loss 124.0776 test Loss 52.7765 with MSE metric 36504.4115\n",
      "Epoch 84 batch 30 train Loss 124.0212 test Loss 52.7542 with MSE metric 36491.2317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 batch 40 train Loss 123.9648 test Loss 52.7319 with MSE metric 36478.2039\n",
      "Epoch 84 batch 50 train Loss 123.9085 test Loss 52.7096 with MSE metric 36464.8137\n",
      "Epoch 84 batch 60 train Loss 123.8522 test Loss 52.6873 with MSE metric 36451.4320\n",
      "Epoch 84 batch 70 train Loss 123.7960 test Loss 52.6650 with MSE metric 36438.3814\n",
      "Epoch 84 batch 80 train Loss 123.7398 test Loss 52.6428 with MSE metric 36425.4428\n",
      "Epoch 84 batch 90 train Loss 123.6837 test Loss 52.6206 with MSE metric 36412.3501\n",
      "Epoch 84 batch 100 train Loss 123.6277 test Loss 52.5984 with MSE metric 36399.2930\n",
      "Epoch 84 batch 110 train Loss 123.5717 test Loss 52.5762 with MSE metric 36386.0748\n",
      "Epoch 84 batch 120 train Loss 123.5157 test Loss 52.5540 with MSE metric 36372.9495\n",
      "Epoch 84 batch 130 train Loss 123.4598 test Loss 52.5319 with MSE metric 36360.0178\n",
      "Epoch 84 batch 140 train Loss 123.4040 test Loss 52.5098 with MSE metric 36346.8503\n",
      "Epoch 84 batch 150 train Loss 123.3482 test Loss 52.4877 with MSE metric 36333.8904\n",
      "Epoch 84 batch 160 train Loss 123.2924 test Loss 52.4656 with MSE metric 36320.8083\n",
      "Epoch 84 batch 170 train Loss 123.2367 test Loss 52.4436 with MSE metric 36307.6770\n",
      "Epoch 84 batch 180 train Loss 123.1811 test Loss 52.4215 with MSE metric 36294.8356\n",
      "Epoch 84 batch 190 train Loss 123.1255 test Loss 52.3995 with MSE metric 36281.8847\n",
      "Epoch 84 batch 200 train Loss 123.0700 test Loss 52.3776 with MSE metric 36268.7868\n",
      "Epoch 84 batch 210 train Loss 123.0145 test Loss 52.3556 with MSE metric 36255.8645\n",
      "Epoch 84 batch 220 train Loss 122.9591 test Loss 52.3337 with MSE metric 36243.3213\n",
      "Epoch 84 batch 230 train Loss 122.9037 test Loss 52.3117 with MSE metric 36230.2997\n",
      "Epoch 84 batch 240 train Loss 122.8484 test Loss 52.2898 with MSE metric 36217.0667\n",
      "Time taken for 1 epoch: 28.473644018173218 secs\n",
      "\n",
      "Epoch 85 batch 0 train Loss 122.7932 test Loss 52.2679 with MSE metric 36204.5103\n",
      "Epoch 85 batch 10 train Loss 122.7379 test Loss 52.2461 with MSE metric 36191.3732\n",
      "Epoch 85 batch 20 train Loss 122.6828 test Loss 52.2243 with MSE metric 36178.8371\n",
      "Epoch 85 batch 30 train Loss 122.6277 test Loss 52.2024 with MSE metric 36165.5853\n",
      "Epoch 85 batch 40 train Loss 122.5726 test Loss 52.1806 with MSE metric 36152.6394\n",
      "Epoch 85 batch 50 train Loss 122.5176 test Loss 52.1588 with MSE metric 36139.9085\n",
      "Epoch 85 batch 60 train Loss 122.4627 test Loss 52.1370 with MSE metric 36127.0976\n",
      "Epoch 85 batch 70 train Loss 122.4077 test Loss 52.1153 with MSE metric 36114.1450\n",
      "Epoch 85 batch 80 train Loss 122.3529 test Loss 52.0935 with MSE metric 36101.3696\n",
      "Epoch 85 batch 90 train Loss 122.2981 test Loss 52.0718 with MSE metric 36088.3648\n",
      "Epoch 85 batch 100 train Loss 122.2434 test Loss 52.0502 with MSE metric 36075.7586\n",
      "Epoch 85 batch 110 train Loss 122.1886 test Loss 52.0285 with MSE metric 36062.9581\n",
      "Epoch 85 batch 120 train Loss 122.1340 test Loss 52.0068 with MSE metric 36050.0883\n",
      "Epoch 85 batch 130 train Loss 122.0794 test Loss 51.9852 with MSE metric 36037.5171\n",
      "Epoch 85 batch 140 train Loss 122.0249 test Loss 51.9636 with MSE metric 36024.7875\n",
      "Epoch 85 batch 150 train Loss 121.9704 test Loss 51.9420 with MSE metric 36011.9313\n",
      "Epoch 85 batch 160 train Loss 121.9159 test Loss 51.9205 with MSE metric 35999.3886\n",
      "Epoch 85 batch 170 train Loss 121.8615 test Loss 51.8989 with MSE metric 35986.6099\n",
      "Epoch 85 batch 180 train Loss 121.8072 test Loss 51.8774 with MSE metric 35974.0050\n",
      "Epoch 85 batch 190 train Loss 121.7529 test Loss 51.8559 with MSE metric 35961.0748\n",
      "Epoch 85 batch 200 train Loss 121.6987 test Loss 51.8344 with MSE metric 35948.4587\n",
      "Epoch 85 batch 210 train Loss 121.6445 test Loss 51.8129 with MSE metric 35935.7789\n",
      "Epoch 85 batch 220 train Loss 121.5903 test Loss 51.7914 with MSE metric 35923.1786\n",
      "Epoch 85 batch 230 train Loss 121.5362 test Loss 51.7700 with MSE metric 35910.2983\n",
      "Epoch 85 batch 240 train Loss 121.4822 test Loss 51.7486 with MSE metric 35897.6131\n",
      "Time taken for 1 epoch: 28.711445093154907 secs\n",
      "\n",
      "Epoch 86 batch 0 train Loss 121.4282 test Loss 51.7272 with MSE metric 35884.7798\n",
      "Epoch 86 batch 10 train Loss 121.3742 test Loss 51.7058 with MSE metric 35872.0719\n",
      "Epoch 86 batch 20 train Loss 121.3203 test Loss 51.6844 with MSE metric 35859.3630\n",
      "Epoch 86 batch 30 train Loss 121.2665 test Loss 51.6631 with MSE metric 35846.6328\n",
      "Epoch 86 batch 40 train Loss 121.2127 test Loss 51.6418 with MSE metric 35834.1344\n",
      "Epoch 86 batch 50 train Loss 121.1590 test Loss 51.6205 with MSE metric 35821.6434\n",
      "Epoch 86 batch 60 train Loss 121.1053 test Loss 51.5992 with MSE metric 35808.7649\n",
      "Epoch 86 batch 70 train Loss 121.0516 test Loss 51.5779 with MSE metric 35796.2711\n",
      "Epoch 86 batch 80 train Loss 120.9980 test Loss 51.5566 with MSE metric 35783.6608\n",
      "Epoch 86 batch 90 train Loss 120.9445 test Loss 51.5354 with MSE metric 35771.0033\n",
      "Epoch 86 batch 100 train Loss 120.8910 test Loss 51.5142 with MSE metric 35758.3220\n",
      "Epoch 86 batch 110 train Loss 120.8376 test Loss 51.4930 with MSE metric 35745.7917\n",
      "Epoch 86 batch 120 train Loss 120.7842 test Loss 51.4718 with MSE metric 35733.3462\n",
      "Epoch 86 batch 130 train Loss 120.7308 test Loss 51.4507 with MSE metric 35720.7724\n",
      "Epoch 86 batch 140 train Loss 120.6775 test Loss 51.4296 with MSE metric 35708.1471\n",
      "Epoch 86 batch 150 train Loss 120.6243 test Loss 51.4085 with MSE metric 35695.8876\n",
      "Epoch 86 batch 160 train Loss 120.5711 test Loss 51.3874 with MSE metric 35683.2825\n",
      "Epoch 86 batch 170 train Loss 120.5180 test Loss 51.3663 with MSE metric 35670.7274\n",
      "Epoch 86 batch 180 train Loss 120.4649 test Loss 51.3452 with MSE metric 35658.4502\n",
      "Epoch 86 batch 190 train Loss 120.4118 test Loss 51.3242 with MSE metric 35645.9689\n",
      "Epoch 86 batch 200 train Loss 120.3588 test Loss 51.3032 with MSE metric 35633.4245\n",
      "Epoch 86 batch 210 train Loss 120.3059 test Loss 51.2822 with MSE metric 35620.9262\n",
      "Epoch 86 batch 220 train Loss 120.2530 test Loss 51.2612 with MSE metric 35608.2648\n",
      "Epoch 86 batch 230 train Loss 120.2001 test Loss 51.2402 with MSE metric 35596.1672\n",
      "Epoch 86 batch 240 train Loss 120.1473 test Loss 51.2193 with MSE metric 35583.7518\n",
      "Time taken for 1 epoch: 26.9031240940094 secs\n",
      "\n",
      "Epoch 87 batch 0 train Loss 120.0945 test Loss 51.1983 with MSE metric 35571.5946\n",
      "Epoch 87 batch 10 train Loss 120.0418 test Loss 51.1774 with MSE metric 35559.0657\n",
      "Epoch 87 batch 20 train Loss 119.9892 test Loss 51.1565 with MSE metric 35546.4849\n",
      "Epoch 87 batch 30 train Loss 119.9366 test Loss 51.1357 with MSE metric 35534.1668\n",
      "Epoch 87 batch 40 train Loss 119.8840 test Loss 51.1148 with MSE metric 35521.8065\n",
      "Epoch 87 batch 50 train Loss 119.8315 test Loss 51.0939 with MSE metric 35509.4280\n",
      "Epoch 87 batch 60 train Loss 119.7790 test Loss 51.0731 with MSE metric 35496.9324\n",
      "Epoch 87 batch 70 train Loss 119.7266 test Loss 51.0524 with MSE metric 35484.8619\n",
      "Epoch 87 batch 80 train Loss 119.6742 test Loss 51.0316 with MSE metric 35472.3143\n",
      "Epoch 87 batch 90 train Loss 119.6219 test Loss 51.0108 with MSE metric 35460.1244\n",
      "Epoch 87 batch 100 train Loss 119.5696 test Loss 50.9901 with MSE metric 35447.9666\n",
      "Epoch 87 batch 110 train Loss 119.5174 test Loss 50.9694 with MSE metric 35435.7025\n",
      "Epoch 87 batch 120 train Loss 119.4652 test Loss 50.9487 with MSE metric 35423.6667\n",
      "Epoch 87 batch 130 train Loss 119.4131 test Loss 50.9280 with MSE metric 35411.4989\n",
      "Epoch 87 batch 140 train Loss 119.3610 test Loss 50.9073 with MSE metric 35399.3227\n",
      "Epoch 87 batch 150 train Loss 119.3090 test Loss 50.8867 with MSE metric 35387.0408\n",
      "Epoch 87 batch 160 train Loss 119.2570 test Loss 50.8660 with MSE metric 35374.5054\n",
      "Epoch 87 batch 170 train Loss 119.2050 test Loss 50.8454 with MSE metric 35362.2514\n",
      "Epoch 87 batch 180 train Loss 119.1531 test Loss 50.8248 with MSE metric 35350.0486\n",
      "Epoch 87 batch 190 train Loss 119.1013 test Loss 50.8042 with MSE metric 35337.8693\n",
      "Epoch 87 batch 200 train Loss 119.0495 test Loss 50.7837 with MSE metric 35325.5976\n",
      "Epoch 87 batch 210 train Loss 118.9977 test Loss 50.7631 with MSE metric 35313.0729\n",
      "Epoch 87 batch 220 train Loss 118.9460 test Loss 50.7426 with MSE metric 35300.8269\n",
      "Epoch 87 batch 230 train Loss 118.8943 test Loss 50.7221 with MSE metric 35288.6504\n",
      "Epoch 87 batch 240 train Loss 118.8427 test Loss 50.7016 with MSE metric 35276.5655\n",
      "Time taken for 1 epoch: 29.772953987121582 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 batch 0 train Loss 118.7912 test Loss 50.6811 with MSE metric 35264.1606\n",
      "Epoch 88 batch 10 train Loss 118.7396 test Loss 50.6607 with MSE metric 35251.7845\n",
      "Epoch 88 batch 20 train Loss 118.6882 test Loss 50.6403 with MSE metric 35239.4349\n",
      "Epoch 88 batch 30 train Loss 118.6367 test Loss 50.6198 with MSE metric 35227.2359\n",
      "Epoch 88 batch 40 train Loss 118.5853 test Loss 50.5994 with MSE metric 35214.9937\n",
      "Epoch 88 batch 50 train Loss 118.5340 test Loss 50.5791 with MSE metric 35202.9079\n",
      "Epoch 88 batch 60 train Loss 118.4827 test Loss 50.5587 with MSE metric 35190.8104\n",
      "Epoch 88 batch 70 train Loss 118.4315 test Loss 50.5383 with MSE metric 35178.8856\n",
      "Epoch 88 batch 80 train Loss 118.3803 test Loss 50.5180 with MSE metric 35166.7100\n",
      "Epoch 88 batch 90 train Loss 118.3291 test Loss 50.4977 with MSE metric 35154.8275\n",
      "Epoch 88 batch 100 train Loss 118.2780 test Loss 50.4774 with MSE metric 35142.9022\n",
      "Epoch 88 batch 110 train Loss 118.2270 test Loss 50.4571 with MSE metric 35130.8062\n",
      "Epoch 88 batch 120 train Loss 118.1760 test Loss 50.4369 with MSE metric 35118.8437\n",
      "Epoch 88 batch 130 train Loss 118.1250 test Loss 50.4167 with MSE metric 35106.8928\n",
      "Epoch 88 batch 140 train Loss 118.0741 test Loss 50.3964 with MSE metric 35094.7706\n",
      "Epoch 88 batch 150 train Loss 118.0232 test Loss 50.3763 with MSE metric 35082.9530\n",
      "Epoch 88 batch 160 train Loss 117.9724 test Loss 50.3561 with MSE metric 35071.1108\n",
      "Epoch 88 batch 170 train Loss 117.9216 test Loss 50.3359 with MSE metric 35059.2271\n",
      "Epoch 88 batch 180 train Loss 117.8709 test Loss 50.3158 with MSE metric 35047.3415\n",
      "Epoch 88 batch 190 train Loss 117.8202 test Loss 50.2956 with MSE metric 35035.3211\n",
      "Epoch 88 batch 200 train Loss 117.7695 test Loss 50.2755 with MSE metric 35023.4286\n",
      "Epoch 88 batch 210 train Loss 117.7189 test Loss 50.2554 with MSE metric 35011.3295\n",
      "Epoch 88 batch 220 train Loss 117.6684 test Loss 50.2353 with MSE metric 34999.4796\n",
      "Epoch 88 batch 230 train Loss 117.6179 test Loss 50.2153 with MSE metric 34987.7064\n",
      "Epoch 88 batch 240 train Loss 117.5674 test Loss 50.1953 with MSE metric 34975.4399\n",
      "Time taken for 1 epoch: 28.711122035980225 secs\n",
      "\n",
      "Epoch 89 batch 0 train Loss 117.5170 test Loss 50.1752 with MSE metric 34963.8214\n",
      "Epoch 89 batch 10 train Loss 117.4666 test Loss 50.1552 with MSE metric 34951.8742\n",
      "Epoch 89 batch 20 train Loss 117.4163 test Loss 50.1353 with MSE metric 34939.8266\n",
      "Epoch 89 batch 30 train Loss 117.3660 test Loss 50.1153 with MSE metric 34927.9616\n",
      "Epoch 89 batch 40 train Loss 117.3158 test Loss 50.0953 with MSE metric 34916.1217\n",
      "Epoch 89 batch 50 train Loss 117.2656 test Loss 50.0754 with MSE metric 34904.2652\n",
      "Epoch 89 batch 60 train Loss 117.2154 test Loss 50.0554 with MSE metric 34892.3672\n",
      "Epoch 89 batch 70 train Loss 117.1653 test Loss 50.0355 with MSE metric 34880.6198\n",
      "Epoch 89 batch 80 train Loss 117.1152 test Loss 50.0156 with MSE metric 34868.6848\n",
      "Epoch 89 batch 90 train Loss 117.0652 test Loss 49.9958 with MSE metric 34857.0178\n",
      "Epoch 89 batch 100 train Loss 117.0153 test Loss 49.9759 with MSE metric 34845.2691\n",
      "Epoch 89 batch 110 train Loss 116.9653 test Loss 49.9561 with MSE metric 34833.3267\n",
      "Epoch 89 batch 120 train Loss 116.9155 test Loss 49.9363 with MSE metric 34821.5429\n",
      "Epoch 89 batch 130 train Loss 116.8656 test Loss 49.9165 with MSE metric 34810.0474\n",
      "Epoch 89 batch 140 train Loss 116.8158 test Loss 49.8967 with MSE metric 34798.1686\n",
      "Epoch 89 batch 150 train Loss 116.7661 test Loss 49.8769 with MSE metric 34786.7227\n",
      "Epoch 89 batch 160 train Loss 116.7164 test Loss 49.8572 with MSE metric 34775.0784\n",
      "Epoch 89 batch 170 train Loss 116.6667 test Loss 49.8375 with MSE metric 34763.0049\n",
      "Epoch 89 batch 180 train Loss 116.6171 test Loss 49.8177 with MSE metric 34751.5088\n",
      "Epoch 89 batch 190 train Loss 116.5675 test Loss 49.7980 with MSE metric 34739.8179\n",
      "Epoch 89 batch 200 train Loss 116.5180 test Loss 49.7783 with MSE metric 34728.2458\n",
      "Epoch 89 batch 210 train Loss 116.4685 test Loss 49.7586 with MSE metric 34716.5814\n",
      "Epoch 89 batch 220 train Loss 116.4191 test Loss 49.7390 with MSE metric 34705.0800\n",
      "Epoch 89 batch 230 train Loss 116.3697 test Loss 49.7194 with MSE metric 34693.3330\n",
      "Epoch 89 batch 240 train Loss 116.3203 test Loss 49.6998 with MSE metric 34681.5996\n",
      "Time taken for 1 epoch: 28.93132185935974 secs\n",
      "\n",
      "Epoch 90 batch 0 train Loss 116.2710 test Loss 49.6802 with MSE metric 34669.6057\n",
      "Epoch 90 batch 10 train Loss 116.2218 test Loss 49.6606 with MSE metric 34657.9557\n",
      "Epoch 90 batch 20 train Loss 116.1726 test Loss 49.6410 with MSE metric 34646.5622\n",
      "Epoch 90 batch 30 train Loss 116.1234 test Loss 49.6215 with MSE metric 34635.0885\n",
      "Epoch 90 batch 40 train Loss 116.0743 test Loss 49.6020 with MSE metric 34623.3107\n",
      "Epoch 90 batch 50 train Loss 116.0252 test Loss 49.5824 with MSE metric 34611.6140\n",
      "Epoch 90 batch 60 train Loss 115.9761 test Loss 49.5629 with MSE metric 34600.2021\n",
      "Epoch 90 batch 70 train Loss 115.9271 test Loss 49.5435 with MSE metric 34588.7032\n",
      "Epoch 90 batch 80 train Loss 115.8782 test Loss 49.5240 with MSE metric 34577.2263\n",
      "Epoch 90 batch 90 train Loss 115.8293 test Loss 49.5045 with MSE metric 34565.6928\n",
      "Epoch 90 batch 100 train Loss 115.7804 test Loss 49.4851 with MSE metric 34553.9790\n",
      "Epoch 90 batch 110 train Loss 115.7316 test Loss 49.4657 with MSE metric 34542.3549\n",
      "Epoch 90 batch 120 train Loss 115.6828 test Loss 49.4463 with MSE metric 34530.8727\n",
      "Epoch 90 batch 130 train Loss 115.6340 test Loss 49.4269 with MSE metric 34519.1732\n",
      "Epoch 90 batch 140 train Loss 115.5853 test Loss 49.4076 with MSE metric 34507.6098\n",
      "Epoch 90 batch 150 train Loss 115.5367 test Loss 49.3882 with MSE metric 34496.0160\n",
      "Epoch 90 batch 160 train Loss 115.4881 test Loss 49.3689 with MSE metric 34484.4661\n",
      "Epoch 90 batch 170 train Loss 115.4395 test Loss 49.3496 with MSE metric 34472.8018\n",
      "Epoch 90 batch 180 train Loss 115.3910 test Loss 49.3303 with MSE metric 34461.4711\n",
      "Epoch 90 batch 190 train Loss 115.3425 test Loss 49.3110 with MSE metric 34450.2122\n",
      "Epoch 90 batch 200 train Loss 115.2940 test Loss 49.2917 with MSE metric 34438.9307\n",
      "Epoch 90 batch 210 train Loss 115.2456 test Loss 49.2725 with MSE metric 34427.2819\n",
      "Epoch 90 batch 220 train Loss 115.1973 test Loss 49.2533 with MSE metric 34415.8056\n",
      "Epoch 90 batch 230 train Loss 115.1490 test Loss 49.2340 with MSE metric 34403.9961\n",
      "Epoch 90 batch 240 train Loss 115.1007 test Loss 49.2149 with MSE metric 34392.8294\n",
      "Time taken for 1 epoch: 28.234251976013184 secs\n",
      "\n",
      "Epoch 91 batch 0 train Loss 115.0525 test Loss 49.1957 with MSE metric 34381.5051\n",
      "Epoch 91 batch 10 train Loss 115.0043 test Loss 49.1765 with MSE metric 34369.7702\n",
      "Epoch 91 batch 20 train Loss 114.9561 test Loss 49.1574 with MSE metric 34358.6047\n",
      "Epoch 91 batch 30 train Loss 114.9080 test Loss 49.1382 with MSE metric 34347.2081\n",
      "Epoch 91 batch 40 train Loss 114.8600 test Loss 49.1191 with MSE metric 34335.6948\n",
      "Epoch 91 batch 50 train Loss 114.8119 test Loss 49.1000 with MSE metric 34324.0664\n",
      "Epoch 91 batch 60 train Loss 114.7640 test Loss 49.0810 with MSE metric 34312.5360\n",
      "Epoch 91 batch 70 train Loss 114.7160 test Loss 49.0619 with MSE metric 34301.2800\n",
      "Epoch 91 batch 80 train Loss 114.6681 test Loss 49.0428 with MSE metric 34290.0468\n",
      "Epoch 91 batch 90 train Loss 114.6203 test Loss 49.0238 with MSE metric 34278.7930\n",
      "Epoch 91 batch 100 train Loss 114.5725 test Loss 49.0048 with MSE metric 34267.7794\n",
      "Epoch 91 batch 110 train Loss 114.5247 test Loss 48.9858 with MSE metric 34256.3674\n",
      "Epoch 91 batch 120 train Loss 114.4770 test Loss 48.9668 with MSE metric 34245.2848\n",
      "Epoch 91 batch 130 train Loss 114.4293 test Loss 48.9478 with MSE metric 34234.0731\n",
      "Epoch 91 batch 140 train Loss 114.3817 test Loss 48.9289 with MSE metric 34222.7388\n",
      "Epoch 91 batch 150 train Loss 114.3341 test Loss 48.9100 with MSE metric 34211.2392\n",
      "Epoch 91 batch 160 train Loss 114.2865 test Loss 48.8910 with MSE metric 34199.9172\n",
      "Epoch 91 batch 170 train Loss 114.2390 test Loss 48.8721 with MSE metric 34188.9039\n",
      "Epoch 91 batch 180 train Loss 114.1915 test Loss 48.8533 with MSE metric 34177.5470\n",
      "Epoch 91 batch 190 train Loss 114.1441 test Loss 48.8344 with MSE metric 34166.3138\n",
      "Epoch 91 batch 200 train Loss 114.0967 test Loss 48.8156 with MSE metric 34155.0892\n",
      "Epoch 91 batch 210 train Loss 114.0494 test Loss 48.7967 with MSE metric 34144.2325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 batch 220 train Loss 114.0021 test Loss 48.7779 with MSE metric 34133.0409\n",
      "Epoch 91 batch 230 train Loss 113.9548 test Loss 48.7591 with MSE metric 34121.9004\n",
      "Epoch 91 batch 240 train Loss 113.9075 test Loss 48.7403 with MSE metric 34110.8586\n",
      "Time taken for 1 epoch: 27.0658917427063 secs\n",
      "\n",
      "Epoch 92 batch 0 train Loss 113.8604 test Loss 48.7215 with MSE metric 34099.5668\n",
      "Epoch 92 batch 10 train Loss 113.8132 test Loss 48.7028 with MSE metric 34088.2863\n",
      "Epoch 92 batch 20 train Loss 113.7661 test Loss 48.6840 with MSE metric 34077.0376\n",
      "Epoch 92 batch 30 train Loss 113.7190 test Loss 48.6653 with MSE metric 34065.7620\n",
      "Epoch 92 batch 40 train Loss 113.6720 test Loss 48.6466 with MSE metric 34054.4325\n",
      "Epoch 92 batch 50 train Loss 113.6250 test Loss 48.6279 with MSE metric 34043.4206\n",
      "Epoch 92 batch 60 train Loss 113.5781 test Loss 48.6092 with MSE metric 34032.3460\n",
      "Epoch 92 batch 70 train Loss 113.5312 test Loss 48.5906 with MSE metric 34021.3367\n",
      "Epoch 92 batch 80 train Loss 113.4843 test Loss 48.5719 with MSE metric 34010.0142\n",
      "Epoch 92 batch 90 train Loss 113.4375 test Loss 48.5533 with MSE metric 33998.8778\n",
      "Epoch 92 batch 100 train Loss 113.3907 test Loss 48.5347 with MSE metric 33987.7504\n",
      "Epoch 92 batch 110 train Loss 113.3440 test Loss 48.5161 with MSE metric 33976.6596\n",
      "Epoch 92 batch 120 train Loss 113.2973 test Loss 48.4975 with MSE metric 33965.5938\n",
      "Epoch 92 batch 130 train Loss 113.2506 test Loss 48.4789 with MSE metric 33954.6676\n",
      "Epoch 92 batch 140 train Loss 113.2040 test Loss 48.4604 with MSE metric 33943.7377\n",
      "Epoch 92 batch 150 train Loss 113.1574 test Loss 48.4419 with MSE metric 33932.5317\n",
      "Epoch 92 batch 160 train Loss 113.1109 test Loss 48.4233 with MSE metric 33921.5122\n",
      "Epoch 92 batch 170 train Loss 113.0644 test Loss 48.4049 with MSE metric 33910.3084\n",
      "Epoch 92 batch 180 train Loss 113.0179 test Loss 48.3864 with MSE metric 33899.2825\n",
      "Epoch 92 batch 190 train Loss 112.9715 test Loss 48.3679 with MSE metric 33888.1899\n",
      "Epoch 92 batch 200 train Loss 112.9251 test Loss 48.3495 with MSE metric 33877.2388\n",
      "Epoch 92 batch 210 train Loss 112.8788 test Loss 48.3310 with MSE metric 33866.1667\n",
      "Epoch 92 batch 220 train Loss 112.8325 test Loss 48.3126 with MSE metric 33855.2464\n",
      "Epoch 92 batch 230 train Loss 112.7862 test Loss 48.2942 with MSE metric 33844.2547\n",
      "Epoch 92 batch 240 train Loss 112.7400 test Loss 48.2758 with MSE metric 33833.3165\n",
      "Time taken for 1 epoch: 30.692069053649902 secs\n",
      "\n",
      "Epoch 93 batch 0 train Loss 112.6938 test Loss 48.2574 with MSE metric 33822.4563\n",
      "Epoch 93 batch 10 train Loss 112.6477 test Loss 48.2390 with MSE metric 33811.5394\n",
      "Epoch 93 batch 20 train Loss 112.6015 test Loss 48.2207 with MSE metric 33800.6440\n",
      "Epoch 93 batch 30 train Loss 112.5555 test Loss 48.2023 with MSE metric 33789.6729\n",
      "Epoch 93 batch 40 train Loss 112.5094 test Loss 48.1840 with MSE metric 33778.6100\n",
      "Epoch 93 batch 50 train Loss 112.4635 test Loss 48.1657 with MSE metric 33767.5643\n",
      "Epoch 93 batch 60 train Loss 112.4175 test Loss 48.1474 with MSE metric 33756.5172\n",
      "Epoch 93 batch 70 train Loss 112.3716 test Loss 48.1292 with MSE metric 33745.6577\n",
      "Epoch 93 batch 80 train Loss 112.3257 test Loss 48.1109 with MSE metric 33734.6148\n",
      "Epoch 93 batch 90 train Loss 112.2799 test Loss 48.0926 with MSE metric 33723.9084\n",
      "Epoch 93 batch 100 train Loss 112.2341 test Loss 48.0744 with MSE metric 33712.7708\n",
      "Epoch 93 batch 110 train Loss 112.1884 test Loss 48.0562 with MSE metric 33701.9712\n",
      "Epoch 93 batch 120 train Loss 112.1427 test Loss 48.0380 with MSE metric 33691.2884\n",
      "Epoch 93 batch 130 train Loss 112.0970 test Loss 48.0198 with MSE metric 33680.2992\n",
      "Epoch 93 batch 140 train Loss 112.0514 test Loss 48.0016 with MSE metric 33669.4677\n",
      "Epoch 93 batch 150 train Loss 112.0058 test Loss 47.9835 with MSE metric 33658.7497\n",
      "Epoch 93 batch 160 train Loss 111.9602 test Loss 47.9654 with MSE metric 33647.8791\n",
      "Epoch 93 batch 170 train Loss 111.9147 test Loss 47.9472 with MSE metric 33636.9411\n",
      "Epoch 93 batch 180 train Loss 111.8692 test Loss 47.9291 with MSE metric 33626.1840\n",
      "Epoch 93 batch 190 train Loss 111.8238 test Loss 47.9110 with MSE metric 33615.5724\n",
      "Epoch 93 batch 200 train Loss 111.7784 test Loss 47.8929 with MSE metric 33604.7079\n",
      "Epoch 93 batch 210 train Loss 111.7330 test Loss 47.8749 with MSE metric 33593.9999\n",
      "Epoch 93 batch 220 train Loss 111.6877 test Loss 47.8569 with MSE metric 33583.0324\n",
      "Epoch 93 batch 230 train Loss 111.6424 test Loss 47.8388 with MSE metric 33572.1539\n",
      "Epoch 93 batch 240 train Loss 111.5972 test Loss 47.8208 with MSE metric 33561.2642\n",
      "Time taken for 1 epoch: 28.66495394706726 secs\n",
      "\n",
      "Epoch 94 batch 0 train Loss 111.5520 test Loss 47.8028 with MSE metric 33550.3150\n",
      "Epoch 94 batch 10 train Loss 111.5068 test Loss 47.7848 with MSE metric 33539.1493\n",
      "Epoch 94 batch 20 train Loss 111.4617 test Loss 47.7669 with MSE metric 33528.2171\n",
      "Epoch 94 batch 30 train Loss 111.4166 test Loss 47.7489 with MSE metric 33517.4185\n",
      "Epoch 94 batch 40 train Loss 111.3716 test Loss 47.7309 with MSE metric 33506.5622\n",
      "Epoch 94 batch 50 train Loss 111.3265 test Loss 47.7130 with MSE metric 33495.8761\n",
      "Epoch 94 batch 60 train Loss 111.2816 test Loss 47.6951 with MSE metric 33485.0852\n",
      "Epoch 94 batch 70 train Loss 111.2366 test Loss 47.6772 with MSE metric 33474.0776\n",
      "Epoch 94 batch 80 train Loss 111.1917 test Loss 47.6593 with MSE metric 33463.1532\n",
      "Epoch 94 batch 90 train Loss 111.1469 test Loss 47.6415 with MSE metric 33452.3178\n",
      "Epoch 94 batch 100 train Loss 111.1020 test Loss 47.6236 with MSE metric 33441.7232\n",
      "Epoch 94 batch 110 train Loss 111.0573 test Loss 47.6058 with MSE metric 33430.7771\n",
      "Epoch 94 batch 120 train Loss 111.0125 test Loss 47.5880 with MSE metric 33419.9746\n",
      "Epoch 94 batch 130 train Loss 110.9678 test Loss 47.5702 with MSE metric 33409.5346\n",
      "Epoch 94 batch 140 train Loss 110.9231 test Loss 47.5524 with MSE metric 33398.8362\n",
      "Epoch 94 batch 150 train Loss 110.8785 test Loss 47.5347 with MSE metric 33388.1409\n",
      "Epoch 94 batch 160 train Loss 110.8339 test Loss 47.5169 with MSE metric 33377.4850\n",
      "Epoch 94 batch 170 train Loss 110.7893 test Loss 47.4991 with MSE metric 33366.8575\n",
      "Epoch 94 batch 180 train Loss 110.7448 test Loss 47.4814 with MSE metric 33356.1893\n",
      "Epoch 94 batch 190 train Loss 110.7003 test Loss 47.4637 with MSE metric 33345.5533\n",
      "Epoch 94 batch 200 train Loss 110.6559 test Loss 47.4460 with MSE metric 33334.7904\n",
      "Epoch 94 batch 210 train Loss 110.6115 test Loss 47.4283 with MSE metric 33323.9235\n",
      "Epoch 94 batch 220 train Loss 110.5671 test Loss 47.4106 with MSE metric 33313.1246\n",
      "Epoch 94 batch 230 train Loss 110.5228 test Loss 47.3930 with MSE metric 33302.5617\n",
      "Epoch 94 batch 240 train Loss 110.4785 test Loss 47.3753 with MSE metric 33292.0231\n",
      "Time taken for 1 epoch: 26.929031133651733 secs\n",
      "\n",
      "Epoch 95 batch 0 train Loss 110.4342 test Loss 47.3577 with MSE metric 33281.3839\n",
      "Epoch 95 batch 10 train Loss 110.3900 test Loss 47.3401 with MSE metric 33270.9367\n",
      "Epoch 95 batch 20 train Loss 110.3458 test Loss 47.3225 with MSE metric 33260.2302\n",
      "Epoch 95 batch 30 train Loss 110.3017 test Loss 47.3049 with MSE metric 33249.5377\n",
      "Epoch 95 batch 40 train Loss 110.2575 test Loss 47.2874 with MSE metric 33238.9940\n",
      "Epoch 95 batch 50 train Loss 110.2135 test Loss 47.2698 with MSE metric 33228.4847\n",
      "Epoch 95 batch 60 train Loss 110.1695 test Loss 47.2523 with MSE metric 33218.1127\n",
      "Epoch 95 batch 70 train Loss 110.1254 test Loss 47.2347 with MSE metric 33207.8191\n",
      "Epoch 95 batch 80 train Loss 110.0815 test Loss 47.2172 with MSE metric 33197.5007\n",
      "Epoch 95 batch 90 train Loss 110.0375 test Loss 47.1997 with MSE metric 33187.0032\n",
      "Epoch 95 batch 100 train Loss 109.9936 test Loss 47.1822 with MSE metric 33176.7044\n",
      "Epoch 95 batch 110 train Loss 109.9498 test Loss 47.1647 with MSE metric 33166.3680\n",
      "Epoch 95 batch 120 train Loss 109.9060 test Loss 47.1473 with MSE metric 33155.7173\n",
      "Epoch 95 batch 130 train Loss 109.8622 test Loss 47.1298 with MSE metric 33145.1189\n",
      "Epoch 95 batch 140 train Loss 109.8184 test Loss 47.1124 with MSE metric 33134.7119\n",
      "Epoch 95 batch 150 train Loss 109.7747 test Loss 47.0950 with MSE metric 33124.1403\n",
      "Epoch 95 batch 160 train Loss 109.7311 test Loss 47.0776 with MSE metric 33113.7431\n",
      "Epoch 95 batch 170 train Loss 109.6874 test Loss 47.0602 with MSE metric 33103.4048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 batch 180 train Loss 109.6438 test Loss 47.0428 with MSE metric 33092.9508\n",
      "Epoch 95 batch 190 train Loss 109.6003 test Loss 47.0255 with MSE metric 33082.4504\n",
      "Epoch 95 batch 200 train Loss 109.5567 test Loss 47.0081 with MSE metric 33072.0017\n",
      "Epoch 95 batch 210 train Loss 109.5132 test Loss 46.9908 with MSE metric 33061.5160\n",
      "Epoch 95 batch 220 train Loss 109.4698 test Loss 46.9735 with MSE metric 33051.1071\n",
      "Epoch 95 batch 230 train Loss 109.4264 test Loss 46.9562 with MSE metric 33040.6384\n",
      "Epoch 95 batch 240 train Loss 109.3830 test Loss 46.9389 with MSE metric 33030.0505\n",
      "Time taken for 1 epoch: 29.40114188194275 secs\n",
      "\n",
      "Epoch 96 batch 0 train Loss 109.3396 test Loss 46.9216 with MSE metric 33019.6527\n",
      "Epoch 96 batch 10 train Loss 109.2963 test Loss 46.9044 with MSE metric 33009.4183\n",
      "Epoch 96 batch 20 train Loss 109.2531 test Loss 46.8872 with MSE metric 32999.1946\n",
      "Epoch 96 batch 30 train Loss 109.2098 test Loss 46.8699 with MSE metric 32988.7520\n",
      "Epoch 96 batch 40 train Loss 109.1666 test Loss 46.8527 with MSE metric 32978.4528\n",
      "Epoch 96 batch 50 train Loss 109.1235 test Loss 46.8355 with MSE metric 32968.3716\n",
      "Epoch 96 batch 60 train Loss 109.0803 test Loss 46.8183 with MSE metric 32958.1123\n",
      "Epoch 96 batch 70 train Loss 109.0373 test Loss 46.8011 with MSE metric 32947.7239\n",
      "Epoch 96 batch 80 train Loss 108.9942 test Loss 46.7840 with MSE metric 32937.4174\n",
      "Epoch 96 batch 90 train Loss 108.9512 test Loss 46.7668 with MSE metric 32927.0755\n",
      "Epoch 96 batch 100 train Loss 108.9082 test Loss 46.7497 with MSE metric 32916.7348\n",
      "Epoch 96 batch 110 train Loss 108.8653 test Loss 46.7326 with MSE metric 32906.4651\n",
      "Epoch 96 batch 120 train Loss 108.8224 test Loss 46.7155 with MSE metric 32896.0444\n",
      "Epoch 96 batch 130 train Loss 108.7795 test Loss 46.6984 with MSE metric 32885.8542\n",
      "Epoch 96 batch 140 train Loss 108.7366 test Loss 46.6813 with MSE metric 32875.5537\n",
      "Epoch 96 batch 150 train Loss 108.6938 test Loss 46.6643 with MSE metric 32865.3991\n",
      "Epoch 96 batch 160 train Loss 108.6510 test Loss 46.6472 with MSE metric 32855.1004\n",
      "Epoch 96 batch 170 train Loss 108.6083 test Loss 46.6302 with MSE metric 32844.9478\n",
      "Epoch 96 batch 180 train Loss 108.5656 test Loss 46.6132 with MSE metric 32834.7217\n",
      "Epoch 96 batch 190 train Loss 108.5229 test Loss 46.5961 with MSE metric 32824.4309\n",
      "Epoch 96 batch 200 train Loss 108.4803 test Loss 46.5791 with MSE metric 32814.3736\n",
      "Epoch 96 batch 210 train Loss 108.4377 test Loss 46.5622 with MSE metric 32804.1907\n",
      "Epoch 96 batch 220 train Loss 108.3951 test Loss 46.5452 with MSE metric 32794.1016\n",
      "Epoch 96 batch 230 train Loss 108.3526 test Loss 46.5283 with MSE metric 32783.9188\n",
      "Epoch 96 batch 240 train Loss 108.3101 test Loss 46.5113 with MSE metric 32773.7319\n",
      "Time taken for 1 epoch: 31.52545714378357 secs\n",
      "\n",
      "Epoch 97 batch 0 train Loss 108.2676 test Loss 46.4944 with MSE metric 32763.4565\n",
      "Epoch 97 batch 10 train Loss 108.2252 test Loss 46.4775 with MSE metric 32753.0587\n",
      "Epoch 97 batch 20 train Loss 108.1828 test Loss 46.4605 with MSE metric 32743.0179\n",
      "Epoch 97 batch 30 train Loss 108.1405 test Loss 46.4437 with MSE metric 32732.7522\n",
      "Epoch 97 batch 40 train Loss 108.0981 test Loss 46.4268 with MSE metric 32722.5412\n",
      "Epoch 97 batch 50 train Loss 108.0559 test Loss 46.4099 with MSE metric 32712.2303\n",
      "Epoch 97 batch 60 train Loss 108.0136 test Loss 46.3931 with MSE metric 32702.2039\n",
      "Epoch 97 batch 70 train Loss 107.9714 test Loss 46.3762 with MSE metric 32692.1065\n",
      "Epoch 97 batch 80 train Loss 107.9292 test Loss 46.3594 with MSE metric 32681.8297\n",
      "Epoch 97 batch 90 train Loss 107.8871 test Loss 46.3426 with MSE metric 32671.6810\n",
      "Epoch 97 batch 100 train Loss 107.8450 test Loss 46.3258 with MSE metric 32661.7245\n",
      "Epoch 97 batch 110 train Loss 107.8029 test Loss 46.3091 with MSE metric 32651.5502\n",
      "Epoch 97 batch 120 train Loss 107.7608 test Loss 46.2923 with MSE metric 32641.4826\n",
      "Epoch 97 batch 130 train Loss 107.7188 test Loss 46.2756 with MSE metric 32631.3543\n",
      "Epoch 97 batch 140 train Loss 107.6769 test Loss 46.2588 with MSE metric 32621.4619\n",
      "Epoch 97 batch 150 train Loss 107.6349 test Loss 46.2421 with MSE metric 32611.3201\n",
      "Epoch 97 batch 160 train Loss 107.5930 test Loss 46.2254 with MSE metric 32601.3200\n",
      "Epoch 97 batch 170 train Loss 107.5511 test Loss 46.2087 with MSE metric 32591.1513\n",
      "Epoch 97 batch 180 train Loss 107.5093 test Loss 46.1921 with MSE metric 32581.1389\n",
      "Epoch 97 batch 190 train Loss 107.4675 test Loss 46.1754 with MSE metric 32571.0197\n",
      "Epoch 97 batch 200 train Loss 107.4257 test Loss 46.1587 with MSE metric 32561.0391\n",
      "Epoch 97 batch 210 train Loss 107.3840 test Loss 46.1421 with MSE metric 32551.0568\n",
      "Epoch 97 batch 220 train Loss 107.3423 test Loss 46.1255 with MSE metric 32540.7121\n",
      "Epoch 97 batch 230 train Loss 107.3007 test Loss 46.1089 with MSE metric 32530.5736\n",
      "Epoch 97 batch 240 train Loss 107.2590 test Loss 46.0923 with MSE metric 32520.4879\n",
      "Time taken for 1 epoch: 28.25147008895874 secs\n",
      "\n",
      "Epoch 98 batch 0 train Loss 107.2174 test Loss 46.0757 with MSE metric 32510.5219\n",
      "Epoch 98 batch 10 train Loss 107.1758 test Loss 46.0591 with MSE metric 32500.6082\n",
      "Epoch 98 batch 20 train Loss 107.1343 test Loss 46.0426 with MSE metric 32490.5739\n",
      "Epoch 98 batch 30 train Loss 107.0928 test Loss 46.0260 with MSE metric 32480.5344\n",
      "Epoch 98 batch 40 train Loss 107.0514 test Loss 46.0095 with MSE metric 32470.3987\n",
      "Epoch 98 batch 50 train Loss 107.0099 test Loss 45.9930 with MSE metric 32460.4937\n",
      "Epoch 98 batch 60 train Loss 106.9685 test Loss 45.9764 with MSE metric 32450.6341\n",
      "Epoch 98 batch 70 train Loss 106.9272 test Loss 45.9600 with MSE metric 32440.7214\n",
      "Epoch 98 batch 80 train Loss 106.8858 test Loss 45.9435 with MSE metric 32430.6171\n",
      "Epoch 98 batch 90 train Loss 106.8446 test Loss 45.9270 with MSE metric 32420.7362\n",
      "Epoch 98 batch 100 train Loss 106.8033 test Loss 45.9106 with MSE metric 32410.8796\n",
      "Epoch 98 batch 110 train Loss 106.7621 test Loss 45.8941 with MSE metric 32400.9207\n",
      "Epoch 98 batch 120 train Loss 106.7209 test Loss 45.8777 with MSE metric 32390.8387\n",
      "Epoch 98 batch 130 train Loss 106.6797 test Loss 45.8613 with MSE metric 32380.8270\n",
      "Epoch 98 batch 140 train Loss 106.6386 test Loss 45.8449 with MSE metric 32370.7229\n",
      "Epoch 98 batch 150 train Loss 106.5975 test Loss 45.8285 with MSE metric 32360.7894\n",
      "Epoch 98 batch 160 train Loss 106.5565 test Loss 45.8121 with MSE metric 32350.8785\n",
      "Epoch 98 batch 170 train Loss 106.5154 test Loss 45.7958 with MSE metric 32340.9212\n",
      "Epoch 98 batch 180 train Loss 106.4744 test Loss 45.7794 with MSE metric 32331.0136\n",
      "Epoch 98 batch 190 train Loss 106.4335 test Loss 45.7631 with MSE metric 32321.3603\n",
      "Epoch 98 batch 200 train Loss 106.3925 test Loss 45.7467 with MSE metric 32311.3489\n",
      "Epoch 98 batch 210 train Loss 106.3516 test Loss 45.7304 with MSE metric 32301.5118\n",
      "Epoch 98 batch 220 train Loss 106.3108 test Loss 45.7141 with MSE metric 32291.7192\n",
      "Epoch 98 batch 230 train Loss 106.2700 test Loss 45.6978 with MSE metric 32282.0460\n",
      "Epoch 98 batch 240 train Loss 106.2292 test Loss 45.6816 with MSE metric 32272.0803\n",
      "Time taken for 1 epoch: 26.038506031036377 secs\n",
      "\n",
      "Epoch 99 batch 0 train Loss 106.1884 test Loss 45.6653 with MSE metric 32262.4008\n",
      "Epoch 99 batch 10 train Loss 106.1477 test Loss 45.6491 with MSE metric 32252.5571\n",
      "Epoch 99 batch 20 train Loss 106.1070 test Loss 45.6328 with MSE metric 32242.7423\n",
      "Epoch 99 batch 30 train Loss 106.0663 test Loss 45.6166 with MSE metric 32232.8288\n",
      "Epoch 99 batch 40 train Loss 106.0257 test Loss 45.6004 with MSE metric 32223.0498\n",
      "Epoch 99 batch 50 train Loss 105.9851 test Loss 45.5843 with MSE metric 32213.2962\n",
      "Epoch 99 batch 60 train Loss 105.9445 test Loss 45.5681 with MSE metric 32203.6243\n",
      "Epoch 99 batch 70 train Loss 105.9040 test Loss 45.5519 with MSE metric 32193.8232\n",
      "Epoch 99 batch 80 train Loss 105.8635 test Loss 45.5358 with MSE metric 32184.1179\n",
      "Epoch 99 batch 90 train Loss 105.8230 test Loss 45.5197 with MSE metric 32174.3057\n",
      "Epoch 99 batch 100 train Loss 105.7826 test Loss 45.5035 with MSE metric 32164.6323\n",
      "Epoch 99 batch 110 train Loss 105.7422 test Loss 45.4875 with MSE metric 32155.0032\n",
      "Epoch 99 batch 120 train Loss 105.7018 test Loss 45.4714 with MSE metric 32145.2894\n",
      "Epoch 99 batch 130 train Loss 105.6615 test Loss 45.4553 with MSE metric 32135.5962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 batch 140 train Loss 105.6212 test Loss 45.4392 with MSE metric 32125.8314\n",
      "Epoch 99 batch 150 train Loss 105.5809 test Loss 45.4231 with MSE metric 32116.0185\n",
      "Epoch 99 batch 160 train Loss 105.5407 test Loss 45.4071 with MSE metric 32106.2467\n",
      "Epoch 99 batch 170 train Loss 105.5004 test Loss 45.3911 with MSE metric 32096.5018\n",
      "Epoch 99 batch 180 train Loss 105.4603 test Loss 45.3751 with MSE metric 32086.8557\n",
      "Epoch 99 batch 190 train Loss 105.4201 test Loss 45.3591 with MSE metric 32077.2450\n",
      "Epoch 99 batch 200 train Loss 105.3800 test Loss 45.3431 with MSE metric 32067.6552\n",
      "Epoch 99 batch 210 train Loss 105.3400 test Loss 45.3271 with MSE metric 32058.0011\n",
      "Epoch 99 batch 220 train Loss 105.2999 test Loss 45.3111 with MSE metric 32048.1450\n",
      "Epoch 99 batch 230 train Loss 105.2599 test Loss 45.2952 with MSE metric 32038.5602\n",
      "Epoch 99 batch 240 train Loss 105.2199 test Loss 45.2792 with MSE metric 32028.8144\n",
      "Time taken for 1 epoch: 27.713969230651855 secs\n",
      "\n",
      "Epoch 100 batch 0 train Loss 105.1800 test Loss 45.2633 with MSE metric 32019.0124\n",
      "Epoch 100 batch 10 train Loss 105.1400 test Loss 45.2474 with MSE metric 32009.3105\n",
      "Epoch 100 batch 20 train Loss 105.1002 test Loss 45.2315 with MSE metric 31999.7504\n",
      "Epoch 100 batch 30 train Loss 105.0603 test Loss 45.2156 with MSE metric 31990.2761\n",
      "Epoch 100 batch 40 train Loss 105.0205 test Loss 45.1997 with MSE metric 31980.8934\n",
      "Epoch 100 batch 50 train Loss 104.9807 test Loss 45.1838 with MSE metric 31971.1068\n",
      "Epoch 100 batch 60 train Loss 104.9409 test Loss 45.1680 with MSE metric 31961.6266\n",
      "Epoch 100 batch 70 train Loss 104.9012 test Loss 45.1521 with MSE metric 31952.1451\n",
      "Epoch 100 batch 80 train Loss 104.8615 test Loss 45.1363 with MSE metric 31942.5863\n",
      "Epoch 100 batch 90 train Loss 104.8218 test Loss 45.1205 with MSE metric 31933.0147\n",
      "Epoch 100 batch 100 train Loss 104.7822 test Loss 45.1047 with MSE metric 31923.4367\n",
      "Epoch 100 batch 110 train Loss 104.7426 test Loss 45.0888 with MSE metric 31913.7974\n",
      "Epoch 100 batch 120 train Loss 104.7030 test Loss 45.0731 with MSE metric 31904.1441\n",
      "Epoch 100 batch 130 train Loss 104.6635 test Loss 45.0573 with MSE metric 31894.5162\n",
      "Epoch 100 batch 140 train Loss 104.6240 test Loss 45.0415 with MSE metric 31884.9041\n",
      "Epoch 100 batch 150 train Loss 104.5845 test Loss 45.0258 with MSE metric 31875.4649\n",
      "Epoch 100 batch 160 train Loss 104.5451 test Loss 45.0101 with MSE metric 31865.8413\n",
      "Epoch 100 batch 170 train Loss 104.5056 test Loss 44.9943 with MSE metric 31856.3958\n",
      "Epoch 100 batch 180 train Loss 104.4663 test Loss 44.9786 with MSE metric 31846.6413\n",
      "Epoch 100 batch 190 train Loss 104.4269 test Loss 44.9629 with MSE metric 31836.9411\n",
      "Epoch 100 batch 200 train Loss 104.3876 test Loss 44.9472 with MSE metric 31827.2294\n",
      "Epoch 100 batch 210 train Loss 104.3483 test Loss 44.9316 with MSE metric 31817.8270\n",
      "Epoch 100 batch 220 train Loss 104.3090 test Loss 44.9159 with MSE metric 31808.2092\n",
      "Epoch 100 batch 230 train Loss 104.2698 test Loss 44.9003 with MSE metric 31798.7636\n",
      "Epoch 100 batch 240 train Loss 104.2306 test Loss 44.8846 with MSE metric 31789.2764\n",
      "Time taken for 1 epoch: 28.345388889312744 secs\n",
      "\n",
      "Epoch 101 batch 0 train Loss 104.1915 test Loss 44.8690 with MSE metric 31779.8286\n",
      "Epoch 101 batch 10 train Loss 104.1523 test Loss 44.8534 with MSE metric 31770.3639\n",
      "Epoch 101 batch 20 train Loss 104.1132 test Loss 44.8378 with MSE metric 31760.8435\n",
      "Epoch 101 batch 30 train Loss 104.0741 test Loss 44.8222 with MSE metric 31751.4755\n",
      "Epoch 101 batch 40 train Loss 104.0351 test Loss 44.8067 with MSE metric 31742.0831\n",
      "Epoch 101 batch 50 train Loss 103.9961 test Loss 44.7911 with MSE metric 31732.8216\n",
      "Epoch 101 batch 60 train Loss 103.9571 test Loss 44.7756 with MSE metric 31723.4606\n",
      "Epoch 101 batch 70 train Loss 103.9182 test Loss 44.7600 with MSE metric 31713.9932\n",
      "Epoch 101 batch 80 train Loss 103.8793 test Loss 44.7445 with MSE metric 31704.8389\n",
      "Epoch 101 batch 90 train Loss 103.8404 test Loss 44.7290 with MSE metric 31695.4479\n",
      "Epoch 101 batch 100 train Loss 103.8015 test Loss 44.7135 with MSE metric 31686.1608\n",
      "Epoch 101 batch 110 train Loss 103.7627 test Loss 44.6980 with MSE metric 31676.7818\n",
      "Epoch 101 batch 120 train Loss 103.7239 test Loss 44.6826 with MSE metric 31667.4839\n",
      "Epoch 101 batch 130 train Loss 103.6851 test Loss 44.6671 with MSE metric 31658.0496\n",
      "Epoch 101 batch 140 train Loss 103.6464 test Loss 44.6516 with MSE metric 31648.7621\n",
      "Epoch 101 batch 150 train Loss 103.6077 test Loss 44.6362 with MSE metric 31639.4699\n",
      "Epoch 101 batch 160 train Loss 103.5690 test Loss 44.6208 with MSE metric 31630.1875\n",
      "Epoch 101 batch 170 train Loss 103.5303 test Loss 44.6054 with MSE metric 31620.8964\n",
      "Epoch 101 batch 180 train Loss 103.4917 test Loss 44.5900 with MSE metric 31611.6224\n",
      "Epoch 101 batch 190 train Loss 103.4532 test Loss 44.5746 with MSE metric 31602.3237\n",
      "Epoch 101 batch 200 train Loss 103.4146 test Loss 44.5592 with MSE metric 31593.0273\n",
      "Epoch 101 batch 210 train Loss 103.3761 test Loss 44.5438 with MSE metric 31583.6892\n",
      "Epoch 101 batch 220 train Loss 103.3376 test Loss 44.5284 with MSE metric 31574.4725\n",
      "Epoch 101 batch 230 train Loss 103.2991 test Loss 44.5130 with MSE metric 31565.0084\n",
      "Epoch 101 batch 240 train Loss 103.2607 test Loss 44.4977 with MSE metric 31555.6931\n",
      "Time taken for 1 epoch: 28.568345069885254 secs\n",
      "\n",
      "Epoch 102 batch 0 train Loss 103.2223 test Loss 44.4824 with MSE metric 31546.3805\n",
      "Epoch 102 batch 10 train Loss 103.1839 test Loss 44.4671 with MSE metric 31537.1168\n",
      "Epoch 102 batch 20 train Loss 103.1455 test Loss 44.4518 with MSE metric 31527.7975\n",
      "Epoch 102 batch 30 train Loss 103.1072 test Loss 44.4365 with MSE metric 31518.7101\n",
      "Epoch 102 batch 40 train Loss 103.0690 test Loss 44.4213 with MSE metric 31509.4590\n",
      "Epoch 102 batch 50 train Loss 103.0307 test Loss 44.4061 with MSE metric 31500.3524\n",
      "Epoch 102 batch 60 train Loss 102.9925 test Loss 44.3908 with MSE metric 31491.1629\n",
      "Epoch 102 batch 70 train Loss 102.9543 test Loss 44.3756 with MSE metric 31482.0019\n",
      "Epoch 102 batch 80 train Loss 102.9161 test Loss 44.3603 with MSE metric 31472.8927\n",
      "Epoch 102 batch 90 train Loss 102.8780 test Loss 44.3451 with MSE metric 31463.6955\n",
      "Epoch 102 batch 100 train Loss 102.8399 test Loss 44.3299 with MSE metric 31454.4179\n",
      "Epoch 102 batch 110 train Loss 102.8018 test Loss 44.3147 with MSE metric 31445.1865\n",
      "Epoch 102 batch 120 train Loss 102.7638 test Loss 44.2996 with MSE metric 31436.0038\n",
      "Epoch 102 batch 130 train Loss 102.7258 test Loss 44.2844 with MSE metric 31426.8498\n",
      "Epoch 102 batch 140 train Loss 102.6878 test Loss 44.2692 with MSE metric 31417.7261\n",
      "Epoch 102 batch 150 train Loss 102.6498 test Loss 44.2541 with MSE metric 31408.5776\n",
      "Epoch 102 batch 160 train Loss 102.6119 test Loss 44.2390 with MSE metric 31399.2372\n",
      "Epoch 102 batch 170 train Loss 102.5740 test Loss 44.2239 with MSE metric 31390.2470\n",
      "Epoch 102 batch 180 train Loss 102.5362 test Loss 44.2088 with MSE metric 31381.2524\n",
      "Epoch 102 batch 190 train Loss 102.4983 test Loss 44.1937 with MSE metric 31372.1437\n",
      "Epoch 102 batch 200 train Loss 102.4605 test Loss 44.1786 with MSE metric 31363.2301\n",
      "Epoch 102 batch 210 train Loss 102.4228 test Loss 44.1635 with MSE metric 31354.1353\n",
      "Epoch 102 batch 220 train Loss 102.3850 test Loss 44.1484 with MSE metric 31345.1429\n",
      "Epoch 102 batch 230 train Loss 102.3473 test Loss 44.1334 with MSE metric 31336.0870\n",
      "Epoch 102 batch 240 train Loss 102.3096 test Loss 44.1183 with MSE metric 31327.0829\n",
      "Time taken for 1 epoch: 26.883676052093506 secs\n",
      "\n",
      "Epoch 103 batch 0 train Loss 102.2719 test Loss 44.1033 with MSE metric 31317.8870\n",
      "Epoch 103 batch 10 train Loss 102.2343 test Loss 44.0883 with MSE metric 31308.8518\n",
      "Epoch 103 batch 20 train Loss 102.1967 test Loss 44.0733 with MSE metric 31299.8989\n",
      "Epoch 103 batch 30 train Loss 102.1591 test Loss 44.0583 with MSE metric 31290.6824\n",
      "Epoch 103 batch 40 train Loss 102.1216 test Loss 44.0434 with MSE metric 31281.7444\n",
      "Epoch 103 batch 50 train Loss 102.0841 test Loss 44.0284 with MSE metric 31272.7202\n",
      "Epoch 103 batch 60 train Loss 102.0466 test Loss 44.0134 with MSE metric 31263.6752\n",
      "Epoch 103 batch 70 train Loss 102.0092 test Loss 43.9985 with MSE metric 31254.6553\n",
      "Epoch 103 batch 80 train Loss 101.9717 test Loss 43.9836 with MSE metric 31245.6754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103 batch 90 train Loss 101.9343 test Loss 43.9686 with MSE metric 31236.6261\n",
      "Epoch 103 batch 100 train Loss 101.8970 test Loss 43.9537 with MSE metric 31227.6646\n",
      "Epoch 103 batch 110 train Loss 101.8596 test Loss 43.9388 with MSE metric 31218.5012\n",
      "Epoch 103 batch 120 train Loss 101.8223 test Loss 43.9239 with MSE metric 31209.4002\n",
      "Epoch 103 batch 130 train Loss 101.7850 test Loss 43.9091 with MSE metric 31200.4025\n",
      "Epoch 103 batch 140 train Loss 101.7478 test Loss 43.8942 with MSE metric 31191.6758\n",
      "Epoch 103 batch 150 train Loss 101.7106 test Loss 43.8793 with MSE metric 31182.8135\n",
      "Epoch 103 batch 160 train Loss 101.6734 test Loss 43.8645 with MSE metric 31173.9150\n",
      "Epoch 103 batch 170 train Loss 101.6362 test Loss 43.8497 with MSE metric 31164.8827\n",
      "Epoch 103 batch 180 train Loss 101.5991 test Loss 43.8349 with MSE metric 31155.7066\n",
      "Epoch 103 batch 190 train Loss 101.5619 test Loss 43.8201 with MSE metric 31146.6242\n",
      "Epoch 103 batch 200 train Loss 101.5249 test Loss 43.8052 with MSE metric 31137.6494\n",
      "Epoch 103 batch 210 train Loss 101.4878 test Loss 43.7905 with MSE metric 31128.4839\n",
      "Epoch 103 batch 220 train Loss 101.4508 test Loss 43.7757 with MSE metric 31119.6643\n",
      "Epoch 103 batch 230 train Loss 101.4138 test Loss 43.7609 with MSE metric 31110.8313\n",
      "Epoch 103 batch 240 train Loss 101.3768 test Loss 43.7462 with MSE metric 31102.0709\n",
      "Time taken for 1 epoch: 28.16719889640808 secs\n",
      "\n",
      "Epoch 104 batch 0 train Loss 101.3399 test Loss 43.7314 with MSE metric 31093.2774\n",
      "Epoch 104 batch 10 train Loss 101.3030 test Loss 43.7167 with MSE metric 31084.2834\n",
      "Epoch 104 batch 20 train Loss 101.2661 test Loss 43.7020 with MSE metric 31075.2284\n",
      "Epoch 104 batch 30 train Loss 101.2292 test Loss 43.6873 with MSE metric 31066.1193\n",
      "Epoch 104 batch 40 train Loss 101.1924 test Loss 43.6726 with MSE metric 31057.2219\n",
      "Epoch 104 batch 50 train Loss 101.1556 test Loss 43.6579 with MSE metric 31048.2675\n",
      "Epoch 104 batch 60 train Loss 101.1188 test Loss 43.6432 with MSE metric 31039.4539\n",
      "Epoch 104 batch 70 train Loss 101.0821 test Loss 43.6285 with MSE metric 31030.7132\n",
      "Epoch 104 batch 80 train Loss 101.0454 test Loss 43.6139 with MSE metric 31021.8059\n",
      "Epoch 104 batch 90 train Loss 101.0087 test Loss 43.5992 with MSE metric 31012.9501\n",
      "Epoch 104 batch 100 train Loss 100.9720 test Loss 43.5846 with MSE metric 31004.0649\n",
      "Epoch 104 batch 110 train Loss 100.9354 test Loss 43.5700 with MSE metric 30995.2820\n",
      "Epoch 104 batch 120 train Loss 100.8988 test Loss 43.5554 with MSE metric 30986.4021\n",
      "Epoch 104 batch 130 train Loss 100.8622 test Loss 43.5408 with MSE metric 30977.5887\n",
      "Epoch 104 batch 140 train Loss 100.8257 test Loss 43.5262 with MSE metric 30968.5798\n",
      "Epoch 104 batch 150 train Loss 100.7892 test Loss 43.5117 with MSE metric 30959.7253\n",
      "Epoch 104 batch 160 train Loss 100.7527 test Loss 43.4971 with MSE metric 30950.8943\n",
      "Epoch 104 batch 170 train Loss 100.7162 test Loss 43.4825 with MSE metric 30941.9972\n",
      "Epoch 104 batch 180 train Loss 100.6798 test Loss 43.4680 with MSE metric 30933.4659\n",
      "Epoch 104 batch 190 train Loss 100.6434 test Loss 43.4535 with MSE metric 30924.5675\n",
      "Epoch 104 batch 200 train Loss 100.6070 test Loss 43.4390 with MSE metric 30915.5761\n",
      "Epoch 104 batch 210 train Loss 100.5707 test Loss 43.4244 with MSE metric 30906.6666\n",
      "Epoch 104 batch 220 train Loss 100.5343 test Loss 43.4099 with MSE metric 30897.8219\n",
      "Epoch 104 batch 230 train Loss 100.4980 test Loss 43.3955 with MSE metric 30889.0290\n",
      "Epoch 104 batch 240 train Loss 100.4618 test Loss 43.3810 with MSE metric 30880.3710\n",
      "Time taken for 1 epoch: 31.459469079971313 secs\n",
      "\n",
      "Epoch 105 batch 0 train Loss 100.4256 test Loss 43.3665 with MSE metric 30871.5848\n",
      "Epoch 105 batch 10 train Loss 100.3893 test Loss 43.3521 with MSE metric 30862.9798\n",
      "Epoch 105 batch 20 train Loss 100.3532 test Loss 43.3376 with MSE metric 30854.2197\n",
      "Epoch 105 batch 30 train Loss 100.3170 test Loss 43.3232 with MSE metric 30845.4978\n",
      "Epoch 105 batch 40 train Loss 100.2809 test Loss 43.3088 with MSE metric 30836.7468\n",
      "Epoch 105 batch 50 train Loss 100.2448 test Loss 43.2944 with MSE metric 30828.1666\n",
      "Epoch 105 batch 60 train Loss 100.2087 test Loss 43.2800 with MSE metric 30819.3921\n",
      "Epoch 105 batch 70 train Loss 100.1727 test Loss 43.2656 with MSE metric 30810.6060\n",
      "Epoch 105 batch 80 train Loss 100.1366 test Loss 43.2513 with MSE metric 30801.8555\n",
      "Epoch 105 batch 90 train Loss 100.1007 test Loss 43.2369 with MSE metric 30793.1158\n",
      "Epoch 105 batch 100 train Loss 100.0647 test Loss 43.2225 with MSE metric 30784.3836\n",
      "Epoch 105 batch 110 train Loss 100.0288 test Loss 43.2082 with MSE metric 30775.7465\n",
      "Epoch 105 batch 120 train Loss 99.9928 test Loss 43.1939 with MSE metric 30767.1545\n",
      "Epoch 105 batch 130 train Loss 99.9570 test Loss 43.1795 with MSE metric 30758.5421\n",
      "Epoch 105 batch 140 train Loss 99.9211 test Loss 43.1652 with MSE metric 30749.9958\n",
      "Epoch 105 batch 150 train Loss 99.8853 test Loss 43.1509 with MSE metric 30741.3122\n",
      "Epoch 105 batch 160 train Loss 99.8495 test Loss 43.1366 with MSE metric 30732.6425\n",
      "Epoch 105 batch 170 train Loss 99.8137 test Loss 43.1223 with MSE metric 30724.2721\n",
      "Epoch 105 batch 180 train Loss 99.7780 test Loss 43.1081 with MSE metric 30715.7169\n",
      "Epoch 105 batch 190 train Loss 99.7423 test Loss 43.0938 with MSE metric 30707.1053\n",
      "Epoch 105 batch 200 train Loss 99.7066 test Loss 43.0796 with MSE metric 30698.6924\n",
      "Epoch 105 batch 210 train Loss 99.6709 test Loss 43.0653 with MSE metric 30690.0142\n",
      "Epoch 105 batch 220 train Loss 99.6353 test Loss 43.0511 with MSE metric 30681.3904\n",
      "Epoch 105 batch 230 train Loss 99.5997 test Loss 43.0369 with MSE metric 30673.0027\n",
      "Epoch 105 batch 240 train Loss 99.5641 test Loss 43.0227 with MSE metric 30664.2181\n",
      "Time taken for 1 epoch: 31.871758937835693 secs\n",
      "\n",
      "Epoch 106 batch 0 train Loss 99.5285 test Loss 43.0086 with MSE metric 30655.7611\n",
      "Epoch 106 batch 10 train Loss 99.4930 test Loss 42.9944 with MSE metric 30647.2913\n",
      "Epoch 106 batch 20 train Loss 99.4575 test Loss 42.9802 with MSE metric 30638.6837\n",
      "Epoch 106 batch 30 train Loss 99.4220 test Loss 42.9660 with MSE metric 30630.0998\n",
      "Epoch 106 batch 40 train Loss 99.3866 test Loss 42.9519 with MSE metric 30621.5408\n",
      "Epoch 106 batch 50 train Loss 99.3511 test Loss 42.9378 with MSE metric 30612.9393\n",
      "Epoch 106 batch 60 train Loss 99.3157 test Loss 42.9236 with MSE metric 30604.4384\n",
      "Epoch 106 batch 70 train Loss 99.2804 test Loss 42.9095 with MSE metric 30595.9048\n",
      "Epoch 106 batch 80 train Loss 99.2450 test Loss 42.8954 with MSE metric 30587.2775\n",
      "Epoch 106 batch 90 train Loss 99.2097 test Loss 42.8813 with MSE metric 30578.6598\n",
      "Epoch 106 batch 100 train Loss 99.1744 test Loss 42.8672 with MSE metric 30570.2668\n",
      "Epoch 106 batch 110 train Loss 99.1392 test Loss 42.8531 with MSE metric 30561.5728\n",
      "Epoch 106 batch 120 train Loss 99.1039 test Loss 42.8391 with MSE metric 30553.1799\n",
      "Epoch 106 batch 130 train Loss 99.0687 test Loss 42.8250 with MSE metric 30544.8469\n",
      "Epoch 106 batch 140 train Loss 99.0335 test Loss 42.8110 with MSE metric 30536.3526\n",
      "Epoch 106 batch 150 train Loss 98.9984 test Loss 42.7969 with MSE metric 30527.8888\n",
      "Epoch 106 batch 160 train Loss 98.9632 test Loss 42.7829 with MSE metric 30519.6177\n",
      "Epoch 106 batch 170 train Loss 98.9281 test Loss 42.7689 with MSE metric 30511.1145\n",
      "Epoch 106 batch 180 train Loss 98.8931 test Loss 42.7549 with MSE metric 30502.6761\n",
      "Epoch 106 batch 190 train Loss 98.8580 test Loss 42.7409 with MSE metric 30494.2303\n",
      "Epoch 106 batch 200 train Loss 98.8230 test Loss 42.7269 with MSE metric 30485.6727\n",
      "Epoch 106 batch 210 train Loss 98.7880 test Loss 42.7129 with MSE metric 30477.1718\n",
      "Epoch 106 batch 220 train Loss 98.7530 test Loss 42.6990 with MSE metric 30468.6497\n",
      "Epoch 106 batch 230 train Loss 98.7181 test Loss 42.6850 with MSE metric 30460.1800\n",
      "Epoch 106 batch 240 train Loss 98.6831 test Loss 42.6711 with MSE metric 30451.7517\n",
      "Time taken for 1 epoch: 25.96045684814453 secs\n",
      "\n",
      "Epoch 107 batch 0 train Loss 98.6482 test Loss 42.6572 with MSE metric 30443.2720\n",
      "Epoch 107 batch 10 train Loss 98.6134 test Loss 42.6432 with MSE metric 30434.6876\n",
      "Epoch 107 batch 20 train Loss 98.5785 test Loss 42.6293 with MSE metric 30426.2550\n",
      "Epoch 107 batch 30 train Loss 98.5437 test Loss 42.6154 with MSE metric 30417.6232\n",
      "Epoch 107 batch 40 train Loss 98.5089 test Loss 42.6015 with MSE metric 30409.2313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 batch 50 train Loss 98.4741 test Loss 42.5876 with MSE metric 30400.7129\n",
      "Epoch 107 batch 60 train Loss 98.4394 test Loss 42.5738 with MSE metric 30392.4174\n",
      "Epoch 107 batch 70 train Loss 98.4047 test Loss 42.5599 with MSE metric 30383.9689\n",
      "Epoch 107 batch 80 train Loss 98.3700 test Loss 42.5461 with MSE metric 30375.6375\n",
      "Epoch 107 batch 90 train Loss 98.3353 test Loss 42.5322 with MSE metric 30367.4440\n",
      "Epoch 107 batch 100 train Loss 98.3007 test Loss 42.5185 with MSE metric 30358.9690\n",
      "Epoch 107 batch 110 train Loss 98.2661 test Loss 42.5046 with MSE metric 30350.3764\n",
      "Epoch 107 batch 120 train Loss 98.2315 test Loss 42.4908 with MSE metric 30341.9188\n",
      "Epoch 107 batch 130 train Loss 98.1969 test Loss 42.4770 with MSE metric 30333.7240\n",
      "Epoch 107 batch 140 train Loss 98.1624 test Loss 42.4632 with MSE metric 30325.4446\n",
      "Epoch 107 batch 150 train Loss 98.1279 test Loss 42.4495 with MSE metric 30317.0796\n",
      "Epoch 107 batch 160 train Loss 98.0934 test Loss 42.4357 with MSE metric 30308.8086\n",
      "Epoch 107 batch 170 train Loss 98.0590 test Loss 42.4220 with MSE metric 30300.4248\n",
      "Epoch 107 batch 180 train Loss 98.0246 test Loss 42.4083 with MSE metric 30292.1254\n",
      "Epoch 107 batch 190 train Loss 97.9902 test Loss 42.3945 with MSE metric 30283.8325\n",
      "Epoch 107 batch 200 train Loss 97.9558 test Loss 42.3808 with MSE metric 30275.4033\n",
      "Epoch 107 batch 210 train Loss 97.9214 test Loss 42.3671 with MSE metric 30267.2297\n",
      "Epoch 107 batch 220 train Loss 97.8871 test Loss 42.3534 with MSE metric 30259.1369\n",
      "Epoch 107 batch 230 train Loss 97.8528 test Loss 42.3397 with MSE metric 30250.9327\n",
      "Epoch 107 batch 240 train Loss 97.8185 test Loss 42.3260 with MSE metric 30242.6013\n",
      "Time taken for 1 epoch: 29.344777822494507 secs\n",
      "\n",
      "Epoch 108 batch 0 train Loss 97.7843 test Loss 42.3124 with MSE metric 30234.2650\n",
      "Epoch 108 batch 10 train Loss 97.7500 test Loss 42.2987 with MSE metric 30225.9878\n",
      "Epoch 108 batch 20 train Loss 97.7159 test Loss 42.2851 with MSE metric 30217.7733\n",
      "Epoch 108 batch 30 train Loss 97.6817 test Loss 42.2714 with MSE metric 30209.6196\n",
      "Epoch 108 batch 40 train Loss 97.6475 test Loss 42.2578 with MSE metric 30201.3845\n",
      "Epoch 108 batch 50 train Loss 97.6134 test Loss 42.2442 with MSE metric 30193.2195\n",
      "Epoch 108 batch 60 train Loss 97.5793 test Loss 42.2306 with MSE metric 30184.7769\n",
      "Epoch 108 batch 70 train Loss 97.5452 test Loss 42.2170 with MSE metric 30176.7613\n",
      "Epoch 108 batch 80 train Loss 97.5112 test Loss 42.2034 with MSE metric 30168.6435\n",
      "Epoch 108 batch 90 train Loss 97.4772 test Loss 42.1898 with MSE metric 30160.4986\n",
      "Epoch 108 batch 100 train Loss 97.4432 test Loss 42.1763 with MSE metric 30152.3472\n",
      "Epoch 108 batch 110 train Loss 97.4092 test Loss 42.1627 with MSE metric 30144.1904\n",
      "Epoch 108 batch 120 train Loss 97.3752 test Loss 42.1492 with MSE metric 30135.9719\n",
      "Epoch 108 batch 130 train Loss 97.3413 test Loss 42.1356 with MSE metric 30127.7147\n",
      "Epoch 108 batch 140 train Loss 97.3074 test Loss 42.1221 with MSE metric 30119.6628\n",
      "Epoch 108 batch 150 train Loss 97.2735 test Loss 42.1086 with MSE metric 30111.4765\n",
      "Epoch 108 batch 160 train Loss 97.2397 test Loss 42.0951 with MSE metric 30103.3353\n",
      "Epoch 108 batch 170 train Loss 97.2059 test Loss 42.0816 with MSE metric 30095.0450\n",
      "Epoch 108 batch 180 train Loss 97.1721 test Loss 42.0681 with MSE metric 30086.8442\n",
      "Epoch 108 batch 190 train Loss 97.1383 test Loss 42.0546 with MSE metric 30078.7788\n",
      "Epoch 108 batch 200 train Loss 97.1045 test Loss 42.0411 with MSE metric 30070.7100\n",
      "Epoch 108 batch 210 train Loss 97.0708 test Loss 42.0277 with MSE metric 30062.4680\n",
      "Epoch 108 batch 220 train Loss 97.0371 test Loss 42.0142 with MSE metric 30054.3321\n",
      "Epoch 108 batch 230 train Loss 97.0034 test Loss 42.0008 with MSE metric 30046.0623\n",
      "Epoch 108 batch 240 train Loss 96.9698 test Loss 41.9874 with MSE metric 30037.9212\n",
      "Time taken for 1 epoch: 28.47303080558777 secs\n",
      "\n",
      "Epoch 109 batch 0 train Loss 96.9362 test Loss 41.9739 with MSE metric 30029.7258\n",
      "Epoch 109 batch 10 train Loss 96.9026 test Loss 41.9605 with MSE metric 30021.5687\n",
      "Epoch 109 batch 20 train Loss 96.8690 test Loss 41.9472 with MSE metric 30013.5248\n",
      "Epoch 109 batch 30 train Loss 96.8354 test Loss 41.9338 with MSE metric 30005.3815\n",
      "Epoch 109 batch 40 train Loss 96.8019 test Loss 41.9204 with MSE metric 29997.2887\n",
      "Epoch 109 batch 50 train Loss 96.7684 test Loss 41.9070 with MSE metric 29989.0086\n",
      "Epoch 109 batch 60 train Loss 96.7349 test Loss 41.8937 with MSE metric 29980.9757\n",
      "Epoch 109 batch 70 train Loss 96.7015 test Loss 41.8803 with MSE metric 29972.9636\n",
      "Epoch 109 batch 80 train Loss 96.6680 test Loss 41.8670 with MSE metric 29964.7830\n",
      "Epoch 109 batch 90 train Loss 96.6346 test Loss 41.8537 with MSE metric 29956.7362\n",
      "Epoch 109 batch 100 train Loss 96.6012 test Loss 41.8403 with MSE metric 29948.4809\n",
      "Epoch 109 batch 110 train Loss 96.5679 test Loss 41.8270 with MSE metric 29940.4427\n",
      "Epoch 109 batch 120 train Loss 96.5346 test Loss 41.8137 with MSE metric 29932.5083\n",
      "Epoch 109 batch 130 train Loss 96.5013 test Loss 41.8005 with MSE metric 29924.4516\n",
      "Epoch 109 batch 140 train Loss 96.4680 test Loss 41.7872 with MSE metric 29916.3992\n",
      "Epoch 109 batch 150 train Loss 96.4347 test Loss 41.7739 with MSE metric 29908.5752\n",
      "Epoch 109 batch 160 train Loss 96.4015 test Loss 41.7606 with MSE metric 29900.5185\n",
      "Epoch 109 batch 170 train Loss 96.3683 test Loss 41.7474 with MSE metric 29892.3733\n",
      "Epoch 109 batch 180 train Loss 96.3351 test Loss 41.7341 with MSE metric 29884.4898\n",
      "Epoch 109 batch 190 train Loss 96.3019 test Loss 41.7209 with MSE metric 29876.4783\n",
      "Epoch 109 batch 200 train Loss 96.2688 test Loss 41.7076 with MSE metric 29868.4907\n",
      "Epoch 109 batch 210 train Loss 96.2357 test Loss 41.6944 with MSE metric 29860.5009\n",
      "Epoch 109 batch 220 train Loss 96.2026 test Loss 41.6812 with MSE metric 29852.3500\n",
      "Epoch 109 batch 230 train Loss 96.1695 test Loss 41.6680 with MSE metric 29844.2969\n",
      "Epoch 109 batch 240 train Loss 96.1365 test Loss 41.6548 with MSE metric 29836.4909\n",
      "Time taken for 1 epoch: 28.596256017684937 secs\n",
      "\n",
      "Epoch 110 batch 0 train Loss 96.1035 test Loss 41.6416 with MSE metric 29828.4297\n",
      "Epoch 110 batch 10 train Loss 96.0705 test Loss 41.6284 with MSE metric 29820.5561\n",
      "Epoch 110 batch 20 train Loss 96.0375 test Loss 41.6153 with MSE metric 29812.6112\n",
      "Epoch 110 batch 30 train Loss 96.0046 test Loss 41.6021 with MSE metric 29804.5617\n",
      "Epoch 110 batch 40 train Loss 95.9716 test Loss 41.5890 with MSE metric 29796.6041\n",
      "Epoch 110 batch 50 train Loss 95.9388 test Loss 41.5759 with MSE metric 29788.7088\n",
      "Epoch 110 batch 60 train Loss 95.9059 test Loss 41.5628 with MSE metric 29780.7663\n",
      "Epoch 110 batch 70 train Loss 95.8730 test Loss 41.5496 with MSE metric 29772.7973\n",
      "Epoch 110 batch 80 train Loss 95.8402 test Loss 41.5365 with MSE metric 29764.7601\n",
      "Epoch 110 batch 90 train Loss 95.8074 test Loss 41.5234 with MSE metric 29756.8775\n",
      "Epoch 110 batch 100 train Loss 95.7746 test Loss 41.5103 with MSE metric 29748.9354\n",
      "Epoch 110 batch 110 train Loss 95.7419 test Loss 41.4972 with MSE metric 29740.9911\n",
      "Epoch 110 batch 120 train Loss 95.7092 test Loss 41.4842 with MSE metric 29733.1425\n",
      "Epoch 110 batch 130 train Loss 95.6765 test Loss 41.4711 with MSE metric 29725.2007\n",
      "Epoch 110 batch 140 train Loss 95.6438 test Loss 41.4581 with MSE metric 29717.4110\n",
      "Epoch 110 batch 150 train Loss 95.6111 test Loss 41.4451 with MSE metric 29709.3724\n",
      "Epoch 110 batch 160 train Loss 95.5785 test Loss 41.4320 with MSE metric 29701.3236\n",
      "Epoch 110 batch 170 train Loss 95.5458 test Loss 41.4190 with MSE metric 29693.3444\n",
      "Epoch 110 batch 180 train Loss 95.5133 test Loss 41.4060 with MSE metric 29685.3438\n",
      "Epoch 110 batch 190 train Loss 95.4807 test Loss 41.3930 with MSE metric 29677.4680\n",
      "Epoch 110 batch 200 train Loss 95.4481 test Loss 41.3801 with MSE metric 29669.5388\n",
      "Epoch 110 batch 210 train Loss 95.4156 test Loss 41.3671 with MSE metric 29661.3802\n",
      "Epoch 110 batch 220 train Loss 95.3831 test Loss 41.3541 with MSE metric 29653.6001\n",
      "Epoch 110 batch 230 train Loss 95.3506 test Loss 41.3412 with MSE metric 29645.7745\n",
      "Epoch 110 batch 240 train Loss 95.3182 test Loss 41.3282 with MSE metric 29637.8652\n",
      "Time taken for 1 epoch: 28.423455953598022 secs\n",
      "\n",
      "Epoch 111 batch 0 train Loss 95.2858 test Loss 41.3153 with MSE metric 29630.0407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111 batch 10 train Loss 95.2533 test Loss 41.3024 with MSE metric 29622.2102\n",
      "Epoch 111 batch 20 train Loss 95.2210 test Loss 41.2894 with MSE metric 29614.4415\n",
      "Epoch 111 batch 30 train Loss 95.1886 test Loss 41.2765 with MSE metric 29606.6714\n",
      "Epoch 111 batch 40 train Loss 95.1563 test Loss 41.2636 with MSE metric 29598.9304\n",
      "Epoch 111 batch 50 train Loss 95.1240 test Loss 41.2508 with MSE metric 29591.1642\n",
      "Epoch 111 batch 60 train Loss 95.0917 test Loss 41.2379 with MSE metric 29583.3331\n",
      "Epoch 111 batch 70 train Loss 95.0594 test Loss 41.2250 with MSE metric 29575.4821\n",
      "Epoch 111 batch 80 train Loss 95.0272 test Loss 41.2121 with MSE metric 29567.6766\n",
      "Epoch 111 batch 90 train Loss 94.9949 test Loss 41.1992 with MSE metric 29559.7282\n",
      "Epoch 111 batch 100 train Loss 94.9627 test Loss 41.1864 with MSE metric 29551.9263\n",
      "Epoch 111 batch 110 train Loss 94.9306 test Loss 41.1736 with MSE metric 29543.9535\n",
      "Epoch 111 batch 120 train Loss 94.8984 test Loss 41.1608 with MSE metric 29536.1938\n",
      "Epoch 111 batch 130 train Loss 94.8663 test Loss 41.1480 with MSE metric 29528.5472\n",
      "Epoch 111 batch 140 train Loss 94.8342 test Loss 41.1351 with MSE metric 29520.8191\n",
      "Epoch 111 batch 150 train Loss 94.8021 test Loss 41.1223 with MSE metric 29513.1049\n",
      "Epoch 111 batch 160 train Loss 94.7701 test Loss 41.1095 with MSE metric 29505.3819\n",
      "Epoch 111 batch 170 train Loss 94.7380 test Loss 41.0968 with MSE metric 29497.6674\n",
      "Epoch 111 batch 180 train Loss 94.7060 test Loss 41.0840 with MSE metric 29489.8868\n",
      "Epoch 111 batch 190 train Loss 94.6740 test Loss 41.0713 with MSE metric 29482.0090\n",
      "Epoch 111 batch 200 train Loss 94.6421 test Loss 41.0585 with MSE metric 29474.4096\n",
      "Epoch 111 batch 210 train Loss 94.6101 test Loss 41.0458 with MSE metric 29466.7602\n",
      "Epoch 111 batch 220 train Loss 94.5782 test Loss 41.0330 with MSE metric 29458.8888\n",
      "Epoch 111 batch 230 train Loss 94.5463 test Loss 41.0203 with MSE metric 29451.0124\n",
      "Epoch 111 batch 240 train Loss 94.5145 test Loss 41.0076 with MSE metric 29443.3682\n",
      "Time taken for 1 epoch: 30.190441846847534 secs\n",
      "\n",
      "Epoch 112 batch 0 train Loss 94.4826 test Loss 40.9948 with MSE metric 29435.7371\n",
      "Epoch 112 batch 10 train Loss 94.4508 test Loss 40.9821 with MSE metric 29428.0271\n",
      "Epoch 112 batch 20 train Loss 94.4190 test Loss 40.9694 with MSE metric 29420.2958\n",
      "Epoch 112 batch 30 train Loss 94.3872 test Loss 40.9567 with MSE metric 29412.7139\n",
      "Epoch 112 batch 40 train Loss 94.3555 test Loss 40.9440 with MSE metric 29405.0170\n",
      "Epoch 112 batch 50 train Loss 94.3237 test Loss 40.9314 with MSE metric 29397.1995\n",
      "Epoch 112 batch 60 train Loss 94.2920 test Loss 40.9187 with MSE metric 29389.6868\n",
      "Epoch 112 batch 70 train Loss 94.2603 test Loss 40.9060 with MSE metric 29382.0077\n",
      "Epoch 112 batch 80 train Loss 94.2286 test Loss 40.8934 with MSE metric 29374.4096\n",
      "Epoch 112 batch 90 train Loss 94.1970 test Loss 40.8808 with MSE metric 29366.8892\n",
      "Epoch 112 batch 100 train Loss 94.1654 test Loss 40.8682 with MSE metric 29359.2003\n",
      "Epoch 112 batch 110 train Loss 94.1338 test Loss 40.8555 with MSE metric 29351.4512\n",
      "Epoch 112 batch 120 train Loss 94.1022 test Loss 40.8429 with MSE metric 29343.8705\n",
      "Epoch 112 batch 130 train Loss 94.0706 test Loss 40.8303 with MSE metric 29336.1849\n",
      "Epoch 112 batch 140 train Loss 94.0391 test Loss 40.8177 with MSE metric 29328.6019\n",
      "Epoch 112 batch 150 train Loss 94.0076 test Loss 40.8052 with MSE metric 29320.9299\n",
      "Epoch 112 batch 160 train Loss 93.9761 test Loss 40.7926 with MSE metric 29313.2429\n",
      "Epoch 112 batch 170 train Loss 93.9446 test Loss 40.7801 with MSE metric 29305.6541\n",
      "Epoch 112 batch 180 train Loss 93.9132 test Loss 40.7676 with MSE metric 29298.1742\n",
      "Epoch 112 batch 190 train Loss 93.8818 test Loss 40.7550 with MSE metric 29290.5524\n",
      "Epoch 112 batch 200 train Loss 93.8503 test Loss 40.7425 with MSE metric 29282.7907\n",
      "Epoch 112 batch 210 train Loss 93.8190 test Loss 40.7300 with MSE metric 29275.1790\n",
      "Epoch 112 batch 220 train Loss 93.7876 test Loss 40.7174 with MSE metric 29267.5365\n",
      "Epoch 112 batch 230 train Loss 93.7563 test Loss 40.7049 with MSE metric 29259.9529\n",
      "Epoch 112 batch 240 train Loss 93.7250 test Loss 40.6924 with MSE metric 29252.4106\n",
      "Time taken for 1 epoch: 29.34632182121277 secs\n",
      "\n",
      "Epoch 113 batch 0 train Loss 93.6937 test Loss 40.6799 with MSE metric 29244.9397\n",
      "Epoch 113 batch 10 train Loss 93.6624 test Loss 40.6674 with MSE metric 29237.3839\n",
      "Epoch 113 batch 20 train Loss 93.6312 test Loss 40.6550 with MSE metric 29229.7010\n",
      "Epoch 113 batch 30 train Loss 93.5999 test Loss 40.6425 with MSE metric 29222.1353\n",
      "Epoch 113 batch 40 train Loss 93.5687 test Loss 40.6300 with MSE metric 29214.4680\n",
      "Epoch 113 batch 50 train Loss 93.5376 test Loss 40.6176 with MSE metric 29206.8698\n",
      "Epoch 113 batch 60 train Loss 93.5064 test Loss 40.6051 with MSE metric 29199.3276\n",
      "Epoch 113 batch 70 train Loss 93.4753 test Loss 40.5927 with MSE metric 29191.8893\n",
      "Epoch 113 batch 80 train Loss 93.4442 test Loss 40.5802 with MSE metric 29184.2635\n",
      "Epoch 113 batch 90 train Loss 93.4131 test Loss 40.5678 with MSE metric 29176.7537\n",
      "Epoch 113 batch 100 train Loss 93.3820 test Loss 40.5554 with MSE metric 29169.2444\n",
      "Epoch 113 batch 110 train Loss 93.3510 test Loss 40.5430 with MSE metric 29161.8714\n",
      "Epoch 113 batch 120 train Loss 93.3199 test Loss 40.5306 with MSE metric 29154.4804\n",
      "Epoch 113 batch 130 train Loss 93.2889 test Loss 40.5183 with MSE metric 29147.0185\n",
      "Epoch 113 batch 140 train Loss 93.2580 test Loss 40.5059 with MSE metric 29139.4912\n",
      "Epoch 113 batch 150 train Loss 93.2270 test Loss 40.4936 with MSE metric 29132.0288\n",
      "Epoch 113 batch 160 train Loss 93.1961 test Loss 40.4813 with MSE metric 29124.4980\n",
      "Epoch 113 batch 170 train Loss 93.1651 test Loss 40.4689 with MSE metric 29116.9954\n",
      "Epoch 113 batch 180 train Loss 93.1343 test Loss 40.4566 with MSE metric 29109.4900\n",
      "Epoch 113 batch 190 train Loss 93.1034 test Loss 40.4442 with MSE metric 29102.1488\n",
      "Epoch 113 batch 200 train Loss 93.0725 test Loss 40.4319 with MSE metric 29094.7156\n",
      "Epoch 113 batch 210 train Loss 93.0417 test Loss 40.4196 with MSE metric 29087.3633\n",
      "Epoch 113 batch 220 train Loss 93.0109 test Loss 40.4073 with MSE metric 29079.9026\n",
      "Epoch 113 batch 230 train Loss 92.9801 test Loss 40.3951 with MSE metric 29072.4350\n",
      "Epoch 113 batch 240 train Loss 92.9493 test Loss 40.3828 with MSE metric 29064.9552\n",
      "Time taken for 1 epoch: 28.517497062683105 secs\n",
      "\n",
      "Epoch 114 batch 0 train Loss 92.9186 test Loss 40.3705 with MSE metric 29057.4565\n",
      "Epoch 114 batch 10 train Loss 92.8879 test Loss 40.3583 with MSE metric 29050.0564\n",
      "Epoch 114 batch 20 train Loss 92.8572 test Loss 40.3460 with MSE metric 29042.6607\n",
      "Epoch 114 batch 30 train Loss 92.8265 test Loss 40.3338 with MSE metric 29035.2392\n",
      "Epoch 114 batch 40 train Loss 92.7958 test Loss 40.3216 with MSE metric 29027.9092\n",
      "Epoch 114 batch 50 train Loss 92.7652 test Loss 40.3094 with MSE metric 29020.5248\n",
      "Epoch 114 batch 60 train Loss 92.7346 test Loss 40.2971 with MSE metric 29013.0846\n",
      "Epoch 114 batch 70 train Loss 92.7040 test Loss 40.2849 with MSE metric 29005.7478\n",
      "Epoch 114 batch 80 train Loss 92.6734 test Loss 40.2727 with MSE metric 28998.3655\n",
      "Epoch 114 batch 90 train Loss 92.6428 test Loss 40.2605 with MSE metric 28991.0286\n",
      "Epoch 114 batch 100 train Loss 92.6123 test Loss 40.2483 with MSE metric 28983.5584\n",
      "Epoch 114 batch 110 train Loss 92.5818 test Loss 40.2361 with MSE metric 28976.0954\n",
      "Epoch 114 batch 120 train Loss 92.5513 test Loss 40.2239 with MSE metric 28968.7238\n",
      "Epoch 114 batch 130 train Loss 92.5209 test Loss 40.2118 with MSE metric 28961.4738\n",
      "Epoch 114 batch 140 train Loss 92.4904 test Loss 40.1996 with MSE metric 28954.1891\n",
      "Epoch 114 batch 150 train Loss 92.4600 test Loss 40.1875 with MSE metric 28946.9186\n",
      "Epoch 114 batch 160 train Loss 92.4296 test Loss 40.1753 with MSE metric 28939.6455\n",
      "Epoch 114 batch 170 train Loss 92.3992 test Loss 40.1632 with MSE metric 28932.2553\n",
      "Epoch 114 batch 180 train Loss 92.3689 test Loss 40.1511 with MSE metric 28924.8088\n",
      "Epoch 114 batch 190 train Loss 92.3385 test Loss 40.1390 with MSE metric 28917.4748\n",
      "Epoch 114 batch 200 train Loss 92.3082 test Loss 40.1269 with MSE metric 28910.1020\n",
      "Epoch 114 batch 210 train Loss 92.2779 test Loss 40.1148 with MSE metric 28902.7845\n",
      "Epoch 114 batch 220 train Loss 92.2476 test Loss 40.1028 with MSE metric 28895.2584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114 batch 230 train Loss 92.2174 test Loss 40.0907 with MSE metric 28887.9765\n",
      "Epoch 114 batch 240 train Loss 92.1872 test Loss 40.0786 with MSE metric 28880.7357\n",
      "Time taken for 1 epoch: 27.59711194038391 secs\n",
      "\n",
      "Epoch 115 batch 0 train Loss 92.1569 test Loss 40.0666 with MSE metric 28873.2700\n",
      "Epoch 115 batch 10 train Loss 92.1268 test Loss 40.0545 with MSE metric 28866.0110\n",
      "Epoch 115 batch 20 train Loss 92.0966 test Loss 40.0425 with MSE metric 28858.7105\n",
      "Epoch 115 batch 30 train Loss 92.0665 test Loss 40.0305 with MSE metric 28851.2848\n",
      "Epoch 115 batch 40 train Loss 92.0363 test Loss 40.0184 with MSE metric 28844.0118\n",
      "Epoch 115 batch 50 train Loss 92.0062 test Loss 40.0064 with MSE metric 28837.0103\n",
      "Epoch 115 batch 60 train Loss 91.9761 test Loss 39.9944 with MSE metric 28829.7175\n",
      "Epoch 115 batch 70 train Loss 91.9461 test Loss 39.9824 with MSE metric 28822.4194\n",
      "Epoch 115 batch 80 train Loss 91.9160 test Loss 39.9704 with MSE metric 28815.0127\n",
      "Epoch 115 batch 90 train Loss 91.8860 test Loss 39.9584 with MSE metric 28807.6541\n",
      "Epoch 115 batch 100 train Loss 91.8560 test Loss 39.9464 with MSE metric 28800.3691\n",
      "Epoch 115 batch 110 train Loss 91.8260 test Loss 39.9345 with MSE metric 28793.1162\n",
      "Epoch 115 batch 120 train Loss 91.7960 test Loss 39.9225 with MSE metric 28785.8626\n",
      "Epoch 115 batch 130 train Loss 91.7661 test Loss 39.9106 with MSE metric 28778.6108\n",
      "Epoch 115 batch 140 train Loss 91.7362 test Loss 39.8986 with MSE metric 28771.4004\n",
      "Epoch 115 batch 150 train Loss 91.7063 test Loss 39.8867 with MSE metric 28764.1281\n",
      "Epoch 115 batch 160 train Loss 91.6764 test Loss 39.8748 with MSE metric 28756.8287\n",
      "Epoch 115 batch 170 train Loss 91.6465 test Loss 39.8629 with MSE metric 28749.4918\n",
      "Epoch 115 batch 180 train Loss 91.6167 test Loss 39.8510 with MSE metric 28742.2827\n",
      "Epoch 115 batch 190 train Loss 91.5869 test Loss 39.8391 with MSE metric 28735.2714\n",
      "Epoch 115 batch 200 train Loss 91.5571 test Loss 39.8272 with MSE metric 28728.0593\n",
      "Epoch 115 batch 210 train Loss 91.5273 test Loss 39.8153 with MSE metric 28720.9207\n",
      "Epoch 115 batch 220 train Loss 91.4976 test Loss 39.8035 with MSE metric 28713.7001\n",
      "Epoch 115 batch 230 train Loss 91.4679 test Loss 39.7916 with MSE metric 28706.5123\n",
      "Epoch 115 batch 240 train Loss 91.4382 test Loss 39.7797 with MSE metric 28699.4528\n",
      "Time taken for 1 epoch: 29.247021913528442 secs\n",
      "\n",
      "Epoch 116 batch 0 train Loss 91.4085 test Loss 39.7679 with MSE metric 28692.3445\n",
      "Epoch 116 batch 10 train Loss 91.3788 test Loss 39.7560 with MSE metric 28685.2238\n",
      "Epoch 116 batch 20 train Loss 91.3492 test Loss 39.7442 with MSE metric 28678.0286\n",
      "Epoch 116 batch 30 train Loss 91.3195 test Loss 39.7324 with MSE metric 28670.7004\n",
      "Epoch 116 batch 40 train Loss 91.2899 test Loss 39.7205 with MSE metric 28663.4653\n",
      "Epoch 116 batch 50 train Loss 91.2603 test Loss 39.7087 with MSE metric 28656.3715\n",
      "Epoch 116 batch 60 train Loss 91.2308 test Loss 39.6969 with MSE metric 28649.2540\n",
      "Epoch 116 batch 70 train Loss 91.2012 test Loss 39.6851 with MSE metric 28641.9643\n",
      "Epoch 116 batch 80 train Loss 91.1717 test Loss 39.6734 with MSE metric 28634.7407\n",
      "Epoch 116 batch 90 train Loss 91.1422 test Loss 39.6616 with MSE metric 28627.6159\n",
      "Epoch 116 batch 100 train Loss 91.1127 test Loss 39.6498 with MSE metric 28620.5132\n",
      "Epoch 116 batch 110 train Loss 91.0832 test Loss 39.6381 with MSE metric 28613.2744\n",
      "Epoch 116 batch 120 train Loss 91.0538 test Loss 39.6263 with MSE metric 28606.1035\n",
      "Epoch 116 batch 130 train Loss 91.0244 test Loss 39.6146 with MSE metric 28598.9904\n",
      "Epoch 116 batch 140 train Loss 90.9949 test Loss 39.6029 with MSE metric 28591.8041\n",
      "Epoch 116 batch 150 train Loss 90.9656 test Loss 39.5912 with MSE metric 28584.5588\n",
      "Epoch 116 batch 160 train Loss 90.9362 test Loss 39.5794 with MSE metric 28577.4865\n",
      "Epoch 116 batch 170 train Loss 90.9068 test Loss 39.5677 with MSE metric 28570.1996\n",
      "Epoch 116 batch 180 train Loss 90.8775 test Loss 39.5560 with MSE metric 28563.1269\n",
      "Epoch 116 batch 190 train Loss 90.8482 test Loss 39.5443 with MSE metric 28556.1203\n",
      "Epoch 116 batch 200 train Loss 90.8189 test Loss 39.5326 with MSE metric 28548.9282\n",
      "Epoch 116 batch 210 train Loss 90.7896 test Loss 39.5209 with MSE metric 28541.7862\n",
      "Epoch 116 batch 220 train Loss 90.7604 test Loss 39.5092 with MSE metric 28534.7224\n",
      "Epoch 116 batch 230 train Loss 90.7311 test Loss 39.4975 with MSE metric 28527.6566\n",
      "Epoch 116 batch 240 train Loss 90.7019 test Loss 39.4859 with MSE metric 28520.5403\n",
      "Time taken for 1 epoch: 28.821264266967773 secs\n",
      "\n",
      "Epoch 117 batch 0 train Loss 90.6728 test Loss 39.4742 with MSE metric 28513.5551\n",
      "Epoch 117 batch 10 train Loss 90.6436 test Loss 39.4626 with MSE metric 28506.5231\n",
      "Epoch 117 batch 20 train Loss 90.6144 test Loss 39.4510 with MSE metric 28499.4034\n",
      "Epoch 117 batch 30 train Loss 90.5853 test Loss 39.4393 with MSE metric 28492.4368\n",
      "Epoch 117 batch 40 train Loss 90.5562 test Loss 39.4277 with MSE metric 28485.4032\n",
      "Epoch 117 batch 50 train Loss 90.5271 test Loss 39.4161 with MSE metric 28478.4185\n",
      "Epoch 117 batch 60 train Loss 90.4981 test Loss 39.4045 with MSE metric 28471.4640\n",
      "Epoch 117 batch 70 train Loss 90.4690 test Loss 39.3929 with MSE metric 28464.3711\n",
      "Epoch 117 batch 80 train Loss 90.4400 test Loss 39.3813 with MSE metric 28457.3163\n",
      "Epoch 117 batch 90 train Loss 90.4110 test Loss 39.3697 with MSE metric 28450.3845\n",
      "Epoch 117 batch 100 train Loss 90.3820 test Loss 39.3582 with MSE metric 28443.3251\n",
      "Epoch 117 batch 110 train Loss 90.3531 test Loss 39.3466 with MSE metric 28436.3912\n",
      "Epoch 117 batch 120 train Loss 90.3241 test Loss 39.3350 with MSE metric 28429.3741\n",
      "Epoch 117 batch 130 train Loss 90.2952 test Loss 39.3235 with MSE metric 28422.3895\n",
      "Epoch 117 batch 140 train Loss 90.2663 test Loss 39.3120 with MSE metric 28415.4140\n",
      "Epoch 117 batch 150 train Loss 90.2374 test Loss 39.3004 with MSE metric 28408.3756\n",
      "Epoch 117 batch 160 train Loss 90.2085 test Loss 39.2889 with MSE metric 28401.3629\n",
      "Epoch 117 batch 170 train Loss 90.1797 test Loss 39.2774 with MSE metric 28394.3503\n",
      "Epoch 117 batch 180 train Loss 90.1509 test Loss 39.2659 with MSE metric 28387.3761\n",
      "Epoch 117 batch 190 train Loss 90.1221 test Loss 39.2544 with MSE metric 28380.4425\n",
      "Epoch 117 batch 200 train Loss 90.0933 test Loss 39.2429 with MSE metric 28373.4222\n",
      "Epoch 117 batch 210 train Loss 90.0645 test Loss 39.2315 with MSE metric 28366.5303\n",
      "Epoch 117 batch 220 train Loss 90.0357 test Loss 39.2200 with MSE metric 28359.6120\n",
      "Epoch 117 batch 230 train Loss 90.0070 test Loss 39.2085 with MSE metric 28352.6424\n",
      "Epoch 117 batch 240 train Loss 89.9783 test Loss 39.1971 with MSE metric 28345.6760\n",
      "Time taken for 1 epoch: 28.30176877975464 secs\n",
      "\n",
      "Epoch 118 batch 0 train Loss 89.9496 test Loss 39.1857 with MSE metric 28338.7099\n",
      "Epoch 118 batch 10 train Loss 89.9209 test Loss 39.1742 with MSE metric 28331.8909\n",
      "Epoch 118 batch 20 train Loss 89.8923 test Loss 39.1628 with MSE metric 28325.0276\n",
      "Epoch 118 batch 30 train Loss 89.8637 test Loss 39.1513 with MSE metric 28318.0073\n",
      "Epoch 118 batch 40 train Loss 89.8350 test Loss 39.1399 with MSE metric 28310.9781\n",
      "Epoch 118 batch 50 train Loss 89.8064 test Loss 39.1285 with MSE metric 28304.0391\n",
      "Epoch 118 batch 60 train Loss 89.7779 test Loss 39.1172 with MSE metric 28297.1605\n",
      "Epoch 118 batch 70 train Loss 89.7493 test Loss 39.1057 with MSE metric 28290.2099\n",
      "Epoch 118 batch 80 train Loss 89.7208 test Loss 39.0943 with MSE metric 28283.4118\n",
      "Epoch 118 batch 90 train Loss 89.6922 test Loss 39.0830 with MSE metric 28276.5662\n",
      "Epoch 118 batch 100 train Loss 89.6637 test Loss 39.0716 with MSE metric 28269.6741\n",
      "Epoch 118 batch 110 train Loss 89.6353 test Loss 39.0603 with MSE metric 28262.7265\n",
      "Epoch 118 batch 120 train Loss 89.6068 test Loss 39.0489 with MSE metric 28255.8280\n",
      "Epoch 118 batch 130 train Loss 89.5784 test Loss 39.0375 with MSE metric 28249.0942\n",
      "Epoch 118 batch 140 train Loss 89.5499 test Loss 39.0262 with MSE metric 28242.2680\n",
      "Epoch 118 batch 150 train Loss 89.5215 test Loss 39.0149 with MSE metric 28235.4069\n",
      "Epoch 118 batch 160 train Loss 89.4932 test Loss 39.0035 with MSE metric 28228.5561\n",
      "Epoch 118 batch 170 train Loss 89.4648 test Loss 38.9922 with MSE metric 28221.6302\n",
      "Epoch 118 batch 180 train Loss 89.4364 test Loss 38.9809 with MSE metric 28214.6177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 batch 190 train Loss 89.4081 test Loss 38.9696 with MSE metric 28207.7923\n",
      "Epoch 118 batch 200 train Loss 89.3798 test Loss 38.9583 with MSE metric 28201.0245\n",
      "Epoch 118 batch 210 train Loss 89.3515 test Loss 38.9470 with MSE metric 28194.1737\n",
      "Epoch 118 batch 220 train Loss 89.3233 test Loss 38.9357 with MSE metric 28187.4148\n",
      "Epoch 118 batch 230 train Loss 89.2950 test Loss 38.9244 with MSE metric 28180.6652\n",
      "Epoch 118 batch 240 train Loss 89.2668 test Loss 38.9132 with MSE metric 28173.8964\n",
      "Time taken for 1 epoch: 27.982534170150757 secs\n",
      "\n",
      "Epoch 119 batch 0 train Loss 89.2386 test Loss 38.9020 with MSE metric 28166.9767\n",
      "Epoch 119 batch 10 train Loss 89.2104 test Loss 38.8907 with MSE metric 28160.1548\n",
      "Epoch 119 batch 20 train Loss 89.1822 test Loss 38.8794 with MSE metric 28153.4326\n",
      "Epoch 119 batch 30 train Loss 89.1541 test Loss 38.8682 with MSE metric 28146.6927\n",
      "Epoch 119 batch 40 train Loss 89.1259 test Loss 38.8569 with MSE metric 28139.8330\n",
      "Epoch 119 batch 50 train Loss 89.0978 test Loss 38.8457 with MSE metric 28133.0390\n",
      "Epoch 119 batch 60 train Loss 89.0697 test Loss 38.8345 with MSE metric 28126.2342\n",
      "Epoch 119 batch 70 train Loss 89.0416 test Loss 38.8233 with MSE metric 28119.2662\n",
      "Epoch 119 batch 80 train Loss 89.0136 test Loss 38.8121 with MSE metric 28112.5257\n",
      "Epoch 119 batch 90 train Loss 88.9855 test Loss 38.8009 with MSE metric 28105.6702\n",
      "Epoch 119 batch 100 train Loss 88.9575 test Loss 38.7897 with MSE metric 28098.8224\n",
      "Epoch 119 batch 110 train Loss 88.9295 test Loss 38.7785 with MSE metric 28092.1314\n",
      "Epoch 119 batch 120 train Loss 88.9015 test Loss 38.7673 with MSE metric 28085.3796\n",
      "Epoch 119 batch 130 train Loss 88.8735 test Loss 38.7562 with MSE metric 28078.7338\n",
      "Epoch 119 batch 140 train Loss 88.8456 test Loss 38.7450 with MSE metric 28071.8057\n",
      "Epoch 119 batch 150 train Loss 88.8177 test Loss 38.7338 with MSE metric 28065.0626\n",
      "Epoch 119 batch 160 train Loss 88.7897 test Loss 38.7227 with MSE metric 28058.2589\n",
      "Epoch 119 batch 170 train Loss 88.7618 test Loss 38.7116 with MSE metric 28051.3246\n",
      "Epoch 119 batch 180 train Loss 88.7340 test Loss 38.7004 with MSE metric 28044.4558\n",
      "Epoch 119 batch 190 train Loss 88.7061 test Loss 38.6894 with MSE metric 28037.6732\n",
      "Epoch 119 batch 200 train Loss 88.6783 test Loss 38.6782 with MSE metric 28030.9819\n",
      "Epoch 119 batch 210 train Loss 88.6505 test Loss 38.6671 with MSE metric 28024.2063\n",
      "Epoch 119 batch 220 train Loss 88.6227 test Loss 38.6561 with MSE metric 28017.6189\n",
      "Epoch 119 batch 230 train Loss 88.5949 test Loss 38.6450 with MSE metric 28010.8480\n",
      "Epoch 119 batch 240 train Loss 88.5671 test Loss 38.6339 with MSE metric 28004.1227\n",
      "Time taken for 1 epoch: 32.7302680015564 secs\n",
      "\n",
      "Epoch 120 batch 0 train Loss 88.5394 test Loss 38.6228 with MSE metric 27997.3128\n",
      "Epoch 120 batch 10 train Loss 88.5116 test Loss 38.6117 with MSE metric 27990.6013\n",
      "Epoch 120 batch 20 train Loss 88.4839 test Loss 38.6007 with MSE metric 27983.8182\n",
      "Epoch 120 batch 30 train Loss 88.4562 test Loss 38.5896 with MSE metric 27977.1350\n",
      "Epoch 120 batch 40 train Loss 88.4286 test Loss 38.5786 with MSE metric 27970.4705\n",
      "Epoch 120 batch 50 train Loss 88.4009 test Loss 38.5675 with MSE metric 27963.9780\n",
      "Epoch 120 batch 60 train Loss 88.3733 test Loss 38.5565 with MSE metric 27957.3578\n",
      "Epoch 120 batch 70 train Loss 88.3457 test Loss 38.5455 with MSE metric 27950.6762\n",
      "Epoch 120 batch 80 train Loss 88.3181 test Loss 38.5344 with MSE metric 27944.0085\n",
      "Epoch 120 batch 90 train Loss 88.2905 test Loss 38.5235 with MSE metric 27937.2402\n",
      "Epoch 120 batch 100 train Loss 88.2629 test Loss 38.5124 with MSE metric 27930.4934\n",
      "Epoch 120 batch 110 train Loss 88.2354 test Loss 38.5014 with MSE metric 27923.8874\n",
      "Epoch 120 batch 120 train Loss 88.2079 test Loss 38.4904 with MSE metric 27917.2720\n",
      "Epoch 120 batch 130 train Loss 88.1803 test Loss 38.4795 with MSE metric 27910.5272\n",
      "Epoch 120 batch 140 train Loss 88.1529 test Loss 38.4685 with MSE metric 27903.8882\n",
      "Epoch 120 batch 150 train Loss 88.1254 test Loss 38.4575 with MSE metric 27897.2627\n",
      "Epoch 120 batch 160 train Loss 88.0979 test Loss 38.4465 with MSE metric 27890.5324\n",
      "Epoch 120 batch 170 train Loss 88.0705 test Loss 38.4356 with MSE metric 27883.8028\n",
      "Epoch 120 batch 180 train Loss 88.0431 test Loss 38.4246 with MSE metric 27877.2307\n",
      "Epoch 120 batch 190 train Loss 88.0157 test Loss 38.4137 with MSE metric 27870.5859\n",
      "Epoch 120 batch 200 train Loss 87.9883 test Loss 38.4028 with MSE metric 27863.9115\n",
      "Epoch 120 batch 210 train Loss 87.9609 test Loss 38.3918 with MSE metric 27857.1899\n",
      "Epoch 120 batch 220 train Loss 87.9336 test Loss 38.3809 with MSE metric 27850.3131\n",
      "Epoch 120 batch 230 train Loss 87.9063 test Loss 38.3700 with MSE metric 27843.6875\n",
      "Epoch 120 batch 240 train Loss 87.8790 test Loss 38.3591 with MSE metric 27837.0643\n",
      "Time taken for 1 epoch: 29.06910514831543 secs\n",
      "\n",
      "Epoch 121 batch 0 train Loss 87.8517 test Loss 38.3482 with MSE metric 27830.4224\n",
      "Epoch 121 batch 10 train Loss 87.8244 test Loss 38.3373 with MSE metric 27823.7639\n",
      "Epoch 121 batch 20 train Loss 87.7972 test Loss 38.3264 with MSE metric 27817.2004\n",
      "Epoch 121 batch 30 train Loss 87.7699 test Loss 38.3155 with MSE metric 27810.5177\n",
      "Epoch 121 batch 40 train Loss 87.7427 test Loss 38.3047 with MSE metric 27803.8710\n",
      "Epoch 121 batch 50 train Loss 87.7155 test Loss 38.2938 with MSE metric 27797.3189\n",
      "Epoch 121 batch 60 train Loss 87.6883 test Loss 38.2830 with MSE metric 27790.5252\n",
      "Epoch 121 batch 70 train Loss 87.6612 test Loss 38.2721 with MSE metric 27783.8572\n",
      "Epoch 121 batch 80 train Loss 87.6340 test Loss 38.2613 with MSE metric 27777.2674\n",
      "Epoch 121 batch 90 train Loss 87.6069 test Loss 38.2504 with MSE metric 27770.7018\n",
      "Epoch 121 batch 100 train Loss 87.5798 test Loss 38.2396 with MSE metric 27764.2665\n",
      "Epoch 121 batch 110 train Loss 87.5527 test Loss 38.2288 with MSE metric 27757.7245\n",
      "Epoch 121 batch 120 train Loss 87.5256 test Loss 38.2180 with MSE metric 27751.2420\n",
      "Epoch 121 batch 130 train Loss 87.4986 test Loss 38.2072 with MSE metric 27744.7205\n",
      "Epoch 121 batch 140 train Loss 87.4715 test Loss 38.1964 with MSE metric 27738.3189\n",
      "Epoch 121 batch 150 train Loss 87.4445 test Loss 38.1857 with MSE metric 27731.6442\n",
      "Epoch 121 batch 160 train Loss 87.4175 test Loss 38.1749 with MSE metric 27725.0851\n",
      "Epoch 121 batch 170 train Loss 87.3905 test Loss 38.1641 with MSE metric 27718.5124\n",
      "Epoch 121 batch 180 train Loss 87.3635 test Loss 38.1533 with MSE metric 27711.9084\n",
      "Epoch 121 batch 190 train Loss 87.3366 test Loss 38.1425 with MSE metric 27705.3642\n",
      "Epoch 121 batch 200 train Loss 87.3097 test Loss 38.1318 with MSE metric 27698.8860\n",
      "Epoch 121 batch 210 train Loss 87.2828 test Loss 38.1211 with MSE metric 27692.4285\n",
      "Epoch 121 batch 220 train Loss 87.2559 test Loss 38.1104 with MSE metric 27685.9611\n",
      "Epoch 121 batch 230 train Loss 87.2290 test Loss 38.0997 with MSE metric 27679.4116\n",
      "Epoch 121 batch 240 train Loss 87.2021 test Loss 38.0890 with MSE metric 27672.8679\n",
      "Time taken for 1 epoch: 26.83838415145874 secs\n",
      "\n",
      "Epoch 122 batch 0 train Loss 87.1753 test Loss 38.0782 with MSE metric 27666.3800\n",
      "Epoch 122 batch 10 train Loss 87.1485 test Loss 38.0675 with MSE metric 27659.9753\n",
      "Epoch 122 batch 20 train Loss 87.1217 test Loss 38.0568 with MSE metric 27653.4693\n",
      "Epoch 122 batch 30 train Loss 87.0949 test Loss 38.0461 with MSE metric 27647.0003\n",
      "Epoch 122 batch 40 train Loss 87.0681 test Loss 38.0354 with MSE metric 27640.4297\n",
      "Epoch 122 batch 50 train Loss 87.0413 test Loss 38.0248 with MSE metric 27633.9600\n",
      "Epoch 122 batch 60 train Loss 87.0146 test Loss 38.0141 with MSE metric 27627.4672\n",
      "Epoch 122 batch 70 train Loss 86.9879 test Loss 38.0034 with MSE metric 27621.1547\n",
      "Epoch 122 batch 80 train Loss 86.9612 test Loss 37.9927 with MSE metric 27614.7017\n",
      "Epoch 122 batch 90 train Loss 86.9345 test Loss 37.9821 with MSE metric 27608.2342\n",
      "Epoch 122 batch 100 train Loss 86.9078 test Loss 37.9715 with MSE metric 27601.7627\n",
      "Epoch 122 batch 110 train Loss 86.8812 test Loss 37.9608 with MSE metric 27595.2569\n",
      "Epoch 122 batch 120 train Loss 86.8546 test Loss 37.9502 with MSE metric 27588.6459\n",
      "Epoch 122 batch 130 train Loss 86.8279 test Loss 37.9396 with MSE metric 27582.2933\n",
      "Epoch 122 batch 140 train Loss 86.8013 test Loss 37.9290 with MSE metric 27575.7631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122 batch 150 train Loss 86.7748 test Loss 37.9184 with MSE metric 27569.4596\n",
      "Epoch 122 batch 160 train Loss 86.7482 test Loss 37.9078 with MSE metric 27562.9950\n",
      "Epoch 122 batch 170 train Loss 86.7217 test Loss 37.8972 with MSE metric 27556.6100\n",
      "Epoch 122 batch 180 train Loss 86.6951 test Loss 37.8866 with MSE metric 27550.1523\n",
      "Epoch 122 batch 190 train Loss 86.6686 test Loss 37.8760 with MSE metric 27543.5999\n",
      "Epoch 122 batch 200 train Loss 86.6421 test Loss 37.8654 with MSE metric 27537.1904\n",
      "Epoch 122 batch 210 train Loss 86.6157 test Loss 37.8549 with MSE metric 27530.6569\n",
      "Epoch 122 batch 220 train Loss 86.5892 test Loss 37.8443 with MSE metric 27524.2387\n",
      "Epoch 122 batch 230 train Loss 86.5628 test Loss 37.8338 with MSE metric 27517.8297\n",
      "Epoch 122 batch 240 train Loss 86.5363 test Loss 37.8233 with MSE metric 27511.3952\n",
      "Time taken for 1 epoch: 33.81719183921814 secs\n",
      "\n",
      "Epoch 123 batch 0 train Loss 86.5099 test Loss 37.8127 with MSE metric 27504.9214\n",
      "Epoch 123 batch 10 train Loss 86.4835 test Loss 37.8021 with MSE metric 27498.5116\n",
      "Epoch 123 batch 20 train Loss 86.4571 test Loss 37.7916 with MSE metric 27492.1599\n",
      "Epoch 123 batch 30 train Loss 86.4308 test Loss 37.7811 with MSE metric 27485.9324\n",
      "Epoch 123 batch 40 train Loss 86.4044 test Loss 37.7706 with MSE metric 27479.6119\n",
      "Epoch 123 batch 50 train Loss 86.3781 test Loss 37.7600 with MSE metric 27473.2467\n",
      "Epoch 123 batch 60 train Loss 86.3518 test Loss 37.7496 with MSE metric 27467.0229\n",
      "Epoch 123 batch 70 train Loss 86.3255 test Loss 37.7391 with MSE metric 27460.7154\n",
      "Epoch 123 batch 80 train Loss 86.2992 test Loss 37.7286 with MSE metric 27454.3033\n",
      "Epoch 123 batch 90 train Loss 86.2730 test Loss 37.7181 with MSE metric 27448.0261\n",
      "Epoch 123 batch 100 train Loss 86.2468 test Loss 37.7076 with MSE metric 27441.6109\n",
      "Epoch 123 batch 110 train Loss 86.2205 test Loss 37.6971 with MSE metric 27435.3220\n",
      "Epoch 123 batch 120 train Loss 86.1943 test Loss 37.6867 with MSE metric 27428.9009\n",
      "Epoch 123 batch 130 train Loss 86.1682 test Loss 37.6762 with MSE metric 27422.4431\n",
      "Epoch 123 batch 140 train Loss 86.1420 test Loss 37.6658 with MSE metric 27416.1451\n",
      "Epoch 123 batch 150 train Loss 86.1158 test Loss 37.6554 with MSE metric 27409.7870\n",
      "Epoch 123 batch 160 train Loss 86.0897 test Loss 37.6449 with MSE metric 27403.3085\n",
      "Epoch 123 batch 170 train Loss 86.0636 test Loss 37.6345 with MSE metric 27396.9652\n",
      "Epoch 123 batch 180 train Loss 86.0375 test Loss 37.6241 with MSE metric 27390.7183\n",
      "Epoch 123 batch 190 train Loss 86.0114 test Loss 37.6136 with MSE metric 27384.4542\n",
      "Epoch 123 batch 200 train Loss 85.9853 test Loss 37.6032 with MSE metric 27378.1336\n",
      "Epoch 123 batch 210 train Loss 85.9593 test Loss 37.5928 with MSE metric 27371.8320\n",
      "Epoch 123 batch 220 train Loss 85.9332 test Loss 37.5824 with MSE metric 27365.5585\n",
      "Epoch 123 batch 230 train Loss 85.9072 test Loss 37.5720 with MSE metric 27359.1640\n",
      "Epoch 123 batch 240 train Loss 85.8812 test Loss 37.5616 with MSE metric 27352.9978\n",
      "Time taken for 1 epoch: 30.51098394393921 secs\n",
      "\n",
      "Epoch 124 batch 0 train Loss 85.8552 test Loss 37.5513 with MSE metric 27346.7509\n",
      "Epoch 124 batch 10 train Loss 85.8293 test Loss 37.5409 with MSE metric 27340.4922\n",
      "Epoch 124 batch 20 train Loss 85.8033 test Loss 37.5305 with MSE metric 27334.1686\n",
      "Epoch 124 batch 30 train Loss 85.7774 test Loss 37.5202 with MSE metric 27327.9085\n",
      "Epoch 124 batch 40 train Loss 85.7515 test Loss 37.5099 with MSE metric 27321.6714\n",
      "Epoch 124 batch 50 train Loss 85.7256 test Loss 37.4995 with MSE metric 27315.5306\n",
      "Epoch 124 batch 60 train Loss 85.6997 test Loss 37.4892 with MSE metric 27309.3164\n",
      "Epoch 124 batch 70 train Loss 85.6738 test Loss 37.4789 with MSE metric 27303.1001\n",
      "Epoch 124 batch 80 train Loss 85.6480 test Loss 37.4686 with MSE metric 27296.8612\n",
      "Epoch 124 batch 90 train Loss 85.6221 test Loss 37.4582 with MSE metric 27290.7512\n",
      "Epoch 124 batch 100 train Loss 85.5963 test Loss 37.4479 with MSE metric 27284.4220\n",
      "Epoch 124 batch 110 train Loss 85.5705 test Loss 37.4376 with MSE metric 27278.0084\n",
      "Epoch 124 batch 120 train Loss 85.5447 test Loss 37.4274 with MSE metric 27271.7846\n",
      "Epoch 124 batch 130 train Loss 85.5190 test Loss 37.4171 with MSE metric 27265.4916\n",
      "Epoch 124 batch 140 train Loss 85.4932 test Loss 37.4068 with MSE metric 27259.1177\n",
      "Epoch 124 batch 150 train Loss 85.4675 test Loss 37.3965 with MSE metric 27252.9791\n",
      "Epoch 124 batch 160 train Loss 85.4418 test Loss 37.3863 with MSE metric 27246.7132\n",
      "Epoch 124 batch 170 train Loss 85.4161 test Loss 37.3760 with MSE metric 27240.5486\n",
      "Epoch 124 batch 180 train Loss 85.3904 test Loss 37.3658 with MSE metric 27234.2337\n",
      "Epoch 124 batch 190 train Loss 85.3647 test Loss 37.3555 with MSE metric 27228.0154\n",
      "Epoch 124 batch 200 train Loss 85.3390 test Loss 37.3453 with MSE metric 27221.7778\n",
      "Epoch 124 batch 210 train Loss 85.3134 test Loss 37.3351 with MSE metric 27215.6472\n",
      "Epoch 124 batch 220 train Loss 85.2878 test Loss 37.3249 with MSE metric 27209.4067\n",
      "Epoch 124 batch 230 train Loss 85.2622 test Loss 37.3146 with MSE metric 27203.2040\n",
      "Epoch 124 batch 240 train Loss 85.2366 test Loss 37.3044 with MSE metric 27196.9495\n",
      "Time taken for 1 epoch: 30.494865894317627 secs\n",
      "\n",
      "Epoch 125 batch 0 train Loss 85.2110 test Loss 37.2942 with MSE metric 27190.7142\n",
      "Epoch 125 batch 10 train Loss 85.1855 test Loss 37.2840 with MSE metric 27184.4516\n",
      "Epoch 125 batch 20 train Loss 85.1599 test Loss 37.2738 with MSE metric 27178.2740\n",
      "Epoch 125 batch 30 train Loss 85.1344 test Loss 37.2636 with MSE metric 27172.0833\n",
      "Epoch 125 batch 40 train Loss 85.1089 test Loss 37.2534 with MSE metric 27165.9299\n",
      "Epoch 125 batch 50 train Loss 85.0834 test Loss 37.2432 with MSE metric 27159.6745\n",
      "Epoch 125 batch 60 train Loss 85.0579 test Loss 37.2331 with MSE metric 27153.6605\n",
      "Epoch 125 batch 70 train Loss 85.0325 test Loss 37.2229 with MSE metric 27147.4313\n",
      "Epoch 125 batch 80 train Loss 85.0070 test Loss 37.2128 with MSE metric 27141.3107\n",
      "Epoch 125 batch 90 train Loss 84.9816 test Loss 37.2026 with MSE metric 27135.0900\n",
      "Epoch 125 batch 100 train Loss 84.9562 test Loss 37.1924 with MSE metric 27128.9566\n",
      "Epoch 125 batch 110 train Loss 84.9308 test Loss 37.1822 with MSE metric 27122.7185\n",
      "Epoch 125 batch 120 train Loss 84.9055 test Loss 37.1722 with MSE metric 27116.7605\n",
      "Epoch 125 batch 130 train Loss 84.8801 test Loss 37.1620 with MSE metric 27110.6002\n",
      "Epoch 125 batch 140 train Loss 84.8548 test Loss 37.1519 with MSE metric 27104.3782\n",
      "Epoch 125 batch 150 train Loss 84.8294 test Loss 37.1418 with MSE metric 27098.2119\n",
      "Epoch 125 batch 160 train Loss 84.8041 test Loss 37.1317 with MSE metric 27092.1064\n",
      "Epoch 125 batch 170 train Loss 84.7789 test Loss 37.1216 with MSE metric 27085.9850\n",
      "Epoch 125 batch 180 train Loss 84.7536 test Loss 37.1115 with MSE metric 27079.7897\n",
      "Epoch 125 batch 190 train Loss 84.7283 test Loss 37.1014 with MSE metric 27073.6783\n",
      "Epoch 125 batch 200 train Loss 84.7030 test Loss 37.0913 with MSE metric 27067.5753\n",
      "Epoch 125 batch 210 train Loss 84.6778 test Loss 37.0812 with MSE metric 27061.6004\n",
      "Epoch 125 batch 220 train Loss 84.6526 test Loss 37.0712 with MSE metric 27055.5197\n",
      "Epoch 125 batch 230 train Loss 84.6274 test Loss 37.0611 with MSE metric 27049.3387\n",
      "Epoch 125 batch 240 train Loss 84.6022 test Loss 37.0511 with MSE metric 27043.1734\n",
      "Time taken for 1 epoch: 28.12926197052002 secs\n",
      "\n",
      "Epoch 126 batch 0 train Loss 84.5771 test Loss 37.0410 with MSE metric 27037.1751\n",
      "Epoch 126 batch 10 train Loss 84.5519 test Loss 37.0310 with MSE metric 27031.1160\n",
      "Epoch 126 batch 20 train Loss 84.5268 test Loss 37.0210 with MSE metric 27024.8790\n",
      "Epoch 126 batch 30 train Loss 84.5017 test Loss 37.0109 with MSE metric 27018.6619\n",
      "Epoch 126 batch 40 train Loss 84.4766 test Loss 37.0009 with MSE metric 27012.6778\n",
      "Epoch 126 batch 50 train Loss 84.4515 test Loss 36.9908 with MSE metric 27006.6533\n",
      "Epoch 126 batch 60 train Loss 84.4264 test Loss 36.9809 with MSE metric 27000.5615\n",
      "Epoch 126 batch 70 train Loss 84.4014 test Loss 36.9709 with MSE metric 26994.5509\n",
      "Epoch 126 batch 80 train Loss 84.3763 test Loss 36.9609 with MSE metric 26988.4827\n",
      "Epoch 126 batch 90 train Loss 84.3513 test Loss 36.9509 with MSE metric 26982.4069\n",
      "Epoch 126 batch 100 train Loss 84.3263 test Loss 36.9409 with MSE metric 26976.2711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126 batch 110 train Loss 84.3013 test Loss 36.9309 with MSE metric 26970.1358\n",
      "Epoch 126 batch 120 train Loss 84.2763 test Loss 36.9210 with MSE metric 26964.0724\n",
      "Epoch 126 batch 130 train Loss 84.2514 test Loss 36.9110 with MSE metric 26958.0586\n",
      "Epoch 126 batch 140 train Loss 84.2264 test Loss 36.9010 with MSE metric 26951.8988\n",
      "Epoch 126 batch 150 train Loss 84.2015 test Loss 36.8911 with MSE metric 26945.8564\n",
      "Epoch 126 batch 160 train Loss 84.1766 test Loss 36.8811 with MSE metric 26939.7884\n",
      "Epoch 126 batch 170 train Loss 84.1517 test Loss 36.8712 with MSE metric 26933.7258\n",
      "Epoch 126 batch 180 train Loss 84.1268 test Loss 36.8612 with MSE metric 26927.7728\n",
      "Epoch 126 batch 190 train Loss 84.1020 test Loss 36.8513 with MSE metric 26921.9377\n",
      "Epoch 126 batch 200 train Loss 84.0771 test Loss 36.8414 with MSE metric 26915.8886\n",
      "Epoch 126 batch 210 train Loss 84.0523 test Loss 36.8315 with MSE metric 26909.7919\n",
      "Epoch 126 batch 220 train Loss 84.0275 test Loss 36.8215 with MSE metric 26903.7120\n",
      "Epoch 126 batch 230 train Loss 84.0026 test Loss 36.8116 with MSE metric 26897.6983\n",
      "Epoch 126 batch 240 train Loss 83.9779 test Loss 36.8017 with MSE metric 26891.7886\n",
      "Time taken for 1 epoch: 29.55920100212097 secs\n",
      "\n",
      "Epoch 127 batch 0 train Loss 83.9531 test Loss 36.7919 with MSE metric 26885.6613\n",
      "Epoch 127 batch 10 train Loss 83.9283 test Loss 36.7820 with MSE metric 26879.6801\n",
      "Epoch 127 batch 20 train Loss 83.9036 test Loss 36.7721 with MSE metric 26873.6210\n",
      "Epoch 127 batch 30 train Loss 83.8789 test Loss 36.7622 with MSE metric 26867.6975\n",
      "Epoch 127 batch 40 train Loss 83.8541 test Loss 36.7524 with MSE metric 26861.7520\n",
      "Epoch 127 batch 50 train Loss 83.8294 test Loss 36.7425 with MSE metric 26855.7261\n",
      "Epoch 127 batch 60 train Loss 83.8048 test Loss 36.7327 with MSE metric 26849.8082\n",
      "Epoch 127 batch 70 train Loss 83.7801 test Loss 36.7229 with MSE metric 26843.8673\n",
      "Epoch 127 batch 80 train Loss 83.7554 test Loss 36.7130 with MSE metric 26837.9205\n",
      "Epoch 127 batch 90 train Loss 83.7308 test Loss 36.7032 with MSE metric 26831.9925\n",
      "Epoch 127 batch 100 train Loss 83.7062 test Loss 36.6933 with MSE metric 26825.9943\n",
      "Epoch 127 batch 110 train Loss 83.6816 test Loss 36.6835 with MSE metric 26820.1429\n",
      "Epoch 127 batch 120 train Loss 83.6570 test Loss 36.6737 with MSE metric 26814.2273\n",
      "Epoch 127 batch 130 train Loss 83.6324 test Loss 36.6639 with MSE metric 26808.1279\n",
      "Epoch 127 batch 140 train Loss 83.6079 test Loss 36.6541 with MSE metric 26802.1095\n",
      "Epoch 127 batch 150 train Loss 83.5834 test Loss 36.6443 with MSE metric 26796.2188\n",
      "Epoch 127 batch 160 train Loss 83.5588 test Loss 36.6345 with MSE metric 26790.1812\n",
      "Epoch 127 batch 170 train Loss 83.5343 test Loss 36.6247 with MSE metric 26784.3419\n",
      "Epoch 127 batch 180 train Loss 83.5098 test Loss 36.6149 with MSE metric 26778.3465\n",
      "Epoch 127 batch 190 train Loss 83.4854 test Loss 36.6052 with MSE metric 26772.4959\n",
      "Epoch 127 batch 200 train Loss 83.4609 test Loss 36.5954 with MSE metric 26766.5360\n",
      "Epoch 127 batch 210 train Loss 83.4364 test Loss 36.5857 with MSE metric 26760.6225\n",
      "Epoch 127 batch 220 train Loss 83.4120 test Loss 36.5760 with MSE metric 26754.6448\n",
      "Epoch 127 batch 230 train Loss 83.3876 test Loss 36.5662 with MSE metric 26748.7613\n",
      "Epoch 127 batch 240 train Loss 83.3632 test Loss 36.5565 with MSE metric 26742.7108\n",
      "Time taken for 1 epoch: 29.727536916732788 secs\n",
      "\n",
      "Epoch 128 batch 0 train Loss 83.3388 test Loss 36.5468 with MSE metric 26736.7194\n",
      "Epoch 128 batch 10 train Loss 83.3144 test Loss 36.5370 with MSE metric 26730.8581\n",
      "Epoch 128 batch 20 train Loss 83.2901 test Loss 36.5273 with MSE metric 26724.8585\n",
      "Epoch 128 batch 30 train Loss 83.2657 test Loss 36.5176 with MSE metric 26718.9024\n",
      "Epoch 128 batch 40 train Loss 83.2414 test Loss 36.5079 with MSE metric 26712.9938\n",
      "Epoch 128 batch 50 train Loss 83.2171 test Loss 36.4982 with MSE metric 26707.1607\n",
      "Epoch 128 batch 60 train Loss 83.1928 test Loss 36.4885 with MSE metric 26701.2023\n",
      "Epoch 128 batch 70 train Loss 83.1685 test Loss 36.4788 with MSE metric 26695.3076\n",
      "Epoch 128 batch 80 train Loss 83.1443 test Loss 36.4691 with MSE metric 26689.4015\n",
      "Epoch 128 batch 90 train Loss 83.1200 test Loss 36.4594 with MSE metric 26683.6319\n",
      "Epoch 128 batch 100 train Loss 83.0958 test Loss 36.4498 with MSE metric 26677.8129\n",
      "Epoch 128 batch 110 train Loss 83.0716 test Loss 36.4402 with MSE metric 26671.8754\n",
      "Epoch 128 batch 120 train Loss 83.0474 test Loss 36.4305 with MSE metric 26666.0435\n",
      "Epoch 128 batch 130 train Loss 83.0232 test Loss 36.4208 with MSE metric 26660.2089\n",
      "Epoch 128 batch 140 train Loss 82.9990 test Loss 36.4112 with MSE metric 26654.2568\n",
      "Epoch 128 batch 150 train Loss 82.9749 test Loss 36.4015 with MSE metric 26648.3747\n",
      "Epoch 128 batch 160 train Loss 82.9507 test Loss 36.3919 with MSE metric 26642.4568\n",
      "Epoch 128 batch 170 train Loss 82.9266 test Loss 36.3823 with MSE metric 26636.5043\n",
      "Epoch 128 batch 180 train Loss 82.9025 test Loss 36.3727 with MSE metric 26630.6097\n",
      "Epoch 128 batch 190 train Loss 82.8784 test Loss 36.3631 with MSE metric 26624.6757\n",
      "Epoch 128 batch 200 train Loss 82.8543 test Loss 36.3535 with MSE metric 26618.9483\n",
      "Epoch 128 batch 210 train Loss 82.8302 test Loss 36.3439 with MSE metric 26613.1944\n",
      "Epoch 128 batch 220 train Loss 82.8062 test Loss 36.3343 with MSE metric 26607.4919\n",
      "Epoch 128 batch 230 train Loss 82.7821 test Loss 36.3247 with MSE metric 26601.7500\n",
      "Epoch 128 batch 240 train Loss 82.7581 test Loss 36.3150 with MSE metric 26595.8950\n",
      "Time taken for 1 epoch: 27.416393756866455 secs\n",
      "\n",
      "Epoch 129 batch 0 train Loss 82.7341 test Loss 36.3055 with MSE metric 26590.1809\n",
      "Epoch 129 batch 10 train Loss 82.7101 test Loss 36.2959 with MSE metric 26584.3414\n",
      "Epoch 129 batch 20 train Loss 82.6861 test Loss 36.2863 with MSE metric 26578.4841\n",
      "Epoch 129 batch 30 train Loss 82.6622 test Loss 36.2768 with MSE metric 26572.7379\n",
      "Epoch 129 batch 40 train Loss 82.6382 test Loss 36.2672 with MSE metric 26566.9669\n",
      "Epoch 129 batch 50 train Loss 82.6143 test Loss 36.2577 with MSE metric 26561.2338\n",
      "Epoch 129 batch 60 train Loss 82.5904 test Loss 36.2482 with MSE metric 26555.4710\n",
      "Epoch 129 batch 70 train Loss 82.5665 test Loss 36.2386 with MSE metric 26549.7215\n",
      "Epoch 129 batch 80 train Loss 82.5426 test Loss 36.2291 with MSE metric 26543.9373\n",
      "Epoch 129 batch 90 train Loss 82.5187 test Loss 36.2196 with MSE metric 26538.2157\n",
      "Epoch 129 batch 100 train Loss 82.4948 test Loss 36.2101 with MSE metric 26532.4013\n",
      "Epoch 129 batch 110 train Loss 82.4710 test Loss 36.2006 with MSE metric 26526.5278\n",
      "Epoch 129 batch 120 train Loss 82.4471 test Loss 36.1911 with MSE metric 26520.7117\n",
      "Epoch 129 batch 130 train Loss 82.4233 test Loss 36.1816 with MSE metric 26514.9449\n",
      "Epoch 129 batch 140 train Loss 82.3995 test Loss 36.1720 with MSE metric 26509.2267\n",
      "Epoch 129 batch 150 train Loss 82.3757 test Loss 36.1626 with MSE metric 26503.3724\n",
      "Epoch 129 batch 160 train Loss 82.3520 test Loss 36.1531 with MSE metric 26497.5164\n",
      "Epoch 129 batch 170 train Loss 82.3282 test Loss 36.1436 with MSE metric 26491.8391\n",
      "Epoch 129 batch 180 train Loss 82.3045 test Loss 36.1342 with MSE metric 26486.2294\n",
      "Epoch 129 batch 190 train Loss 82.2807 test Loss 36.1247 with MSE metric 26480.5619\n",
      "Epoch 129 batch 200 train Loss 82.2570 test Loss 36.1152 with MSE metric 26474.7351\n",
      "Epoch 129 batch 210 train Loss 82.2333 test Loss 36.1058 with MSE metric 26469.0635\n",
      "Epoch 129 batch 220 train Loss 82.2097 test Loss 36.0963 with MSE metric 26463.4009\n",
      "Epoch 129 batch 230 train Loss 82.1860 test Loss 36.0869 with MSE metric 26457.6274\n",
      "Epoch 129 batch 240 train Loss 82.1623 test Loss 36.0774 with MSE metric 26451.8163\n",
      "Time taken for 1 epoch: 29.210986137390137 secs\n",
      "\n",
      "Epoch 130 batch 0 train Loss 82.1387 test Loss 36.0680 with MSE metric 26445.9242\n",
      "Epoch 130 batch 10 train Loss 82.1150 test Loss 36.0586 with MSE metric 26440.1916\n",
      "Epoch 130 batch 20 train Loss 82.0914 test Loss 36.0491 with MSE metric 26434.4334\n",
      "Epoch 130 batch 30 train Loss 82.0678 test Loss 36.0397 with MSE metric 26428.6613\n",
      "Epoch 130 batch 40 train Loss 82.0442 test Loss 36.0303 with MSE metric 26422.9740\n",
      "Epoch 130 batch 50 train Loss 82.0207 test Loss 36.0209 with MSE metric 26417.3172\n",
      "Epoch 130 batch 60 train Loss 81.9971 test Loss 36.0115 with MSE metric 26411.5834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130 batch 70 train Loss 81.9736 test Loss 36.0021 with MSE metric 26405.7328\n",
      "Epoch 130 batch 80 train Loss 81.9501 test Loss 35.9927 with MSE metric 26400.0933\n",
      "Epoch 130 batch 90 train Loss 81.9265 test Loss 35.9833 with MSE metric 26394.4222\n",
      "Epoch 130 batch 100 train Loss 81.9031 test Loss 35.9740 with MSE metric 26388.7200\n",
      "Epoch 130 batch 110 train Loss 81.8796 test Loss 35.9646 with MSE metric 26382.9804\n",
      "Epoch 130 batch 120 train Loss 81.8561 test Loss 35.9552 with MSE metric 26377.1933\n",
      "Epoch 130 batch 130 train Loss 81.8327 test Loss 35.9459 with MSE metric 26371.5630\n",
      "Epoch 130 batch 140 train Loss 81.8092 test Loss 35.9365 with MSE metric 26365.9300\n",
      "Epoch 130 batch 150 train Loss 81.7858 test Loss 35.9272 with MSE metric 26360.2619\n",
      "Epoch 130 batch 160 train Loss 81.7624 test Loss 35.9178 with MSE metric 26354.5930\n",
      "Epoch 130 batch 170 train Loss 81.7390 test Loss 35.9085 with MSE metric 26348.9874\n",
      "Epoch 130 batch 180 train Loss 81.7156 test Loss 35.8991 with MSE metric 26343.3804\n",
      "Epoch 130 batch 190 train Loss 81.6923 test Loss 35.8898 with MSE metric 26337.7696\n",
      "Epoch 130 batch 200 train Loss 81.6689 test Loss 35.8805 with MSE metric 26332.2108\n",
      "Epoch 130 batch 210 train Loss 81.6456 test Loss 35.8712 with MSE metric 26326.4695\n",
      "Epoch 130 batch 220 train Loss 81.6222 test Loss 35.8619 with MSE metric 26320.8374\n",
      "Epoch 130 batch 230 train Loss 81.5989 test Loss 35.8525 with MSE metric 26315.1168\n",
      "Epoch 130 batch 240 train Loss 81.5756 test Loss 35.8432 with MSE metric 26309.4191\n",
      "Time taken for 1 epoch: 28.365396976470947 secs\n",
      "\n",
      "Epoch 131 batch 0 train Loss 81.5524 test Loss 35.8339 with MSE metric 26303.7383\n",
      "Epoch 131 batch 10 train Loss 81.5291 test Loss 35.8247 with MSE metric 26297.9286\n",
      "Epoch 131 batch 20 train Loss 81.5058 test Loss 35.8154 with MSE metric 26292.2433\n",
      "Epoch 131 batch 30 train Loss 81.4826 test Loss 35.8061 with MSE metric 26286.6154\n",
      "Epoch 131 batch 40 train Loss 81.4594 test Loss 35.7968 with MSE metric 26280.9925\n",
      "Epoch 131 batch 50 train Loss 81.4362 test Loss 35.7876 with MSE metric 26275.4610\n",
      "Epoch 131 batch 60 train Loss 81.4130 test Loss 35.7783 with MSE metric 26269.8449\n",
      "Epoch 131 batch 70 train Loss 81.3898 test Loss 35.7691 with MSE metric 26264.1250\n",
      "Epoch 131 batch 80 train Loss 81.3666 test Loss 35.7599 with MSE metric 26258.4683\n",
      "Epoch 131 batch 90 train Loss 81.3434 test Loss 35.7507 with MSE metric 26252.8406\n",
      "Epoch 131 batch 100 train Loss 81.3203 test Loss 35.7414 with MSE metric 26247.4197\n",
      "Epoch 131 batch 110 train Loss 81.2972 test Loss 35.7322 with MSE metric 26241.9298\n",
      "Epoch 131 batch 120 train Loss 81.2741 test Loss 35.7231 with MSE metric 26236.2002\n",
      "Epoch 131 batch 130 train Loss 81.2510 test Loss 35.7138 with MSE metric 26230.7969\n",
      "Epoch 131 batch 140 train Loss 81.2279 test Loss 35.7046 with MSE metric 26225.2591\n",
      "Epoch 131 batch 150 train Loss 81.2049 test Loss 35.6954 with MSE metric 26219.7055\n",
      "Epoch 131 batch 160 train Loss 81.1818 test Loss 35.6862 with MSE metric 26214.1165\n",
      "Epoch 131 batch 170 train Loss 81.1588 test Loss 35.6770 with MSE metric 26208.6038\n",
      "Epoch 131 batch 180 train Loss 81.1357 test Loss 35.6679 with MSE metric 26202.9957\n",
      "Epoch 131 batch 190 train Loss 81.1127 test Loss 35.6587 with MSE metric 26197.3215\n",
      "Epoch 131 batch 200 train Loss 81.0897 test Loss 35.6495 with MSE metric 26191.7867\n",
      "Epoch 131 batch 210 train Loss 81.0667 test Loss 35.6403 with MSE metric 26186.3160\n",
      "Epoch 131 batch 220 train Loss 81.0437 test Loss 35.6312 with MSE metric 26180.6810\n",
      "Epoch 131 batch 230 train Loss 81.0208 test Loss 35.6220 with MSE metric 26175.1084\n",
      "Epoch 131 batch 240 train Loss 80.9978 test Loss 35.6129 with MSE metric 26169.6626\n",
      "Time taken for 1 epoch: 27.152642250061035 secs\n",
      "\n",
      "Epoch 132 batch 0 train Loss 80.9749 test Loss 35.6037 with MSE metric 26164.1478\n",
      "Epoch 132 batch 10 train Loss 80.9520 test Loss 35.5946 with MSE metric 26158.6005\n",
      "Epoch 132 batch 20 train Loss 80.9291 test Loss 35.5854 with MSE metric 26152.9396\n",
      "Epoch 132 batch 30 train Loss 80.9062 test Loss 35.5763 with MSE metric 26147.4439\n",
      "Epoch 132 batch 40 train Loss 80.8833 test Loss 35.5672 with MSE metric 26141.8979\n",
      "Epoch 132 batch 50 train Loss 80.8604 test Loss 35.5580 with MSE metric 26136.3325\n",
      "Epoch 132 batch 60 train Loss 80.8376 test Loss 35.5489 with MSE metric 26130.9449\n",
      "Epoch 132 batch 70 train Loss 80.8148 test Loss 35.5398 with MSE metric 26125.4929\n",
      "Epoch 132 batch 80 train Loss 80.7920 test Loss 35.5307 with MSE metric 26119.9682\n",
      "Epoch 132 batch 90 train Loss 80.7692 test Loss 35.5216 with MSE metric 26114.4288\n",
      "Epoch 132 batch 100 train Loss 80.7464 test Loss 35.5125 with MSE metric 26108.9423\n",
      "Epoch 132 batch 110 train Loss 80.7236 test Loss 35.5034 with MSE metric 26103.4995\n",
      "Epoch 132 batch 120 train Loss 80.7008 test Loss 35.4943 with MSE metric 26098.1114\n",
      "Epoch 132 batch 130 train Loss 80.6781 test Loss 35.4852 with MSE metric 26092.5760\n",
      "Epoch 132 batch 140 train Loss 80.6553 test Loss 35.4762 with MSE metric 26087.0949\n",
      "Epoch 132 batch 150 train Loss 80.6326 test Loss 35.4671 with MSE metric 26081.6027\n",
      "Epoch 132 batch 160 train Loss 80.6099 test Loss 35.4581 with MSE metric 26076.1427\n",
      "Epoch 132 batch 170 train Loss 80.5872 test Loss 35.4490 with MSE metric 26070.6614\n",
      "Epoch 132 batch 180 train Loss 80.5645 test Loss 35.4400 with MSE metric 26065.1979\n",
      "Epoch 132 batch 190 train Loss 80.5419 test Loss 35.4309 with MSE metric 26059.6720\n",
      "Epoch 132 batch 200 train Loss 80.5192 test Loss 35.4219 with MSE metric 26054.2368\n",
      "Epoch 132 batch 210 train Loss 80.4966 test Loss 35.4128 with MSE metric 26048.7588\n",
      "Epoch 132 batch 220 train Loss 80.4739 test Loss 35.4038 with MSE metric 26043.2798\n",
      "Epoch 132 batch 230 train Loss 80.4513 test Loss 35.3947 with MSE metric 26037.7704\n",
      "Epoch 132 batch 240 train Loss 80.4287 test Loss 35.3857 with MSE metric 26032.3767\n",
      "Time taken for 1 epoch: 27.889068126678467 secs\n",
      "\n",
      "Epoch 133 batch 0 train Loss 80.4061 test Loss 35.3766 with MSE metric 26026.8404\n",
      "Epoch 133 batch 10 train Loss 80.3836 test Loss 35.3676 with MSE metric 26021.3534\n",
      "Epoch 133 batch 20 train Loss 80.3610 test Loss 35.3586 with MSE metric 26015.9103\n",
      "Epoch 133 batch 30 train Loss 80.3385 test Loss 35.3496 with MSE metric 26010.3642\n",
      "Epoch 133 batch 40 train Loss 80.3160 test Loss 35.3406 with MSE metric 26004.8527\n",
      "Epoch 133 batch 50 train Loss 80.2934 test Loss 35.3316 with MSE metric 25999.4118\n",
      "Epoch 133 batch 60 train Loss 80.2709 test Loss 35.3226 with MSE metric 25993.8886\n",
      "Epoch 133 batch 70 train Loss 80.2484 test Loss 35.3137 with MSE metric 25988.4431\n",
      "Epoch 133 batch 80 train Loss 80.2260 test Loss 35.3047 with MSE metric 25982.9222\n",
      "Epoch 133 batch 90 train Loss 80.2035 test Loss 35.2957 with MSE metric 25977.4931\n",
      "Epoch 133 batch 100 train Loss 80.1810 test Loss 35.2868 with MSE metric 25972.0440\n",
      "Epoch 133 batch 110 train Loss 80.1586 test Loss 35.2778 with MSE metric 25966.5614\n",
      "Epoch 133 batch 120 train Loss 80.1362 test Loss 35.2689 with MSE metric 25961.0795\n",
      "Epoch 133 batch 130 train Loss 80.1138 test Loss 35.2600 with MSE metric 25955.6418\n",
      "Epoch 133 batch 140 train Loss 80.0914 test Loss 35.2510 with MSE metric 25950.1630\n",
      "Epoch 133 batch 150 train Loss 80.0690 test Loss 35.2421 with MSE metric 25944.6273\n",
      "Epoch 133 batch 160 train Loss 80.0466 test Loss 35.2332 with MSE metric 25939.1263\n",
      "Epoch 133 batch 170 train Loss 80.0242 test Loss 35.2243 with MSE metric 25933.6685\n",
      "Epoch 133 batch 180 train Loss 80.0019 test Loss 35.2154 with MSE metric 25928.3219\n",
      "Epoch 133 batch 190 train Loss 79.9796 test Loss 35.2064 with MSE metric 25922.8807\n",
      "Epoch 133 batch 200 train Loss 79.9573 test Loss 35.1975 with MSE metric 25917.3624\n",
      "Epoch 133 batch 210 train Loss 79.9350 test Loss 35.1886 with MSE metric 25911.9651\n",
      "Epoch 133 batch 220 train Loss 79.9127 test Loss 35.1798 with MSE metric 25906.5078\n",
      "Epoch 133 batch 230 train Loss 79.8904 test Loss 35.1709 with MSE metric 25901.1469\n",
      "Epoch 133 batch 240 train Loss 79.8681 test Loss 35.1620 with MSE metric 25895.7015\n",
      "Time taken for 1 epoch: 28.49120306968689 secs\n",
      "\n",
      "Epoch 134 batch 0 train Loss 79.8459 test Loss 35.1531 with MSE metric 25890.3742\n",
      "Epoch 134 batch 10 train Loss 79.8236 test Loss 35.1443 with MSE metric 25884.9459\n",
      "Epoch 134 batch 20 train Loss 79.8014 test Loss 35.1354 with MSE metric 25879.4756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134 batch 30 train Loss 79.7792 test Loss 35.1266 with MSE metric 25874.0669\n",
      "Epoch 134 batch 40 train Loss 79.7570 test Loss 35.1177 with MSE metric 25868.7295\n",
      "Epoch 134 batch 50 train Loss 79.7348 test Loss 35.1089 with MSE metric 25863.4031\n",
      "Epoch 134 batch 60 train Loss 79.7127 test Loss 35.1000 with MSE metric 25858.1110\n",
      "Epoch 134 batch 70 train Loss 79.6905 test Loss 35.0911 with MSE metric 25852.8749\n",
      "Epoch 134 batch 80 train Loss 79.6684 test Loss 35.0823 with MSE metric 25847.4875\n",
      "Epoch 134 batch 90 train Loss 79.6463 test Loss 35.0735 with MSE metric 25842.0956\n",
      "Epoch 134 batch 100 train Loss 79.6241 test Loss 35.0646 with MSE metric 25836.7356\n",
      "Epoch 134 batch 110 train Loss 79.6020 test Loss 35.0558 with MSE metric 25831.4325\n",
      "Epoch 134 batch 120 train Loss 79.5799 test Loss 35.0470 with MSE metric 25825.9455\n",
      "Epoch 134 batch 130 train Loss 79.5579 test Loss 35.0381 with MSE metric 25820.6783\n",
      "Epoch 134 batch 140 train Loss 79.5358 test Loss 35.0293 with MSE metric 25815.2680\n",
      "Epoch 134 batch 150 train Loss 79.5137 test Loss 35.0205 with MSE metric 25809.9402\n",
      "Epoch 134 batch 160 train Loss 79.4917 test Loss 35.0117 with MSE metric 25804.4938\n",
      "Epoch 134 batch 170 train Loss 79.4697 test Loss 35.0029 with MSE metric 25799.0932\n",
      "Epoch 134 batch 180 train Loss 79.4476 test Loss 34.9941 with MSE metric 25793.8312\n",
      "Epoch 134 batch 190 train Loss 79.4256 test Loss 34.9853 with MSE metric 25788.5446\n",
      "Epoch 134 batch 200 train Loss 79.4036 test Loss 34.9765 with MSE metric 25783.2729\n",
      "Epoch 134 batch 210 train Loss 79.3817 test Loss 34.9678 with MSE metric 25777.9982\n",
      "Epoch 134 batch 220 train Loss 79.3597 test Loss 34.9590 with MSE metric 25772.5800\n",
      "Epoch 134 batch 230 train Loss 79.3378 test Loss 34.9503 with MSE metric 25767.2663\n",
      "Epoch 134 batch 240 train Loss 79.3158 test Loss 34.9415 with MSE metric 25761.9377\n",
      "Time taken for 1 epoch: 29.51839804649353 secs\n",
      "\n",
      "Epoch 135 batch 0 train Loss 79.2939 test Loss 34.9327 with MSE metric 25756.5683\n",
      "Epoch 135 batch 10 train Loss 79.2720 test Loss 34.9240 with MSE metric 25751.1836\n",
      "Epoch 135 batch 20 train Loss 79.2501 test Loss 34.9152 with MSE metric 25745.9078\n",
      "Epoch 135 batch 30 train Loss 79.2282 test Loss 34.9065 with MSE metric 25740.5466\n",
      "Epoch 135 batch 40 train Loss 79.2063 test Loss 34.8978 with MSE metric 25735.2527\n",
      "Epoch 135 batch 50 train Loss 79.1845 test Loss 34.8891 with MSE metric 25729.9019\n",
      "Epoch 135 batch 60 train Loss 79.1626 test Loss 34.8803 with MSE metric 25724.4452\n",
      "Epoch 135 batch 70 train Loss 79.1408 test Loss 34.8716 with MSE metric 25719.1359\n",
      "Epoch 135 batch 80 train Loss 79.1190 test Loss 34.8629 with MSE metric 25713.7171\n",
      "Epoch 135 batch 90 train Loss 79.0972 test Loss 34.8542 with MSE metric 25708.4072\n",
      "Epoch 135 batch 100 train Loss 79.0754 test Loss 34.8455 with MSE metric 25703.0425\n",
      "Epoch 135 batch 110 train Loss 79.0536 test Loss 34.8368 with MSE metric 25697.8293\n",
      "Epoch 135 batch 120 train Loss 79.0318 test Loss 34.8281 with MSE metric 25692.4908\n",
      "Epoch 135 batch 130 train Loss 79.0101 test Loss 34.8194 with MSE metric 25687.3220\n",
      "Epoch 135 batch 140 train Loss 78.9883 test Loss 34.8108 with MSE metric 25682.0777\n",
      "Epoch 135 batch 150 train Loss 78.9666 test Loss 34.8021 with MSE metric 25676.7198\n",
      "Epoch 135 batch 160 train Loss 78.9449 test Loss 34.7934 with MSE metric 25671.5355\n",
      "Epoch 135 batch 170 train Loss 78.9232 test Loss 34.7848 with MSE metric 25666.2140\n",
      "Epoch 135 batch 180 train Loss 78.9015 test Loss 34.7762 with MSE metric 25660.8932\n",
      "Epoch 135 batch 190 train Loss 78.8798 test Loss 34.7675 with MSE metric 25655.5498\n",
      "Epoch 135 batch 200 train Loss 78.8581 test Loss 34.7589 with MSE metric 25650.3608\n",
      "Epoch 135 batch 210 train Loss 78.8365 test Loss 34.7502 with MSE metric 25645.0433\n",
      "Epoch 135 batch 220 train Loss 78.8148 test Loss 34.7416 with MSE metric 25639.8004\n",
      "Epoch 135 batch 230 train Loss 78.7932 test Loss 34.7330 with MSE metric 25634.5509\n",
      "Epoch 135 batch 240 train Loss 78.7716 test Loss 34.7244 with MSE metric 25629.3188\n",
      "Time taken for 1 epoch: 26.238627195358276 secs\n",
      "\n",
      "Epoch 136 batch 0 train Loss 78.7500 test Loss 34.7158 with MSE metric 25624.0854\n",
      "Epoch 136 batch 10 train Loss 78.7284 test Loss 34.7072 with MSE metric 25618.9734\n",
      "Epoch 136 batch 20 train Loss 78.7068 test Loss 34.6985 with MSE metric 25613.6844\n",
      "Epoch 136 batch 30 train Loss 78.6853 test Loss 34.6899 with MSE metric 25608.5133\n",
      "Epoch 136 batch 40 train Loss 78.6637 test Loss 34.6813 with MSE metric 25603.1819\n",
      "Epoch 136 batch 50 train Loss 78.6422 test Loss 34.6727 with MSE metric 25597.9809\n",
      "Epoch 136 batch 60 train Loss 78.6207 test Loss 34.6642 with MSE metric 25592.7383\n",
      "Epoch 136 batch 70 train Loss 78.5992 test Loss 34.6556 with MSE metric 25587.5366\n",
      "Epoch 136 batch 80 train Loss 78.5777 test Loss 34.6470 with MSE metric 25582.3309\n",
      "Epoch 136 batch 90 train Loss 78.5562 test Loss 34.6384 with MSE metric 25577.0367\n",
      "Epoch 136 batch 100 train Loss 78.5347 test Loss 34.6299 with MSE metric 25571.7479\n",
      "Epoch 136 batch 110 train Loss 78.5132 test Loss 34.6213 with MSE metric 25566.5123\n",
      "Epoch 136 batch 120 train Loss 78.4918 test Loss 34.6128 with MSE metric 25561.2719\n",
      "Epoch 136 batch 130 train Loss 78.4703 test Loss 34.6042 with MSE metric 25555.9186\n",
      "Epoch 136 batch 140 train Loss 78.4489 test Loss 34.5957 with MSE metric 25550.6869\n",
      "Epoch 136 batch 150 train Loss 78.4275 test Loss 34.5871 with MSE metric 25545.4888\n",
      "Epoch 136 batch 160 train Loss 78.4061 test Loss 34.5786 with MSE metric 25540.2776\n",
      "Epoch 136 batch 170 train Loss 78.3847 test Loss 34.5700 with MSE metric 25534.9778\n",
      "Epoch 136 batch 180 train Loss 78.3633 test Loss 34.5615 with MSE metric 25529.7759\n",
      "Epoch 136 batch 190 train Loss 78.3420 test Loss 34.5529 with MSE metric 25524.4785\n",
      "Epoch 136 batch 200 train Loss 78.3206 test Loss 34.5444 with MSE metric 25519.2131\n",
      "Epoch 136 batch 210 train Loss 78.2993 test Loss 34.5359 with MSE metric 25514.0537\n",
      "Epoch 136 batch 220 train Loss 78.2780 test Loss 34.5274 with MSE metric 25508.8872\n",
      "Epoch 136 batch 230 train Loss 78.2567 test Loss 34.5189 with MSE metric 25503.7922\n",
      "Epoch 136 batch 240 train Loss 78.2354 test Loss 34.5104 with MSE metric 25498.4827\n",
      "Time taken for 1 epoch: 24.546266794204712 secs\n",
      "\n",
      "Epoch 137 batch 0 train Loss 78.2141 test Loss 34.5019 with MSE metric 25493.2470\n",
      "Epoch 137 batch 10 train Loss 78.1928 test Loss 34.4934 with MSE metric 25487.9786\n",
      "Epoch 137 batch 20 train Loss 78.1715 test Loss 34.4849 with MSE metric 25482.8685\n",
      "Epoch 137 batch 30 train Loss 78.1503 test Loss 34.4764 with MSE metric 25477.9236\n",
      "Epoch 137 batch 40 train Loss 78.1291 test Loss 34.4679 with MSE metric 25472.7291\n",
      "Epoch 137 batch 50 train Loss 78.1078 test Loss 34.4595 with MSE metric 25467.6436\n",
      "Epoch 137 batch 60 train Loss 78.0866 test Loss 34.4510 with MSE metric 25462.5255\n",
      "Epoch 137 batch 70 train Loss 78.0654 test Loss 34.4426 with MSE metric 25457.4902\n",
      "Epoch 137 batch 80 train Loss 78.0442 test Loss 34.4341 with MSE metric 25452.4199\n",
      "Epoch 137 batch 90 train Loss 78.0231 test Loss 34.4256 with MSE metric 25447.1934\n",
      "Epoch 137 batch 100 train Loss 78.0019 test Loss 34.4172 with MSE metric 25442.0606\n",
      "Epoch 137 batch 110 train Loss 77.9808 test Loss 34.4087 with MSE metric 25436.8540\n",
      "Epoch 137 batch 120 train Loss 77.9596 test Loss 34.4003 with MSE metric 25431.7993\n",
      "Epoch 137 batch 130 train Loss 77.9385 test Loss 34.3918 with MSE metric 25426.7237\n",
      "Epoch 137 batch 140 train Loss 77.9174 test Loss 34.3834 with MSE metric 25421.6228\n",
      "Epoch 137 batch 150 train Loss 77.8963 test Loss 34.3750 with MSE metric 25416.4535\n",
      "Epoch 137 batch 160 train Loss 77.8752 test Loss 34.3666 with MSE metric 25411.4338\n",
      "Epoch 137 batch 170 train Loss 77.8541 test Loss 34.3582 with MSE metric 25406.4515\n",
      "Epoch 137 batch 180 train Loss 77.8331 test Loss 34.3498 with MSE metric 25401.3126\n",
      "Epoch 137 batch 190 train Loss 77.8120 test Loss 34.3413 with MSE metric 25396.2245\n",
      "Epoch 137 batch 200 train Loss 77.7910 test Loss 34.3329 with MSE metric 25391.0720\n",
      "Epoch 137 batch 210 train Loss 77.7699 test Loss 34.3245 with MSE metric 25386.0710\n",
      "Epoch 137 batch 220 train Loss 77.7489 test Loss 34.3162 with MSE metric 25381.0012\n",
      "Epoch 137 batch 230 train Loss 77.7279 test Loss 34.3078 with MSE metric 25375.9243\n",
      "Epoch 137 batch 240 train Loss 77.7069 test Loss 34.2994 with MSE metric 25370.7721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 24.199883699417114 secs\n",
      "\n",
      "Epoch 138 batch 0 train Loss 77.6859 test Loss 34.2910 with MSE metric 25365.6653\n",
      "Epoch 138 batch 10 train Loss 77.6650 test Loss 34.2826 with MSE metric 25360.6146\n",
      "Epoch 138 batch 20 train Loss 77.6440 test Loss 34.2743 with MSE metric 25355.5820\n",
      "Epoch 138 batch 30 train Loss 77.6231 test Loss 34.2659 with MSE metric 25350.5373\n",
      "Epoch 138 batch 40 train Loss 77.6021 test Loss 34.2576 with MSE metric 25345.5554\n",
      "Epoch 138 batch 50 train Loss 77.5812 test Loss 34.2492 with MSE metric 25340.4687\n",
      "Epoch 138 batch 60 train Loss 77.5603 test Loss 34.2409 with MSE metric 25335.3297\n",
      "Epoch 138 batch 70 train Loss 77.5394 test Loss 34.2325 with MSE metric 25330.2843\n",
      "Epoch 138 batch 80 train Loss 77.5185 test Loss 34.2242 with MSE metric 25325.0618\n",
      "Epoch 138 batch 90 train Loss 77.4977 test Loss 34.2159 with MSE metric 25319.9729\n",
      "Epoch 138 batch 100 train Loss 77.4768 test Loss 34.2075 with MSE metric 25315.0424\n",
      "Epoch 138 batch 110 train Loss 77.4560 test Loss 34.1992 with MSE metric 25309.8970\n",
      "Epoch 138 batch 120 train Loss 77.4351 test Loss 34.1909 with MSE metric 25304.7812\n",
      "Epoch 138 batch 130 train Loss 77.4143 test Loss 34.1826 with MSE metric 25299.7961\n",
      "Epoch 138 batch 140 train Loss 77.3935 test Loss 34.1743 with MSE metric 25294.7609\n",
      "Epoch 138 batch 150 train Loss 77.3727 test Loss 34.1660 with MSE metric 25289.6968\n",
      "Epoch 138 batch 160 train Loss 77.3519 test Loss 34.1577 with MSE metric 25284.6369\n",
      "Epoch 138 batch 170 train Loss 77.3311 test Loss 34.1494 with MSE metric 25279.4685\n",
      "Epoch 138 batch 180 train Loss 77.3104 test Loss 34.1411 with MSE metric 25274.4225\n",
      "Epoch 138 batch 190 train Loss 77.2896 test Loss 34.1329 with MSE metric 25269.3138\n",
      "Epoch 138 batch 200 train Loss 77.2689 test Loss 34.1246 with MSE metric 25264.2625\n",
      "Epoch 138 batch 210 train Loss 77.2481 test Loss 34.1163 with MSE metric 25259.1399\n",
      "Epoch 138 batch 220 train Loss 77.2274 test Loss 34.1081 with MSE metric 25254.1421\n",
      "Epoch 138 batch 230 train Loss 77.2067 test Loss 34.0998 with MSE metric 25249.0201\n",
      "Epoch 138 batch 240 train Loss 77.1860 test Loss 34.0915 with MSE metric 25244.0583\n",
      "Time taken for 1 epoch: 24.382557153701782 secs\n",
      "\n",
      "Epoch 139 batch 0 train Loss 77.1654 test Loss 34.0833 with MSE metric 25239.1073\n",
      "Epoch 139 batch 10 train Loss 77.1447 test Loss 34.0750 with MSE metric 25234.0348\n",
      "Epoch 139 batch 20 train Loss 77.1240 test Loss 34.0668 with MSE metric 25228.9755\n",
      "Epoch 139 batch 30 train Loss 77.1034 test Loss 34.0585 with MSE metric 25223.9187\n",
      "Epoch 139 batch 40 train Loss 77.0828 test Loss 34.0503 with MSE metric 25218.9499\n",
      "Epoch 139 batch 50 train Loss 77.0621 test Loss 34.0420 with MSE metric 25213.9705\n",
      "Epoch 139 batch 60 train Loss 77.0415 test Loss 34.0338 with MSE metric 25208.9418\n",
      "Epoch 139 batch 70 train Loss 77.0209 test Loss 34.0256 with MSE metric 25203.8476\n",
      "Epoch 139 batch 80 train Loss 77.0003 test Loss 34.0174 with MSE metric 25198.9197\n",
      "Epoch 139 batch 90 train Loss 76.9798 test Loss 34.0092 with MSE metric 25193.8136\n",
      "Epoch 139 batch 100 train Loss 76.9592 test Loss 34.0009 with MSE metric 25188.9640\n",
      "Epoch 139 batch 110 train Loss 76.9387 test Loss 33.9927 with MSE metric 25183.9933\n",
      "Epoch 139 batch 120 train Loss 76.9181 test Loss 33.9846 with MSE metric 25179.0312\n",
      "Epoch 139 batch 130 train Loss 76.8976 test Loss 33.9764 with MSE metric 25174.1490\n",
      "Epoch 139 batch 140 train Loss 76.8771 test Loss 33.9682 with MSE metric 25169.2013\n",
      "Epoch 139 batch 150 train Loss 76.8566 test Loss 33.9600 with MSE metric 25164.2892\n",
      "Epoch 139 batch 160 train Loss 76.8361 test Loss 33.9518 with MSE metric 25159.2533\n",
      "Epoch 139 batch 170 train Loss 76.8156 test Loss 33.9437 with MSE metric 25154.3264\n",
      "Epoch 139 batch 180 train Loss 76.7952 test Loss 33.9355 with MSE metric 25149.3283\n",
      "Epoch 139 batch 190 train Loss 76.7747 test Loss 33.9274 with MSE metric 25144.4057\n",
      "Epoch 139 batch 200 train Loss 76.7543 test Loss 33.9192 with MSE metric 25139.4995\n",
      "Epoch 139 batch 210 train Loss 76.7338 test Loss 33.9110 with MSE metric 25134.6504\n",
      "Epoch 139 batch 220 train Loss 76.7134 test Loss 33.9029 with MSE metric 25129.7450\n",
      "Epoch 139 batch 230 train Loss 76.6930 test Loss 33.8948 with MSE metric 25124.7819\n",
      "Epoch 139 batch 240 train Loss 76.6726 test Loss 33.8866 with MSE metric 25119.8320\n",
      "Time taken for 1 epoch: 24.744321584701538 secs\n",
      "\n",
      "Epoch 140 batch 0 train Loss 76.6523 test Loss 33.8785 with MSE metric 25115.0006\n",
      "Epoch 140 batch 10 train Loss 76.6319 test Loss 33.8703 with MSE metric 25110.0148\n",
      "Epoch 140 batch 20 train Loss 76.6115 test Loss 33.8622 with MSE metric 25105.0572\n",
      "Epoch 140 batch 30 train Loss 76.5912 test Loss 33.8541 with MSE metric 25100.1742\n",
      "Epoch 140 batch 40 train Loss 76.5708 test Loss 33.8459 with MSE metric 25095.3593\n",
      "Epoch 140 batch 50 train Loss 76.5505 test Loss 33.8379 with MSE metric 25090.5025\n",
      "Epoch 140 batch 60 train Loss 76.5302 test Loss 33.8297 with MSE metric 25085.5205\n",
      "Epoch 140 batch 70 train Loss 76.5099 test Loss 33.8216 with MSE metric 25080.5925\n",
      "Epoch 140 batch 80 train Loss 76.4896 test Loss 33.8135 with MSE metric 25075.6943\n",
      "Epoch 140 batch 90 train Loss 76.4693 test Loss 33.8054 with MSE metric 25070.7258\n",
      "Epoch 140 batch 100 train Loss 76.4491 test Loss 33.7973 with MSE metric 25065.7358\n",
      "Epoch 140 batch 110 train Loss 76.4288 test Loss 33.7892 with MSE metric 25060.8257\n",
      "Epoch 140 batch 120 train Loss 76.4086 test Loss 33.7812 with MSE metric 25055.8546\n",
      "Epoch 140 batch 130 train Loss 76.3883 test Loss 33.7731 with MSE metric 25051.0178\n",
      "Epoch 140 batch 140 train Loss 76.3681 test Loss 33.7650 with MSE metric 25046.0439\n",
      "Epoch 140 batch 150 train Loss 76.3479 test Loss 33.7570 with MSE metric 25041.0953\n",
      "Epoch 140 batch 160 train Loss 76.3277 test Loss 33.7489 with MSE metric 25036.1320\n",
      "Epoch 140 batch 170 train Loss 76.3075 test Loss 33.7409 with MSE metric 25031.2386\n",
      "Epoch 140 batch 180 train Loss 76.2873 test Loss 33.7329 with MSE metric 25026.3347\n",
      "Epoch 140 batch 190 train Loss 76.2672 test Loss 33.7248 with MSE metric 25021.4443\n",
      "Epoch 140 batch 200 train Loss 76.2470 test Loss 33.7168 with MSE metric 25016.5456\n",
      "Epoch 140 batch 210 train Loss 76.2269 test Loss 33.7087 with MSE metric 25011.7586\n",
      "Epoch 140 batch 220 train Loss 76.2067 test Loss 33.7007 with MSE metric 25006.8961\n",
      "Epoch 140 batch 230 train Loss 76.1866 test Loss 33.6927 with MSE metric 25002.0203\n",
      "Epoch 140 batch 240 train Loss 76.1665 test Loss 33.6847 with MSE metric 24997.0547\n",
      "Time taken for 1 epoch: 24.941713094711304 secs\n",
      "\n",
      "Epoch 141 batch 0 train Loss 76.1464 test Loss 33.6766 with MSE metric 24992.1384\n",
      "Epoch 141 batch 10 train Loss 76.1263 test Loss 33.6686 with MSE metric 24987.2803\n",
      "Epoch 141 batch 20 train Loss 76.1063 test Loss 33.6606 with MSE metric 24982.3483\n",
      "Epoch 141 batch 30 train Loss 76.0862 test Loss 33.6526 with MSE metric 24977.3754\n",
      "Epoch 141 batch 40 train Loss 76.0661 test Loss 33.6446 with MSE metric 24972.5207\n",
      "Epoch 141 batch 50 train Loss 76.0461 test Loss 33.6366 with MSE metric 24967.6977\n",
      "Epoch 141 batch 60 train Loss 76.0261 test Loss 33.6286 with MSE metric 24962.8635\n",
      "Epoch 141 batch 70 train Loss 76.0061 test Loss 33.6207 with MSE metric 24958.0472\n",
      "Epoch 141 batch 80 train Loss 75.9861 test Loss 33.6127 with MSE metric 24953.1461\n",
      "Epoch 141 batch 90 train Loss 75.9661 test Loss 33.6047 with MSE metric 24948.3204\n",
      "Epoch 141 batch 100 train Loss 75.9461 test Loss 33.5967 with MSE metric 24943.5262\n",
      "Epoch 141 batch 110 train Loss 75.9261 test Loss 33.5888 with MSE metric 24938.7476\n",
      "Epoch 141 batch 120 train Loss 75.9062 test Loss 33.5808 with MSE metric 24933.8040\n",
      "Epoch 141 batch 130 train Loss 75.8862 test Loss 33.5728 with MSE metric 24929.0002\n",
      "Epoch 141 batch 140 train Loss 75.8663 test Loss 33.5649 with MSE metric 24924.0589\n",
      "Epoch 141 batch 150 train Loss 75.8464 test Loss 33.5569 with MSE metric 24919.2771\n",
      "Epoch 141 batch 160 train Loss 75.8264 test Loss 33.5490 with MSE metric 24914.4375\n",
      "Epoch 141 batch 170 train Loss 75.8065 test Loss 33.5411 with MSE metric 24909.6171\n",
      "Epoch 141 batch 180 train Loss 75.7866 test Loss 33.5332 with MSE metric 24904.7000\n",
      "Epoch 141 batch 190 train Loss 75.7668 test Loss 33.5253 with MSE metric 24899.7762\n",
      "Epoch 141 batch 200 train Loss 75.7469 test Loss 33.5174 with MSE metric 24895.0162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141 batch 210 train Loss 75.7270 test Loss 33.5094 with MSE metric 24890.1360\n",
      "Epoch 141 batch 220 train Loss 75.7072 test Loss 33.5015 with MSE metric 24885.2757\n",
      "Epoch 141 batch 230 train Loss 75.6873 test Loss 33.4936 with MSE metric 24880.5000\n",
      "Epoch 141 batch 240 train Loss 75.6675 test Loss 33.4857 with MSE metric 24875.6948\n",
      "Time taken for 1 epoch: 24.328299045562744 secs\n",
      "\n",
      "Epoch 142 batch 0 train Loss 75.6477 test Loss 33.4778 with MSE metric 24870.9259\n",
      "Epoch 142 batch 10 train Loss 75.6279 test Loss 33.4699 with MSE metric 24866.1084\n",
      "Epoch 142 batch 20 train Loss 75.6081 test Loss 33.4620 with MSE metric 24861.2389\n",
      "Epoch 142 batch 30 train Loss 75.5883 test Loss 33.4541 with MSE metric 24856.4235\n",
      "Epoch 142 batch 40 train Loss 75.5686 test Loss 33.4462 with MSE metric 24851.5631\n",
      "Epoch 142 batch 50 train Loss 75.5488 test Loss 33.4383 with MSE metric 24846.6711\n",
      "Epoch 142 batch 60 train Loss 75.5291 test Loss 33.4305 with MSE metric 24841.7122\n",
      "Epoch 142 batch 70 train Loss 75.5093 test Loss 33.4226 with MSE metric 24836.9098\n",
      "Epoch 142 batch 80 train Loss 75.4896 test Loss 33.4147 with MSE metric 24832.1127\n",
      "Epoch 142 batch 90 train Loss 75.4699 test Loss 33.4068 with MSE metric 24827.3156\n",
      "Epoch 142 batch 100 train Loss 75.4502 test Loss 33.3990 with MSE metric 24822.5312\n",
      "Epoch 142 batch 110 train Loss 75.4305 test Loss 33.3911 with MSE metric 24817.6872\n",
      "Epoch 142 batch 120 train Loss 75.4108 test Loss 33.3833 with MSE metric 24812.9875\n",
      "Epoch 142 batch 130 train Loss 75.3912 test Loss 33.3754 with MSE metric 24808.1854\n",
      "Epoch 142 batch 140 train Loss 75.3715 test Loss 33.3676 with MSE metric 24803.4666\n",
      "Epoch 142 batch 150 train Loss 75.3518 test Loss 33.3597 with MSE metric 24798.6202\n",
      "Epoch 142 batch 160 train Loss 75.3322 test Loss 33.3519 with MSE metric 24793.8104\n",
      "Epoch 142 batch 170 train Loss 75.3126 test Loss 33.3441 with MSE metric 24788.9152\n",
      "Epoch 142 batch 180 train Loss 75.2929 test Loss 33.3363 with MSE metric 24784.1168\n",
      "Epoch 142 batch 190 train Loss 75.2733 test Loss 33.3284 with MSE metric 24779.4641\n",
      "Epoch 142 batch 200 train Loss 75.2537 test Loss 33.3206 with MSE metric 24774.5651\n",
      "Epoch 142 batch 210 train Loss 75.2342 test Loss 33.3128 with MSE metric 24769.8441\n",
      "Epoch 142 batch 220 train Loss 75.2146 test Loss 33.3050 with MSE metric 24765.0739\n",
      "Epoch 142 batch 230 train Loss 75.1950 test Loss 33.2972 with MSE metric 24760.2630\n",
      "Epoch 142 batch 240 train Loss 75.1755 test Loss 33.2894 with MSE metric 24755.4939\n",
      "Time taken for 1 epoch: 24.614437103271484 secs\n",
      "\n",
      "Epoch 143 batch 0 train Loss 75.1559 test Loss 33.2816 with MSE metric 24750.7828\n",
      "Epoch 143 batch 10 train Loss 75.1364 test Loss 33.2739 with MSE metric 24746.0625\n",
      "Epoch 143 batch 20 train Loss 75.1169 test Loss 33.2661 with MSE metric 24741.2171\n",
      "Epoch 143 batch 30 train Loss 75.0974 test Loss 33.2583 with MSE metric 24736.6090\n",
      "Epoch 143 batch 40 train Loss 75.0779 test Loss 33.2506 with MSE metric 24731.7699\n",
      "Epoch 143 batch 50 train Loss 75.0584 test Loss 33.2428 with MSE metric 24727.0859\n",
      "Epoch 143 batch 60 train Loss 75.0389 test Loss 33.2350 with MSE metric 24722.3928\n",
      "Epoch 143 batch 70 train Loss 75.0195 test Loss 33.2273 with MSE metric 24717.6807\n",
      "Epoch 143 batch 80 train Loss 75.0000 test Loss 33.2195 with MSE metric 24712.9881\n",
      "Epoch 143 batch 90 train Loss 74.9806 test Loss 33.2118 with MSE metric 24708.3242\n",
      "Epoch 143 batch 100 train Loss 74.9612 test Loss 33.2040 with MSE metric 24703.5790\n",
      "Epoch 143 batch 110 train Loss 74.9418 test Loss 33.1962 with MSE metric 24698.8976\n",
      "Epoch 143 batch 120 train Loss 74.9224 test Loss 33.1885 with MSE metric 24694.1563\n",
      "Epoch 143 batch 130 train Loss 74.9030 test Loss 33.1807 with MSE metric 24689.3837\n",
      "Epoch 143 batch 140 train Loss 74.8836 test Loss 33.1730 with MSE metric 24684.6942\n",
      "Epoch 143 batch 150 train Loss 74.8642 test Loss 33.1653 with MSE metric 24679.9405\n",
      "Epoch 143 batch 160 train Loss 74.8448 test Loss 33.1576 with MSE metric 24675.2475\n",
      "Epoch 143 batch 170 train Loss 74.8255 test Loss 33.1498 with MSE metric 24670.5180\n",
      "Epoch 143 batch 180 train Loss 74.8061 test Loss 33.1421 with MSE metric 24665.9049\n",
      "Epoch 143 batch 190 train Loss 74.7868 test Loss 33.1344 with MSE metric 24661.2626\n",
      "Epoch 143 batch 200 train Loss 74.7675 test Loss 33.1267 with MSE metric 24656.5923\n",
      "Epoch 143 batch 210 train Loss 74.7482 test Loss 33.1190 with MSE metric 24651.9648\n",
      "Epoch 143 batch 220 train Loss 74.7289 test Loss 33.1113 with MSE metric 24647.3107\n",
      "Epoch 143 batch 230 train Loss 74.7096 test Loss 33.1036 with MSE metric 24642.6905\n",
      "Epoch 143 batch 240 train Loss 74.6903 test Loss 33.0960 with MSE metric 24638.0299\n",
      "Time taken for 1 epoch: 24.851551055908203 secs\n",
      "\n",
      "Epoch 144 batch 0 train Loss 74.6710 test Loss 33.0883 with MSE metric 24633.2971\n",
      "Epoch 144 batch 10 train Loss 74.6518 test Loss 33.0806 with MSE metric 24628.5153\n",
      "Epoch 144 batch 20 train Loss 74.6325 test Loss 33.0729 with MSE metric 24623.8382\n",
      "Epoch 144 batch 30 train Loss 74.6133 test Loss 33.0652 with MSE metric 24619.1579\n",
      "Epoch 144 batch 40 train Loss 74.5941 test Loss 33.0576 with MSE metric 24614.5511\n",
      "Epoch 144 batch 50 train Loss 74.5748 test Loss 33.0499 with MSE metric 24609.8961\n",
      "Epoch 144 batch 60 train Loss 74.5556 test Loss 33.0422 with MSE metric 24605.2320\n",
      "Epoch 144 batch 70 train Loss 74.5364 test Loss 33.0345 with MSE metric 24600.5932\n",
      "Epoch 144 batch 80 train Loss 74.5173 test Loss 33.0269 with MSE metric 24595.8647\n",
      "Epoch 144 batch 90 train Loss 74.4981 test Loss 33.0192 with MSE metric 24591.2547\n",
      "Epoch 144 batch 100 train Loss 74.4789 test Loss 33.0116 with MSE metric 24586.5094\n",
      "Epoch 144 batch 110 train Loss 74.4598 test Loss 33.0039 with MSE metric 24581.9042\n",
      "Epoch 144 batch 120 train Loss 74.4406 test Loss 32.9963 with MSE metric 24577.3066\n",
      "Epoch 144 batch 130 train Loss 74.4215 test Loss 32.9887 with MSE metric 24572.7496\n",
      "Epoch 144 batch 140 train Loss 74.4024 test Loss 32.9810 with MSE metric 24568.0981\n",
      "Epoch 144 batch 150 train Loss 74.3833 test Loss 32.9734 with MSE metric 24563.4553\n",
      "Epoch 144 batch 160 train Loss 74.3642 test Loss 32.9658 with MSE metric 24558.8436\n",
      "Epoch 144 batch 170 train Loss 74.3451 test Loss 32.9581 with MSE metric 24554.2629\n",
      "Epoch 144 batch 180 train Loss 74.3260 test Loss 32.9505 with MSE metric 24549.7346\n",
      "Epoch 144 batch 190 train Loss 74.3070 test Loss 32.9429 with MSE metric 24545.0222\n",
      "Epoch 144 batch 200 train Loss 74.2879 test Loss 32.9353 with MSE metric 24540.3255\n",
      "Epoch 144 batch 210 train Loss 74.2688 test Loss 32.9277 with MSE metric 24535.6776\n",
      "Epoch 144 batch 220 train Loss 74.2498 test Loss 32.9201 with MSE metric 24531.0055\n",
      "Epoch 144 batch 230 train Loss 74.2308 test Loss 32.9125 with MSE metric 24526.4242\n",
      "Epoch 144 batch 240 train Loss 74.2118 test Loss 32.9049 with MSE metric 24521.7929\n",
      "Time taken for 1 epoch: 24.435301780700684 secs\n",
      "\n",
      "Epoch 145 batch 0 train Loss 74.1928 test Loss 32.8973 with MSE metric 24517.1606\n",
      "Epoch 145 batch 10 train Loss 74.1738 test Loss 32.8897 with MSE metric 24512.5281\n",
      "Epoch 145 batch 20 train Loss 74.1548 test Loss 32.8822 with MSE metric 24508.0468\n",
      "Epoch 145 batch 30 train Loss 74.1358 test Loss 32.8746 with MSE metric 24503.3761\n",
      "Epoch 145 batch 40 train Loss 74.1169 test Loss 32.8671 with MSE metric 24498.6697\n",
      "Epoch 145 batch 50 train Loss 74.0979 test Loss 32.8595 with MSE metric 24494.1465\n",
      "Epoch 145 batch 60 train Loss 74.0790 test Loss 32.8519 with MSE metric 24489.5235\n",
      "Epoch 145 batch 70 train Loss 74.0600 test Loss 32.8444 with MSE metric 24484.9326\n",
      "Epoch 145 batch 80 train Loss 74.0411 test Loss 32.8368 with MSE metric 24480.3586\n",
      "Epoch 145 batch 90 train Loss 74.0222 test Loss 32.8293 with MSE metric 24475.7789\n",
      "Epoch 145 batch 100 train Loss 74.0033 test Loss 32.8217 with MSE metric 24471.1388\n",
      "Epoch 145 batch 110 train Loss 73.9844 test Loss 32.8142 with MSE metric 24466.5109\n",
      "Epoch 145 batch 120 train Loss 73.9655 test Loss 32.8066 with MSE metric 24461.9098\n",
      "Epoch 145 batch 130 train Loss 73.9467 test Loss 32.7991 with MSE metric 24457.3699\n",
      "Epoch 145 batch 140 train Loss 73.9278 test Loss 32.7916 with MSE metric 24452.7451\n",
      "Epoch 145 batch 150 train Loss 73.9090 test Loss 32.7840 with MSE metric 24448.1197\n",
      "Epoch 145 batch 160 train Loss 73.8901 test Loss 32.7765 with MSE metric 24443.4680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145 batch 170 train Loss 73.8713 test Loss 32.7690 with MSE metric 24438.9423\n",
      "Epoch 145 batch 180 train Loss 73.8525 test Loss 32.7615 with MSE metric 24434.2931\n",
      "Epoch 145 batch 190 train Loss 73.8337 test Loss 32.7540 with MSE metric 24429.6422\n",
      "Epoch 145 batch 200 train Loss 73.8149 test Loss 32.7465 with MSE metric 24425.0028\n",
      "Epoch 145 batch 210 train Loss 73.7961 test Loss 32.7390 with MSE metric 24420.3445\n",
      "Epoch 145 batch 220 train Loss 73.7773 test Loss 32.7315 with MSE metric 24415.8053\n",
      "Epoch 145 batch 230 train Loss 73.7586 test Loss 32.7240 with MSE metric 24411.1150\n",
      "Epoch 145 batch 240 train Loss 73.7398 test Loss 32.7165 with MSE metric 24406.4486\n",
      "Time taken for 1 epoch: 25.1923770904541 secs\n",
      "\n",
      "Epoch 146 batch 0 train Loss 73.7210 test Loss 32.7091 with MSE metric 24401.9152\n",
      "Epoch 146 batch 10 train Loss 73.7023 test Loss 32.7016 with MSE metric 24397.3176\n",
      "Epoch 146 batch 20 train Loss 73.6836 test Loss 32.6941 with MSE metric 24392.7946\n",
      "Epoch 146 batch 30 train Loss 73.6649 test Loss 32.6866 with MSE metric 24388.2306\n",
      "Epoch 146 batch 40 train Loss 73.6462 test Loss 32.6792 with MSE metric 24383.6439\n",
      "Epoch 146 batch 50 train Loss 73.6275 test Loss 32.6717 with MSE metric 24379.0964\n",
      "Epoch 146 batch 60 train Loss 73.6088 test Loss 32.6643 with MSE metric 24374.5674\n",
      "Epoch 146 batch 70 train Loss 73.5901 test Loss 32.6568 with MSE metric 24370.0557\n",
      "Epoch 146 batch 80 train Loss 73.5715 test Loss 32.6494 with MSE metric 24365.4675\n",
      "Epoch 146 batch 90 train Loss 73.5528 test Loss 32.6419 with MSE metric 24360.9883\n",
      "Epoch 146 batch 100 train Loss 73.5342 test Loss 32.6345 with MSE metric 24356.4451\n",
      "Epoch 146 batch 110 train Loss 73.5155 test Loss 32.6270 with MSE metric 24351.9073\n",
      "Epoch 146 batch 120 train Loss 73.4969 test Loss 32.6196 with MSE metric 24347.3890\n",
      "Epoch 146 batch 130 train Loss 73.4783 test Loss 32.6122 with MSE metric 24342.8736\n",
      "Epoch 146 batch 140 train Loss 73.4597 test Loss 32.6048 with MSE metric 24338.3268\n",
      "Epoch 146 batch 150 train Loss 73.4411 test Loss 32.5973 with MSE metric 24333.7697\n",
      "Epoch 146 batch 160 train Loss 73.4225 test Loss 32.5899 with MSE metric 24329.3372\n",
      "Epoch 146 batch 170 train Loss 73.4040 test Loss 32.5825 with MSE metric 24324.8150\n",
      "Epoch 146 batch 180 train Loss 73.3854 test Loss 32.5751 with MSE metric 24320.2410\n",
      "Epoch 146 batch 190 train Loss 73.3668 test Loss 32.5677 with MSE metric 24315.7784\n",
      "Epoch 146 batch 200 train Loss 73.3483 test Loss 32.5603 with MSE metric 24311.3586\n",
      "Epoch 146 batch 210 train Loss 73.3298 test Loss 32.5529 with MSE metric 24306.8194\n",
      "Epoch 146 batch 220 train Loss 73.3113 test Loss 32.5455 with MSE metric 24302.3434\n",
      "Epoch 146 batch 230 train Loss 73.2928 test Loss 32.5382 with MSE metric 24297.7746\n",
      "Epoch 146 batch 240 train Loss 73.2742 test Loss 32.5308 with MSE metric 24293.1468\n",
      "Time taken for 1 epoch: 24.46507716178894 secs\n",
      "\n",
      "Epoch 147 batch 0 train Loss 73.2558 test Loss 32.5234 with MSE metric 24288.6187\n",
      "Epoch 147 batch 10 train Loss 73.2373 test Loss 32.5160 with MSE metric 24284.0871\n",
      "Epoch 147 batch 20 train Loss 73.2188 test Loss 32.5086 with MSE metric 24279.5823\n",
      "Epoch 147 batch 30 train Loss 73.2003 test Loss 32.5013 with MSE metric 24275.1341\n",
      "Epoch 147 batch 40 train Loss 73.1819 test Loss 32.4939 with MSE metric 24270.6063\n",
      "Epoch 147 batch 50 train Loss 73.1635 test Loss 32.4866 with MSE metric 24266.1834\n",
      "Epoch 147 batch 60 train Loss 73.1450 test Loss 32.4792 with MSE metric 24261.7338\n",
      "Epoch 147 batch 70 train Loss 73.1266 test Loss 32.4718 with MSE metric 24257.1653\n",
      "Epoch 147 batch 80 train Loss 73.1082 test Loss 32.4645 with MSE metric 24252.6542\n",
      "Epoch 147 batch 90 train Loss 73.0898 test Loss 32.4572 with MSE metric 24248.1578\n",
      "Epoch 147 batch 100 train Loss 73.0714 test Loss 32.4498 with MSE metric 24243.6169\n",
      "Epoch 147 batch 110 train Loss 73.0530 test Loss 32.4425 with MSE metric 24239.2124\n",
      "Epoch 147 batch 120 train Loss 73.0347 test Loss 32.4352 with MSE metric 24234.7328\n",
      "Epoch 147 batch 130 train Loss 73.0163 test Loss 32.4278 with MSE metric 24230.3336\n",
      "Epoch 147 batch 140 train Loss 72.9980 test Loss 32.4205 with MSE metric 24225.8573\n",
      "Epoch 147 batch 150 train Loss 72.9796 test Loss 32.4132 with MSE metric 24221.4188\n",
      "Epoch 147 batch 160 train Loss 72.9613 test Loss 32.4059 with MSE metric 24216.8563\n",
      "Epoch 147 batch 170 train Loss 72.9430 test Loss 32.3986 with MSE metric 24212.3497\n",
      "Epoch 147 batch 180 train Loss 72.9246 test Loss 32.3913 with MSE metric 24207.8512\n",
      "Epoch 147 batch 190 train Loss 72.9063 test Loss 32.3840 with MSE metric 24203.3723\n",
      "Epoch 147 batch 200 train Loss 72.8881 test Loss 32.3767 with MSE metric 24198.9420\n",
      "Epoch 147 batch 210 train Loss 72.8698 test Loss 32.3694 with MSE metric 24194.6204\n",
      "Epoch 147 batch 220 train Loss 72.8515 test Loss 32.3621 with MSE metric 24190.1667\n",
      "Epoch 147 batch 230 train Loss 72.8333 test Loss 32.3548 with MSE metric 24185.7695\n",
      "Epoch 147 batch 240 train Loss 72.8150 test Loss 32.3476 with MSE metric 24181.3151\n",
      "Time taken for 1 epoch: 24.521133184432983 secs\n",
      "\n",
      "Epoch 148 batch 0 train Loss 72.7968 test Loss 32.3403 with MSE metric 24176.9412\n",
      "Epoch 148 batch 10 train Loss 72.7785 test Loss 32.3330 with MSE metric 24172.5221\n",
      "Epoch 148 batch 20 train Loss 72.7603 test Loss 32.3257 with MSE metric 24168.0282\n",
      "Epoch 148 batch 30 train Loss 72.7421 test Loss 32.3185 with MSE metric 24163.5484\n",
      "Epoch 148 batch 40 train Loss 72.7239 test Loss 32.3112 with MSE metric 24159.0869\n",
      "Epoch 148 batch 50 train Loss 72.7057 test Loss 32.3040 with MSE metric 24154.6348\n",
      "Epoch 148 batch 60 train Loss 72.6875 test Loss 32.2967 with MSE metric 24150.1849\n",
      "Epoch 148 batch 70 train Loss 72.6693 test Loss 32.2895 with MSE metric 24145.7961\n",
      "Epoch 148 batch 80 train Loss 72.6512 test Loss 32.2822 with MSE metric 24141.3919\n",
      "Epoch 148 batch 90 train Loss 72.6330 test Loss 32.2750 with MSE metric 24136.9897\n",
      "Epoch 148 batch 100 train Loss 72.6149 test Loss 32.2677 with MSE metric 24132.4984\n",
      "Epoch 148 batch 110 train Loss 72.5968 test Loss 32.2605 with MSE metric 24128.1130\n",
      "Epoch 148 batch 120 train Loss 72.5786 test Loss 32.2532 with MSE metric 24123.7590\n",
      "Epoch 148 batch 130 train Loss 72.5605 test Loss 32.2460 with MSE metric 24119.3895\n",
      "Epoch 148 batch 140 train Loss 72.5424 test Loss 32.2388 with MSE metric 24114.9104\n",
      "Epoch 148 batch 150 train Loss 72.5243 test Loss 32.2315 with MSE metric 24110.5227\n",
      "Epoch 148 batch 160 train Loss 72.5062 test Loss 32.2243 with MSE metric 24106.1620\n",
      "Epoch 148 batch 170 train Loss 72.4882 test Loss 32.2171 with MSE metric 24101.8303\n",
      "Epoch 148 batch 180 train Loss 72.4701 test Loss 32.2099 with MSE metric 24097.4192\n",
      "Epoch 148 batch 190 train Loss 72.4520 test Loss 32.2027 with MSE metric 24093.0339\n",
      "Epoch 148 batch 200 train Loss 72.4340 test Loss 32.1955 with MSE metric 24088.6437\n",
      "Epoch 148 batch 210 train Loss 72.4160 test Loss 32.1883 with MSE metric 24084.2281\n",
      "Epoch 148 batch 220 train Loss 72.3979 test Loss 32.1811 with MSE metric 24079.8194\n",
      "Epoch 148 batch 230 train Loss 72.3799 test Loss 32.1739 with MSE metric 24075.4338\n",
      "Epoch 148 batch 240 train Loss 72.3619 test Loss 32.1667 with MSE metric 24071.0930\n",
      "Time taken for 1 epoch: 24.51261615753174 secs\n",
      "\n",
      "Epoch 149 batch 0 train Loss 72.3439 test Loss 32.1595 with MSE metric 24066.7483\n",
      "Epoch 149 batch 10 train Loss 72.3259 test Loss 32.1524 with MSE metric 24062.2946\n",
      "Epoch 149 batch 20 train Loss 72.3080 test Loss 32.1452 with MSE metric 24057.8834\n",
      "Epoch 149 batch 30 train Loss 72.2900 test Loss 32.1380 with MSE metric 24053.5661\n",
      "Epoch 149 batch 40 train Loss 72.2720 test Loss 32.1308 with MSE metric 24049.1617\n",
      "Epoch 149 batch 50 train Loss 72.2541 test Loss 32.1237 with MSE metric 24044.9322\n",
      "Epoch 149 batch 60 train Loss 72.2362 test Loss 32.1165 with MSE metric 24040.6833\n",
      "Epoch 149 batch 70 train Loss 72.2182 test Loss 32.1093 with MSE metric 24036.2966\n",
      "Epoch 149 batch 80 train Loss 72.2003 test Loss 32.1022 with MSE metric 24032.0201\n",
      "Epoch 149 batch 90 train Loss 72.1824 test Loss 32.0950 with MSE metric 24027.5800\n",
      "Epoch 149 batch 100 train Loss 72.1645 test Loss 32.0879 with MSE metric 24023.1513\n",
      "Epoch 149 batch 110 train Loss 72.1466 test Loss 32.0807 with MSE metric 24018.7478\n",
      "Epoch 149 batch 120 train Loss 72.1287 test Loss 32.0736 with MSE metric 24014.4846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 batch 130 train Loss 72.1109 test Loss 32.0665 with MSE metric 24010.0225\n",
      "Epoch 149 batch 140 train Loss 72.0930 test Loss 32.0593 with MSE metric 24005.7174\n",
      "Epoch 149 batch 150 train Loss 72.0751 test Loss 32.0522 with MSE metric 24001.4127\n",
      "Epoch 149 batch 160 train Loss 72.0573 test Loss 32.0451 with MSE metric 23997.1488\n",
      "Epoch 149 batch 170 train Loss 72.0395 test Loss 32.0380 with MSE metric 23992.7672\n",
      "Epoch 149 batch 180 train Loss 72.0216 test Loss 32.0309 with MSE metric 23988.4053\n",
      "Epoch 149 batch 190 train Loss 72.0038 test Loss 32.0237 with MSE metric 23983.9895\n",
      "Epoch 149 batch 200 train Loss 71.9860 test Loss 32.0166 with MSE metric 23979.6844\n",
      "Epoch 149 batch 210 train Loss 71.9682 test Loss 32.0095 with MSE metric 23975.2811\n",
      "Epoch 149 batch 220 train Loss 71.9504 test Loss 32.0024 with MSE metric 23970.9111\n",
      "Epoch 149 batch 230 train Loss 71.9327 test Loss 31.9953 with MSE metric 23966.5716\n",
      "Epoch 149 batch 240 train Loss 71.9149 test Loss 31.9882 with MSE metric 23962.2356\n",
      "Time taken for 1 epoch: 24.88361096382141 secs\n",
      "\n",
      "Epoch 150 batch 0 train Loss 71.8972 test Loss 31.9811 with MSE metric 23957.8454\n",
      "Epoch 150 batch 10 train Loss 71.8794 test Loss 31.9740 with MSE metric 23953.5077\n",
      "Epoch 150 batch 20 train Loss 71.8617 test Loss 31.9669 with MSE metric 23949.2376\n",
      "Epoch 150 batch 30 train Loss 71.8439 test Loss 31.9599 with MSE metric 23944.9398\n",
      "Epoch 150 batch 40 train Loss 71.8262 test Loss 31.9528 with MSE metric 23940.6475\n",
      "Epoch 150 batch 50 train Loss 71.8085 test Loss 31.9457 with MSE metric 23936.3849\n",
      "Epoch 150 batch 60 train Loss 71.7908 test Loss 31.9387 with MSE metric 23932.0888\n",
      "Epoch 150 batch 70 train Loss 71.7731 test Loss 31.9316 with MSE metric 23927.7308\n",
      "Epoch 150 batch 80 train Loss 71.7554 test Loss 31.9245 with MSE metric 23923.2826\n",
      "Epoch 150 batch 90 train Loss 71.7378 test Loss 31.9175 with MSE metric 23918.9956\n",
      "Epoch 150 batch 100 train Loss 71.7201 test Loss 31.9104 with MSE metric 23914.6957\n",
      "Epoch 150 batch 110 train Loss 71.7024 test Loss 31.9034 with MSE metric 23910.3982\n",
      "Epoch 150 batch 120 train Loss 71.6848 test Loss 31.8963 with MSE metric 23906.0635\n",
      "Epoch 150 batch 130 train Loss 71.6672 test Loss 31.8893 with MSE metric 23901.8992\n",
      "Epoch 150 batch 140 train Loss 71.6495 test Loss 31.8823 with MSE metric 23897.5987\n",
      "Epoch 150 batch 150 train Loss 71.6319 test Loss 31.8752 with MSE metric 23893.3064\n",
      "Epoch 150 batch 160 train Loss 71.6143 test Loss 31.8682 with MSE metric 23889.0842\n",
      "Epoch 150 batch 170 train Loss 71.5967 test Loss 31.8612 with MSE metric 23884.8348\n",
      "Epoch 150 batch 180 train Loss 71.5791 test Loss 31.8541 with MSE metric 23880.5278\n",
      "Epoch 150 batch 190 train Loss 71.5616 test Loss 31.8471 with MSE metric 23876.3434\n",
      "Epoch 150 batch 200 train Loss 71.5440 test Loss 31.8401 with MSE metric 23872.0110\n",
      "Epoch 150 batch 210 train Loss 71.5264 test Loss 31.8332 with MSE metric 23867.6374\n",
      "Epoch 150 batch 220 train Loss 71.5089 test Loss 31.8262 with MSE metric 23863.2666\n",
      "Epoch 150 batch 230 train Loss 71.4913 test Loss 31.8192 with MSE metric 23858.9259\n",
      "Epoch 150 batch 240 train Loss 71.4738 test Loss 31.8122 with MSE metric 23854.6966\n",
      "Time taken for 1 epoch: 24.510573148727417 secs\n",
      "\n",
      "Epoch 151 batch 0 train Loss 71.4563 test Loss 31.8052 with MSE metric 23850.4981\n",
      "Epoch 151 batch 10 train Loss 71.4388 test Loss 31.7982 with MSE metric 23846.1688\n",
      "Epoch 151 batch 20 train Loss 71.4213 test Loss 31.7912 with MSE metric 23841.9706\n",
      "Epoch 151 batch 30 train Loss 71.4038 test Loss 31.7842 with MSE metric 23837.7635\n",
      "Epoch 151 batch 40 train Loss 71.3863 test Loss 31.7773 with MSE metric 23833.4457\n",
      "Epoch 151 batch 50 train Loss 71.3688 test Loss 31.7703 with MSE metric 23829.2667\n",
      "Epoch 151 batch 60 train Loss 71.3513 test Loss 31.7633 with MSE metric 23825.0091\n",
      "Epoch 151 batch 70 train Loss 71.3339 test Loss 31.7563 with MSE metric 23820.6712\n",
      "Epoch 151 batch 80 train Loss 71.3164 test Loss 31.7494 with MSE metric 23816.4611\n",
      "Epoch 151 batch 90 train Loss 71.2990 test Loss 31.7424 with MSE metric 23812.2253\n",
      "Epoch 151 batch 100 train Loss 71.2816 test Loss 31.7355 with MSE metric 23807.9600\n",
      "Epoch 151 batch 110 train Loss 71.2642 test Loss 31.7286 with MSE metric 23803.7124\n",
      "Epoch 151 batch 120 train Loss 71.2467 test Loss 31.7216 with MSE metric 23799.4589\n",
      "Epoch 151 batch 130 train Loss 71.2293 test Loss 31.7146 with MSE metric 23795.2379\n",
      "Epoch 151 batch 140 train Loss 71.2120 test Loss 31.7077 with MSE metric 23791.0707\n",
      "Epoch 151 batch 150 train Loss 71.1946 test Loss 31.7008 with MSE metric 23786.8914\n",
      "Epoch 151 batch 160 train Loss 71.1772 test Loss 31.6939 with MSE metric 23782.6713\n",
      "Epoch 151 batch 170 train Loss 71.1598 test Loss 31.6870 with MSE metric 23778.4192\n",
      "Epoch 151 batch 180 train Loss 71.1425 test Loss 31.6800 with MSE metric 23774.2058\n",
      "Epoch 151 batch 190 train Loss 71.1251 test Loss 31.6731 with MSE metric 23770.0134\n",
      "Epoch 151 batch 200 train Loss 71.1078 test Loss 31.6662 with MSE metric 23765.6755\n",
      "Epoch 151 batch 210 train Loss 71.0905 test Loss 31.6592 with MSE metric 23761.4291\n",
      "Epoch 151 batch 220 train Loss 71.0731 test Loss 31.6523 with MSE metric 23757.2099\n",
      "Epoch 151 batch 230 train Loss 71.0558 test Loss 31.6454 with MSE metric 23752.9364\n",
      "Epoch 151 batch 240 train Loss 71.0385 test Loss 31.6385 with MSE metric 23748.6596\n",
      "Time taken for 1 epoch: 25.26799488067627 secs\n",
      "\n",
      "Epoch 152 batch 0 train Loss 71.0212 test Loss 31.6316 with MSE metric 23744.4322\n",
      "Epoch 152 batch 10 train Loss 71.0039 test Loss 31.6247 with MSE metric 23740.2910\n",
      "Epoch 152 batch 20 train Loss 70.9867 test Loss 31.6178 with MSE metric 23736.0876\n",
      "Epoch 152 batch 30 train Loss 70.9694 test Loss 31.6109 with MSE metric 23731.8283\n",
      "Epoch 152 batch 40 train Loss 70.9521 test Loss 31.6040 with MSE metric 23727.6479\n",
      "Epoch 152 batch 50 train Loss 70.9349 test Loss 31.5972 with MSE metric 23723.4343\n",
      "Epoch 152 batch 60 train Loss 70.9177 test Loss 31.5903 with MSE metric 23719.2960\n",
      "Epoch 152 batch 70 train Loss 70.9004 test Loss 31.5834 with MSE metric 23714.9947\n",
      "Epoch 152 batch 80 train Loss 70.8832 test Loss 31.5765 with MSE metric 23710.7954\n",
      "Epoch 152 batch 90 train Loss 70.8660 test Loss 31.5697 with MSE metric 23706.6291\n",
      "Epoch 152 batch 100 train Loss 70.8488 test Loss 31.5628 with MSE metric 23702.4631\n",
      "Epoch 152 batch 110 train Loss 70.8316 test Loss 31.5559 with MSE metric 23698.3055\n",
      "Epoch 152 batch 120 train Loss 70.8144 test Loss 31.5491 with MSE metric 23694.2586\n",
      "Epoch 152 batch 130 train Loss 70.7973 test Loss 31.5422 with MSE metric 23690.1506\n",
      "Epoch 152 batch 140 train Loss 70.7801 test Loss 31.5354 with MSE metric 23685.9550\n",
      "Epoch 152 batch 150 train Loss 70.7630 test Loss 31.5285 with MSE metric 23681.8122\n",
      "Epoch 152 batch 160 train Loss 70.7458 test Loss 31.5217 with MSE metric 23677.5576\n",
      "Epoch 152 batch 170 train Loss 70.7287 test Loss 31.5148 with MSE metric 23673.3128\n",
      "Epoch 152 batch 180 train Loss 70.7115 test Loss 31.5080 with MSE metric 23669.1430\n",
      "Epoch 152 batch 190 train Loss 70.6944 test Loss 31.5012 with MSE metric 23664.9548\n",
      "Epoch 152 batch 200 train Loss 70.6773 test Loss 31.4943 with MSE metric 23660.7120\n",
      "Epoch 152 batch 210 train Loss 70.6602 test Loss 31.4875 with MSE metric 23656.5428\n",
      "Epoch 152 batch 220 train Loss 70.6431 test Loss 31.4807 with MSE metric 23652.3630\n",
      "Epoch 152 batch 230 train Loss 70.6260 test Loss 31.4739 with MSE metric 23648.2436\n",
      "Epoch 152 batch 240 train Loss 70.6089 test Loss 31.4671 with MSE metric 23644.0598\n",
      "Time taken for 1 epoch: 26.558639764785767 secs\n",
      "\n",
      "Epoch 153 batch 0 train Loss 70.5918 test Loss 31.4603 with MSE metric 23639.8993\n",
      "Epoch 153 batch 10 train Loss 70.5748 test Loss 31.4535 with MSE metric 23635.8123\n",
      "Epoch 153 batch 20 train Loss 70.5577 test Loss 31.4467 with MSE metric 23631.6723\n",
      "Epoch 153 batch 30 train Loss 70.5407 test Loss 31.4399 with MSE metric 23627.5327\n",
      "Epoch 153 batch 40 train Loss 70.5237 test Loss 31.4332 with MSE metric 23623.4801\n",
      "Epoch 153 batch 50 train Loss 70.5067 test Loss 31.4264 with MSE metric 23619.3833\n",
      "Epoch 153 batch 60 train Loss 70.4896 test Loss 31.4196 with MSE metric 23615.2702\n",
      "Epoch 153 batch 70 train Loss 70.4726 test Loss 31.4128 with MSE metric 23611.0498\n",
      "Epoch 153 batch 80 train Loss 70.4556 test Loss 31.4060 with MSE metric 23606.8276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153 batch 90 train Loss 70.4386 test Loss 31.3992 with MSE metric 23602.6221\n",
      "Epoch 153 batch 100 train Loss 70.4217 test Loss 31.3924 with MSE metric 23598.4642\n",
      "Epoch 153 batch 110 train Loss 70.4047 test Loss 31.3857 with MSE metric 23594.3075\n",
      "Epoch 153 batch 120 train Loss 70.3877 test Loss 31.3789 with MSE metric 23590.1726\n",
      "Epoch 153 batch 130 train Loss 70.3708 test Loss 31.3721 with MSE metric 23585.9709\n",
      "Epoch 153 batch 140 train Loss 70.3538 test Loss 31.3654 with MSE metric 23581.7662\n",
      "Epoch 153 batch 150 train Loss 70.3369 test Loss 31.3586 with MSE metric 23577.6375\n",
      "Epoch 153 batch 160 train Loss 70.3200 test Loss 31.3519 with MSE metric 23573.4799\n",
      "Epoch 153 batch 170 train Loss 70.3030 test Loss 31.3451 with MSE metric 23569.3567\n",
      "Epoch 153 batch 180 train Loss 70.2861 test Loss 31.3384 with MSE metric 23565.2846\n",
      "Epoch 153 batch 190 train Loss 70.2692 test Loss 31.3316 with MSE metric 23561.1597\n",
      "Epoch 153 batch 200 train Loss 70.2524 test Loss 31.3248 with MSE metric 23557.0870\n",
      "Epoch 153 batch 210 train Loss 70.2355 test Loss 31.3181 with MSE metric 23552.9660\n",
      "Epoch 153 batch 220 train Loss 70.2186 test Loss 31.3114 with MSE metric 23548.9255\n",
      "Epoch 153 batch 230 train Loss 70.2017 test Loss 31.3046 with MSE metric 23544.7227\n",
      "Epoch 153 batch 240 train Loss 70.1849 test Loss 31.2979 with MSE metric 23540.4954\n",
      "Time taken for 1 epoch: 25.758726119995117 secs\n",
      "\n",
      "Epoch 154 batch 0 train Loss 70.1680 test Loss 31.2912 with MSE metric 23536.3588\n",
      "Epoch 154 batch 10 train Loss 70.1512 test Loss 31.2845 with MSE metric 23532.2651\n",
      "Epoch 154 batch 20 train Loss 70.1344 test Loss 31.2777 with MSE metric 23528.3438\n",
      "Epoch 154 batch 30 train Loss 70.1175 test Loss 31.2710 with MSE metric 23524.3436\n",
      "Epoch 154 batch 40 train Loss 70.1007 test Loss 31.2643 with MSE metric 23520.2471\n",
      "Epoch 154 batch 50 train Loss 70.0839 test Loss 31.2576 with MSE metric 23516.1775\n",
      "Epoch 154 batch 60 train Loss 70.0671 test Loss 31.2509 with MSE metric 23512.1009\n",
      "Epoch 154 batch 70 train Loss 70.0503 test Loss 31.2442 with MSE metric 23508.0603\n",
      "Epoch 154 batch 80 train Loss 70.0336 test Loss 31.2375 with MSE metric 23503.9804\n",
      "Epoch 154 batch 90 train Loss 70.0168 test Loss 31.2308 with MSE metric 23499.8184\n",
      "Epoch 154 batch 100 train Loss 70.0000 test Loss 31.2241 with MSE metric 23495.7206\n",
      "Epoch 154 batch 110 train Loss 69.9833 test Loss 31.2174 with MSE metric 23491.5779\n",
      "Epoch 154 batch 120 train Loss 69.9665 test Loss 31.2107 with MSE metric 23487.4978\n",
      "Epoch 154 batch 130 train Loss 69.9498 test Loss 31.2040 with MSE metric 23483.4461\n",
      "Epoch 154 batch 140 train Loss 69.9331 test Loss 31.1974 with MSE metric 23479.3386\n",
      "Epoch 154 batch 150 train Loss 69.9164 test Loss 31.1907 with MSE metric 23475.2554\n",
      "Epoch 154 batch 160 train Loss 69.8997 test Loss 31.1840 with MSE metric 23471.2030\n",
      "Epoch 154 batch 170 train Loss 69.8830 test Loss 31.1773 with MSE metric 23467.1039\n",
      "Epoch 154 batch 180 train Loss 69.8663 test Loss 31.1707 with MSE metric 23463.0616\n",
      "Epoch 154 batch 190 train Loss 69.8496 test Loss 31.1640 with MSE metric 23458.8855\n",
      "Epoch 154 batch 200 train Loss 69.8329 test Loss 31.1574 with MSE metric 23454.7727\n",
      "Epoch 154 batch 210 train Loss 69.8162 test Loss 31.1507 with MSE metric 23450.7310\n",
      "Epoch 154 batch 220 train Loss 69.7996 test Loss 31.1441 with MSE metric 23446.6938\n",
      "Epoch 154 batch 230 train Loss 69.7829 test Loss 31.1374 with MSE metric 23442.6895\n",
      "Epoch 154 batch 240 train Loss 69.7663 test Loss 31.1308 with MSE metric 23438.5917\n",
      "Time taken for 1 epoch: 23.247355937957764 secs\n",
      "\n",
      "Epoch 155 batch 0 train Loss 69.7497 test Loss 31.1241 with MSE metric 23434.5385\n",
      "Epoch 155 batch 10 train Loss 69.7330 test Loss 31.1175 with MSE metric 23430.5627\n",
      "Epoch 155 batch 20 train Loss 69.7164 test Loss 31.1108 with MSE metric 23426.5623\n",
      "Epoch 155 batch 30 train Loss 69.6998 test Loss 31.1042 with MSE metric 23422.5576\n",
      "Epoch 155 batch 40 train Loss 69.6832 test Loss 31.0976 with MSE metric 23418.5405\n",
      "Epoch 155 batch 50 train Loss 69.6666 test Loss 31.0910 with MSE metric 23414.4595\n",
      "Epoch 155 batch 60 train Loss 69.6501 test Loss 31.0843 with MSE metric 23410.4023\n",
      "Epoch 155 batch 70 train Loss 69.6335 test Loss 31.0777 with MSE metric 23406.4554\n",
      "Epoch 155 batch 80 train Loss 69.6169 test Loss 31.0711 with MSE metric 23402.4108\n",
      "Epoch 155 batch 90 train Loss 69.6004 test Loss 31.0645 with MSE metric 23398.2949\n",
      "Epoch 155 batch 100 train Loss 69.5838 test Loss 31.0579 with MSE metric 23394.3027\n",
      "Epoch 155 batch 110 train Loss 69.5673 test Loss 31.0513 with MSE metric 23390.2559\n",
      "Epoch 155 batch 120 train Loss 69.5508 test Loss 31.0447 with MSE metric 23386.1963\n",
      "Epoch 155 batch 130 train Loss 69.5342 test Loss 31.0381 with MSE metric 23382.1483\n",
      "Epoch 155 batch 140 train Loss 69.5177 test Loss 31.0315 with MSE metric 23378.0751\n",
      "Epoch 155 batch 150 train Loss 69.5012 test Loss 31.0249 with MSE metric 23374.0544\n",
      "Epoch 155 batch 160 train Loss 69.4847 test Loss 31.0183 with MSE metric 23370.0344\n",
      "Epoch 155 batch 170 train Loss 69.4682 test Loss 31.0118 with MSE metric 23366.0072\n",
      "Epoch 155 batch 180 train Loss 69.4518 test Loss 31.0051 with MSE metric 23361.9073\n",
      "Epoch 155 batch 190 train Loss 69.4353 test Loss 30.9986 with MSE metric 23357.9025\n",
      "Epoch 155 batch 200 train Loss 69.4188 test Loss 30.9920 with MSE metric 23353.8800\n",
      "Epoch 155 batch 210 train Loss 69.4024 test Loss 30.9854 with MSE metric 23349.8326\n",
      "Epoch 155 batch 220 train Loss 69.3860 test Loss 30.9789 with MSE metric 23345.8606\n",
      "Epoch 155 batch 230 train Loss 69.3695 test Loss 30.9723 with MSE metric 23341.9136\n",
      "Epoch 155 batch 240 train Loss 69.3531 test Loss 30.9658 with MSE metric 23337.7879\n",
      "Time taken for 1 epoch: 23.58967113494873 secs\n",
      "\n",
      "Epoch 156 batch 0 train Loss 69.3367 test Loss 30.9592 with MSE metric 23333.6823\n",
      "Epoch 156 batch 10 train Loss 69.3203 test Loss 30.9526 with MSE metric 23329.7062\n",
      "Epoch 156 batch 20 train Loss 69.3039 test Loss 30.9461 with MSE metric 23325.7658\n",
      "Epoch 156 batch 30 train Loss 69.2875 test Loss 30.9396 with MSE metric 23321.7427\n",
      "Epoch 156 batch 40 train Loss 69.2711 test Loss 30.9330 with MSE metric 23317.6587\n",
      "Epoch 156 batch 50 train Loss 69.2547 test Loss 30.9265 with MSE metric 23313.7016\n",
      "Epoch 156 batch 60 train Loss 69.2383 test Loss 30.9200 with MSE metric 23309.6486\n",
      "Epoch 156 batch 70 train Loss 69.2220 test Loss 30.9135 with MSE metric 23305.6339\n",
      "Epoch 156 batch 80 train Loss 69.2056 test Loss 30.9070 with MSE metric 23301.7139\n",
      "Epoch 156 batch 90 train Loss 69.1893 test Loss 30.9005 with MSE metric 23297.6806\n",
      "Epoch 156 batch 100 train Loss 69.1729 test Loss 30.8939 with MSE metric 23293.7234\n",
      "Epoch 156 batch 110 train Loss 69.1566 test Loss 30.8874 with MSE metric 23289.6703\n",
      "Epoch 156 batch 120 train Loss 69.1403 test Loss 30.8809 with MSE metric 23285.7480\n",
      "Epoch 156 batch 130 train Loss 69.1240 test Loss 30.8744 with MSE metric 23281.6878\n",
      "Epoch 156 batch 140 train Loss 69.1077 test Loss 30.8679 with MSE metric 23277.8343\n",
      "Epoch 156 batch 150 train Loss 69.0914 test Loss 30.8614 with MSE metric 23273.8160\n",
      "Epoch 156 batch 160 train Loss 69.0751 test Loss 30.8549 with MSE metric 23269.7716\n",
      "Epoch 156 batch 170 train Loss 69.0588 test Loss 30.8484 with MSE metric 23265.9556\n",
      "Epoch 156 batch 180 train Loss 69.0426 test Loss 30.8419 with MSE metric 23261.9487\n",
      "Epoch 156 batch 190 train Loss 69.0263 test Loss 30.8354 with MSE metric 23257.9535\n",
      "Epoch 156 batch 200 train Loss 69.0100 test Loss 30.8289 with MSE metric 23253.9616\n",
      "Epoch 156 batch 210 train Loss 68.9938 test Loss 30.8224 with MSE metric 23250.0048\n",
      "Epoch 156 batch 220 train Loss 68.9776 test Loss 30.8159 with MSE metric 23246.0728\n",
      "Epoch 156 batch 230 train Loss 68.9613 test Loss 30.8094 with MSE metric 23242.0416\n",
      "Epoch 156 batch 240 train Loss 68.9451 test Loss 30.8029 with MSE metric 23238.0719\n",
      "Time taken for 1 epoch: 23.96631669998169 secs\n",
      "\n",
      "Epoch 157 batch 0 train Loss 68.9289 test Loss 30.7965 with MSE metric 23234.1206\n",
      "Epoch 157 batch 10 train Loss 68.9127 test Loss 30.7900 with MSE metric 23230.1241\n",
      "Epoch 157 batch 20 train Loss 68.8965 test Loss 30.7835 with MSE metric 23226.1485\n",
      "Epoch 157 batch 30 train Loss 68.8803 test Loss 30.7770 with MSE metric 23222.2101\n",
      "Epoch 157 batch 40 train Loss 68.8641 test Loss 30.7706 with MSE metric 23218.3414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157 batch 50 train Loss 68.8480 test Loss 30.7641 with MSE metric 23214.3497\n",
      "Epoch 157 batch 60 train Loss 68.8318 test Loss 30.7577 with MSE metric 23210.5449\n",
      "Epoch 157 batch 70 train Loss 68.8157 test Loss 30.7512 with MSE metric 23206.6009\n",
      "Epoch 157 batch 80 train Loss 68.7995 test Loss 30.7448 with MSE metric 23202.7014\n",
      "Epoch 157 batch 90 train Loss 68.7834 test Loss 30.7383 with MSE metric 23198.7679\n",
      "Epoch 157 batch 100 train Loss 68.7673 test Loss 30.7319 with MSE metric 23194.8515\n",
      "Epoch 157 batch 110 train Loss 68.7511 test Loss 30.7254 with MSE metric 23190.9681\n",
      "Epoch 157 batch 120 train Loss 68.7350 test Loss 30.7190 with MSE metric 23187.0221\n",
      "Epoch 157 batch 130 train Loss 68.7189 test Loss 30.7125 with MSE metric 23183.1634\n",
      "Epoch 157 batch 140 train Loss 68.7028 test Loss 30.7061 with MSE metric 23179.2916\n",
      "Epoch 157 batch 150 train Loss 68.6868 test Loss 30.6997 with MSE metric 23175.4284\n",
      "Epoch 157 batch 160 train Loss 68.6707 test Loss 30.6932 with MSE metric 23171.5667\n",
      "Epoch 157 batch 170 train Loss 68.6546 test Loss 30.6868 with MSE metric 23167.6905\n",
      "Epoch 157 batch 180 train Loss 68.6385 test Loss 30.6804 with MSE metric 23163.8577\n",
      "Epoch 157 batch 190 train Loss 68.6225 test Loss 30.6740 with MSE metric 23159.9257\n",
      "Epoch 157 batch 200 train Loss 68.6064 test Loss 30.6676 with MSE metric 23155.9520\n",
      "Epoch 157 batch 210 train Loss 68.5904 test Loss 30.6612 with MSE metric 23151.9752\n",
      "Epoch 157 batch 220 train Loss 68.5744 test Loss 30.6548 with MSE metric 23148.0568\n",
      "Epoch 157 batch 230 train Loss 68.5584 test Loss 30.6484 with MSE metric 23144.2259\n",
      "Epoch 157 batch 240 train Loss 68.5423 test Loss 30.6420 with MSE metric 23140.3004\n",
      "Time taken for 1 epoch: 24.155724048614502 secs\n",
      "\n",
      "Epoch 158 batch 0 train Loss 68.5263 test Loss 30.6356 with MSE metric 23136.3992\n",
      "Epoch 158 batch 10 train Loss 68.5104 test Loss 30.6292 with MSE metric 23132.4389\n",
      "Epoch 158 batch 20 train Loss 68.4944 test Loss 30.6228 with MSE metric 23128.6173\n",
      "Epoch 158 batch 30 train Loss 68.4784 test Loss 30.6164 with MSE metric 23124.7894\n",
      "Epoch 158 batch 40 train Loss 68.4624 test Loss 30.6100 with MSE metric 23120.8917\n",
      "Epoch 158 batch 50 train Loss 68.4465 test Loss 30.6037 with MSE metric 23117.0536\n",
      "Epoch 158 batch 60 train Loss 68.4305 test Loss 30.5973 with MSE metric 23113.1978\n",
      "Epoch 158 batch 70 train Loss 68.4146 test Loss 30.5909 with MSE metric 23109.3312\n",
      "Epoch 158 batch 80 train Loss 68.3986 test Loss 30.5846 with MSE metric 23105.4337\n",
      "Epoch 158 batch 90 train Loss 68.3827 test Loss 30.5782 with MSE metric 23101.5837\n",
      "Epoch 158 batch 100 train Loss 68.3668 test Loss 30.5718 with MSE metric 23097.8377\n",
      "Epoch 158 batch 110 train Loss 68.3509 test Loss 30.5655 with MSE metric 23093.9807\n",
      "Epoch 158 batch 120 train Loss 68.3350 test Loss 30.5592 with MSE metric 23090.0350\n",
      "Epoch 158 batch 130 train Loss 68.3191 test Loss 30.5528 with MSE metric 23086.0470\n",
      "Epoch 158 batch 140 train Loss 68.3032 test Loss 30.5465 with MSE metric 23082.2591\n",
      "Epoch 158 batch 150 train Loss 68.2873 test Loss 30.5401 with MSE metric 23078.3839\n",
      "Epoch 158 batch 160 train Loss 68.2714 test Loss 30.5338 with MSE metric 23074.5211\n",
      "Epoch 158 batch 170 train Loss 68.2555 test Loss 30.5275 with MSE metric 23070.6316\n",
      "Epoch 158 batch 180 train Loss 68.2397 test Loss 30.5212 with MSE metric 23066.8400\n",
      "Epoch 158 batch 190 train Loss 68.2238 test Loss 30.5148 with MSE metric 23063.0278\n",
      "Epoch 158 batch 200 train Loss 68.2080 test Loss 30.5086 with MSE metric 23059.2331\n",
      "Epoch 158 batch 210 train Loss 68.1922 test Loss 30.5023 with MSE metric 23055.3258\n",
      "Epoch 158 batch 220 train Loss 68.1763 test Loss 30.4959 with MSE metric 23051.4709\n",
      "Epoch 158 batch 230 train Loss 68.1605 test Loss 30.4896 with MSE metric 23047.6339\n",
      "Epoch 158 batch 240 train Loss 68.1447 test Loss 30.4833 with MSE metric 23043.8099\n",
      "Time taken for 1 epoch: 24.009319067001343 secs\n",
      "\n",
      "Epoch 159 batch 0 train Loss 68.1289 test Loss 30.4770 with MSE metric 23040.0604\n",
      "Epoch 159 batch 10 train Loss 68.1131 test Loss 30.4707 with MSE metric 23036.2372\n",
      "Epoch 159 batch 20 train Loss 68.0973 test Loss 30.4644 with MSE metric 23032.3990\n",
      "Epoch 159 batch 30 train Loss 68.0815 test Loss 30.4581 with MSE metric 23028.5990\n",
      "Epoch 159 batch 40 train Loss 68.0658 test Loss 30.4518 with MSE metric 23024.7443\n",
      "Epoch 159 batch 50 train Loss 68.0500 test Loss 30.4455 with MSE metric 23020.9062\n",
      "Epoch 159 batch 60 train Loss 68.0342 test Loss 30.4392 with MSE metric 23016.9699\n",
      "Epoch 159 batch 70 train Loss 68.0185 test Loss 30.4330 with MSE metric 23013.0733\n",
      "Epoch 159 batch 80 train Loss 68.0028 test Loss 30.4267 with MSE metric 23009.3259\n",
      "Epoch 159 batch 90 train Loss 67.9870 test Loss 30.4204 with MSE metric 23005.4195\n",
      "Epoch 159 batch 100 train Loss 67.9713 test Loss 30.4141 with MSE metric 23001.5464\n",
      "Epoch 159 batch 110 train Loss 67.9556 test Loss 30.4078 with MSE metric 22997.7458\n",
      "Epoch 159 batch 120 train Loss 67.9399 test Loss 30.4016 with MSE metric 22993.8436\n",
      "Epoch 159 batch 130 train Loss 67.9242 test Loss 30.3953 with MSE metric 22990.0151\n",
      "Epoch 159 batch 140 train Loss 67.9085 test Loss 30.3891 with MSE metric 22986.2003\n",
      "Epoch 159 batch 150 train Loss 67.8928 test Loss 30.3828 with MSE metric 22982.3124\n",
      "Epoch 159 batch 160 train Loss 67.8771 test Loss 30.3765 with MSE metric 22978.5184\n",
      "Epoch 159 batch 170 train Loss 67.8614 test Loss 30.3703 with MSE metric 22974.6298\n",
      "Epoch 159 batch 180 train Loss 67.8458 test Loss 30.3640 with MSE metric 22970.7132\n",
      "Epoch 159 batch 190 train Loss 67.8301 test Loss 30.3578 with MSE metric 22966.9077\n",
      "Epoch 159 batch 200 train Loss 67.8145 test Loss 30.3515 with MSE metric 22963.1274\n",
      "Epoch 159 batch 210 train Loss 67.7988 test Loss 30.3453 with MSE metric 22959.2664\n",
      "Epoch 159 batch 220 train Loss 67.7832 test Loss 30.3391 with MSE metric 22955.4348\n",
      "Epoch 159 batch 230 train Loss 67.7676 test Loss 30.3329 with MSE metric 22951.7078\n",
      "Epoch 159 batch 240 train Loss 67.7520 test Loss 30.3266 with MSE metric 22947.8957\n",
      "Time taken for 1 epoch: 23.883249044418335 secs\n",
      "\n",
      "Epoch 160 batch 0 train Loss 67.7363 test Loss 30.3204 with MSE metric 22944.0389\n",
      "Epoch 160 batch 10 train Loss 67.7207 test Loss 30.3141 with MSE metric 22940.2540\n",
      "Epoch 160 batch 20 train Loss 67.7052 test Loss 30.3079 with MSE metric 22936.5004\n",
      "Epoch 160 batch 30 train Loss 67.6896 test Loss 30.3016 with MSE metric 22932.6694\n",
      "Epoch 160 batch 40 train Loss 67.6740 test Loss 30.2954 with MSE metric 22928.8319\n",
      "Epoch 160 batch 50 train Loss 67.6584 test Loss 30.2892 with MSE metric 22925.0088\n",
      "Epoch 160 batch 60 train Loss 67.6429 test Loss 30.2830 with MSE metric 22921.2505\n",
      "Epoch 160 batch 70 train Loss 67.6273 test Loss 30.2768 with MSE metric 22917.4347\n",
      "Epoch 160 batch 80 train Loss 67.6118 test Loss 30.2706 with MSE metric 22913.6286\n",
      "Epoch 160 batch 90 train Loss 67.5962 test Loss 30.2644 with MSE metric 22909.8273\n",
      "Epoch 160 batch 100 train Loss 67.5807 test Loss 30.2582 with MSE metric 22906.0552\n",
      "Epoch 160 batch 110 train Loss 67.5652 test Loss 30.2520 with MSE metric 22902.2805\n",
      "Epoch 160 batch 120 train Loss 67.5497 test Loss 30.2458 with MSE metric 22898.4373\n",
      "Epoch 160 batch 130 train Loss 67.5341 test Loss 30.2397 with MSE metric 22894.4942\n",
      "Epoch 160 batch 140 train Loss 67.5187 test Loss 30.2335 with MSE metric 22890.7573\n",
      "Epoch 160 batch 150 train Loss 67.5032 test Loss 30.2273 with MSE metric 22887.0071\n",
      "Epoch 160 batch 160 train Loss 67.4877 test Loss 30.2211 with MSE metric 22883.2688\n",
      "Epoch 160 batch 170 train Loss 67.4722 test Loss 30.2149 with MSE metric 22879.4845\n",
      "Epoch 160 batch 180 train Loss 67.4567 test Loss 30.2088 with MSE metric 22875.7960\n",
      "Epoch 160 batch 190 train Loss 67.4413 test Loss 30.2026 with MSE metric 22872.0230\n",
      "Epoch 160 batch 200 train Loss 67.4258 test Loss 30.1964 with MSE metric 22868.2713\n",
      "Epoch 160 batch 210 train Loss 67.4104 test Loss 30.1903 with MSE metric 22864.5539\n",
      "Epoch 160 batch 220 train Loss 67.3950 test Loss 30.1841 with MSE metric 22860.8168\n",
      "Epoch 160 batch 230 train Loss 67.3795 test Loss 30.1779 with MSE metric 22857.0311\n",
      "Epoch 160 batch 240 train Loss 67.3641 test Loss 30.1717 with MSE metric 22853.1913\n",
      "Time taken for 1 epoch: 24.21878409385681 secs\n",
      "\n",
      "Epoch 161 batch 0 train Loss 67.3487 test Loss 30.1656 with MSE metric 22849.4607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 batch 10 train Loss 67.3333 test Loss 30.1595 with MSE metric 22845.6386\n",
      "Epoch 161 batch 20 train Loss 67.3179 test Loss 30.1533 with MSE metric 22841.9306\n",
      "Epoch 161 batch 30 train Loss 67.3025 test Loss 30.1472 with MSE metric 22838.1709\n",
      "Epoch 161 batch 40 train Loss 67.2871 test Loss 30.1410 with MSE metric 22834.4269\n",
      "Epoch 161 batch 50 train Loss 67.2717 test Loss 30.1349 with MSE metric 22830.6339\n",
      "Epoch 161 batch 60 train Loss 67.2564 test Loss 30.1287 with MSE metric 22826.8039\n",
      "Epoch 161 batch 70 train Loss 67.2410 test Loss 30.1226 with MSE metric 22822.9866\n",
      "Epoch 161 batch 80 train Loss 67.2256 test Loss 30.1165 with MSE metric 22819.2339\n",
      "Epoch 161 batch 90 train Loss 67.2103 test Loss 30.1103 with MSE metric 22815.4624\n",
      "Epoch 161 batch 100 train Loss 67.1950 test Loss 30.1042 with MSE metric 22811.7143\n",
      "Epoch 161 batch 110 train Loss 67.1796 test Loss 30.0981 with MSE metric 22807.9176\n",
      "Epoch 161 batch 120 train Loss 67.1643 test Loss 30.0920 with MSE metric 22804.2076\n",
      "Epoch 161 batch 130 train Loss 67.1490 test Loss 30.0859 with MSE metric 22800.4630\n",
      "Epoch 161 batch 140 train Loss 67.1337 test Loss 30.0798 with MSE metric 22796.7237\n",
      "Epoch 161 batch 150 train Loss 67.1184 test Loss 30.0737 with MSE metric 22793.0151\n",
      "Epoch 161 batch 160 train Loss 67.1031 test Loss 30.0676 with MSE metric 22789.2473\n",
      "Epoch 161 batch 170 train Loss 67.0878 test Loss 30.0615 with MSE metric 22785.5156\n",
      "Epoch 161 batch 180 train Loss 67.0725 test Loss 30.0554 with MSE metric 22781.7657\n",
      "Epoch 161 batch 190 train Loss 67.0572 test Loss 30.0493 with MSE metric 22777.9770\n",
      "Epoch 161 batch 200 train Loss 67.0420 test Loss 30.0432 with MSE metric 22774.1872\n",
      "Epoch 161 batch 210 train Loss 67.0267 test Loss 30.0372 with MSE metric 22770.4961\n",
      "Epoch 161 batch 220 train Loss 67.0115 test Loss 30.0311 with MSE metric 22766.7989\n",
      "Epoch 161 batch 230 train Loss 66.9962 test Loss 30.0250 with MSE metric 22763.0864\n",
      "Epoch 161 batch 240 train Loss 66.9810 test Loss 30.0189 with MSE metric 22759.4745\n",
      "Time taken for 1 epoch: 24.325680255889893 secs\n",
      "\n",
      "Epoch 162 batch 0 train Loss 66.9658 test Loss 30.0128 with MSE metric 22755.7426\n",
      "Epoch 162 batch 10 train Loss 66.9506 test Loss 30.0067 with MSE metric 22752.0902\n",
      "Epoch 162 batch 20 train Loss 66.9354 test Loss 30.0007 with MSE metric 22748.3765\n",
      "Epoch 162 batch 30 train Loss 66.9202 test Loss 29.9946 with MSE metric 22744.6971\n",
      "Epoch 162 batch 40 train Loss 66.9050 test Loss 29.9885 with MSE metric 22740.9551\n",
      "Epoch 162 batch 50 train Loss 66.8898 test Loss 29.9824 with MSE metric 22737.2148\n",
      "Epoch 162 batch 60 train Loss 66.8746 test Loss 29.9764 with MSE metric 22733.5133\n",
      "Epoch 162 batch 70 train Loss 66.8595 test Loss 29.9703 with MSE metric 22729.7513\n",
      "Epoch 162 batch 80 train Loss 66.8443 test Loss 29.9642 with MSE metric 22726.0466\n",
      "Epoch 162 batch 90 train Loss 66.8291 test Loss 29.9582 with MSE metric 22722.3662\n",
      "Epoch 162 batch 100 train Loss 66.8140 test Loss 29.9521 with MSE metric 22718.6218\n",
      "Epoch 162 batch 110 train Loss 66.7988 test Loss 29.9461 with MSE metric 22714.9296\n",
      "Epoch 162 batch 120 train Loss 66.7837 test Loss 29.9400 with MSE metric 22711.2598\n",
      "Epoch 162 batch 130 train Loss 66.7686 test Loss 29.9340 with MSE metric 22707.5934\n",
      "Epoch 162 batch 140 train Loss 66.7535 test Loss 29.9280 with MSE metric 22703.9225\n",
      "Epoch 162 batch 150 train Loss 66.7384 test Loss 29.9219 with MSE metric 22700.2141\n",
      "Epoch 162 batch 160 train Loss 66.7233 test Loss 29.9159 with MSE metric 22696.5281\n",
      "Epoch 162 batch 170 train Loss 66.7082 test Loss 29.9099 with MSE metric 22692.8802\n",
      "Epoch 162 batch 180 train Loss 66.6931 test Loss 29.9038 with MSE metric 22689.2116\n",
      "Epoch 162 batch 190 train Loss 66.6780 test Loss 29.8978 with MSE metric 22685.3544\n",
      "Epoch 162 batch 200 train Loss 66.6629 test Loss 29.8918 with MSE metric 22681.6581\n",
      "Epoch 162 batch 210 train Loss 66.6478 test Loss 29.8858 with MSE metric 22677.9906\n",
      "Epoch 162 batch 220 train Loss 66.6328 test Loss 29.8798 with MSE metric 22674.1899\n",
      "Epoch 162 batch 230 train Loss 66.6177 test Loss 29.8738 with MSE metric 22670.4509\n",
      "Epoch 162 batch 240 train Loss 66.6027 test Loss 29.8678 with MSE metric 22666.7654\n",
      "Time taken for 1 epoch: 24.28379487991333 secs\n",
      "\n",
      "Epoch 163 batch 0 train Loss 66.5876 test Loss 29.8617 with MSE metric 22663.0148\n",
      "Epoch 163 batch 10 train Loss 66.5726 test Loss 29.8557 with MSE metric 22659.3595\n",
      "Epoch 163 batch 20 train Loss 66.5576 test Loss 29.8497 with MSE metric 22655.6985\n",
      "Epoch 163 batch 30 train Loss 66.5426 test Loss 29.8437 with MSE metric 22652.0766\n",
      "Epoch 163 batch 40 train Loss 66.5275 test Loss 29.8377 with MSE metric 22648.4341\n",
      "Epoch 163 batch 50 train Loss 66.5125 test Loss 29.8317 with MSE metric 22644.7769\n",
      "Epoch 163 batch 60 train Loss 66.4976 test Loss 29.8257 with MSE metric 22641.0899\n",
      "Epoch 163 batch 70 train Loss 66.4826 test Loss 29.8198 with MSE metric 22637.3920\n",
      "Epoch 163 batch 80 train Loss 66.4676 test Loss 29.8138 with MSE metric 22633.7157\n",
      "Epoch 163 batch 90 train Loss 66.4526 test Loss 29.8078 with MSE metric 22630.0685\n",
      "Epoch 163 batch 100 train Loss 66.4377 test Loss 29.8018 with MSE metric 22626.5475\n",
      "Epoch 163 batch 110 train Loss 66.4227 test Loss 29.7958 with MSE metric 22622.8669\n",
      "Epoch 163 batch 120 train Loss 66.4078 test Loss 29.7899 with MSE metric 22619.1989\n",
      "Epoch 163 batch 130 train Loss 66.3928 test Loss 29.7839 with MSE metric 22615.6069\n",
      "Epoch 163 batch 140 train Loss 66.3779 test Loss 29.7780 with MSE metric 22611.9137\n",
      "Epoch 163 batch 150 train Loss 66.3629 test Loss 29.7720 with MSE metric 22608.2159\n",
      "Epoch 163 batch 160 train Loss 66.3480 test Loss 29.7660 with MSE metric 22604.5469\n",
      "Epoch 163 batch 170 train Loss 66.3331 test Loss 29.7601 with MSE metric 22600.9208\n",
      "Epoch 163 batch 180 train Loss 66.3182 test Loss 29.7542 with MSE metric 22597.2763\n",
      "Epoch 163 batch 190 train Loss 66.3033 test Loss 29.7482 with MSE metric 22593.5846\n",
      "Epoch 163 batch 200 train Loss 66.2884 test Loss 29.7423 with MSE metric 22589.8592\n",
      "Epoch 163 batch 210 train Loss 66.2735 test Loss 29.7363 with MSE metric 22586.2360\n",
      "Epoch 163 batch 220 train Loss 66.2586 test Loss 29.7304 with MSE metric 22582.6698\n",
      "Epoch 163 batch 230 train Loss 66.2438 test Loss 29.7245 with MSE metric 22579.0530\n",
      "Epoch 163 batch 240 train Loss 66.2289 test Loss 29.7185 with MSE metric 22575.4663\n",
      "Time taken for 1 epoch: 23.937822103500366 secs\n",
      "\n",
      "Epoch 164 batch 0 train Loss 66.2141 test Loss 29.7126 with MSE metric 22571.9225\n",
      "Epoch 164 batch 10 train Loss 66.1992 test Loss 29.7067 with MSE metric 22568.2227\n",
      "Epoch 164 batch 20 train Loss 66.1844 test Loss 29.7008 with MSE metric 22564.6242\n",
      "Epoch 164 batch 30 train Loss 66.1695 test Loss 29.6949 with MSE metric 22561.0129\n",
      "Epoch 164 batch 40 train Loss 66.1547 test Loss 29.6889 with MSE metric 22557.4192\n",
      "Epoch 164 batch 50 train Loss 66.1399 test Loss 29.6830 with MSE metric 22553.8670\n",
      "Epoch 164 batch 60 train Loss 66.1251 test Loss 29.6771 with MSE metric 22550.2042\n",
      "Epoch 164 batch 70 train Loss 66.1103 test Loss 29.6712 with MSE metric 22546.5645\n",
      "Epoch 164 batch 80 train Loss 66.0955 test Loss 29.6654 with MSE metric 22542.9332\n",
      "Epoch 164 batch 90 train Loss 66.0807 test Loss 29.6595 with MSE metric 22539.2770\n",
      "Epoch 164 batch 100 train Loss 66.0659 test Loss 29.6536 with MSE metric 22535.6088\n",
      "Epoch 164 batch 110 train Loss 66.0511 test Loss 29.6477 with MSE metric 22532.0131\n",
      "Epoch 164 batch 120 train Loss 66.0364 test Loss 29.6418 with MSE metric 22528.4621\n",
      "Epoch 164 batch 130 train Loss 66.0216 test Loss 29.6359 with MSE metric 22524.8104\n",
      "Epoch 164 batch 140 train Loss 66.0068 test Loss 29.6300 with MSE metric 22521.2667\n",
      "Epoch 164 batch 150 train Loss 65.9921 test Loss 29.6241 with MSE metric 22517.7174\n",
      "Epoch 164 batch 160 train Loss 65.9773 test Loss 29.6182 with MSE metric 22514.0987\n",
      "Epoch 164 batch 170 train Loss 65.9626 test Loss 29.6124 with MSE metric 22510.3880\n",
      "Epoch 164 batch 180 train Loss 65.9479 test Loss 29.6065 with MSE metric 22506.8493\n",
      "Epoch 164 batch 190 train Loss 65.9332 test Loss 29.6007 with MSE metric 22503.3160\n",
      "Epoch 164 batch 200 train Loss 65.9185 test Loss 29.5948 with MSE metric 22499.7046\n",
      "Epoch 164 batch 210 train Loss 65.9037 test Loss 29.5890 with MSE metric 22496.1005\n",
      "Epoch 164 batch 220 train Loss 65.8891 test Loss 29.5831 with MSE metric 22492.4714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164 batch 230 train Loss 65.8744 test Loss 29.5772 with MSE metric 22488.8400\n",
      "Epoch 164 batch 240 train Loss 65.8597 test Loss 29.5714 with MSE metric 22485.2462\n",
      "Time taken for 1 epoch: 24.320390939712524 secs\n",
      "\n",
      "Epoch 165 batch 0 train Loss 65.8450 test Loss 29.5655 with MSE metric 22481.7042\n",
      "Epoch 165 batch 10 train Loss 65.8303 test Loss 29.5597 with MSE metric 22478.1385\n",
      "Epoch 165 batch 20 train Loss 65.8157 test Loss 29.5538 with MSE metric 22474.5111\n",
      "Epoch 165 batch 30 train Loss 65.8010 test Loss 29.5479 with MSE metric 22470.9576\n",
      "Epoch 165 batch 40 train Loss 65.7864 test Loss 29.5421 with MSE metric 22467.3898\n",
      "Epoch 165 batch 50 train Loss 65.7717 test Loss 29.5362 with MSE metric 22463.8051\n",
      "Epoch 165 batch 60 train Loss 65.7571 test Loss 29.5304 with MSE metric 22460.2338\n",
      "Epoch 165 batch 70 train Loss 65.7425 test Loss 29.5245 with MSE metric 22456.6139\n",
      "Epoch 165 batch 80 train Loss 65.7278 test Loss 29.5187 with MSE metric 22453.0109\n",
      "Epoch 165 batch 90 train Loss 65.7132 test Loss 29.5129 with MSE metric 22449.4322\n",
      "Epoch 165 batch 100 train Loss 65.6986 test Loss 29.5070 with MSE metric 22445.8253\n",
      "Epoch 165 batch 110 train Loss 65.6840 test Loss 29.5012 with MSE metric 22442.2803\n",
      "Epoch 165 batch 120 train Loss 65.6694 test Loss 29.4954 with MSE metric 22438.7519\n",
      "Epoch 165 batch 130 train Loss 65.6548 test Loss 29.4896 with MSE metric 22435.1376\n",
      "Epoch 165 batch 140 train Loss 65.6403 test Loss 29.4838 with MSE metric 22431.5644\n",
      "Epoch 165 batch 150 train Loss 65.6257 test Loss 29.4779 with MSE metric 22428.0326\n",
      "Epoch 165 batch 160 train Loss 65.6111 test Loss 29.4721 with MSE metric 22424.5276\n",
      "Epoch 165 batch 170 train Loss 65.5966 test Loss 29.4663 with MSE metric 22420.9277\n",
      "Epoch 165 batch 180 train Loss 65.5820 test Loss 29.4605 with MSE metric 22417.4580\n",
      "Epoch 165 batch 190 train Loss 65.5675 test Loss 29.4547 with MSE metric 22413.8517\n",
      "Epoch 165 batch 200 train Loss 65.5530 test Loss 29.4489 with MSE metric 22410.2899\n",
      "Epoch 165 batch 210 train Loss 65.5384 test Loss 29.4431 with MSE metric 22406.7731\n",
      "Epoch 165 batch 220 train Loss 65.5239 test Loss 29.4373 with MSE metric 22403.3158\n",
      "Epoch 165 batch 230 train Loss 65.5094 test Loss 29.4315 with MSE metric 22399.7826\n",
      "Epoch 165 batch 240 train Loss 65.4949 test Loss 29.4257 with MSE metric 22396.1290\n",
      "Time taken for 1 epoch: 24.093408584594727 secs\n",
      "\n",
      "Epoch 166 batch 0 train Loss 65.4804 test Loss 29.4199 with MSE metric 22392.5955\n",
      "Epoch 166 batch 10 train Loss 65.4659 test Loss 29.4141 with MSE metric 22389.0625\n",
      "Epoch 166 batch 20 train Loss 65.4514 test Loss 29.4083 with MSE metric 22385.5751\n",
      "Epoch 166 batch 30 train Loss 65.4370 test Loss 29.4025 with MSE metric 22382.0688\n",
      "Epoch 166 batch 40 train Loss 65.4225 test Loss 29.3968 with MSE metric 22378.5574\n",
      "Epoch 166 batch 50 train Loss 65.4080 test Loss 29.3910 with MSE metric 22374.9777\n",
      "Epoch 166 batch 60 train Loss 65.3936 test Loss 29.3852 with MSE metric 22371.3975\n",
      "Epoch 166 batch 70 train Loss 65.3791 test Loss 29.3795 with MSE metric 22367.8843\n",
      "Epoch 166 batch 80 train Loss 65.3647 test Loss 29.3738 with MSE metric 22364.3854\n",
      "Epoch 166 batch 90 train Loss 65.3502 test Loss 29.3680 with MSE metric 22360.8529\n",
      "Epoch 166 batch 100 train Loss 65.3358 test Loss 29.3622 with MSE metric 22357.4183\n",
      "Epoch 166 batch 110 train Loss 65.3214 test Loss 29.3565 with MSE metric 22353.9372\n",
      "Epoch 166 batch 120 train Loss 65.3070 test Loss 29.3507 with MSE metric 22350.4155\n",
      "Epoch 166 batch 130 train Loss 65.2926 test Loss 29.3449 with MSE metric 22346.9396\n",
      "Epoch 166 batch 140 train Loss 65.2782 test Loss 29.3392 with MSE metric 22343.4393\n",
      "Epoch 166 batch 150 train Loss 65.2638 test Loss 29.3334 with MSE metric 22339.9260\n",
      "Epoch 166 batch 160 train Loss 65.2494 test Loss 29.3276 with MSE metric 22336.3627\n",
      "Epoch 166 batch 170 train Loss 65.2350 test Loss 29.3219 with MSE metric 22332.9111\n",
      "Epoch 166 batch 180 train Loss 65.2206 test Loss 29.3162 with MSE metric 22329.3860\n",
      "Epoch 166 batch 190 train Loss 65.2063 test Loss 29.3105 with MSE metric 22325.9055\n",
      "Epoch 166 batch 200 train Loss 65.1919 test Loss 29.3047 with MSE metric 22322.3775\n",
      "Epoch 166 batch 210 train Loss 65.1775 test Loss 29.2990 with MSE metric 22318.8667\n",
      "Epoch 166 batch 220 train Loss 65.1632 test Loss 29.2933 with MSE metric 22315.4977\n",
      "Epoch 166 batch 230 train Loss 65.1489 test Loss 29.2876 with MSE metric 22311.9951\n",
      "Epoch 166 batch 240 train Loss 65.1345 test Loss 29.2818 with MSE metric 22308.5729\n",
      "Time taken for 1 epoch: 24.05841898918152 secs\n",
      "\n",
      "Epoch 167 batch 0 train Loss 65.1202 test Loss 29.2761 with MSE metric 22305.0837\n",
      "Epoch 167 batch 10 train Loss 65.1059 test Loss 29.2704 with MSE metric 22301.6066\n",
      "Epoch 167 batch 20 train Loss 65.0916 test Loss 29.2647 with MSE metric 22298.1510\n",
      "Epoch 167 batch 30 train Loss 65.0773 test Loss 29.2590 with MSE metric 22294.6773\n",
      "Epoch 167 batch 40 train Loss 65.0630 test Loss 29.2533 with MSE metric 22291.1259\n",
      "Epoch 167 batch 50 train Loss 65.0487 test Loss 29.2476 with MSE metric 22287.6805\n",
      "Epoch 167 batch 60 train Loss 65.0344 test Loss 29.2419 with MSE metric 22284.2511\n",
      "Epoch 167 batch 70 train Loss 65.0201 test Loss 29.2362 with MSE metric 22280.7778\n",
      "Epoch 167 batch 80 train Loss 65.0059 test Loss 29.2305 with MSE metric 22277.3103\n",
      "Epoch 167 batch 90 train Loss 64.9916 test Loss 29.2248 with MSE metric 22273.8908\n",
      "Epoch 167 batch 100 train Loss 64.9773 test Loss 29.2191 with MSE metric 22270.3920\n",
      "Epoch 167 batch 110 train Loss 64.9631 test Loss 29.2134 with MSE metric 22266.9041\n",
      "Epoch 167 batch 120 train Loss 64.9488 test Loss 29.2077 with MSE metric 22263.4395\n",
      "Epoch 167 batch 130 train Loss 64.9346 test Loss 29.2020 with MSE metric 22260.0333\n",
      "Epoch 167 batch 140 train Loss 64.9204 test Loss 29.1963 with MSE metric 22256.5866\n",
      "Epoch 167 batch 150 train Loss 64.9061 test Loss 29.1906 with MSE metric 22253.1533\n",
      "Epoch 167 batch 160 train Loss 64.8919 test Loss 29.1849 with MSE metric 22249.6908\n",
      "Epoch 167 batch 170 train Loss 64.8777 test Loss 29.1792 with MSE metric 22246.2513\n",
      "Epoch 167 batch 180 train Loss 64.8635 test Loss 29.1735 with MSE metric 22242.7528\n",
      "Epoch 167 batch 190 train Loss 64.8493 test Loss 29.1679 with MSE metric 22239.2877\n",
      "Epoch 167 batch 200 train Loss 64.8351 test Loss 29.1622 with MSE metric 22235.8410\n",
      "Epoch 167 batch 210 train Loss 64.8209 test Loss 29.1566 with MSE metric 22232.4484\n",
      "Epoch 167 batch 220 train Loss 64.8068 test Loss 29.1509 with MSE metric 22229.0195\n",
      "Epoch 167 batch 230 train Loss 64.7926 test Loss 29.1452 with MSE metric 22225.5252\n",
      "Epoch 167 batch 240 train Loss 64.7784 test Loss 29.1396 with MSE metric 22222.0996\n",
      "Time taken for 1 epoch: 24.21499991416931 secs\n",
      "\n",
      "Epoch 168 batch 0 train Loss 64.7643 test Loss 29.1340 with MSE metric 22218.5757\n",
      "Epoch 168 batch 10 train Loss 64.7501 test Loss 29.1283 with MSE metric 22215.2017\n",
      "Epoch 168 batch 20 train Loss 64.7360 test Loss 29.1227 with MSE metric 22211.8184\n",
      "Epoch 168 batch 30 train Loss 64.7219 test Loss 29.1170 with MSE metric 22208.4618\n",
      "Epoch 168 batch 40 train Loss 64.7077 test Loss 29.1114 with MSE metric 22205.0229\n",
      "Epoch 168 batch 50 train Loss 64.6936 test Loss 29.1058 with MSE metric 22201.5366\n",
      "Epoch 168 batch 60 train Loss 64.6795 test Loss 29.1001 with MSE metric 22198.1201\n",
      "Epoch 168 batch 70 train Loss 64.6654 test Loss 29.0945 with MSE metric 22194.6741\n",
      "Epoch 168 batch 80 train Loss 64.6513 test Loss 29.0889 with MSE metric 22191.2173\n",
      "Epoch 168 batch 90 train Loss 64.6372 test Loss 29.0833 with MSE metric 22187.7595\n",
      "Epoch 168 batch 100 train Loss 64.6231 test Loss 29.0776 with MSE metric 22184.2944\n",
      "Epoch 168 batch 110 train Loss 64.6090 test Loss 29.0720 with MSE metric 22180.9223\n",
      "Epoch 168 batch 120 train Loss 64.5949 test Loss 29.0664 with MSE metric 22177.5325\n",
      "Epoch 168 batch 130 train Loss 64.5809 test Loss 29.0607 with MSE metric 22174.0596\n",
      "Epoch 168 batch 140 train Loss 64.5668 test Loss 29.0551 with MSE metric 22170.6716\n",
      "Epoch 168 batch 150 train Loss 64.5527 test Loss 29.0495 with MSE metric 22167.2660\n",
      "Epoch 168 batch 160 train Loss 64.5387 test Loss 29.0439 with MSE metric 22163.8224\n",
      "Epoch 168 batch 170 train Loss 64.5247 test Loss 29.0383 with MSE metric 22160.4321\n",
      "Epoch 168 batch 180 train Loss 64.5106 test Loss 29.0327 with MSE metric 22156.9741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168 batch 190 train Loss 64.4966 test Loss 29.0271 with MSE metric 22153.5316\n",
      "Epoch 168 batch 200 train Loss 64.4826 test Loss 29.0215 with MSE metric 22150.0671\n",
      "Epoch 168 batch 210 train Loss 64.4685 test Loss 29.0159 with MSE metric 22146.5325\n",
      "Epoch 168 batch 220 train Loss 64.4545 test Loss 29.0103 with MSE metric 22143.1354\n",
      "Epoch 168 batch 230 train Loss 64.4405 test Loss 29.0047 with MSE metric 22139.7420\n",
      "Epoch 168 batch 240 train Loss 64.4265 test Loss 28.9991 with MSE metric 22136.2298\n",
      "Time taken for 1 epoch: 24.92238998413086 secs\n",
      "\n",
      "Epoch 169 batch 0 train Loss 64.4126 test Loss 28.9936 with MSE metric 22132.8240\n",
      "Epoch 169 batch 10 train Loss 64.3986 test Loss 28.9880 with MSE metric 22129.4537\n",
      "Epoch 169 batch 20 train Loss 64.3846 test Loss 28.9824 with MSE metric 22125.9822\n",
      "Epoch 169 batch 30 train Loss 64.3706 test Loss 28.9768 with MSE metric 22122.6215\n",
      "Epoch 169 batch 40 train Loss 64.3567 test Loss 28.9712 with MSE metric 22119.2511\n",
      "Epoch 169 batch 50 train Loss 64.3427 test Loss 28.9657 with MSE metric 22115.8493\n",
      "Epoch 169 batch 60 train Loss 64.3287 test Loss 28.9601 with MSE metric 22112.4671\n",
      "Epoch 169 batch 70 train Loss 64.3148 test Loss 28.9545 with MSE metric 22108.9966\n",
      "Epoch 169 batch 80 train Loss 64.3009 test Loss 28.9490 with MSE metric 22105.5593\n",
      "Epoch 169 batch 90 train Loss 64.2869 test Loss 28.9434 with MSE metric 22102.1225\n",
      "Epoch 169 batch 100 train Loss 64.2730 test Loss 28.9379 with MSE metric 22098.7523\n",
      "Epoch 169 batch 110 train Loss 64.2591 test Loss 28.9323 with MSE metric 22095.3672\n",
      "Epoch 169 batch 120 train Loss 64.2452 test Loss 28.9267 with MSE metric 22092.0021\n",
      "Epoch 169 batch 130 train Loss 64.2313 test Loss 28.9212 with MSE metric 22088.6933\n",
      "Epoch 169 batch 140 train Loss 64.2174 test Loss 28.9157 with MSE metric 22085.3393\n",
      "Epoch 169 batch 150 train Loss 64.2035 test Loss 28.9101 with MSE metric 22081.9495\n",
      "Epoch 169 batch 160 train Loss 64.1896 test Loss 28.9046 with MSE metric 22078.5756\n",
      "Epoch 169 batch 170 train Loss 64.1757 test Loss 28.8990 with MSE metric 22075.1325\n",
      "Epoch 169 batch 180 train Loss 64.1619 test Loss 28.8935 with MSE metric 22071.6937\n",
      "Epoch 169 batch 190 train Loss 64.1480 test Loss 28.8880 with MSE metric 22068.2809\n",
      "Epoch 169 batch 200 train Loss 64.1341 test Loss 28.8824 with MSE metric 22064.8528\n",
      "Epoch 169 batch 210 train Loss 64.1203 test Loss 28.8769 with MSE metric 22061.5503\n",
      "Epoch 169 batch 220 train Loss 64.1064 test Loss 28.8714 with MSE metric 22058.1351\n",
      "Epoch 169 batch 230 train Loss 64.0926 test Loss 28.8658 with MSE metric 22054.7474\n",
      "Epoch 169 batch 240 train Loss 64.0788 test Loss 28.8603 with MSE metric 22051.3861\n",
      "Time taken for 1 epoch: 24.63450288772583 secs\n",
      "\n",
      "Epoch 170 batch 0 train Loss 64.0649 test Loss 28.8548 with MSE metric 22047.9750\n",
      "Epoch 170 batch 10 train Loss 64.0511 test Loss 28.8493 with MSE metric 22044.5680\n",
      "Epoch 170 batch 20 train Loss 64.0373 test Loss 28.8438 with MSE metric 22041.2621\n",
      "Epoch 170 batch 30 train Loss 64.0235 test Loss 28.8383 with MSE metric 22037.9108\n",
      "Epoch 170 batch 40 train Loss 64.0097 test Loss 28.8328 with MSE metric 22034.5403\n",
      "Epoch 170 batch 50 train Loss 63.9959 test Loss 28.8272 with MSE metric 22031.2570\n",
      "Epoch 170 batch 60 train Loss 63.9821 test Loss 28.8218 with MSE metric 22027.9274\n",
      "Epoch 170 batch 70 train Loss 63.9684 test Loss 28.8163 with MSE metric 22024.5300\n",
      "Epoch 170 batch 80 train Loss 63.9546 test Loss 28.8108 with MSE metric 22021.1598\n",
      "Epoch 170 batch 90 train Loss 63.9408 test Loss 28.8053 with MSE metric 22017.7768\n",
      "Epoch 170 batch 100 train Loss 63.9271 test Loss 28.7998 with MSE metric 22014.4602\n",
      "Epoch 170 batch 110 train Loss 63.9133 test Loss 28.7943 with MSE metric 22011.1623\n",
      "Epoch 170 batch 120 train Loss 63.8996 test Loss 28.7888 with MSE metric 22007.7971\n",
      "Epoch 170 batch 130 train Loss 63.8858 test Loss 28.7833 with MSE metric 22004.4753\n",
      "Epoch 170 batch 140 train Loss 63.8721 test Loss 28.7778 with MSE metric 22001.1066\n",
      "Epoch 170 batch 150 train Loss 63.8584 test Loss 28.7723 with MSE metric 21997.7899\n",
      "Epoch 170 batch 160 train Loss 63.8446 test Loss 28.7668 with MSE metric 21994.4538\n",
      "Epoch 170 batch 170 train Loss 63.8309 test Loss 28.7614 with MSE metric 21991.0794\n",
      "Epoch 170 batch 180 train Loss 63.8172 test Loss 28.7559 with MSE metric 21987.8830\n",
      "Epoch 170 batch 190 train Loss 63.8035 test Loss 28.7505 with MSE metric 21984.5002\n",
      "Epoch 170 batch 200 train Loss 63.7898 test Loss 28.7450 with MSE metric 21981.1581\n",
      "Epoch 170 batch 210 train Loss 63.7761 test Loss 28.7395 with MSE metric 21977.7816\n",
      "Epoch 170 batch 220 train Loss 63.7625 test Loss 28.7340 with MSE metric 21974.4713\n",
      "Epoch 170 batch 230 train Loss 63.7488 test Loss 28.7286 with MSE metric 21971.1796\n",
      "Epoch 170 batch 240 train Loss 63.7351 test Loss 28.7232 with MSE metric 21967.8365\n",
      "Time taken for 1 epoch: 24.551342964172363 secs\n",
      "\n",
      "Epoch 171 batch 0 train Loss 63.7214 test Loss 28.7177 with MSE metric 21964.4636\n",
      "Epoch 171 batch 10 train Loss 63.7078 test Loss 28.7122 with MSE metric 21961.1327\n",
      "Epoch 171 batch 20 train Loss 63.6941 test Loss 28.7068 with MSE metric 21957.8716\n",
      "Epoch 171 batch 30 train Loss 63.6805 test Loss 28.7013 with MSE metric 21954.5773\n",
      "Epoch 171 batch 40 train Loss 63.6669 test Loss 28.6959 with MSE metric 21951.2440\n",
      "Epoch 171 batch 50 train Loss 63.6532 test Loss 28.6904 with MSE metric 21947.9123\n",
      "Epoch 171 batch 60 train Loss 63.6396 test Loss 28.6850 with MSE metric 21944.4687\n",
      "Epoch 171 batch 70 train Loss 63.6260 test Loss 28.6796 with MSE metric 21941.1538\n",
      "Epoch 171 batch 80 train Loss 63.6124 test Loss 28.6742 with MSE metric 21937.7342\n",
      "Epoch 171 batch 90 train Loss 63.5987 test Loss 28.6687 with MSE metric 21934.4491\n",
      "Epoch 171 batch 100 train Loss 63.5851 test Loss 28.6633 with MSE metric 21931.0468\n",
      "Epoch 171 batch 110 train Loss 63.5716 test Loss 28.6579 with MSE metric 21927.7495\n",
      "Epoch 171 batch 120 train Loss 63.5580 test Loss 28.6524 with MSE metric 21924.3653\n",
      "Epoch 171 batch 130 train Loss 63.5444 test Loss 28.6470 with MSE metric 21921.0181\n",
      "Epoch 171 batch 140 train Loss 63.5308 test Loss 28.6416 with MSE metric 21917.6546\n",
      "Epoch 171 batch 150 train Loss 63.5172 test Loss 28.6362 with MSE metric 21914.3001\n",
      "Epoch 171 batch 160 train Loss 63.5037 test Loss 28.6308 with MSE metric 21910.9666\n",
      "Epoch 171 batch 170 train Loss 63.4901 test Loss 28.6253 with MSE metric 21907.6479\n",
      "Epoch 171 batch 180 train Loss 63.4766 test Loss 28.6199 with MSE metric 21904.2517\n",
      "Epoch 171 batch 190 train Loss 63.4630 test Loss 28.6145 with MSE metric 21900.9441\n",
      "Epoch 171 batch 200 train Loss 63.4495 test Loss 28.6091 with MSE metric 21897.6692\n",
      "Epoch 171 batch 210 train Loss 63.4359 test Loss 28.6037 with MSE metric 21894.4024\n",
      "Epoch 171 batch 220 train Loss 63.4224 test Loss 28.5983 with MSE metric 21891.0368\n",
      "Epoch 171 batch 230 train Loss 63.4089 test Loss 28.5929 with MSE metric 21887.7658\n",
      "Epoch 171 batch 240 train Loss 63.3954 test Loss 28.5876 with MSE metric 21884.5018\n",
      "Time taken for 1 epoch: 24.501386165618896 secs\n",
      "\n",
      "Epoch 172 batch 0 train Loss 63.3819 test Loss 28.5822 with MSE metric 21881.2062\n",
      "Epoch 172 batch 10 train Loss 63.3684 test Loss 28.5768 with MSE metric 21877.9320\n",
      "Epoch 172 batch 20 train Loss 63.3549 test Loss 28.5715 with MSE metric 21874.6755\n",
      "Epoch 172 batch 30 train Loss 63.3414 test Loss 28.5661 with MSE metric 21871.3962\n",
      "Epoch 172 batch 40 train Loss 63.3279 test Loss 28.5607 with MSE metric 21868.0618\n",
      "Epoch 172 batch 50 train Loss 63.3144 test Loss 28.5553 with MSE metric 21864.8105\n",
      "Epoch 172 batch 60 train Loss 63.3010 test Loss 28.5499 with MSE metric 21861.4973\n",
      "Epoch 172 batch 70 train Loss 63.2875 test Loss 28.5445 with MSE metric 21858.1929\n",
      "Epoch 172 batch 80 train Loss 63.2741 test Loss 28.5392 with MSE metric 21854.9746\n",
      "Epoch 172 batch 90 train Loss 63.2606 test Loss 28.5338 with MSE metric 21851.6131\n",
      "Epoch 172 batch 100 train Loss 63.2472 test Loss 28.5284 with MSE metric 21848.4519\n",
      "Epoch 172 batch 110 train Loss 63.2337 test Loss 28.5230 with MSE metric 21845.0980\n",
      "Epoch 172 batch 120 train Loss 63.2203 test Loss 28.5177 with MSE metric 21841.8219\n",
      "Epoch 172 batch 130 train Loss 63.2069 test Loss 28.5123 with MSE metric 21838.5559\n",
      "Epoch 172 batch 140 train Loss 63.1935 test Loss 28.5070 with MSE metric 21835.2240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 batch 150 train Loss 63.1800 test Loss 28.5016 with MSE metric 21831.9385\n",
      "Epoch 172 batch 160 train Loss 63.1666 test Loss 28.4962 with MSE metric 21828.6239\n",
      "Epoch 172 batch 170 train Loss 63.1532 test Loss 28.4909 with MSE metric 21825.3135\n",
      "Epoch 172 batch 180 train Loss 63.1398 test Loss 28.4855 with MSE metric 21822.0523\n",
      "Epoch 172 batch 190 train Loss 63.1265 test Loss 28.4802 with MSE metric 21818.7603\n",
      "Epoch 172 batch 200 train Loss 63.1131 test Loss 28.4749 with MSE metric 21815.4802\n",
      "Epoch 172 batch 210 train Loss 63.0997 test Loss 28.4695 with MSE metric 21812.1982\n",
      "Epoch 172 batch 220 train Loss 63.0863 test Loss 28.4642 with MSE metric 21808.9914\n",
      "Epoch 172 batch 230 train Loss 63.0730 test Loss 28.4588 with MSE metric 21805.7404\n",
      "Epoch 172 batch 240 train Loss 63.0596 test Loss 28.4535 with MSE metric 21802.5307\n",
      "Time taken for 1 epoch: 24.78684687614441 secs\n",
      "\n",
      "Epoch 173 batch 0 train Loss 63.0463 test Loss 28.4482 with MSE metric 21799.2634\n",
      "Epoch 173 batch 10 train Loss 63.0329 test Loss 28.4429 with MSE metric 21796.0554\n",
      "Epoch 173 batch 20 train Loss 63.0196 test Loss 28.4375 with MSE metric 21792.7003\n",
      "Epoch 173 batch 30 train Loss 63.0063 test Loss 28.4322 with MSE metric 21789.3698\n",
      "Epoch 173 batch 40 train Loss 62.9929 test Loss 28.4269 with MSE metric 21786.0511\n",
      "Epoch 173 batch 50 train Loss 62.9796 test Loss 28.4216 with MSE metric 21782.7337\n",
      "Epoch 173 batch 60 train Loss 62.9663 test Loss 28.4163 with MSE metric 21779.5106\n",
      "Epoch 173 batch 70 train Loss 62.9530 test Loss 28.4110 with MSE metric 21776.1756\n",
      "Epoch 173 batch 80 train Loss 62.9397 test Loss 28.4057 with MSE metric 21772.9035\n",
      "Epoch 173 batch 90 train Loss 62.9264 test Loss 28.4004 with MSE metric 21769.6634\n",
      "Epoch 173 batch 100 train Loss 62.9131 test Loss 28.3951 with MSE metric 21766.4535\n",
      "Epoch 173 batch 110 train Loss 62.8998 test Loss 28.3898 with MSE metric 21763.2074\n",
      "Epoch 173 batch 120 train Loss 62.8865 test Loss 28.3845 with MSE metric 21759.9477\n",
      "Epoch 173 batch 130 train Loss 62.8733 test Loss 28.3792 with MSE metric 21756.7058\n",
      "Epoch 173 batch 140 train Loss 62.8600 test Loss 28.3739 with MSE metric 21753.4890\n",
      "Epoch 173 batch 150 train Loss 62.8467 test Loss 28.3686 with MSE metric 21750.2743\n",
      "Epoch 173 batch 160 train Loss 62.8335 test Loss 28.3633 with MSE metric 21747.0443\n",
      "Epoch 173 batch 170 train Loss 62.8202 test Loss 28.3580 with MSE metric 21743.7915\n",
      "Epoch 173 batch 180 train Loss 62.8070 test Loss 28.3527 with MSE metric 21740.5233\n",
      "Epoch 173 batch 190 train Loss 62.7938 test Loss 28.3474 with MSE metric 21737.2659\n",
      "Epoch 173 batch 200 train Loss 62.7805 test Loss 28.3422 with MSE metric 21734.0415\n",
      "Epoch 173 batch 210 train Loss 62.7673 test Loss 28.3369 with MSE metric 21730.8955\n",
      "Epoch 173 batch 220 train Loss 62.7541 test Loss 28.3316 with MSE metric 21727.6812\n",
      "Epoch 173 batch 230 train Loss 62.7409 test Loss 28.3263 with MSE metric 21724.4817\n",
      "Epoch 173 batch 240 train Loss 62.7277 test Loss 28.3210 with MSE metric 21721.1606\n",
      "Time taken for 1 epoch: 25.463083028793335 secs\n",
      "\n",
      "Epoch 174 batch 0 train Loss 62.7145 test Loss 28.3158 with MSE metric 21717.8999\n",
      "Epoch 174 batch 10 train Loss 62.7013 test Loss 28.3106 with MSE metric 21714.6923\n",
      "Epoch 174 batch 20 train Loss 62.6881 test Loss 28.3053 with MSE metric 21711.4566\n",
      "Epoch 174 batch 30 train Loss 62.6749 test Loss 28.3001 with MSE metric 21708.2258\n",
      "Epoch 174 batch 40 train Loss 62.6618 test Loss 28.2948 with MSE metric 21705.0801\n",
      "Epoch 174 batch 50 train Loss 62.6486 test Loss 28.2896 with MSE metric 21701.9558\n",
      "Epoch 174 batch 60 train Loss 62.6355 test Loss 28.2843 with MSE metric 21698.7827\n",
      "Epoch 174 batch 70 train Loss 62.6223 test Loss 28.2791 with MSE metric 21695.5972\n",
      "Epoch 174 batch 80 train Loss 62.6092 test Loss 28.2738 with MSE metric 21692.4155\n",
      "Epoch 174 batch 90 train Loss 62.5960 test Loss 28.2686 with MSE metric 21689.2439\n",
      "Epoch 174 batch 100 train Loss 62.5829 test Loss 28.2633 with MSE metric 21686.0498\n",
      "Epoch 174 batch 110 train Loss 62.5698 test Loss 28.2581 with MSE metric 21682.8349\n",
      "Epoch 174 batch 120 train Loss 62.5566 test Loss 28.2528 with MSE metric 21679.5793\n",
      "Epoch 174 batch 130 train Loss 62.5435 test Loss 28.2476 with MSE metric 21676.4051\n",
      "Epoch 174 batch 140 train Loss 62.5304 test Loss 28.2424 with MSE metric 21673.2271\n",
      "Epoch 174 batch 150 train Loss 62.5173 test Loss 28.2371 with MSE metric 21670.1114\n",
      "Epoch 174 batch 160 train Loss 62.5042 test Loss 28.2319 with MSE metric 21666.9554\n",
      "Epoch 174 batch 170 train Loss 62.4911 test Loss 28.2267 with MSE metric 21663.7618\n",
      "Epoch 174 batch 180 train Loss 62.4780 test Loss 28.2215 with MSE metric 21660.5054\n",
      "Epoch 174 batch 190 train Loss 62.4649 test Loss 28.2163 with MSE metric 21657.2547\n",
      "Epoch 174 batch 200 train Loss 62.4518 test Loss 28.2110 with MSE metric 21654.0567\n",
      "Epoch 174 batch 210 train Loss 62.4388 test Loss 28.2058 with MSE metric 21650.8340\n",
      "Epoch 174 batch 220 train Loss 62.4257 test Loss 28.2006 with MSE metric 21647.6750\n",
      "Epoch 174 batch 230 train Loss 62.4126 test Loss 28.1954 with MSE metric 21644.3759\n",
      "Epoch 174 batch 240 train Loss 62.3996 test Loss 28.1901 with MSE metric 21641.1044\n",
      "Time taken for 1 epoch: 25.632454872131348 secs\n",
      "\n",
      "Epoch 175 batch 0 train Loss 62.3865 test Loss 28.1849 with MSE metric 21637.8684\n",
      "Epoch 175 batch 10 train Loss 62.3735 test Loss 28.1797 with MSE metric 21634.6749\n",
      "Epoch 175 batch 20 train Loss 62.3605 test Loss 28.1745 with MSE metric 21631.4660\n",
      "Epoch 175 batch 30 train Loss 62.3474 test Loss 28.1693 with MSE metric 21628.3093\n",
      "Epoch 175 batch 40 train Loss 62.3344 test Loss 28.1641 with MSE metric 21625.1191\n",
      "Epoch 175 batch 50 train Loss 62.3214 test Loss 28.1589 with MSE metric 21621.9417\n",
      "Epoch 175 batch 60 train Loss 62.3084 test Loss 28.1537 with MSE metric 21618.6617\n",
      "Epoch 175 batch 70 train Loss 62.2954 test Loss 28.1485 with MSE metric 21615.4597\n",
      "Epoch 175 batch 80 train Loss 62.2824 test Loss 28.1433 with MSE metric 21612.2827\n",
      "Epoch 175 batch 90 train Loss 62.2694 test Loss 28.1382 with MSE metric 21609.1488\n",
      "Epoch 175 batch 100 train Loss 62.2564 test Loss 28.1330 with MSE metric 21605.9927\n",
      "Epoch 175 batch 110 train Loss 62.2434 test Loss 28.1278 with MSE metric 21602.7868\n",
      "Epoch 175 batch 120 train Loss 62.2304 test Loss 28.1226 with MSE metric 21599.6237\n",
      "Epoch 175 batch 130 train Loss 62.2175 test Loss 28.1174 with MSE metric 21596.4472\n",
      "Epoch 175 batch 140 train Loss 62.2045 test Loss 28.1122 with MSE metric 21593.3352\n",
      "Epoch 175 batch 150 train Loss 62.1915 test Loss 28.1070 with MSE metric 21590.1905\n",
      "Epoch 175 batch 160 train Loss 62.1786 test Loss 28.1018 with MSE metric 21587.0135\n",
      "Epoch 175 batch 170 train Loss 62.1657 test Loss 28.0967 with MSE metric 21583.8675\n",
      "Epoch 175 batch 180 train Loss 62.1527 test Loss 28.0915 with MSE metric 21580.6863\n",
      "Epoch 175 batch 190 train Loss 62.1398 test Loss 28.0863 with MSE metric 21577.5119\n",
      "Epoch 175 batch 200 train Loss 62.1268 test Loss 28.0812 with MSE metric 21574.2762\n",
      "Epoch 175 batch 210 train Loss 62.1139 test Loss 28.0760 with MSE metric 21571.1057\n",
      "Epoch 175 batch 220 train Loss 62.1010 test Loss 28.0709 with MSE metric 21567.9577\n",
      "Epoch 175 batch 230 train Loss 62.0881 test Loss 28.0657 with MSE metric 21564.7969\n",
      "Epoch 175 batch 240 train Loss 62.0752 test Loss 28.0606 with MSE metric 21561.6175\n",
      "Time taken for 1 epoch: 24.643140077590942 secs\n",
      "\n",
      "Epoch 176 batch 0 train Loss 62.0623 test Loss 28.0554 with MSE metric 21558.4397\n",
      "Epoch 176 batch 10 train Loss 62.0494 test Loss 28.0503 with MSE metric 21555.3055\n",
      "Epoch 176 batch 20 train Loss 62.0365 test Loss 28.0451 with MSE metric 21552.1101\n",
      "Epoch 176 batch 30 train Loss 62.0236 test Loss 28.0400 with MSE metric 21548.9502\n",
      "Epoch 176 batch 40 train Loss 62.0107 test Loss 28.0349 with MSE metric 21545.8009\n",
      "Epoch 176 batch 50 train Loss 61.9979 test Loss 28.0297 with MSE metric 21542.6502\n",
      "Epoch 176 batch 60 train Loss 61.9850 test Loss 28.0246 with MSE metric 21539.5497\n",
      "Epoch 176 batch 70 train Loss 61.9722 test Loss 28.0194 with MSE metric 21536.3986\n",
      "Epoch 176 batch 80 train Loss 61.9593 test Loss 28.0143 with MSE metric 21533.1564\n",
      "Epoch 176 batch 90 train Loss 61.9464 test Loss 28.0092 with MSE metric 21529.8785\n",
      "Epoch 176 batch 100 train Loss 61.9336 test Loss 28.0040 with MSE metric 21526.7706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176 batch 110 train Loss 61.9208 test Loss 27.9989 with MSE metric 21523.6445\n",
      "Epoch 176 batch 120 train Loss 61.9079 test Loss 27.9938 with MSE metric 21520.4598\n",
      "Epoch 176 batch 130 train Loss 61.8951 test Loss 27.9887 with MSE metric 21517.2650\n",
      "Epoch 176 batch 140 train Loss 61.8823 test Loss 27.9835 with MSE metric 21514.1856\n",
      "Epoch 176 batch 150 train Loss 61.8695 test Loss 27.9784 with MSE metric 21511.0620\n",
      "Epoch 176 batch 160 train Loss 61.8567 test Loss 27.9733 with MSE metric 21507.9322\n",
      "Epoch 176 batch 170 train Loss 61.8439 test Loss 27.9682 with MSE metric 21504.8479\n",
      "Epoch 176 batch 180 train Loss 61.8311 test Loss 27.9631 with MSE metric 21501.7121\n",
      "Epoch 176 batch 190 train Loss 61.8183 test Loss 27.9580 with MSE metric 21498.5431\n",
      "Epoch 176 batch 200 train Loss 61.8055 test Loss 27.9529 with MSE metric 21495.4333\n",
      "Epoch 176 batch 210 train Loss 61.7927 test Loss 27.9478 with MSE metric 21492.3017\n",
      "Epoch 176 batch 220 train Loss 61.7800 test Loss 27.9427 with MSE metric 21489.1374\n",
      "Epoch 176 batch 230 train Loss 61.7672 test Loss 27.9376 with MSE metric 21486.0406\n",
      "Epoch 176 batch 240 train Loss 61.7545 test Loss 27.9325 with MSE metric 21482.9730\n",
      "Time taken for 1 epoch: 24.948307991027832 secs\n",
      "\n",
      "Epoch 177 batch 0 train Loss 61.7417 test Loss 27.9274 with MSE metric 21479.8303\n",
      "Epoch 177 batch 10 train Loss 61.7290 test Loss 27.9223 with MSE metric 21476.7609\n",
      "Epoch 177 batch 20 train Loss 61.7162 test Loss 27.9172 with MSE metric 21473.5732\n",
      "Epoch 177 batch 30 train Loss 61.7035 test Loss 27.9121 with MSE metric 21470.4048\n",
      "Epoch 177 batch 40 train Loss 61.6907 test Loss 27.9071 with MSE metric 21467.2946\n",
      "Epoch 177 batch 50 train Loss 61.6780 test Loss 27.9020 with MSE metric 21464.1863\n",
      "Epoch 177 batch 60 train Loss 61.6653 test Loss 27.8969 with MSE metric 21461.0934\n",
      "Epoch 177 batch 70 train Loss 61.6526 test Loss 27.8918 with MSE metric 21458.0362\n",
      "Epoch 177 batch 80 train Loss 61.6399 test Loss 27.8868 with MSE metric 21454.9504\n",
      "Epoch 177 batch 90 train Loss 61.6272 test Loss 27.8817 with MSE metric 21451.8862\n",
      "Epoch 177 batch 100 train Loss 61.6145 test Loss 27.8766 with MSE metric 21448.8637\n",
      "Epoch 177 batch 110 train Loss 61.6018 test Loss 27.8715 with MSE metric 21445.7531\n",
      "Epoch 177 batch 120 train Loss 61.5891 test Loss 27.8665 with MSE metric 21442.5969\n",
      "Epoch 177 batch 130 train Loss 61.5764 test Loss 27.8614 with MSE metric 21439.4590\n",
      "Epoch 177 batch 140 train Loss 61.5638 test Loss 27.8564 with MSE metric 21436.2989\n",
      "Epoch 177 batch 150 train Loss 61.5511 test Loss 27.8513 with MSE metric 21433.2069\n",
      "Epoch 177 batch 160 train Loss 61.5384 test Loss 27.8463 with MSE metric 21430.1652\n",
      "Epoch 177 batch 170 train Loss 61.5258 test Loss 27.8412 with MSE metric 21427.1032\n",
      "Epoch 177 batch 180 train Loss 61.5131 test Loss 27.8362 with MSE metric 21423.9769\n",
      "Epoch 177 batch 190 train Loss 61.5005 test Loss 27.8311 with MSE metric 21420.8706\n",
      "Epoch 177 batch 200 train Loss 61.4878 test Loss 27.8261 with MSE metric 21417.8213\n",
      "Epoch 177 batch 210 train Loss 61.4752 test Loss 27.8210 with MSE metric 21414.6823\n",
      "Epoch 177 batch 220 train Loss 61.4626 test Loss 27.8160 with MSE metric 21411.6175\n",
      "Epoch 177 batch 230 train Loss 61.4500 test Loss 27.8109 with MSE metric 21408.6121\n",
      "Epoch 177 batch 240 train Loss 61.4373 test Loss 27.8059 with MSE metric 21405.6064\n",
      "Time taken for 1 epoch: 24.267904043197632 secs\n",
      "\n",
      "Epoch 178 batch 0 train Loss 61.4247 test Loss 27.8009 with MSE metric 21402.5493\n",
      "Epoch 178 batch 10 train Loss 61.4121 test Loss 27.7959 with MSE metric 21399.4806\n",
      "Epoch 178 batch 20 train Loss 61.3995 test Loss 27.7909 with MSE metric 21396.3969\n",
      "Epoch 178 batch 30 train Loss 61.3869 test Loss 27.7859 with MSE metric 21393.3212\n",
      "Epoch 178 batch 40 train Loss 61.3744 test Loss 27.7808 with MSE metric 21390.3226\n",
      "Epoch 178 batch 50 train Loss 61.3618 test Loss 27.7758 with MSE metric 21387.2248\n",
      "Epoch 178 batch 60 train Loss 61.3492 test Loss 27.7708 with MSE metric 21384.1668\n",
      "Epoch 178 batch 70 train Loss 61.3366 test Loss 27.7658 with MSE metric 21381.0848\n",
      "Epoch 178 batch 80 train Loss 61.3241 test Loss 27.7608 with MSE metric 21377.9732\n",
      "Epoch 178 batch 90 train Loss 61.3115 test Loss 27.7558 with MSE metric 21374.8775\n",
      "Epoch 178 batch 100 train Loss 61.2990 test Loss 27.7507 with MSE metric 21371.8391\n",
      "Epoch 178 batch 110 train Loss 61.2864 test Loss 27.7457 with MSE metric 21368.8481\n",
      "Epoch 178 batch 120 train Loss 61.2739 test Loss 27.7407 with MSE metric 21365.8533\n",
      "Epoch 178 batch 130 train Loss 61.2613 test Loss 27.7357 with MSE metric 21362.7707\n",
      "Epoch 178 batch 140 train Loss 61.2488 test Loss 27.7307 with MSE metric 21359.7481\n",
      "Epoch 178 batch 150 train Loss 61.2363 test Loss 27.7257 with MSE metric 21356.7550\n",
      "Epoch 178 batch 160 train Loss 61.2238 test Loss 27.7207 with MSE metric 21353.6677\n",
      "Epoch 178 batch 170 train Loss 61.2112 test Loss 27.7157 with MSE metric 21350.6359\n",
      "Epoch 178 batch 180 train Loss 61.1987 test Loss 27.7107 with MSE metric 21347.5470\n",
      "Epoch 178 batch 190 train Loss 61.1862 test Loss 27.7057 with MSE metric 21344.4858\n",
      "Epoch 178 batch 200 train Loss 61.1737 test Loss 27.7007 with MSE metric 21341.3779\n",
      "Epoch 178 batch 210 train Loss 61.1613 test Loss 27.6957 with MSE metric 21338.3926\n",
      "Epoch 178 batch 220 train Loss 61.1488 test Loss 27.6907 with MSE metric 21335.2734\n",
      "Epoch 178 batch 230 train Loss 61.1363 test Loss 27.6857 with MSE metric 21332.1947\n",
      "Epoch 178 batch 240 train Loss 61.1238 test Loss 27.6807 with MSE metric 21329.1662\n",
      "Time taken for 1 epoch: 24.051868200302124 secs\n",
      "\n",
      "Epoch 179 batch 0 train Loss 61.1113 test Loss 27.6757 with MSE metric 21326.0818\n",
      "Epoch 179 batch 10 train Loss 61.0989 test Loss 27.6708 with MSE metric 21323.0510\n",
      "Epoch 179 batch 20 train Loss 61.0864 test Loss 27.6658 with MSE metric 21319.9941\n",
      "Epoch 179 batch 30 train Loss 61.0740 test Loss 27.6608 with MSE metric 21316.9469\n",
      "Epoch 179 batch 40 train Loss 61.0615 test Loss 27.6559 with MSE metric 21313.9473\n",
      "Epoch 179 batch 50 train Loss 61.0491 test Loss 27.6509 with MSE metric 21310.8950\n",
      "Epoch 179 batch 60 train Loss 61.0367 test Loss 27.6459 with MSE metric 21307.8920\n",
      "Epoch 179 batch 70 train Loss 61.0242 test Loss 27.6410 with MSE metric 21304.8186\n",
      "Epoch 179 batch 80 train Loss 61.0118 test Loss 27.6360 with MSE metric 21301.7145\n",
      "Epoch 179 batch 90 train Loss 60.9994 test Loss 27.6310 with MSE metric 21298.6755\n",
      "Epoch 179 batch 100 train Loss 60.9869 test Loss 27.6261 with MSE metric 21295.6780\n",
      "Epoch 179 batch 110 train Loss 60.9745 test Loss 27.6211 with MSE metric 21292.6246\n",
      "Epoch 179 batch 120 train Loss 60.9621 test Loss 27.6162 with MSE metric 21289.6213\n",
      "Epoch 179 batch 130 train Loss 60.9497 test Loss 27.6112 with MSE metric 21286.5651\n",
      "Epoch 179 batch 140 train Loss 60.9373 test Loss 27.6063 with MSE metric 21283.4481\n",
      "Epoch 179 batch 150 train Loss 60.9249 test Loss 27.6013 with MSE metric 21280.3978\n",
      "Epoch 179 batch 160 train Loss 60.9126 test Loss 27.5964 with MSE metric 21277.4259\n",
      "Epoch 179 batch 170 train Loss 60.9002 test Loss 27.5914 with MSE metric 21274.4021\n",
      "Epoch 179 batch 180 train Loss 60.8878 test Loss 27.5865 with MSE metric 21271.3060\n",
      "Epoch 179 batch 190 train Loss 60.8754 test Loss 27.5815 with MSE metric 21268.2312\n",
      "Epoch 179 batch 200 train Loss 60.8631 test Loss 27.5766 with MSE metric 21265.2351\n",
      "Epoch 179 batch 210 train Loss 60.8507 test Loss 27.5717 with MSE metric 21262.2239\n",
      "Epoch 179 batch 220 train Loss 60.8384 test Loss 27.5668 with MSE metric 21259.1782\n",
      "Epoch 179 batch 230 train Loss 60.8261 test Loss 27.5618 with MSE metric 21256.2022\n",
      "Epoch 179 batch 240 train Loss 60.8137 test Loss 27.5569 with MSE metric 21253.1336\n",
      "Time taken for 1 epoch: 24.79691219329834 secs\n",
      "\n",
      "Epoch 180 batch 0 train Loss 60.8014 test Loss 27.5520 with MSE metric 21250.0988\n",
      "Epoch 180 batch 10 train Loss 60.7891 test Loss 27.5470 with MSE metric 21247.0980\n",
      "Epoch 180 batch 20 train Loss 60.7767 test Loss 27.5421 with MSE metric 21244.0295\n",
      "Epoch 180 batch 30 train Loss 60.7644 test Loss 27.5372 with MSE metric 21241.0594\n",
      "Epoch 180 batch 40 train Loss 60.7521 test Loss 27.5322 with MSE metric 21238.0648\n",
      "Epoch 180 batch 50 train Loss 60.7398 test Loss 27.5273 with MSE metric 21235.0546\n",
      "Epoch 180 batch 60 train Loss 60.7275 test Loss 27.5224 with MSE metric 21232.1280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180 batch 70 train Loss 60.7152 test Loss 27.5176 with MSE metric 21229.1702\n",
      "Epoch 180 batch 80 train Loss 60.7029 test Loss 27.5127 with MSE metric 21226.1590\n",
      "Epoch 180 batch 90 train Loss 60.6907 test Loss 27.5078 with MSE metric 21223.2083\n",
      "Epoch 180 batch 100 train Loss 60.6784 test Loss 27.5028 with MSE metric 21220.2534\n",
      "Epoch 180 batch 110 train Loss 60.6661 test Loss 27.4980 with MSE metric 21217.2534\n",
      "Epoch 180 batch 120 train Loss 60.6538 test Loss 27.4931 with MSE metric 21214.2411\n",
      "Epoch 180 batch 130 train Loss 60.6416 test Loss 27.4882 with MSE metric 21211.2321\n",
      "Epoch 180 batch 140 train Loss 60.6293 test Loss 27.4833 with MSE metric 21208.3549\n",
      "Epoch 180 batch 150 train Loss 60.6171 test Loss 27.4784 with MSE metric 21205.2903\n",
      "Epoch 180 batch 160 train Loss 60.6048 test Loss 27.4735 with MSE metric 21202.3230\n",
      "Epoch 180 batch 170 train Loss 60.5926 test Loss 27.4686 with MSE metric 21199.3990\n",
      "Epoch 180 batch 180 train Loss 60.5803 test Loss 27.4637 with MSE metric 21196.3693\n",
      "Epoch 180 batch 190 train Loss 60.5681 test Loss 27.4588 with MSE metric 21193.3495\n",
      "Epoch 180 batch 200 train Loss 60.5559 test Loss 27.4540 with MSE metric 21190.3173\n",
      "Epoch 180 batch 210 train Loss 60.5437 test Loss 27.4491 with MSE metric 21187.3334\n",
      "Epoch 180 batch 220 train Loss 60.5315 test Loss 27.4442 with MSE metric 21184.2930\n",
      "Epoch 180 batch 230 train Loss 60.5192 test Loss 27.4393 with MSE metric 21181.2940\n",
      "Epoch 180 batch 240 train Loss 60.5071 test Loss 27.4345 with MSE metric 21178.3469\n",
      "Time taken for 1 epoch: 25.46008276939392 secs\n",
      "\n",
      "Epoch 181 batch 0 train Loss 60.4949 test Loss 27.4296 with MSE metric 21175.4073\n",
      "Epoch 181 batch 10 train Loss 60.4827 test Loss 27.4247 with MSE metric 21172.4075\n",
      "Epoch 181 batch 20 train Loss 60.4705 test Loss 27.4199 with MSE metric 21169.4741\n",
      "Epoch 181 batch 30 train Loss 60.4583 test Loss 27.4150 with MSE metric 21166.4967\n",
      "Epoch 181 batch 40 train Loss 60.4461 test Loss 27.4101 with MSE metric 21163.4939\n",
      "Epoch 181 batch 50 train Loss 60.4340 test Loss 27.4053 with MSE metric 21160.5924\n",
      "Epoch 181 batch 60 train Loss 60.4218 test Loss 27.4004 with MSE metric 21157.6141\n",
      "Epoch 181 batch 70 train Loss 60.4097 test Loss 27.3956 with MSE metric 21154.6919\n",
      "Epoch 181 batch 80 train Loss 60.3975 test Loss 27.3907 with MSE metric 21151.6794\n",
      "Epoch 181 batch 90 train Loss 60.3854 test Loss 27.3859 with MSE metric 21148.7919\n",
      "Epoch 181 batch 100 train Loss 60.3732 test Loss 27.3810 with MSE metric 21145.8270\n",
      "Epoch 181 batch 110 train Loss 60.3611 test Loss 27.3762 with MSE metric 21142.8571\n",
      "Epoch 181 batch 120 train Loss 60.3490 test Loss 27.3713 with MSE metric 21139.9179\n",
      "Epoch 181 batch 130 train Loss 60.3368 test Loss 27.3665 with MSE metric 21137.0319\n",
      "Epoch 181 batch 140 train Loss 60.3247 test Loss 27.3616 with MSE metric 21134.0701\n",
      "Epoch 181 batch 150 train Loss 60.3126 test Loss 27.3568 with MSE metric 21131.1196\n",
      "Epoch 181 batch 160 train Loss 60.3005 test Loss 27.3520 with MSE metric 21128.1584\n",
      "Epoch 181 batch 170 train Loss 60.2884 test Loss 27.3471 with MSE metric 21125.1432\n",
      "Epoch 181 batch 180 train Loss 60.2763 test Loss 27.3423 with MSE metric 21122.1262\n",
      "Epoch 181 batch 190 train Loss 60.2642 test Loss 27.3375 with MSE metric 21119.1557\n",
      "Epoch 181 batch 200 train Loss 60.2521 test Loss 27.3327 with MSE metric 21116.2040\n",
      "Epoch 181 batch 210 train Loss 60.2400 test Loss 27.3279 with MSE metric 21113.2802\n",
      "Epoch 181 batch 220 train Loss 60.2279 test Loss 27.3231 with MSE metric 21110.3129\n",
      "Epoch 181 batch 230 train Loss 60.2159 test Loss 27.3182 with MSE metric 21107.3716\n",
      "Epoch 181 batch 240 train Loss 60.2038 test Loss 27.3134 with MSE metric 21104.4174\n",
      "Time taken for 1 epoch: 25.589177131652832 secs\n",
      "\n",
      "Epoch 182 batch 0 train Loss 60.1917 test Loss 27.3086 with MSE metric 21101.4572\n",
      "Epoch 182 batch 10 train Loss 60.1797 test Loss 27.3038 with MSE metric 21098.5236\n",
      "Epoch 182 batch 20 train Loss 60.1676 test Loss 27.2990 with MSE metric 21095.5520\n",
      "Epoch 182 batch 30 train Loss 60.1556 test Loss 27.2942 with MSE metric 21092.5677\n",
      "Epoch 182 batch 40 train Loss 60.1435 test Loss 27.2893 with MSE metric 21089.6309\n",
      "Epoch 182 batch 50 train Loss 60.1315 test Loss 27.2845 with MSE metric 21086.7308\n",
      "Epoch 182 batch 60 train Loss 60.1195 test Loss 27.2797 with MSE metric 21083.7806\n",
      "Epoch 182 batch 70 train Loss 60.1074 test Loss 27.2749 with MSE metric 21080.8245\n",
      "Epoch 182 batch 80 train Loss 60.0954 test Loss 27.2701 with MSE metric 21077.8597\n",
      "Epoch 182 batch 90 train Loss 60.0834 test Loss 27.2653 with MSE metric 21074.8562\n",
      "Epoch 182 batch 100 train Loss 60.0714 test Loss 27.2605 with MSE metric 21071.9031\n",
      "Epoch 182 batch 110 train Loss 60.0594 test Loss 27.2558 with MSE metric 21069.0018\n",
      "Epoch 182 batch 120 train Loss 60.0474 test Loss 27.2510 with MSE metric 21066.0133\n",
      "Epoch 182 batch 130 train Loss 60.0354 test Loss 27.2462 with MSE metric 21063.0759\n",
      "Epoch 182 batch 140 train Loss 60.0234 test Loss 27.2414 with MSE metric 21060.1072\n",
      "Epoch 182 batch 150 train Loss 60.0114 test Loss 27.2366 with MSE metric 21057.1961\n",
      "Epoch 182 batch 160 train Loss 59.9994 test Loss 27.2319 with MSE metric 21054.2777\n",
      "Epoch 182 batch 170 train Loss 59.9875 test Loss 27.2271 with MSE metric 21051.2736\n",
      "Epoch 182 batch 180 train Loss 59.9755 test Loss 27.2223 with MSE metric 21048.3585\n",
      "Epoch 182 batch 190 train Loss 59.9635 test Loss 27.2176 with MSE metric 21045.4141\n",
      "Epoch 182 batch 200 train Loss 59.9516 test Loss 27.2128 with MSE metric 21042.6500\n",
      "Epoch 182 batch 210 train Loss 59.9396 test Loss 27.2080 with MSE metric 21039.6828\n",
      "Epoch 182 batch 220 train Loss 59.9277 test Loss 27.2032 with MSE metric 21036.7271\n",
      "Epoch 182 batch 230 train Loss 59.9157 test Loss 27.1985 with MSE metric 21033.8061\n",
      "Epoch 182 batch 240 train Loss 59.9038 test Loss 27.1937 with MSE metric 21030.9463\n",
      "Time taken for 1 epoch: 25.209463119506836 secs\n",
      "\n",
      "Epoch 183 batch 0 train Loss 59.8919 test Loss 27.1889 with MSE metric 21028.0340\n",
      "Epoch 183 batch 10 train Loss 59.8800 test Loss 27.1842 with MSE metric 21025.1120\n",
      "Epoch 183 batch 20 train Loss 59.8681 test Loss 27.1794 with MSE metric 21022.2449\n",
      "Epoch 183 batch 30 train Loss 59.8561 test Loss 27.1747 with MSE metric 21019.2927\n",
      "Epoch 183 batch 40 train Loss 59.8442 test Loss 27.1699 with MSE metric 21016.3802\n",
      "Epoch 183 batch 50 train Loss 59.8323 test Loss 27.1652 with MSE metric 21013.5640\n",
      "Epoch 183 batch 60 train Loss 59.8204 test Loss 27.1604 with MSE metric 21010.6689\n",
      "Epoch 183 batch 70 train Loss 59.8085 test Loss 27.1557 with MSE metric 21007.7119\n",
      "Epoch 183 batch 80 train Loss 59.7966 test Loss 27.1510 with MSE metric 21004.7779\n",
      "Epoch 183 batch 90 train Loss 59.7848 test Loss 27.1462 with MSE metric 21001.8859\n",
      "Epoch 183 batch 100 train Loss 59.7729 test Loss 27.1415 with MSE metric 20998.9790\n",
      "Epoch 183 batch 110 train Loss 59.7610 test Loss 27.1368 with MSE metric 20996.1119\n",
      "Epoch 183 batch 120 train Loss 59.7491 test Loss 27.1320 with MSE metric 20993.1994\n",
      "Epoch 183 batch 130 train Loss 59.7373 test Loss 27.1273 with MSE metric 20990.1930\n",
      "Epoch 183 batch 140 train Loss 59.7254 test Loss 27.1226 with MSE metric 20987.2997\n",
      "Epoch 183 batch 150 train Loss 59.7136 test Loss 27.1178 with MSE metric 20984.4009\n",
      "Epoch 183 batch 160 train Loss 59.7017 test Loss 27.1131 with MSE metric 20981.4632\n",
      "Epoch 183 batch 170 train Loss 59.6899 test Loss 27.1084 with MSE metric 20978.5925\n",
      "Epoch 183 batch 180 train Loss 59.6780 test Loss 27.1036 with MSE metric 20975.7264\n",
      "Epoch 183 batch 190 train Loss 59.6662 test Loss 27.0989 with MSE metric 20972.8272\n",
      "Epoch 183 batch 200 train Loss 59.6544 test Loss 27.0942 with MSE metric 20969.9261\n",
      "Epoch 183 batch 210 train Loss 59.6425 test Loss 27.0895 with MSE metric 20966.9790\n",
      "Epoch 183 batch 220 train Loss 59.6307 test Loss 27.0848 with MSE metric 20964.0745\n",
      "Epoch 183 batch 230 train Loss 59.6189 test Loss 27.0800 with MSE metric 20961.1738\n",
      "Epoch 183 batch 240 train Loss 59.6071 test Loss 27.0753 with MSE metric 20958.2540\n",
      "Time taken for 1 epoch: 24.637139320373535 secs\n",
      "\n",
      "Epoch 184 batch 0 train Loss 59.5953 test Loss 27.0706 with MSE metric 20955.3073\n",
      "Epoch 184 batch 10 train Loss 59.5835 test Loss 27.0659 with MSE metric 20952.3838\n",
      "Epoch 184 batch 20 train Loss 59.5717 test Loss 27.0612 with MSE metric 20949.5654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184 batch 30 train Loss 59.5599 test Loss 27.0565 with MSE metric 20946.6691\n",
      "Epoch 184 batch 40 train Loss 59.5481 test Loss 27.0518 with MSE metric 20943.8020\n",
      "Epoch 184 batch 50 train Loss 59.5364 test Loss 27.0471 with MSE metric 20940.9595\n",
      "Epoch 184 batch 60 train Loss 59.5246 test Loss 27.0424 with MSE metric 20938.0922\n",
      "Epoch 184 batch 70 train Loss 59.5128 test Loss 27.0377 with MSE metric 20935.2275\n",
      "Epoch 184 batch 80 train Loss 59.5011 test Loss 27.0330 with MSE metric 20932.3323\n",
      "Epoch 184 batch 90 train Loss 59.4893 test Loss 27.0283 with MSE metric 20929.4930\n",
      "Epoch 184 batch 100 train Loss 59.4776 test Loss 27.0236 with MSE metric 20926.6220\n",
      "Epoch 184 batch 110 train Loss 59.4658 test Loss 27.0189 with MSE metric 20923.8405\n",
      "Epoch 184 batch 120 train Loss 59.4541 test Loss 27.0142 with MSE metric 20921.0501\n",
      "Epoch 184 batch 130 train Loss 59.4423 test Loss 27.0095 with MSE metric 20918.1457\n",
      "Epoch 184 batch 140 train Loss 59.4306 test Loss 27.0048 with MSE metric 20915.2586\n",
      "Epoch 184 batch 150 train Loss 59.4189 test Loss 27.0002 with MSE metric 20912.3773\n",
      "Epoch 184 batch 160 train Loss 59.4072 test Loss 26.9955 with MSE metric 20909.5312\n",
      "Epoch 184 batch 170 train Loss 59.3955 test Loss 26.9908 with MSE metric 20906.6149\n",
      "Epoch 184 batch 180 train Loss 59.3837 test Loss 26.9861 with MSE metric 20903.7741\n",
      "Epoch 184 batch 190 train Loss 59.3720 test Loss 26.9815 with MSE metric 20900.9454\n",
      "Epoch 184 batch 200 train Loss 59.3603 test Loss 26.9768 with MSE metric 20898.0243\n",
      "Epoch 184 batch 210 train Loss 59.3487 test Loss 26.9721 with MSE metric 20895.1756\n",
      "Epoch 184 batch 220 train Loss 59.3370 test Loss 26.9675 with MSE metric 20892.2904\n",
      "Epoch 184 batch 230 train Loss 59.3253 test Loss 26.9628 with MSE metric 20889.4144\n",
      "Epoch 184 batch 240 train Loss 59.3136 test Loss 26.9582 with MSE metric 20886.5791\n",
      "Time taken for 1 epoch: 24.24854588508606 secs\n",
      "\n",
      "Epoch 185 batch 0 train Loss 59.3019 test Loss 26.9535 with MSE metric 20883.7192\n",
      "Epoch 185 batch 10 train Loss 59.2903 test Loss 26.9488 with MSE metric 20880.9306\n",
      "Epoch 185 batch 20 train Loss 59.2786 test Loss 26.9442 with MSE metric 20878.1045\n",
      "Epoch 185 batch 30 train Loss 59.2669 test Loss 26.9395 with MSE metric 20875.2261\n",
      "Epoch 185 batch 40 train Loss 59.2553 test Loss 26.9349 with MSE metric 20872.3282\n",
      "Epoch 185 batch 50 train Loss 59.2436 test Loss 26.9302 with MSE metric 20869.3858\n",
      "Epoch 185 batch 60 train Loss 59.2320 test Loss 26.9256 with MSE metric 20866.4888\n",
      "Epoch 185 batch 70 train Loss 59.2203 test Loss 26.9209 with MSE metric 20863.6381\n",
      "Epoch 185 batch 80 train Loss 59.2087 test Loss 26.9163 with MSE metric 20860.7186\n",
      "Epoch 185 batch 90 train Loss 59.1971 test Loss 26.9116 with MSE metric 20857.8866\n",
      "Epoch 185 batch 100 train Loss 59.1854 test Loss 26.9070 with MSE metric 20855.0997\n",
      "Epoch 185 batch 110 train Loss 59.1738 test Loss 26.9023 with MSE metric 20852.2668\n",
      "Epoch 185 batch 120 train Loss 59.1622 test Loss 26.8977 with MSE metric 20849.4632\n",
      "Epoch 185 batch 130 train Loss 59.1506 test Loss 26.8931 with MSE metric 20846.6378\n",
      "Epoch 185 batch 140 train Loss 59.1390 test Loss 26.8884 with MSE metric 20843.7701\n",
      "Epoch 185 batch 150 train Loss 59.1274 test Loss 26.8838 with MSE metric 20840.9040\n",
      "Epoch 185 batch 160 train Loss 59.1158 test Loss 26.8792 with MSE metric 20838.0162\n",
      "Epoch 185 batch 170 train Loss 59.1042 test Loss 26.8746 with MSE metric 20835.1876\n",
      "Epoch 185 batch 180 train Loss 59.0926 test Loss 26.8700 with MSE metric 20832.4114\n",
      "Epoch 185 batch 190 train Loss 59.0810 test Loss 26.8653 with MSE metric 20829.5950\n",
      "Epoch 185 batch 200 train Loss 59.0695 test Loss 26.8607 with MSE metric 20826.7772\n",
      "Epoch 185 batch 210 train Loss 59.0579 test Loss 26.8561 with MSE metric 20823.9501\n",
      "Epoch 185 batch 220 train Loss 59.0463 test Loss 26.8515 with MSE metric 20821.1381\n",
      "Epoch 185 batch 230 train Loss 59.0348 test Loss 26.8469 with MSE metric 20818.3158\n",
      "Epoch 185 batch 240 train Loss 59.0232 test Loss 26.8423 with MSE metric 20815.5065\n",
      "Time taken for 1 epoch: 23.844959020614624 secs\n",
      "\n",
      "Epoch 186 batch 0 train Loss 59.0117 test Loss 26.8377 with MSE metric 20812.6656\n",
      "Epoch 186 batch 10 train Loss 59.0001 test Loss 26.8331 with MSE metric 20809.8560\n",
      "Epoch 186 batch 20 train Loss 58.9886 test Loss 26.8285 with MSE metric 20807.0099\n",
      "Epoch 186 batch 30 train Loss 58.9770 test Loss 26.8239 with MSE metric 20804.1627\n",
      "Epoch 186 batch 40 train Loss 58.9655 test Loss 26.8193 with MSE metric 20801.2777\n",
      "Epoch 186 batch 50 train Loss 58.9540 test Loss 26.8146 with MSE metric 20798.4372\n",
      "Epoch 186 batch 60 train Loss 58.9425 test Loss 26.8101 with MSE metric 20795.6380\n",
      "Epoch 186 batch 70 train Loss 58.9310 test Loss 26.8055 with MSE metric 20792.8398\n",
      "Epoch 186 batch 80 train Loss 58.9195 test Loss 26.8009 with MSE metric 20789.9886\n",
      "Epoch 186 batch 90 train Loss 58.9079 test Loss 26.7963 with MSE metric 20787.1436\n",
      "Epoch 186 batch 100 train Loss 58.8964 test Loss 26.7917 with MSE metric 20784.2945\n",
      "Epoch 186 batch 110 train Loss 58.8850 test Loss 26.7871 with MSE metric 20781.4982\n",
      "Epoch 186 batch 120 train Loss 58.8735 test Loss 26.7825 with MSE metric 20778.7730\n",
      "Epoch 186 batch 130 train Loss 58.8620 test Loss 26.7779 with MSE metric 20775.9506\n",
      "Epoch 186 batch 140 train Loss 58.8505 test Loss 26.7733 with MSE metric 20773.0924\n",
      "Epoch 186 batch 150 train Loss 58.8390 test Loss 26.7687 with MSE metric 20770.2446\n",
      "Epoch 186 batch 160 train Loss 58.8276 test Loss 26.7641 with MSE metric 20767.4178\n",
      "Epoch 186 batch 170 train Loss 58.8161 test Loss 26.7595 with MSE metric 20764.7035\n",
      "Epoch 186 batch 180 train Loss 58.8046 test Loss 26.7550 with MSE metric 20761.9084\n",
      "Epoch 186 batch 190 train Loss 58.7932 test Loss 26.7504 with MSE metric 20759.0731\n",
      "Epoch 186 batch 200 train Loss 58.7817 test Loss 26.7458 with MSE metric 20756.3281\n",
      "Epoch 186 batch 210 train Loss 58.7703 test Loss 26.7412 with MSE metric 20753.4640\n",
      "Epoch 186 batch 220 train Loss 58.7589 test Loss 26.7367 with MSE metric 20750.7152\n",
      "Epoch 186 batch 230 train Loss 58.7474 test Loss 26.7321 with MSE metric 20747.9112\n",
      "Epoch 186 batch 240 train Loss 58.7360 test Loss 26.7276 with MSE metric 20745.1096\n",
      "Time taken for 1 epoch: 24.422606229782104 secs\n",
      "\n",
      "Epoch 187 batch 0 train Loss 58.7246 test Loss 26.7230 with MSE metric 20742.3493\n",
      "Epoch 187 batch 10 train Loss 58.7132 test Loss 26.7184 with MSE metric 20739.5346\n",
      "Epoch 187 batch 20 train Loss 58.7017 test Loss 26.7139 with MSE metric 20736.7892\n",
      "Epoch 187 batch 30 train Loss 58.6903 test Loss 26.7093 with MSE metric 20733.9496\n",
      "Epoch 187 batch 40 train Loss 58.6789 test Loss 26.7048 with MSE metric 20731.1599\n",
      "Epoch 187 batch 50 train Loss 58.6675 test Loss 26.7002 with MSE metric 20728.3225\n",
      "Epoch 187 batch 60 train Loss 58.6561 test Loss 26.6957 with MSE metric 20725.5052\n",
      "Epoch 187 batch 70 train Loss 58.6447 test Loss 26.6912 with MSE metric 20722.7572\n",
      "Epoch 187 batch 80 train Loss 58.6334 test Loss 26.6866 with MSE metric 20719.9283\n",
      "Epoch 187 batch 90 train Loss 58.6220 test Loss 26.6821 with MSE metric 20717.1834\n",
      "Epoch 187 batch 100 train Loss 58.6106 test Loss 26.6775 with MSE metric 20714.4698\n",
      "Epoch 187 batch 110 train Loss 58.5992 test Loss 26.6730 with MSE metric 20711.6449\n",
      "Epoch 187 batch 120 train Loss 58.5879 test Loss 26.6685 with MSE metric 20708.8808\n",
      "Epoch 187 batch 130 train Loss 58.5765 test Loss 26.6639 with MSE metric 20706.1539\n",
      "Epoch 187 batch 140 train Loss 58.5651 test Loss 26.6594 with MSE metric 20703.4305\n",
      "Epoch 187 batch 150 train Loss 58.5538 test Loss 26.6548 with MSE metric 20700.6258\n",
      "Epoch 187 batch 160 train Loss 58.5424 test Loss 26.6503 with MSE metric 20697.8579\n",
      "Epoch 187 batch 170 train Loss 58.5311 test Loss 26.6458 with MSE metric 20695.1296\n",
      "Epoch 187 batch 180 train Loss 58.5198 test Loss 26.6413 with MSE metric 20692.3176\n",
      "Epoch 187 batch 190 train Loss 58.5084 test Loss 26.6367 with MSE metric 20689.5698\n",
      "Epoch 187 batch 200 train Loss 58.4971 test Loss 26.6322 with MSE metric 20686.7802\n",
      "Epoch 187 batch 210 train Loss 58.4858 test Loss 26.6277 with MSE metric 20683.9931\n",
      "Epoch 187 batch 220 train Loss 58.4745 test Loss 26.6232 with MSE metric 20681.2394\n",
      "Epoch 187 batch 230 train Loss 58.4631 test Loss 26.6186 with MSE metric 20678.5382\n",
      "Epoch 187 batch 240 train Loss 58.4518 test Loss 26.6141 with MSE metric 20675.7648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 26.134443998336792 secs\n",
      "\n",
      "Epoch 188 batch 0 train Loss 58.4405 test Loss 26.6096 with MSE metric 20672.9529\n",
      "Epoch 188 batch 10 train Loss 58.4292 test Loss 26.6051 with MSE metric 20670.1348\n",
      "Epoch 188 batch 20 train Loss 58.4179 test Loss 26.6006 with MSE metric 20667.3541\n",
      "Epoch 188 batch 30 train Loss 58.4066 test Loss 26.5961 with MSE metric 20664.5860\n",
      "Epoch 188 batch 40 train Loss 58.3953 test Loss 26.5916 with MSE metric 20661.9005\n",
      "Epoch 188 batch 50 train Loss 58.3841 test Loss 26.5871 with MSE metric 20659.0923\n",
      "Epoch 188 batch 60 train Loss 58.3728 test Loss 26.5826 with MSE metric 20656.3645\n",
      "Epoch 188 batch 70 train Loss 58.3615 test Loss 26.5781 with MSE metric 20653.6077\n",
      "Epoch 188 batch 80 train Loss 58.3503 test Loss 26.5736 with MSE metric 20650.8816\n",
      "Epoch 188 batch 90 train Loss 58.3390 test Loss 26.5691 with MSE metric 20648.1696\n",
      "Epoch 188 batch 100 train Loss 58.3278 test Loss 26.5646 with MSE metric 20645.4166\n",
      "Epoch 188 batch 110 train Loss 58.3165 test Loss 26.5601 with MSE metric 20642.6389\n",
      "Epoch 188 batch 120 train Loss 58.3053 test Loss 26.5556 with MSE metric 20639.9050\n",
      "Epoch 188 batch 130 train Loss 58.2940 test Loss 26.5511 with MSE metric 20637.1079\n",
      "Epoch 188 batch 140 train Loss 58.2828 test Loss 26.5466 with MSE metric 20634.3779\n",
      "Epoch 188 batch 150 train Loss 58.2716 test Loss 26.5422 with MSE metric 20631.6621\n",
      "Epoch 188 batch 160 train Loss 58.2603 test Loss 26.5377 with MSE metric 20628.8256\n",
      "Epoch 188 batch 170 train Loss 58.2491 test Loss 26.5332 with MSE metric 20626.1148\n",
      "Epoch 188 batch 180 train Loss 58.2379 test Loss 26.5287 with MSE metric 20623.3136\n",
      "Epoch 188 batch 190 train Loss 58.2267 test Loss 26.5242 with MSE metric 20620.5800\n",
      "Epoch 188 batch 200 train Loss 58.2155 test Loss 26.5198 with MSE metric 20617.8600\n",
      "Epoch 188 batch 210 train Loss 58.2042 test Loss 26.5153 with MSE metric 20615.0964\n",
      "Epoch 188 batch 220 train Loss 58.1930 test Loss 26.5108 with MSE metric 20612.3885\n",
      "Epoch 188 batch 230 train Loss 58.1819 test Loss 26.5063 with MSE metric 20609.6211\n",
      "Epoch 188 batch 240 train Loss 58.1707 test Loss 26.5018 with MSE metric 20606.8950\n",
      "Time taken for 1 epoch: 25.96453595161438 secs\n",
      "\n",
      "Epoch 189 batch 0 train Loss 58.1595 test Loss 26.4974 with MSE metric 20604.1245\n",
      "Epoch 189 batch 10 train Loss 58.1483 test Loss 26.4929 with MSE metric 20601.4069\n",
      "Epoch 189 batch 20 train Loss 58.1371 test Loss 26.4884 with MSE metric 20598.5978\n",
      "Epoch 189 batch 30 train Loss 58.1259 test Loss 26.4840 with MSE metric 20595.8793\n",
      "Epoch 189 batch 40 train Loss 58.1148 test Loss 26.4795 with MSE metric 20593.1398\n",
      "Epoch 189 batch 50 train Loss 58.1036 test Loss 26.4750 with MSE metric 20590.4323\n",
      "Epoch 189 batch 60 train Loss 58.0925 test Loss 26.4706 with MSE metric 20587.7782\n",
      "Epoch 189 batch 70 train Loss 58.0813 test Loss 26.4661 with MSE metric 20585.1020\n",
      "Epoch 189 batch 80 train Loss 58.0702 test Loss 26.4617 with MSE metric 20582.2867\n",
      "Epoch 189 batch 90 train Loss 58.0590 test Loss 26.4572 with MSE metric 20579.6214\n",
      "Epoch 189 batch 100 train Loss 58.0479 test Loss 26.4528 with MSE metric 20576.9831\n",
      "Epoch 189 batch 110 train Loss 58.0368 test Loss 26.4483 with MSE metric 20574.2655\n",
      "Epoch 189 batch 120 train Loss 58.0256 test Loss 26.4438 with MSE metric 20571.5221\n",
      "Epoch 189 batch 130 train Loss 58.0145 test Loss 26.4394 with MSE metric 20568.7774\n",
      "Epoch 189 batch 140 train Loss 58.0034 test Loss 26.4349 with MSE metric 20565.9702\n",
      "Epoch 189 batch 150 train Loss 57.9923 test Loss 26.4305 with MSE metric 20563.2901\n",
      "Epoch 189 batch 160 train Loss 57.9812 test Loss 26.4261 with MSE metric 20560.5627\n",
      "Epoch 189 batch 170 train Loss 57.9701 test Loss 26.4216 with MSE metric 20557.7658\n",
      "Epoch 189 batch 180 train Loss 57.9590 test Loss 26.4172 with MSE metric 20555.0793\n",
      "Epoch 189 batch 190 train Loss 57.9479 test Loss 26.4128 with MSE metric 20552.3973\n",
      "Epoch 189 batch 200 train Loss 57.9368 test Loss 26.4083 with MSE metric 20549.7353\n",
      "Epoch 189 batch 210 train Loss 57.9257 test Loss 26.4039 with MSE metric 20547.0076\n",
      "Epoch 189 batch 220 train Loss 57.9146 test Loss 26.3995 with MSE metric 20544.2669\n",
      "Epoch 189 batch 230 train Loss 57.9035 test Loss 26.3950 with MSE metric 20541.6068\n",
      "Epoch 189 batch 240 train Loss 57.8924 test Loss 26.3906 with MSE metric 20538.8926\n",
      "Time taken for 1 epoch: 24.430814027786255 secs\n",
      "\n",
      "Epoch 190 batch 0 train Loss 57.8814 test Loss 26.3862 with MSE metric 20536.1987\n",
      "Epoch 190 batch 10 train Loss 57.8703 test Loss 26.3818 with MSE metric 20533.5352\n",
      "Epoch 190 batch 20 train Loss 57.8593 test Loss 26.3774 with MSE metric 20530.8574\n",
      "Epoch 190 batch 30 train Loss 57.8482 test Loss 26.3730 with MSE metric 20528.1620\n",
      "Epoch 190 batch 40 train Loss 57.8372 test Loss 26.3686 with MSE metric 20525.4646\n",
      "Epoch 190 batch 50 train Loss 57.8261 test Loss 26.3642 with MSE metric 20522.7668\n",
      "Epoch 190 batch 60 train Loss 57.8151 test Loss 26.3597 with MSE metric 20520.0731\n",
      "Epoch 190 batch 70 train Loss 57.8040 test Loss 26.3553 with MSE metric 20517.3748\n",
      "Epoch 190 batch 80 train Loss 57.7930 test Loss 26.3509 with MSE metric 20514.5938\n",
      "Epoch 190 batch 90 train Loss 57.7820 test Loss 26.3465 with MSE metric 20511.8692\n",
      "Epoch 190 batch 100 train Loss 57.7710 test Loss 26.3421 with MSE metric 20509.1766\n",
      "Epoch 190 batch 110 train Loss 57.7599 test Loss 26.3377 with MSE metric 20506.4467\n",
      "Epoch 190 batch 120 train Loss 57.7489 test Loss 26.3333 with MSE metric 20503.7463\n",
      "Epoch 190 batch 130 train Loss 57.7379 test Loss 26.3289 with MSE metric 20501.0429\n",
      "Epoch 190 batch 140 train Loss 57.7269 test Loss 26.3246 with MSE metric 20498.3338\n",
      "Epoch 190 batch 150 train Loss 57.7159 test Loss 26.3202 with MSE metric 20495.6336\n",
      "Epoch 190 batch 160 train Loss 57.7049 test Loss 26.3158 with MSE metric 20492.9541\n",
      "Epoch 190 batch 170 train Loss 57.6939 test Loss 26.3114 with MSE metric 20490.2044\n",
      "Epoch 190 batch 180 train Loss 57.6830 test Loss 26.3070 with MSE metric 20487.4453\n",
      "Epoch 190 batch 190 train Loss 57.6720 test Loss 26.3026 with MSE metric 20484.7766\n",
      "Epoch 190 batch 200 train Loss 57.6610 test Loss 26.2983 with MSE metric 20482.1074\n",
      "Epoch 190 batch 210 train Loss 57.6500 test Loss 26.2939 with MSE metric 20479.3232\n",
      "Epoch 190 batch 220 train Loss 57.6391 test Loss 26.2895 with MSE metric 20476.6023\n",
      "Epoch 190 batch 230 train Loss 57.6281 test Loss 26.2852 with MSE metric 20473.9213\n",
      "Epoch 190 batch 240 train Loss 57.6171 test Loss 26.2808 with MSE metric 20471.2290\n",
      "Time taken for 1 epoch: 24.263030767440796 secs\n",
      "\n",
      "Epoch 191 batch 0 train Loss 57.6062 test Loss 26.2764 with MSE metric 20468.5454\n",
      "Epoch 191 batch 10 train Loss 57.5952 test Loss 26.2720 with MSE metric 20465.8505\n",
      "Epoch 191 batch 20 train Loss 57.5843 test Loss 26.2677 with MSE metric 20463.1953\n",
      "Epoch 191 batch 30 train Loss 57.5734 test Loss 26.2633 with MSE metric 20460.4644\n",
      "Epoch 191 batch 40 train Loss 57.5624 test Loss 26.2589 with MSE metric 20457.7649\n",
      "Epoch 191 batch 50 train Loss 57.5515 test Loss 26.2546 with MSE metric 20454.9608\n",
      "Epoch 191 batch 60 train Loss 57.5406 test Loss 26.2502 with MSE metric 20452.2637\n",
      "Epoch 191 batch 70 train Loss 57.5296 test Loss 26.2458 with MSE metric 20449.6173\n",
      "Epoch 191 batch 80 train Loss 57.5187 test Loss 26.2415 with MSE metric 20446.9361\n",
      "Epoch 191 batch 90 train Loss 57.5078 test Loss 26.2371 with MSE metric 20444.2467\n",
      "Epoch 191 batch 100 train Loss 57.4969 test Loss 26.2327 with MSE metric 20441.6159\n",
      "Epoch 191 batch 110 train Loss 57.4860 test Loss 26.2284 with MSE metric 20438.8768\n",
      "Epoch 191 batch 120 train Loss 57.4751 test Loss 26.2240 with MSE metric 20436.2103\n",
      "Epoch 191 batch 130 train Loss 57.4642 test Loss 26.2197 with MSE metric 20433.5342\n",
      "Epoch 191 batch 140 train Loss 57.4533 test Loss 26.2153 with MSE metric 20430.8246\n",
      "Epoch 191 batch 150 train Loss 57.4424 test Loss 26.2110 with MSE metric 20428.1580\n",
      "Epoch 191 batch 160 train Loss 57.4316 test Loss 26.2067 with MSE metric 20425.4463\n",
      "Epoch 191 batch 170 train Loss 57.4207 test Loss 26.2023 with MSE metric 20422.7161\n",
      "Epoch 191 batch 180 train Loss 57.4098 test Loss 26.1980 with MSE metric 20420.0322\n",
      "Epoch 191 batch 190 train Loss 57.3989 test Loss 26.1937 with MSE metric 20417.3759\n",
      "Epoch 191 batch 200 train Loss 57.3881 test Loss 26.1893 with MSE metric 20414.6269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191 batch 210 train Loss 57.3772 test Loss 26.1850 with MSE metric 20412.0044\n",
      "Epoch 191 batch 220 train Loss 57.3664 test Loss 26.1807 with MSE metric 20409.4542\n",
      "Epoch 191 batch 230 train Loss 57.3555 test Loss 26.1763 with MSE metric 20406.7981\n",
      "Epoch 191 batch 240 train Loss 57.3447 test Loss 26.1720 with MSE metric 20404.0900\n",
      "Time taken for 1 epoch: 24.243361949920654 secs\n",
      "\n",
      "Epoch 192 batch 0 train Loss 57.3338 test Loss 26.1677 with MSE metric 20401.4685\n",
      "Epoch 192 batch 10 train Loss 57.3230 test Loss 26.1634 with MSE metric 20398.8579\n",
      "Epoch 192 batch 20 train Loss 57.3122 test Loss 26.1590 with MSE metric 20396.2741\n",
      "Epoch 192 batch 30 train Loss 57.3013 test Loss 26.1547 with MSE metric 20393.5933\n",
      "Epoch 192 batch 40 train Loss 57.2905 test Loss 26.1504 with MSE metric 20390.9795\n",
      "Epoch 192 batch 50 train Loss 57.2797 test Loss 26.1461 with MSE metric 20388.3429\n",
      "Epoch 192 batch 60 train Loss 57.2689 test Loss 26.1417 with MSE metric 20385.6847\n",
      "Epoch 192 batch 70 train Loss 57.2581 test Loss 26.1374 with MSE metric 20383.0678\n",
      "Epoch 192 batch 80 train Loss 57.2473 test Loss 26.1331 with MSE metric 20380.4439\n",
      "Epoch 192 batch 90 train Loss 57.2365 test Loss 26.1288 with MSE metric 20377.8663\n",
      "Epoch 192 batch 100 train Loss 57.2257 test Loss 26.1245 with MSE metric 20375.2167\n",
      "Epoch 192 batch 110 train Loss 57.2149 test Loss 26.1202 with MSE metric 20372.5819\n",
      "Epoch 192 batch 120 train Loss 57.2041 test Loss 26.1159 with MSE metric 20369.8799\n",
      "Epoch 192 batch 130 train Loss 57.1934 test Loss 26.1116 with MSE metric 20367.1993\n",
      "Epoch 192 batch 140 train Loss 57.1826 test Loss 26.1073 with MSE metric 20364.5502\n",
      "Epoch 192 batch 150 train Loss 57.1718 test Loss 26.1030 with MSE metric 20361.9595\n",
      "Epoch 192 batch 160 train Loss 57.1610 test Loss 26.0987 with MSE metric 20359.3449\n",
      "Epoch 192 batch 170 train Loss 57.1503 test Loss 26.0944 with MSE metric 20356.7768\n",
      "Epoch 192 batch 180 train Loss 57.1395 test Loss 26.0901 with MSE metric 20354.1927\n",
      "Epoch 192 batch 190 train Loss 57.1288 test Loss 26.0858 with MSE metric 20351.5487\n",
      "Epoch 192 batch 200 train Loss 57.1180 test Loss 26.0815 with MSE metric 20348.9641\n",
      "Epoch 192 batch 210 train Loss 57.1073 test Loss 26.0772 with MSE metric 20346.3399\n",
      "Epoch 192 batch 220 train Loss 57.0965 test Loss 26.0729 with MSE metric 20343.6773\n",
      "Epoch 192 batch 230 train Loss 57.0858 test Loss 26.0686 with MSE metric 20341.0344\n",
      "Epoch 192 batch 240 train Loss 57.0751 test Loss 26.0643 with MSE metric 20338.4342\n",
      "Time taken for 1 epoch: 24.230401039123535 secs\n",
      "\n",
      "Epoch 193 batch 0 train Loss 57.0644 test Loss 26.0601 with MSE metric 20335.8255\n",
      "Epoch 193 batch 10 train Loss 57.0536 test Loss 26.0558 with MSE metric 20333.1541\n",
      "Epoch 193 batch 20 train Loss 57.0429 test Loss 26.0515 with MSE metric 20330.5280\n",
      "Epoch 193 batch 30 train Loss 57.0322 test Loss 26.0472 with MSE metric 20327.9621\n",
      "Epoch 193 batch 40 train Loss 57.0215 test Loss 26.0429 with MSE metric 20325.3720\n",
      "Epoch 193 batch 50 train Loss 57.0108 test Loss 26.0387 with MSE metric 20322.7443\n",
      "Epoch 193 batch 60 train Loss 57.0001 test Loss 26.0344 with MSE metric 20320.0888\n",
      "Epoch 193 batch 70 train Loss 56.9894 test Loss 26.0301 with MSE metric 20317.4887\n",
      "Epoch 193 batch 80 train Loss 56.9787 test Loss 26.0259 with MSE metric 20314.9302\n",
      "Epoch 193 batch 90 train Loss 56.9680 test Loss 26.0216 with MSE metric 20312.2871\n",
      "Epoch 193 batch 100 train Loss 56.9573 test Loss 26.0173 with MSE metric 20309.6809\n",
      "Epoch 193 batch 110 train Loss 56.9467 test Loss 26.0131 with MSE metric 20307.0512\n",
      "Epoch 193 batch 120 train Loss 56.9360 test Loss 26.0088 with MSE metric 20304.5424\n",
      "Epoch 193 batch 130 train Loss 56.9253 test Loss 26.0045 with MSE metric 20301.9306\n",
      "Epoch 193 batch 140 train Loss 56.9147 test Loss 26.0003 with MSE metric 20299.4088\n",
      "Epoch 193 batch 150 train Loss 56.9040 test Loss 25.9960 with MSE metric 20296.7919\n",
      "Epoch 193 batch 160 train Loss 56.8933 test Loss 25.9918 with MSE metric 20294.1626\n",
      "Epoch 193 batch 170 train Loss 56.8827 test Loss 25.9875 with MSE metric 20291.4993\n",
      "Epoch 193 batch 180 train Loss 56.8720 test Loss 25.9833 with MSE metric 20288.8744\n",
      "Epoch 193 batch 190 train Loss 56.8614 test Loss 25.9791 with MSE metric 20286.3040\n",
      "Epoch 193 batch 200 train Loss 56.8508 test Loss 25.9748 with MSE metric 20283.6331\n",
      "Epoch 193 batch 210 train Loss 56.8401 test Loss 25.9706 with MSE metric 20280.9601\n",
      "Epoch 193 batch 220 train Loss 56.8295 test Loss 25.9663 with MSE metric 20278.3895\n",
      "Epoch 193 batch 230 train Loss 56.8189 test Loss 25.9621 with MSE metric 20275.8267\n",
      "Epoch 193 batch 240 train Loss 56.8082 test Loss 25.9579 with MSE metric 20273.2410\n",
      "Time taken for 1 epoch: 24.26750898361206 secs\n",
      "\n",
      "Epoch 194 batch 0 train Loss 56.7976 test Loss 25.9536 with MSE metric 20270.6847\n",
      "Epoch 194 batch 10 train Loss 56.7870 test Loss 25.9494 with MSE metric 20268.1279\n",
      "Epoch 194 batch 20 train Loss 56.7764 test Loss 25.9451 with MSE metric 20265.5448\n",
      "Epoch 194 batch 30 train Loss 56.7658 test Loss 25.9409 with MSE metric 20262.9539\n",
      "Epoch 194 batch 40 train Loss 56.7552 test Loss 25.9367 with MSE metric 20260.3191\n",
      "Epoch 194 batch 50 train Loss 56.7446 test Loss 25.9325 with MSE metric 20257.7141\n",
      "Epoch 194 batch 60 train Loss 56.7340 test Loss 25.9282 with MSE metric 20255.1422\n",
      "Epoch 194 batch 70 train Loss 56.7235 test Loss 25.9240 with MSE metric 20252.5647\n",
      "Epoch 194 batch 80 train Loss 56.7129 test Loss 25.9198 with MSE metric 20249.9103\n",
      "Epoch 194 batch 90 train Loss 56.7023 test Loss 25.9156 with MSE metric 20247.2259\n",
      "Epoch 194 batch 100 train Loss 56.6917 test Loss 25.9113 with MSE metric 20244.5945\n",
      "Epoch 194 batch 110 train Loss 56.6811 test Loss 25.9071 with MSE metric 20242.0056\n",
      "Epoch 194 batch 120 train Loss 56.6706 test Loss 25.9029 with MSE metric 20239.4346\n",
      "Epoch 194 batch 130 train Loss 56.6600 test Loss 25.8987 with MSE metric 20236.8301\n",
      "Epoch 194 batch 140 train Loss 56.6495 test Loss 25.8945 with MSE metric 20234.2718\n",
      "Epoch 194 batch 150 train Loss 56.6389 test Loss 25.8902 with MSE metric 20231.7776\n",
      "Epoch 194 batch 160 train Loss 56.6284 test Loss 25.8860 with MSE metric 20229.1884\n",
      "Epoch 194 batch 170 train Loss 56.6178 test Loss 25.8818 with MSE metric 20226.6039\n",
      "Epoch 194 batch 180 train Loss 56.6073 test Loss 25.8776 with MSE metric 20224.1593\n",
      "Epoch 194 batch 190 train Loss 56.5968 test Loss 25.8734 with MSE metric 20221.6257\n",
      "Epoch 194 batch 200 train Loss 56.5862 test Loss 25.8692 with MSE metric 20219.0100\n",
      "Epoch 194 batch 210 train Loss 56.5757 test Loss 25.8650 with MSE metric 20216.4537\n",
      "Epoch 194 batch 220 train Loss 56.5652 test Loss 25.8608 with MSE metric 20213.9147\n",
      "Epoch 194 batch 230 train Loss 56.5547 test Loss 25.8566 with MSE metric 20211.3628\n",
      "Epoch 194 batch 240 train Loss 56.5442 test Loss 25.8524 with MSE metric 20208.8274\n",
      "Time taken for 1 epoch: 24.530982971191406 secs\n",
      "\n",
      "Epoch 195 batch 0 train Loss 56.5337 test Loss 25.8481 with MSE metric 20206.2729\n",
      "Epoch 195 batch 10 train Loss 56.5232 test Loss 25.8440 with MSE metric 20203.7101\n",
      "Epoch 195 batch 20 train Loss 56.5127 test Loss 25.8398 with MSE metric 20201.1666\n",
      "Epoch 195 batch 30 train Loss 56.5022 test Loss 25.8356 with MSE metric 20198.6146\n",
      "Epoch 195 batch 40 train Loss 56.4917 test Loss 25.8314 with MSE metric 20196.1252\n",
      "Epoch 195 batch 50 train Loss 56.4812 test Loss 25.8272 with MSE metric 20193.5416\n",
      "Epoch 195 batch 60 train Loss 56.4707 test Loss 25.8230 with MSE metric 20191.0162\n",
      "Epoch 195 batch 70 train Loss 56.4603 test Loss 25.8188 with MSE metric 20188.4888\n",
      "Epoch 195 batch 80 train Loss 56.4498 test Loss 25.8147 with MSE metric 20185.8978\n",
      "Epoch 195 batch 90 train Loss 56.4393 test Loss 25.8105 with MSE metric 20183.3431\n",
      "Epoch 195 batch 100 train Loss 56.4288 test Loss 25.8063 with MSE metric 20180.7905\n",
      "Epoch 195 batch 110 train Loss 56.4184 test Loss 25.8021 with MSE metric 20178.1928\n",
      "Epoch 195 batch 120 train Loss 56.4079 test Loss 25.7980 with MSE metric 20175.6581\n",
      "Epoch 195 batch 130 train Loss 56.3975 test Loss 25.7938 with MSE metric 20173.0683\n",
      "Epoch 195 batch 140 train Loss 56.3870 test Loss 25.7896 with MSE metric 20170.4449\n",
      "Epoch 195 batch 150 train Loss 56.3766 test Loss 25.7855 with MSE metric 20167.9199\n",
      "Epoch 195 batch 160 train Loss 56.3662 test Loss 25.7813 with MSE metric 20165.4160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195 batch 170 train Loss 56.3557 test Loss 25.7772 with MSE metric 20162.8661\n",
      "Epoch 195 batch 180 train Loss 56.3453 test Loss 25.7730 with MSE metric 20160.3229\n",
      "Epoch 195 batch 190 train Loss 56.3349 test Loss 25.7688 with MSE metric 20157.7472\n",
      "Epoch 195 batch 200 train Loss 56.3245 test Loss 25.7647 with MSE metric 20155.1717\n",
      "Epoch 195 batch 210 train Loss 56.3140 test Loss 25.7605 with MSE metric 20152.6080\n",
      "Epoch 195 batch 220 train Loss 56.3036 test Loss 25.7563 with MSE metric 20150.0517\n",
      "Epoch 195 batch 230 train Loss 56.2932 test Loss 25.7522 with MSE metric 20147.4624\n",
      "Epoch 195 batch 240 train Loss 56.2828 test Loss 25.7480 with MSE metric 20144.9190\n",
      "Time taken for 1 epoch: 24.31041717529297 secs\n",
      "\n",
      "Epoch 196 batch 0 train Loss 56.2724 test Loss 25.7439 with MSE metric 20142.3433\n",
      "Epoch 196 batch 10 train Loss 56.2620 test Loss 25.7397 with MSE metric 20139.8556\n",
      "Epoch 196 batch 20 train Loss 56.2516 test Loss 25.7355 with MSE metric 20137.3563\n",
      "Epoch 196 batch 30 train Loss 56.2412 test Loss 25.7314 with MSE metric 20134.7485\n",
      "Epoch 196 batch 40 train Loss 56.2309 test Loss 25.7273 with MSE metric 20132.1683\n",
      "Epoch 196 batch 50 train Loss 56.2205 test Loss 25.7231 with MSE metric 20129.6556\n",
      "Epoch 196 batch 60 train Loss 56.2101 test Loss 25.7190 with MSE metric 20127.1117\n",
      "Epoch 196 batch 70 train Loss 56.1997 test Loss 25.7148 with MSE metric 20124.6448\n",
      "Epoch 196 batch 80 train Loss 56.1894 test Loss 25.7107 with MSE metric 20122.0844\n",
      "Epoch 196 batch 90 train Loss 56.1790 test Loss 25.7066 with MSE metric 20119.5223\n",
      "Epoch 196 batch 100 train Loss 56.1687 test Loss 25.7024 with MSE metric 20116.9881\n",
      "Epoch 196 batch 110 train Loss 56.1583 test Loss 25.6983 with MSE metric 20114.5368\n",
      "Epoch 196 batch 120 train Loss 56.1480 test Loss 25.6942 with MSE metric 20112.0092\n",
      "Epoch 196 batch 130 train Loss 56.1376 test Loss 25.6900 with MSE metric 20109.5432\n",
      "Epoch 196 batch 140 train Loss 56.1273 test Loss 25.6859 with MSE metric 20106.9664\n",
      "Epoch 196 batch 150 train Loss 56.1169 test Loss 25.6818 with MSE metric 20104.4591\n",
      "Epoch 196 batch 160 train Loss 56.1066 test Loss 25.6777 with MSE metric 20101.9000\n",
      "Epoch 196 batch 170 train Loss 56.0963 test Loss 25.6735 with MSE metric 20099.3686\n",
      "Epoch 196 batch 180 train Loss 56.0859 test Loss 25.6694 with MSE metric 20096.8630\n",
      "Epoch 196 batch 190 train Loss 56.0756 test Loss 25.6653 with MSE metric 20094.3491\n",
      "Epoch 196 batch 200 train Loss 56.0653 test Loss 25.6612 with MSE metric 20091.8198\n",
      "Epoch 196 batch 210 train Loss 56.0550 test Loss 25.6571 with MSE metric 20089.3188\n",
      "Epoch 196 batch 220 train Loss 56.0447 test Loss 25.6529 with MSE metric 20086.7408\n",
      "Epoch 196 batch 230 train Loss 56.0344 test Loss 25.6488 with MSE metric 20084.2879\n",
      "Epoch 196 batch 240 train Loss 56.0241 test Loss 25.6447 with MSE metric 20081.7934\n",
      "Time taken for 1 epoch: 24.481933116912842 secs\n",
      "\n",
      "Epoch 197 batch 0 train Loss 56.0138 test Loss 25.6406 with MSE metric 20079.2348\n",
      "Epoch 197 batch 10 train Loss 56.0035 test Loss 25.6365 with MSE metric 20076.7157\n",
      "Epoch 197 batch 20 train Loss 55.9932 test Loss 25.6324 with MSE metric 20074.2094\n",
      "Epoch 197 batch 30 train Loss 55.9829 test Loss 25.6283 with MSE metric 20071.6926\n",
      "Epoch 197 batch 40 train Loss 55.9726 test Loss 25.6242 with MSE metric 20069.1073\n",
      "Epoch 197 batch 50 train Loss 55.9624 test Loss 25.6201 with MSE metric 20066.5851\n",
      "Epoch 197 batch 60 train Loss 55.9521 test Loss 25.6160 with MSE metric 20064.0629\n",
      "Epoch 197 batch 70 train Loss 55.9418 test Loss 25.6119 with MSE metric 20061.5778\n",
      "Epoch 197 batch 80 train Loss 55.9316 test Loss 25.6078 with MSE metric 20059.0626\n",
      "Epoch 197 batch 90 train Loss 55.9213 test Loss 25.6037 with MSE metric 20056.5598\n",
      "Epoch 197 batch 100 train Loss 55.9111 test Loss 25.5996 with MSE metric 20054.0910\n",
      "Epoch 197 batch 110 train Loss 55.9008 test Loss 25.5956 with MSE metric 20051.5860\n",
      "Epoch 197 batch 120 train Loss 55.8906 test Loss 25.5915 with MSE metric 20049.0470\n",
      "Epoch 197 batch 130 train Loss 55.8803 test Loss 25.5874 with MSE metric 20046.4438\n",
      "Epoch 197 batch 140 train Loss 55.8701 test Loss 25.5833 with MSE metric 20043.9270\n",
      "Epoch 197 batch 150 train Loss 55.8598 test Loss 25.5792 with MSE metric 20041.4079\n",
      "Epoch 197 batch 160 train Loss 55.8496 test Loss 25.5751 with MSE metric 20038.8576\n",
      "Epoch 197 batch 170 train Loss 55.8394 test Loss 25.5710 with MSE metric 20036.3059\n",
      "Epoch 197 batch 180 train Loss 55.8292 test Loss 25.5670 with MSE metric 20033.8330\n",
      "Epoch 197 batch 190 train Loss 55.8189 test Loss 25.5629 with MSE metric 20031.3449\n",
      "Epoch 197 batch 200 train Loss 55.8087 test Loss 25.5588 with MSE metric 20028.8093\n",
      "Epoch 197 batch 210 train Loss 55.7985 test Loss 25.5547 with MSE metric 20026.3092\n",
      "Epoch 197 batch 220 train Loss 55.7883 test Loss 25.5506 with MSE metric 20023.8829\n",
      "Epoch 197 batch 230 train Loss 55.7781 test Loss 25.5466 with MSE metric 20021.3625\n",
      "Epoch 197 batch 240 train Loss 55.7679 test Loss 25.5425 with MSE metric 20018.8731\n",
      "Time taken for 1 epoch: 24.362313985824585 secs\n",
      "\n",
      "Epoch 198 batch 0 train Loss 55.7577 test Loss 25.5384 with MSE metric 20016.3083\n",
      "Epoch 198 batch 10 train Loss 55.7475 test Loss 25.5343 with MSE metric 20013.8461\n",
      "Epoch 198 batch 20 train Loss 55.7374 test Loss 25.5303 with MSE metric 20011.3804\n",
      "Epoch 198 batch 30 train Loss 55.7272 test Loss 25.5262 with MSE metric 20008.8760\n",
      "Epoch 198 batch 40 train Loss 55.7170 test Loss 25.5221 with MSE metric 20006.3650\n",
      "Epoch 198 batch 50 train Loss 55.7068 test Loss 25.5180 with MSE metric 20003.8765\n",
      "Epoch 198 batch 60 train Loss 55.6967 test Loss 25.5140 with MSE metric 20001.4042\n",
      "Epoch 198 batch 70 train Loss 55.6865 test Loss 25.5100 with MSE metric 19998.8638\n",
      "Epoch 198 batch 80 train Loss 55.6763 test Loss 25.5059 with MSE metric 19996.3492\n",
      "Epoch 198 batch 90 train Loss 55.6662 test Loss 25.5019 with MSE metric 19993.8726\n",
      "Epoch 198 batch 100 train Loss 55.6560 test Loss 25.4978 with MSE metric 19991.3755\n",
      "Epoch 198 batch 110 train Loss 55.6459 test Loss 25.4938 with MSE metric 19988.8758\n",
      "Epoch 198 batch 120 train Loss 55.6357 test Loss 25.4897 with MSE metric 19986.4495\n",
      "Epoch 198 batch 130 train Loss 55.6256 test Loss 25.4856 with MSE metric 19983.9784\n",
      "Epoch 198 batch 140 train Loss 55.6155 test Loss 25.4816 with MSE metric 19981.4586\n",
      "Epoch 198 batch 150 train Loss 55.6053 test Loss 25.4776 with MSE metric 19978.9398\n",
      "Epoch 198 batch 160 train Loss 55.5952 test Loss 25.4735 with MSE metric 19976.4848\n",
      "Epoch 198 batch 170 train Loss 55.5851 test Loss 25.4695 with MSE metric 19974.0016\n",
      "Epoch 198 batch 180 train Loss 55.5750 test Loss 25.4654 with MSE metric 19971.6033\n",
      "Epoch 198 batch 190 train Loss 55.5649 test Loss 25.4614 with MSE metric 19969.0896\n",
      "Epoch 198 batch 200 train Loss 55.5547 test Loss 25.4574 with MSE metric 19966.5734\n",
      "Epoch 198 batch 210 train Loss 55.5446 test Loss 25.4533 with MSE metric 19964.1301\n",
      "Epoch 198 batch 220 train Loss 55.5345 test Loss 25.4493 with MSE metric 19961.6308\n",
      "Epoch 198 batch 230 train Loss 55.5244 test Loss 25.4453 with MSE metric 19959.1564\n",
      "Epoch 198 batch 240 train Loss 55.5143 test Loss 25.4412 with MSE metric 19956.6874\n",
      "Time taken for 1 epoch: 24.719050884246826 secs\n",
      "\n",
      "Epoch 199 batch 0 train Loss 55.5043 test Loss 25.4372 with MSE metric 19954.2348\n",
      "Epoch 199 batch 10 train Loss 55.4942 test Loss 25.4331 with MSE metric 19951.7661\n",
      "Epoch 199 batch 20 train Loss 55.4841 test Loss 25.4291 with MSE metric 19949.2742\n",
      "Epoch 199 batch 30 train Loss 55.4740 test Loss 25.4250 with MSE metric 19946.8178\n",
      "Epoch 199 batch 40 train Loss 55.4639 test Loss 25.4210 with MSE metric 19944.3181\n",
      "Epoch 199 batch 50 train Loss 55.4539 test Loss 25.4170 with MSE metric 19941.8578\n",
      "Epoch 199 batch 60 train Loss 55.4438 test Loss 25.4130 with MSE metric 19939.4239\n",
      "Epoch 199 batch 70 train Loss 55.4337 test Loss 25.4090 with MSE metric 19937.0503\n",
      "Epoch 199 batch 80 train Loss 55.4237 test Loss 25.4049 with MSE metric 19934.6023\n",
      "Epoch 199 batch 90 train Loss 55.4136 test Loss 25.4009 with MSE metric 19932.1678\n",
      "Epoch 199 batch 100 train Loss 55.4036 test Loss 25.3969 with MSE metric 19929.6758\n",
      "Epoch 199 batch 110 train Loss 55.3935 test Loss 25.3929 with MSE metric 19927.1681\n",
      "Epoch 199 batch 120 train Loss 55.3835 test Loss 25.3889 with MSE metric 19924.7397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199 batch 130 train Loss 55.3735 test Loss 25.3849 with MSE metric 19922.2934\n",
      "Epoch 199 batch 140 train Loss 55.3634 test Loss 25.3809 with MSE metric 19919.8637\n",
      "Epoch 199 batch 150 train Loss 55.3534 test Loss 25.3769 with MSE metric 19917.4139\n",
      "Epoch 199 batch 160 train Loss 55.3434 test Loss 25.3729 with MSE metric 19914.9843\n",
      "Epoch 199 batch 170 train Loss 55.3334 test Loss 25.3689 with MSE metric 19912.5261\n",
      "Epoch 199 batch 180 train Loss 55.3233 test Loss 25.3649 with MSE metric 19910.0578\n",
      "Epoch 199 batch 190 train Loss 55.3133 test Loss 25.3608 with MSE metric 19907.6350\n",
      "Epoch 199 batch 200 train Loss 55.3033 test Loss 25.3568 with MSE metric 19905.2199\n",
      "Epoch 199 batch 210 train Loss 55.2933 test Loss 25.3528 with MSE metric 19902.7515\n",
      "Epoch 199 batch 220 train Loss 55.2833 test Loss 25.3488 with MSE metric 19900.2799\n",
      "Epoch 199 batch 230 train Loss 55.2733 test Loss 25.3448 with MSE metric 19897.8117\n",
      "Epoch 199 batch 240 train Loss 55.2633 test Loss 25.3409 with MSE metric 19895.3139\n",
      "Time taken for 1 epoch: 26.155203819274902 secs\n",
      "\n",
      "Epoch 200 batch 0 train Loss 55.2533 test Loss 25.3369 with MSE metric 19892.8330\n",
      "Epoch 200 batch 10 train Loss 55.2433 test Loss 25.3329 with MSE metric 19890.4540\n",
      "Epoch 200 batch 20 train Loss 55.2334 test Loss 25.3289 with MSE metric 19887.9695\n",
      "Epoch 200 batch 30 train Loss 55.2234 test Loss 25.3249 with MSE metric 19885.5159\n",
      "Epoch 200 batch 40 train Loss 55.2134 test Loss 25.3209 with MSE metric 19883.0327\n",
      "Epoch 200 batch 50 train Loss 55.2034 test Loss 25.3170 with MSE metric 19880.5826\n",
      "Epoch 200 batch 60 train Loss 55.1935 test Loss 25.3130 with MSE metric 19878.1009\n",
      "Epoch 200 batch 70 train Loss 55.1835 test Loss 25.3090 with MSE metric 19875.5989\n",
      "Epoch 200 batch 80 train Loss 55.1735 test Loss 25.3050 with MSE metric 19873.1343\n",
      "Epoch 200 batch 90 train Loss 55.1636 test Loss 25.3011 with MSE metric 19870.7105\n",
      "Epoch 200 batch 100 train Loss 55.1536 test Loss 25.2971 with MSE metric 19868.1917\n",
      "Epoch 200 batch 110 train Loss 55.1437 test Loss 25.2931 with MSE metric 19865.8165\n",
      "Epoch 200 batch 120 train Loss 55.1337 test Loss 25.2892 with MSE metric 19863.3538\n",
      "Epoch 200 batch 130 train Loss 55.1238 test Loss 25.2852 with MSE metric 19860.9383\n",
      "Epoch 200 batch 140 train Loss 55.1139 test Loss 25.2812 with MSE metric 19858.5004\n",
      "Epoch 200 batch 150 train Loss 55.1039 test Loss 25.2773 with MSE metric 19856.0863\n",
      "Epoch 200 batch 160 train Loss 55.0940 test Loss 25.2733 with MSE metric 19853.6174\n",
      "Epoch 200 batch 170 train Loss 55.0841 test Loss 25.2694 with MSE metric 19851.1673\n",
      "Epoch 200 batch 180 train Loss 55.0742 test Loss 25.2654 with MSE metric 19848.7288\n",
      "Epoch 200 batch 190 train Loss 55.0643 test Loss 25.2614 with MSE metric 19846.2108\n",
      "Epoch 200 batch 200 train Loss 55.0544 test Loss 25.2575 with MSE metric 19843.7719\n",
      "Epoch 200 batch 210 train Loss 55.0444 test Loss 25.2535 with MSE metric 19841.3640\n",
      "Epoch 200 batch 220 train Loss 55.0345 test Loss 25.2496 with MSE metric 19838.9539\n",
      "Epoch 200 batch 230 train Loss 55.0247 test Loss 25.2456 with MSE metric 19836.5290\n",
      "Epoch 200 batch 240 train Loss 55.0148 test Loss 25.2416 with MSE metric 19834.1162\n",
      "Time taken for 1 epoch: 24.97882318496704 secs\n",
      "\n",
      "Epoch 201 batch 0 train Loss 55.0049 test Loss 25.2377 with MSE metric 19831.6756\n",
      "Epoch 201 batch 10 train Loss 54.9950 test Loss 25.2337 with MSE metric 19829.2019\n",
      "Epoch 201 batch 20 train Loss 54.9851 test Loss 25.2298 with MSE metric 19826.7845\n",
      "Epoch 201 batch 30 train Loss 54.9752 test Loss 25.2259 with MSE metric 19824.3943\n",
      "Epoch 201 batch 40 train Loss 54.9653 test Loss 25.2219 with MSE metric 19822.0187\n",
      "Epoch 201 batch 50 train Loss 54.9555 test Loss 25.2180 with MSE metric 19819.5099\n",
      "Epoch 201 batch 60 train Loss 54.9456 test Loss 25.2140 with MSE metric 19817.0146\n",
      "Epoch 201 batch 70 train Loss 54.9357 test Loss 25.2101 with MSE metric 19814.5964\n",
      "Epoch 201 batch 80 train Loss 54.9259 test Loss 25.2062 with MSE metric 19812.2126\n",
      "Epoch 201 batch 90 train Loss 54.9160 test Loss 25.2022 with MSE metric 19809.8300\n",
      "Epoch 201 batch 100 train Loss 54.9062 test Loss 25.1983 with MSE metric 19807.4369\n",
      "Epoch 201 batch 110 train Loss 54.8963 test Loss 25.1943 with MSE metric 19805.0447\n",
      "Epoch 201 batch 120 train Loss 54.8865 test Loss 25.1904 with MSE metric 19802.6610\n",
      "Epoch 201 batch 130 train Loss 54.8767 test Loss 25.1865 with MSE metric 19800.2356\n",
      "Epoch 201 batch 140 train Loss 54.8668 test Loss 25.1826 with MSE metric 19797.7948\n",
      "Epoch 201 batch 150 train Loss 54.8570 test Loss 25.1787 with MSE metric 19795.3773\n",
      "Epoch 201 batch 160 train Loss 54.8472 test Loss 25.1747 with MSE metric 19793.0010\n",
      "Epoch 201 batch 170 train Loss 54.8373 test Loss 25.1708 with MSE metric 19790.5826\n",
      "Epoch 201 batch 180 train Loss 54.8275 test Loss 25.1669 with MSE metric 19788.2614\n",
      "Epoch 201 batch 190 train Loss 54.8177 test Loss 25.1630 with MSE metric 19785.8233\n",
      "Epoch 201 batch 200 train Loss 54.8079 test Loss 25.1591 with MSE metric 19783.4008\n",
      "Epoch 201 batch 210 train Loss 54.7981 test Loss 25.1551 with MSE metric 19781.0223\n",
      "Epoch 201 batch 220 train Loss 54.7883 test Loss 25.1512 with MSE metric 19778.6033\n",
      "Epoch 201 batch 230 train Loss 54.7785 test Loss 25.1473 with MSE metric 19776.1775\n",
      "Epoch 201 batch 240 train Loss 54.7687 test Loss 25.1434 with MSE metric 19773.7540\n",
      "Time taken for 1 epoch: 25.70627999305725 secs\n",
      "\n",
      "Epoch 202 batch 0 train Loss 54.7589 test Loss 25.1395 with MSE metric 19771.3931\n",
      "Epoch 202 batch 10 train Loss 54.7491 test Loss 25.1355 with MSE metric 19768.9956\n",
      "Epoch 202 batch 20 train Loss 54.7393 test Loss 25.1316 with MSE metric 19766.6247\n",
      "Epoch 202 batch 30 train Loss 54.7295 test Loss 25.1277 with MSE metric 19764.2366\n",
      "Epoch 202 batch 40 train Loss 54.7198 test Loss 25.1238 with MSE metric 19761.8262\n",
      "Epoch 202 batch 50 train Loss 54.7100 test Loss 25.1199 with MSE metric 19759.4528\n",
      "Epoch 202 batch 60 train Loss 54.7002 test Loss 25.1160 with MSE metric 19757.0095\n",
      "Epoch 202 batch 70 train Loss 54.6905 test Loss 25.1121 with MSE metric 19754.6411\n",
      "Epoch 202 batch 80 train Loss 54.6807 test Loss 25.1082 with MSE metric 19752.2467\n",
      "Epoch 202 batch 90 train Loss 54.6709 test Loss 25.1043 with MSE metric 19749.9261\n",
      "Epoch 202 batch 100 train Loss 54.6612 test Loss 25.1004 with MSE metric 19747.5296\n",
      "Epoch 202 batch 110 train Loss 54.6514 test Loss 25.0965 with MSE metric 19745.1008\n",
      "Epoch 202 batch 120 train Loss 54.6417 test Loss 25.0927 with MSE metric 19742.7833\n",
      "Epoch 202 batch 130 train Loss 54.6320 test Loss 25.0888 with MSE metric 19740.3325\n",
      "Epoch 202 batch 140 train Loss 54.6222 test Loss 25.0849 with MSE metric 19737.9543\n",
      "Epoch 202 batch 150 train Loss 54.6125 test Loss 25.0810 with MSE metric 19735.6366\n",
      "Epoch 202 batch 160 train Loss 54.6027 test Loss 25.0772 with MSE metric 19733.2118\n",
      "Epoch 202 batch 170 train Loss 54.5930 test Loss 25.0733 with MSE metric 19730.7966\n",
      "Epoch 202 batch 180 train Loss 54.5833 test Loss 25.0694 with MSE metric 19728.3693\n",
      "Epoch 202 batch 190 train Loss 54.5736 test Loss 25.0655 with MSE metric 19725.9500\n",
      "Epoch 202 batch 200 train Loss 54.5639 test Loss 25.0616 with MSE metric 19723.6594\n",
      "Epoch 202 batch 210 train Loss 54.5542 test Loss 25.0577 with MSE metric 19721.2929\n",
      "Epoch 202 batch 220 train Loss 54.5444 test Loss 25.0539 with MSE metric 19718.9016\n",
      "Epoch 202 batch 230 train Loss 54.5347 test Loss 25.0500 with MSE metric 19716.5748\n",
      "Epoch 202 batch 240 train Loss 54.5250 test Loss 25.0461 with MSE metric 19714.1746\n",
      "Time taken for 1 epoch: 24.562053203582764 secs\n",
      "\n",
      "Epoch 203 batch 0 train Loss 54.5153 test Loss 25.0422 with MSE metric 19711.7376\n",
      "Epoch 203 batch 10 train Loss 54.5056 test Loss 25.0384 with MSE metric 19709.2778\n",
      "Epoch 203 batch 20 train Loss 54.4960 test Loss 25.0345 with MSE metric 19706.8812\n",
      "Epoch 203 batch 30 train Loss 54.4863 test Loss 25.0306 with MSE metric 19704.6026\n",
      "Epoch 203 batch 40 train Loss 54.4766 test Loss 25.0268 with MSE metric 19702.3462\n",
      "Epoch 203 batch 50 train Loss 54.4669 test Loss 25.0229 with MSE metric 19699.9885\n",
      "Epoch 203 batch 60 train Loss 54.4572 test Loss 25.0190 with MSE metric 19697.5750\n",
      "Epoch 203 batch 70 train Loss 54.4476 test Loss 25.0152 with MSE metric 19695.1787\n",
      "Epoch 203 batch 80 train Loss 54.4379 test Loss 25.0113 with MSE metric 19692.7707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 batch 90 train Loss 54.4282 test Loss 25.0075 with MSE metric 19690.3766\n",
      "Epoch 203 batch 100 train Loss 54.4186 test Loss 25.0036 with MSE metric 19687.9998\n",
      "Epoch 203 batch 110 train Loss 54.4089 test Loss 24.9997 with MSE metric 19685.6136\n",
      "Epoch 203 batch 120 train Loss 54.3993 test Loss 24.9959 with MSE metric 19683.3174\n",
      "Epoch 203 batch 130 train Loss 54.3896 test Loss 24.9920 with MSE metric 19681.0014\n",
      "Epoch 203 batch 140 train Loss 54.3800 test Loss 24.9882 with MSE metric 19678.5475\n",
      "Epoch 203 batch 150 train Loss 54.3704 test Loss 24.9843 with MSE metric 19676.2407\n",
      "Epoch 203 batch 160 train Loss 54.3607 test Loss 24.9805 with MSE metric 19673.8426\n",
      "Epoch 203 batch 170 train Loss 54.3511 test Loss 24.9766 with MSE metric 19671.4559\n",
      "Epoch 203 batch 180 train Loss 54.3415 test Loss 24.9728 with MSE metric 19669.0698\n",
      "Epoch 203 batch 190 train Loss 54.3318 test Loss 24.9690 with MSE metric 19666.7179\n",
      "Epoch 203 batch 200 train Loss 54.3222 test Loss 24.9651 with MSE metric 19664.3456\n",
      "Epoch 203 batch 210 train Loss 54.3126 test Loss 24.9613 with MSE metric 19661.9841\n",
      "Epoch 203 batch 220 train Loss 54.3030 test Loss 24.9574 with MSE metric 19659.6304\n",
      "Epoch 203 batch 230 train Loss 54.2934 test Loss 24.9536 with MSE metric 19657.3036\n",
      "Epoch 203 batch 240 train Loss 54.2838 test Loss 24.9497 with MSE metric 19654.9857\n",
      "Time taken for 1 epoch: 24.432424068450928 secs\n",
      "\n",
      "Epoch 204 batch 0 train Loss 54.2742 test Loss 24.9459 with MSE metric 19652.6890\n",
      "Epoch 204 batch 10 train Loss 54.2646 test Loss 24.9421 with MSE metric 19650.2902\n",
      "Epoch 204 batch 20 train Loss 54.2550 test Loss 24.9382 with MSE metric 19647.9165\n",
      "Epoch 204 batch 30 train Loss 54.2454 test Loss 24.9344 with MSE metric 19645.6082\n",
      "Epoch 204 batch 40 train Loss 54.2358 test Loss 24.9305 with MSE metric 19643.2278\n",
      "Epoch 204 batch 50 train Loss 54.2262 test Loss 24.9267 with MSE metric 19640.8947\n",
      "Epoch 204 batch 60 train Loss 54.2166 test Loss 24.9229 with MSE metric 19638.4899\n",
      "Epoch 204 batch 70 train Loss 54.2071 test Loss 24.9190 with MSE metric 19636.0603\n",
      "Epoch 204 batch 80 train Loss 54.1975 test Loss 24.9152 with MSE metric 19633.6959\n",
      "Epoch 204 batch 90 train Loss 54.1879 test Loss 24.9114 with MSE metric 19631.3903\n",
      "Epoch 204 batch 100 train Loss 54.1784 test Loss 24.9075 with MSE metric 19629.0576\n",
      "Epoch 204 batch 110 train Loss 54.1688 test Loss 24.9037 with MSE metric 19626.6215\n",
      "Epoch 204 batch 120 train Loss 54.1592 test Loss 24.8999 with MSE metric 19624.1756\n",
      "Epoch 204 batch 130 train Loss 54.1497 test Loss 24.8961 with MSE metric 19621.8758\n",
      "Epoch 204 batch 140 train Loss 54.1401 test Loss 24.8923 with MSE metric 19619.5061\n",
      "Epoch 204 batch 150 train Loss 54.1306 test Loss 24.8885 with MSE metric 19617.1239\n",
      "Epoch 204 batch 160 train Loss 54.1210 test Loss 24.8847 with MSE metric 19614.7116\n",
      "Epoch 204 batch 170 train Loss 54.1115 test Loss 24.8809 with MSE metric 19612.3890\n",
      "Epoch 204 batch 180 train Loss 54.1020 test Loss 24.8771 with MSE metric 19610.0344\n",
      "Epoch 204 batch 190 train Loss 54.0924 test Loss 24.8733 with MSE metric 19607.7239\n",
      "Epoch 204 batch 200 train Loss 54.0829 test Loss 24.8695 with MSE metric 19605.3974\n",
      "Epoch 204 batch 210 train Loss 54.0734 test Loss 24.8656 with MSE metric 19602.9977\n",
      "Epoch 204 batch 220 train Loss 54.0639 test Loss 24.8619 with MSE metric 19600.6295\n",
      "Epoch 204 batch 230 train Loss 54.0544 test Loss 24.8581 with MSE metric 19598.3642\n",
      "Epoch 204 batch 240 train Loss 54.0448 test Loss 24.8543 with MSE metric 19595.9973\n",
      "Time taken for 1 epoch: 25.12465000152588 secs\n",
      "\n",
      "Epoch 205 batch 0 train Loss 54.0353 test Loss 24.8505 with MSE metric 19593.6738\n",
      "Epoch 205 batch 10 train Loss 54.0258 test Loss 24.8467 with MSE metric 19591.3643\n",
      "Epoch 205 batch 20 train Loss 54.0163 test Loss 24.8429 with MSE metric 19589.0385\n",
      "Epoch 205 batch 30 train Loss 54.0068 test Loss 24.8391 with MSE metric 19586.7595\n",
      "Epoch 205 batch 40 train Loss 53.9973 test Loss 24.8353 with MSE metric 19584.4523\n",
      "Epoch 205 batch 50 train Loss 53.9878 test Loss 24.8315 with MSE metric 19582.1461\n",
      "Epoch 205 batch 60 train Loss 53.9784 test Loss 24.8277 with MSE metric 19579.8855\n",
      "Epoch 205 batch 70 train Loss 53.9689 test Loss 24.8239 with MSE metric 19577.6059\n",
      "Epoch 205 batch 80 train Loss 53.9594 test Loss 24.8201 with MSE metric 19575.2814\n",
      "Epoch 205 batch 90 train Loss 53.9499 test Loss 24.8163 with MSE metric 19572.8956\n",
      "Epoch 205 batch 100 train Loss 53.9405 test Loss 24.8126 with MSE metric 19570.6326\n",
      "Epoch 205 batch 110 train Loss 53.9310 test Loss 24.8088 with MSE metric 19568.3508\n",
      "Epoch 205 batch 120 train Loss 53.9215 test Loss 24.8050 with MSE metric 19566.0562\n",
      "Epoch 205 batch 130 train Loss 53.9121 test Loss 24.8012 with MSE metric 19563.7226\n",
      "Epoch 205 batch 140 train Loss 53.9026 test Loss 24.7975 with MSE metric 19561.3264\n",
      "Epoch 205 batch 150 train Loss 53.8932 test Loss 24.7937 with MSE metric 19558.9694\n",
      "Epoch 205 batch 160 train Loss 53.8837 test Loss 24.7899 with MSE metric 19556.6912\n",
      "Epoch 205 batch 170 train Loss 53.8743 test Loss 24.7862 with MSE metric 19554.4182\n",
      "Epoch 205 batch 180 train Loss 53.8648 test Loss 24.7824 with MSE metric 19552.0702\n",
      "Epoch 205 batch 190 train Loss 53.8554 test Loss 24.7786 with MSE metric 19549.7564\n",
      "Epoch 205 batch 200 train Loss 53.8460 test Loss 24.7748 with MSE metric 19547.4668\n",
      "Epoch 205 batch 210 train Loss 53.8365 test Loss 24.7711 with MSE metric 19545.1168\n",
      "Epoch 205 batch 220 train Loss 53.8271 test Loss 24.7673 with MSE metric 19542.8217\n",
      "Epoch 205 batch 230 train Loss 53.8177 test Loss 24.7635 with MSE metric 19540.5419\n",
      "Epoch 205 batch 240 train Loss 53.8083 test Loss 24.7598 with MSE metric 19538.2024\n",
      "Time taken for 1 epoch: 24.55048704147339 secs\n",
      "\n",
      "Epoch 206 batch 0 train Loss 53.7989 test Loss 24.7560 with MSE metric 19535.8421\n",
      "Epoch 206 batch 10 train Loss 53.7894 test Loss 24.7522 with MSE metric 19533.5335\n",
      "Epoch 206 batch 20 train Loss 53.7800 test Loss 24.7485 with MSE metric 19531.2818\n",
      "Epoch 206 batch 30 train Loss 53.7706 test Loss 24.7447 with MSE metric 19528.9334\n",
      "Epoch 206 batch 40 train Loss 53.7612 test Loss 24.7409 with MSE metric 19526.6837\n",
      "Epoch 206 batch 50 train Loss 53.7518 test Loss 24.7372 with MSE metric 19524.3655\n",
      "Epoch 206 batch 60 train Loss 53.7424 test Loss 24.7334 with MSE metric 19522.0509\n",
      "Epoch 206 batch 70 train Loss 53.7330 test Loss 24.7297 with MSE metric 19519.7519\n",
      "Epoch 206 batch 80 train Loss 53.7237 test Loss 24.7259 with MSE metric 19517.4575\n",
      "Epoch 206 batch 90 train Loss 53.7143 test Loss 24.7222 with MSE metric 19515.1365\n",
      "Epoch 206 batch 100 train Loss 53.7049 test Loss 24.7184 with MSE metric 19512.8446\n",
      "Epoch 206 batch 110 train Loss 53.6955 test Loss 24.7147 with MSE metric 19510.5143\n",
      "Epoch 206 batch 120 train Loss 53.6861 test Loss 24.7110 with MSE metric 19508.1921\n",
      "Epoch 206 batch 130 train Loss 53.6768 test Loss 24.7072 with MSE metric 19505.9096\n",
      "Epoch 206 batch 140 train Loss 53.6674 test Loss 24.7035 with MSE metric 19503.5572\n",
      "Epoch 206 batch 150 train Loss 53.6580 test Loss 24.6997 with MSE metric 19501.1753\n",
      "Epoch 206 batch 160 train Loss 53.6487 test Loss 24.6960 with MSE metric 19498.9363\n",
      "Epoch 206 batch 170 train Loss 53.6393 test Loss 24.6923 with MSE metric 19496.6435\n",
      "Epoch 206 batch 180 train Loss 53.6300 test Loss 24.6885 with MSE metric 19494.3739\n",
      "Epoch 206 batch 190 train Loss 53.6206 test Loss 24.6848 with MSE metric 19492.0767\n",
      "Epoch 206 batch 200 train Loss 53.6113 test Loss 24.6810 with MSE metric 19489.7868\n",
      "Epoch 206 batch 210 train Loss 53.6020 test Loss 24.6773 with MSE metric 19487.5471\n",
      "Epoch 206 batch 220 train Loss 53.5926 test Loss 24.6736 with MSE metric 19485.2606\n",
      "Epoch 206 batch 230 train Loss 53.5833 test Loss 24.6698 with MSE metric 19482.9355\n",
      "Epoch 206 batch 240 train Loss 53.5740 test Loss 24.6661 with MSE metric 19480.6269\n",
      "Time taken for 1 epoch: 24.480300188064575 secs\n",
      "\n",
      "Epoch 207 batch 0 train Loss 53.5646 test Loss 24.6624 with MSE metric 19478.2553\n",
      "Epoch 207 batch 10 train Loss 53.5553 test Loss 24.6587 with MSE metric 19475.9710\n",
      "Epoch 207 batch 20 train Loss 53.5460 test Loss 24.6550 with MSE metric 19473.6770\n",
      "Epoch 207 batch 30 train Loss 53.5367 test Loss 24.6512 with MSE metric 19471.3755\n",
      "Epoch 207 batch 40 train Loss 53.5274 test Loss 24.6475 with MSE metric 19469.1280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207 batch 50 train Loss 53.5180 test Loss 24.6438 with MSE metric 19466.9093\n",
      "Epoch 207 batch 60 train Loss 53.5087 test Loss 24.6401 with MSE metric 19464.6308\n",
      "Epoch 207 batch 70 train Loss 53.4994 test Loss 24.6364 with MSE metric 19462.3212\n",
      "Epoch 207 batch 80 train Loss 53.4901 test Loss 24.6326 with MSE metric 19460.0058\n",
      "Epoch 207 batch 90 train Loss 53.4808 test Loss 24.6289 with MSE metric 19457.7171\n",
      "Epoch 207 batch 100 train Loss 53.4716 test Loss 24.6252 with MSE metric 19455.4603\n",
      "Epoch 207 batch 110 train Loss 53.4623 test Loss 24.6215 with MSE metric 19453.1755\n",
      "Epoch 207 batch 120 train Loss 53.4530 test Loss 24.6178 with MSE metric 19450.8925\n",
      "Epoch 207 batch 130 train Loss 53.4437 test Loss 24.6141 with MSE metric 19448.5944\n",
      "Epoch 207 batch 140 train Loss 53.4344 test Loss 24.6104 with MSE metric 19446.3420\n",
      "Epoch 207 batch 150 train Loss 53.4252 test Loss 24.6067 with MSE metric 19444.0937\n",
      "Epoch 207 batch 160 train Loss 53.4159 test Loss 24.6030 with MSE metric 19441.8396\n",
      "Epoch 207 batch 170 train Loss 53.4066 test Loss 24.5993 with MSE metric 19439.5295\n",
      "Epoch 207 batch 180 train Loss 53.3974 test Loss 24.5956 with MSE metric 19437.2462\n",
      "Epoch 207 batch 190 train Loss 53.3881 test Loss 24.5919 with MSE metric 19435.0038\n",
      "Epoch 207 batch 200 train Loss 53.3789 test Loss 24.5882 with MSE metric 19432.7385\n",
      "Epoch 207 batch 210 train Loss 53.3696 test Loss 24.5845 with MSE metric 19430.4785\n",
      "Epoch 207 batch 220 train Loss 53.3604 test Loss 24.5808 with MSE metric 19428.2196\n",
      "Epoch 207 batch 230 train Loss 53.3511 test Loss 24.5771 with MSE metric 19425.9542\n",
      "Epoch 207 batch 240 train Loss 53.3419 test Loss 24.5735 with MSE metric 19423.7183\n",
      "Time taken for 1 epoch: 24.167922973632812 secs\n",
      "\n",
      "Epoch 208 batch 0 train Loss 53.3327 test Loss 24.5698 with MSE metric 19421.4514\n",
      "Epoch 208 batch 10 train Loss 53.3234 test Loss 24.5661 with MSE metric 19419.1796\n",
      "Epoch 208 batch 20 train Loss 53.3142 test Loss 24.5624 with MSE metric 19416.9435\n",
      "Epoch 208 batch 30 train Loss 53.3050 test Loss 24.5587 with MSE metric 19414.6514\n",
      "Epoch 208 batch 40 train Loss 53.2957 test Loss 24.5550 with MSE metric 19412.3930\n",
      "Epoch 208 batch 50 train Loss 53.2865 test Loss 24.5514 with MSE metric 19410.0587\n",
      "Epoch 208 batch 60 train Loss 53.2773 test Loss 24.5477 with MSE metric 19407.8869\n",
      "Epoch 208 batch 70 train Loss 53.2681 test Loss 24.5440 with MSE metric 19405.5651\n",
      "Epoch 208 batch 80 train Loss 53.2589 test Loss 24.5403 with MSE metric 19403.3840\n",
      "Epoch 208 batch 90 train Loss 53.2497 test Loss 24.5367 with MSE metric 19401.0919\n",
      "Epoch 208 batch 100 train Loss 53.2405 test Loss 24.5330 with MSE metric 19398.8220\n",
      "Epoch 208 batch 110 train Loss 53.2313 test Loss 24.5293 with MSE metric 19396.6105\n",
      "Epoch 208 batch 120 train Loss 53.2221 test Loss 24.5256 with MSE metric 19394.3615\n",
      "Epoch 208 batch 130 train Loss 53.2129 test Loss 24.5220 with MSE metric 19392.0438\n",
      "Epoch 208 batch 140 train Loss 53.2037 test Loss 24.5183 with MSE metric 19389.8249\n",
      "Epoch 208 batch 150 train Loss 53.1945 test Loss 24.5147 with MSE metric 19387.5514\n",
      "Epoch 208 batch 160 train Loss 53.1854 test Loss 24.5110 with MSE metric 19385.2616\n",
      "Epoch 208 batch 170 train Loss 53.1762 test Loss 24.5073 with MSE metric 19383.0146\n",
      "Epoch 208 batch 180 train Loss 53.1670 test Loss 24.5036 with MSE metric 19380.7593\n",
      "Epoch 208 batch 190 train Loss 53.1578 test Loss 24.5000 with MSE metric 19378.4517\n",
      "Epoch 208 batch 200 train Loss 53.1487 test Loss 24.4963 with MSE metric 19376.2146\n",
      "Epoch 208 batch 210 train Loss 53.1395 test Loss 24.4927 with MSE metric 19373.9182\n",
      "Epoch 208 batch 220 train Loss 53.1304 test Loss 24.4890 with MSE metric 19371.6967\n",
      "Epoch 208 batch 230 train Loss 53.1212 test Loss 24.4853 with MSE metric 19369.4586\n",
      "Epoch 208 batch 240 train Loss 53.1121 test Loss 24.4817 with MSE metric 19367.2593\n",
      "Time taken for 1 epoch: 23.947566986083984 secs\n",
      "\n",
      "Epoch 209 batch 0 train Loss 53.1029 test Loss 24.4780 with MSE metric 19365.0184\n",
      "Epoch 209 batch 10 train Loss 53.0938 test Loss 24.4744 with MSE metric 19362.7561\n",
      "Epoch 209 batch 20 train Loss 53.0846 test Loss 24.4707 with MSE metric 19360.4746\n",
      "Epoch 209 batch 30 train Loss 53.0755 test Loss 24.4671 with MSE metric 19358.1894\n",
      "Epoch 209 batch 40 train Loss 53.0664 test Loss 24.4634 with MSE metric 19355.9116\n",
      "Epoch 209 batch 50 train Loss 53.0572 test Loss 24.4598 with MSE metric 19353.7062\n",
      "Epoch 209 batch 60 train Loss 53.0481 test Loss 24.4561 with MSE metric 19351.4793\n",
      "Epoch 209 batch 70 train Loss 53.0390 test Loss 24.4525 with MSE metric 19349.2354\n",
      "Epoch 209 batch 80 train Loss 53.0299 test Loss 24.4489 with MSE metric 19346.9879\n",
      "Epoch 209 batch 90 train Loss 53.0207 test Loss 24.4452 with MSE metric 19344.7611\n",
      "Epoch 209 batch 100 train Loss 53.0116 test Loss 24.4416 with MSE metric 19342.5511\n",
      "Epoch 209 batch 110 train Loss 53.0025 test Loss 24.4379 with MSE metric 19340.3231\n",
      "Epoch 209 batch 120 train Loss 52.9934 test Loss 24.4343 with MSE metric 19338.1658\n",
      "Epoch 209 batch 130 train Loss 52.9843 test Loss 24.4307 with MSE metric 19335.9861\n",
      "Epoch 209 batch 140 train Loss 52.9752 test Loss 24.4271 with MSE metric 19333.7554\n",
      "Epoch 209 batch 150 train Loss 52.9661 test Loss 24.4234 with MSE metric 19331.6070\n",
      "Epoch 209 batch 160 train Loss 52.9570 test Loss 24.4198 with MSE metric 19329.4192\n",
      "Epoch 209 batch 170 train Loss 52.9480 test Loss 24.4161 with MSE metric 19327.2021\n",
      "Epoch 209 batch 180 train Loss 52.9389 test Loss 24.4125 with MSE metric 19324.9504\n",
      "Epoch 209 batch 190 train Loss 52.9298 test Loss 24.4089 with MSE metric 19322.7427\n",
      "Epoch 209 batch 200 train Loss 52.9207 test Loss 24.4053 with MSE metric 19320.4633\n",
      "Epoch 209 batch 210 train Loss 52.9116 test Loss 24.4017 with MSE metric 19318.2214\n",
      "Epoch 209 batch 220 train Loss 52.9026 test Loss 24.3980 with MSE metric 19316.0379\n",
      "Epoch 209 batch 230 train Loss 52.8935 test Loss 24.3944 with MSE metric 19313.7769\n",
      "Epoch 209 batch 240 train Loss 52.8844 test Loss 24.3908 with MSE metric 19311.4982\n",
      "Time taken for 1 epoch: 23.84233593940735 secs\n",
      "\n",
      "Epoch 210 batch 0 train Loss 52.8754 test Loss 24.3872 with MSE metric 19309.2990\n",
      "Epoch 210 batch 10 train Loss 52.8663 test Loss 24.3836 with MSE metric 19307.0846\n",
      "Epoch 210 batch 20 train Loss 52.8573 test Loss 24.3800 with MSE metric 19304.9178\n",
      "Epoch 210 batch 30 train Loss 52.8482 test Loss 24.3763 with MSE metric 19302.6829\n",
      "Epoch 210 batch 40 train Loss 52.8392 test Loss 24.3727 with MSE metric 19300.4684\n",
      "Epoch 210 batch 50 train Loss 52.8301 test Loss 24.3691 with MSE metric 19298.2120\n",
      "Epoch 210 batch 60 train Loss 52.8211 test Loss 24.3655 with MSE metric 19296.0681\n",
      "Epoch 210 batch 70 train Loss 52.8121 test Loss 24.3619 with MSE metric 19293.8749\n",
      "Epoch 210 batch 80 train Loss 52.8030 test Loss 24.3583 with MSE metric 19291.6535\n",
      "Epoch 210 batch 90 train Loss 52.7940 test Loss 24.3547 with MSE metric 19289.4454\n",
      "Epoch 210 batch 100 train Loss 52.7850 test Loss 24.3511 with MSE metric 19287.2100\n",
      "Epoch 210 batch 110 train Loss 52.7759 test Loss 24.3475 with MSE metric 19285.0097\n",
      "Epoch 210 batch 120 train Loss 52.7669 test Loss 24.3439 with MSE metric 19282.7930\n",
      "Epoch 210 batch 130 train Loss 52.7579 test Loss 24.3403 with MSE metric 19280.5749\n",
      "Epoch 210 batch 140 train Loss 52.7489 test Loss 24.3367 with MSE metric 19278.3022\n",
      "Epoch 210 batch 150 train Loss 52.7399 test Loss 24.3331 with MSE metric 19276.0933\n",
      "Epoch 210 batch 160 train Loss 52.7309 test Loss 24.3295 with MSE metric 19273.8498\n",
      "Epoch 210 batch 170 train Loss 52.7219 test Loss 24.3259 with MSE metric 19271.6296\n",
      "Epoch 210 batch 180 train Loss 52.7129 test Loss 24.3223 with MSE metric 19269.3848\n",
      "Epoch 210 batch 190 train Loss 52.7039 test Loss 24.3187 with MSE metric 19267.1566\n",
      "Epoch 210 batch 200 train Loss 52.6949 test Loss 24.3151 with MSE metric 19264.9111\n",
      "Epoch 210 batch 210 train Loss 52.6859 test Loss 24.3115 with MSE metric 19262.6892\n",
      "Epoch 210 batch 220 train Loss 52.6769 test Loss 24.3079 with MSE metric 19260.4792\n",
      "Epoch 210 batch 230 train Loss 52.6679 test Loss 24.3043 with MSE metric 19258.2441\n",
      "Epoch 210 batch 240 train Loss 52.6590 test Loss 24.3008 with MSE metric 19256.0317\n",
      "Time taken for 1 epoch: 23.73912811279297 secs\n",
      "\n",
      "Epoch 211 batch 0 train Loss 52.6500 test Loss 24.2972 with MSE metric 19253.8069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211 batch 10 train Loss 52.6410 test Loss 24.2936 with MSE metric 19251.6501\n",
      "Epoch 211 batch 20 train Loss 52.6320 test Loss 24.2900 with MSE metric 19249.4707\n",
      "Epoch 211 batch 30 train Loss 52.6231 test Loss 24.2865 with MSE metric 19247.2499\n",
      "Epoch 211 batch 40 train Loss 52.6141 test Loss 24.2829 with MSE metric 19245.0466\n",
      "Epoch 211 batch 50 train Loss 52.6052 test Loss 24.2793 with MSE metric 19242.8685\n",
      "Epoch 211 batch 60 train Loss 52.5962 test Loss 24.2757 with MSE metric 19240.6418\n",
      "Epoch 211 batch 70 train Loss 52.5873 test Loss 24.2722 with MSE metric 19238.4054\n",
      "Epoch 211 batch 80 train Loss 52.5783 test Loss 24.2686 with MSE metric 19236.1809\n",
      "Epoch 211 batch 90 train Loss 52.5694 test Loss 24.2650 with MSE metric 19233.9924\n",
      "Epoch 211 batch 100 train Loss 52.5604 test Loss 24.2615 with MSE metric 19231.7586\n",
      "Epoch 211 batch 110 train Loss 52.5515 test Loss 24.2579 with MSE metric 19229.5781\n",
      "Epoch 211 batch 120 train Loss 52.5425 test Loss 24.2543 with MSE metric 19227.4312\n",
      "Epoch 211 batch 130 train Loss 52.5336 test Loss 24.2507 with MSE metric 19225.2816\n",
      "Epoch 211 batch 140 train Loss 52.5247 test Loss 24.2472 with MSE metric 19223.0758\n",
      "Epoch 211 batch 150 train Loss 52.5158 test Loss 24.2436 with MSE metric 19220.9088\n",
      "Epoch 211 batch 160 train Loss 52.5068 test Loss 24.2400 with MSE metric 19218.7334\n",
      "Epoch 211 batch 170 train Loss 52.4979 test Loss 24.2365 with MSE metric 19216.5969\n",
      "Epoch 211 batch 180 train Loss 52.4890 test Loss 24.2329 with MSE metric 19214.4492\n",
      "Epoch 211 batch 190 train Loss 52.4801 test Loss 24.2293 with MSE metric 19212.2940\n",
      "Epoch 211 batch 200 train Loss 52.4712 test Loss 24.2258 with MSE metric 19210.0939\n",
      "Epoch 211 batch 210 train Loss 52.4623 test Loss 24.2222 with MSE metric 19207.9003\n",
      "Epoch 211 batch 220 train Loss 52.4534 test Loss 24.2187 with MSE metric 19205.7296\n",
      "Epoch 211 batch 230 train Loss 52.4445 test Loss 24.2151 with MSE metric 19203.5334\n",
      "Epoch 211 batch 240 train Loss 52.4356 test Loss 24.2116 with MSE metric 19201.3844\n",
      "Time taken for 1 epoch: 24.103047847747803 secs\n",
      "\n",
      "Epoch 212 batch 0 train Loss 52.4267 test Loss 24.2080 with MSE metric 19199.1629\n",
      "Epoch 212 batch 10 train Loss 52.4178 test Loss 24.2045 with MSE metric 19196.9695\n",
      "Epoch 212 batch 20 train Loss 52.4089 test Loss 24.2009 with MSE metric 19194.8266\n",
      "Epoch 212 batch 30 train Loss 52.4001 test Loss 24.1974 with MSE metric 19192.6371\n",
      "Epoch 212 batch 40 train Loss 52.3912 test Loss 24.1938 with MSE metric 19190.4436\n",
      "Epoch 212 batch 50 train Loss 52.3823 test Loss 24.1903 with MSE metric 19188.3159\n",
      "Epoch 212 batch 60 train Loss 52.3734 test Loss 24.1868 with MSE metric 19186.1518\n",
      "Epoch 212 batch 70 train Loss 52.3646 test Loss 24.1832 with MSE metric 19183.9805\n",
      "Epoch 212 batch 80 train Loss 52.3557 test Loss 24.1797 with MSE metric 19181.8369\n",
      "Epoch 212 batch 90 train Loss 52.3469 test Loss 24.1761 with MSE metric 19179.6846\n",
      "Epoch 212 batch 100 train Loss 52.3380 test Loss 24.1726 with MSE metric 19177.4850\n",
      "Epoch 212 batch 110 train Loss 52.3291 test Loss 24.1691 with MSE metric 19175.2861\n",
      "Epoch 212 batch 120 train Loss 52.3203 test Loss 24.1655 with MSE metric 19173.0609\n",
      "Epoch 212 batch 130 train Loss 52.3114 test Loss 24.1620 with MSE metric 19170.9021\n",
      "Epoch 212 batch 140 train Loss 52.3026 test Loss 24.1585 with MSE metric 19168.7649\n",
      "Epoch 212 batch 150 train Loss 52.2938 test Loss 24.1549 with MSE metric 19166.5562\n",
      "Epoch 212 batch 160 train Loss 52.2849 test Loss 24.1514 with MSE metric 19164.3671\n",
      "Epoch 212 batch 170 train Loss 52.2761 test Loss 24.1479 with MSE metric 19162.1924\n",
      "Epoch 212 batch 180 train Loss 52.2673 test Loss 24.1444 with MSE metric 19160.0384\n",
      "Epoch 212 batch 190 train Loss 52.2584 test Loss 24.1409 with MSE metric 19157.9397\n",
      "Epoch 212 batch 200 train Loss 52.2496 test Loss 24.1373 with MSE metric 19155.7265\n",
      "Epoch 212 batch 210 train Loss 52.2408 test Loss 24.1338 with MSE metric 19153.5351\n",
      "Epoch 212 batch 220 train Loss 52.2320 test Loss 24.1303 with MSE metric 19151.4026\n",
      "Epoch 212 batch 230 train Loss 52.2232 test Loss 24.1268 with MSE metric 19149.2798\n",
      "Epoch 212 batch 240 train Loss 52.2143 test Loss 24.1233 with MSE metric 19147.1134\n",
      "Time taken for 1 epoch: 24.39232897758484 secs\n",
      "\n",
      "Epoch 213 batch 0 train Loss 52.2055 test Loss 24.1197 with MSE metric 19144.9459\n",
      "Epoch 213 batch 10 train Loss 52.1967 test Loss 24.1162 with MSE metric 19142.8370\n",
      "Epoch 213 batch 20 train Loss 52.1879 test Loss 24.1127 with MSE metric 19140.6319\n",
      "Epoch 213 batch 30 train Loss 52.1791 test Loss 24.1092 with MSE metric 19138.5206\n",
      "Epoch 213 batch 40 train Loss 52.1703 test Loss 24.1057 with MSE metric 19136.3824\n",
      "Epoch 213 batch 50 train Loss 52.1616 test Loss 24.1022 with MSE metric 19134.2102\n",
      "Epoch 213 batch 60 train Loss 52.1528 test Loss 24.0987 with MSE metric 19132.0259\n",
      "Epoch 213 batch 70 train Loss 52.1440 test Loss 24.0952 with MSE metric 19129.8169\n",
      "Epoch 213 batch 80 train Loss 52.1352 test Loss 24.0916 with MSE metric 19127.6793\n",
      "Epoch 213 batch 90 train Loss 52.1264 test Loss 24.0881 with MSE metric 19125.5506\n",
      "Epoch 213 batch 100 train Loss 52.1176 test Loss 24.0846 with MSE metric 19123.3837\n",
      "Epoch 213 batch 110 train Loss 52.1089 test Loss 24.0811 with MSE metric 19121.2143\n",
      "Epoch 213 batch 120 train Loss 52.1001 test Loss 24.0776 with MSE metric 19119.0844\n",
      "Epoch 213 batch 130 train Loss 52.0913 test Loss 24.0741 with MSE metric 19116.9818\n",
      "Epoch 213 batch 140 train Loss 52.0826 test Loss 24.0706 with MSE metric 19114.8277\n",
      "Epoch 213 batch 150 train Loss 52.0738 test Loss 24.0671 with MSE metric 19112.8004\n",
      "Epoch 213 batch 160 train Loss 52.0651 test Loss 24.0636 with MSE metric 19110.6658\n",
      "Epoch 213 batch 170 train Loss 52.0563 test Loss 24.0601 with MSE metric 19108.5124\n",
      "Epoch 213 batch 180 train Loss 52.0476 test Loss 24.0566 with MSE metric 19106.3623\n",
      "Epoch 213 batch 190 train Loss 52.0388 test Loss 24.0531 with MSE metric 19104.1824\n",
      "Epoch 213 batch 200 train Loss 52.0301 test Loss 24.0496 with MSE metric 19101.9909\n",
      "Epoch 213 batch 210 train Loss 52.0213 test Loss 24.0461 with MSE metric 19099.8298\n",
      "Epoch 213 batch 220 train Loss 52.0126 test Loss 24.0427 with MSE metric 19097.7125\n",
      "Epoch 213 batch 230 train Loss 52.0039 test Loss 24.0392 with MSE metric 19095.5521\n",
      "Epoch 213 batch 240 train Loss 51.9952 test Loss 24.0357 with MSE metric 19093.3587\n",
      "Time taken for 1 epoch: 24.536451816558838 secs\n",
      "\n",
      "Epoch 214 batch 0 train Loss 51.9864 test Loss 24.0322 with MSE metric 19091.2085\n",
      "Epoch 214 batch 10 train Loss 51.9777 test Loss 24.0287 with MSE metric 19089.0499\n",
      "Epoch 214 batch 20 train Loss 51.9690 test Loss 24.0253 with MSE metric 19086.9422\n",
      "Epoch 214 batch 30 train Loss 51.9603 test Loss 24.0218 with MSE metric 19084.7709\n",
      "Epoch 214 batch 40 train Loss 51.9516 test Loss 24.0183 with MSE metric 19082.5933\n",
      "Epoch 214 batch 50 train Loss 51.9428 test Loss 24.0149 with MSE metric 19080.4652\n",
      "Epoch 214 batch 60 train Loss 51.9341 test Loss 24.0114 with MSE metric 19078.3254\n",
      "Epoch 214 batch 70 train Loss 51.9254 test Loss 24.0079 with MSE metric 19076.1969\n",
      "Epoch 214 batch 80 train Loss 51.9167 test Loss 24.0044 with MSE metric 19073.9991\n",
      "Epoch 214 batch 90 train Loss 51.9081 test Loss 24.0009 with MSE metric 19071.8901\n",
      "Epoch 214 batch 100 train Loss 51.8994 test Loss 23.9974 with MSE metric 19069.7836\n",
      "Epoch 214 batch 110 train Loss 51.8907 test Loss 23.9940 with MSE metric 19067.6694\n",
      "Epoch 214 batch 120 train Loss 51.8820 test Loss 23.9905 with MSE metric 19065.5603\n",
      "Epoch 214 batch 130 train Loss 51.8733 test Loss 23.9870 with MSE metric 19063.4061\n",
      "Epoch 214 batch 140 train Loss 51.8646 test Loss 23.9836 with MSE metric 19061.2889\n",
      "Epoch 214 batch 150 train Loss 51.8559 test Loss 23.9801 with MSE metric 19059.1896\n",
      "Epoch 214 batch 160 train Loss 51.8473 test Loss 23.9766 with MSE metric 19057.0303\n",
      "Epoch 214 batch 170 train Loss 51.8386 test Loss 23.9731 with MSE metric 19054.9515\n",
      "Epoch 214 batch 180 train Loss 51.8299 test Loss 23.9697 with MSE metric 19052.9113\n",
      "Epoch 214 batch 190 train Loss 51.8213 test Loss 23.9662 with MSE metric 19050.7573\n",
      "Epoch 214 batch 200 train Loss 51.8126 test Loss 23.9627 with MSE metric 19048.6680\n",
      "Epoch 214 batch 210 train Loss 51.8040 test Loss 23.9593 with MSE metric 19046.5286\n",
      "Epoch 214 batch 220 train Loss 51.7953 test Loss 23.9558 with MSE metric 19044.4161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 batch 230 train Loss 51.7867 test Loss 23.9524 with MSE metric 19042.2261\n",
      "Epoch 214 batch 240 train Loss 51.7780 test Loss 23.9489 with MSE metric 19040.0889\n",
      "Time taken for 1 epoch: 24.47244906425476 secs\n",
      "\n",
      "Epoch 215 batch 0 train Loss 51.7694 test Loss 23.9455 with MSE metric 19037.9834\n",
      "Epoch 215 batch 10 train Loss 51.7607 test Loss 23.9420 with MSE metric 19035.9704\n",
      "Epoch 215 batch 20 train Loss 51.7521 test Loss 23.9386 with MSE metric 19033.7975\n",
      "Epoch 215 batch 30 train Loss 51.7435 test Loss 23.9351 with MSE metric 19031.6647\n",
      "Epoch 215 batch 40 train Loss 51.7348 test Loss 23.9317 with MSE metric 19029.5907\n",
      "Epoch 215 batch 50 train Loss 51.7262 test Loss 23.9282 with MSE metric 19027.5356\n",
      "Epoch 215 batch 60 train Loss 51.7176 test Loss 23.9248 with MSE metric 19025.3811\n",
      "Epoch 215 batch 70 train Loss 51.7090 test Loss 23.9213 with MSE metric 19023.2808\n",
      "Epoch 215 batch 80 train Loss 51.7003 test Loss 23.9179 with MSE metric 19021.1074\n",
      "Epoch 215 batch 90 train Loss 51.6917 test Loss 23.9145 with MSE metric 19018.9890\n",
      "Epoch 215 batch 100 train Loss 51.6831 test Loss 23.9110 with MSE metric 19016.9374\n",
      "Epoch 215 batch 110 train Loss 51.6745 test Loss 23.9076 with MSE metric 19014.8770\n",
      "Epoch 215 batch 120 train Loss 51.6659 test Loss 23.9042 with MSE metric 19012.7754\n",
      "Epoch 215 batch 130 train Loss 51.6573 test Loss 23.9007 with MSE metric 19010.6160\n",
      "Epoch 215 batch 140 train Loss 51.6487 test Loss 23.8973 with MSE metric 19008.5422\n",
      "Epoch 215 batch 150 train Loss 51.6401 test Loss 23.8939 with MSE metric 19006.4418\n",
      "Epoch 215 batch 160 train Loss 51.6315 test Loss 23.8904 with MSE metric 19004.3481\n",
      "Epoch 215 batch 170 train Loss 51.6229 test Loss 23.8870 with MSE metric 19002.2155\n",
      "Epoch 215 batch 180 train Loss 51.6143 test Loss 23.8835 with MSE metric 19000.0808\n",
      "Epoch 215 batch 190 train Loss 51.6057 test Loss 23.8801 with MSE metric 18997.9708\n",
      "Epoch 215 batch 200 train Loss 51.5971 test Loss 23.8767 with MSE metric 18995.8027\n",
      "Epoch 215 batch 210 train Loss 51.5886 test Loss 23.8733 with MSE metric 18993.6443\n",
      "Epoch 215 batch 220 train Loss 51.5800 test Loss 23.8698 with MSE metric 18991.5088\n",
      "Epoch 215 batch 230 train Loss 51.5714 test Loss 23.8664 with MSE metric 18989.3620\n",
      "Epoch 215 batch 240 train Loss 51.5629 test Loss 23.8630 with MSE metric 18987.2772\n",
      "Time taken for 1 epoch: 24.336596965789795 secs\n",
      "\n",
      "Epoch 216 batch 0 train Loss 51.5543 test Loss 23.8596 with MSE metric 18985.1398\n",
      "Epoch 216 batch 10 train Loss 51.5457 test Loss 23.8562 with MSE metric 18983.0621\n",
      "Epoch 216 batch 20 train Loss 51.5372 test Loss 23.8527 with MSE metric 18980.8984\n",
      "Epoch 216 batch 30 train Loss 51.5286 test Loss 23.8493 with MSE metric 18978.7355\n",
      "Epoch 216 batch 40 train Loss 51.5201 test Loss 23.8459 with MSE metric 18976.6098\n",
      "Epoch 216 batch 50 train Loss 51.5115 test Loss 23.8425 with MSE metric 18974.5123\n",
      "Epoch 216 batch 60 train Loss 51.5030 test Loss 23.8391 with MSE metric 18972.4309\n",
      "Epoch 216 batch 70 train Loss 51.4944 test Loss 23.8357 with MSE metric 18970.3282\n",
      "Epoch 216 batch 80 train Loss 51.4859 test Loss 23.8323 with MSE metric 18968.2927\n",
      "Epoch 216 batch 90 train Loss 51.4774 test Loss 23.8289 with MSE metric 18966.2097\n",
      "Epoch 216 batch 100 train Loss 51.4688 test Loss 23.8255 with MSE metric 18964.0568\n",
      "Epoch 216 batch 110 train Loss 51.4603 test Loss 23.8220 with MSE metric 18961.9664\n",
      "Epoch 216 batch 120 train Loss 51.4518 test Loss 23.8186 with MSE metric 18959.8923\n",
      "Epoch 216 batch 130 train Loss 51.4432 test Loss 23.8152 with MSE metric 18957.8186\n",
      "Epoch 216 batch 140 train Loss 51.4347 test Loss 23.8118 with MSE metric 18955.7646\n",
      "Epoch 216 batch 150 train Loss 51.4262 test Loss 23.8084 with MSE metric 18953.6158\n",
      "Epoch 216 batch 160 train Loss 51.4177 test Loss 23.8050 with MSE metric 18951.5334\n",
      "Epoch 216 batch 170 train Loss 51.4092 test Loss 23.8016 with MSE metric 18949.4554\n",
      "Epoch 216 batch 180 train Loss 51.4007 test Loss 23.7982 with MSE metric 18947.3533\n",
      "Epoch 216 batch 190 train Loss 51.3922 test Loss 23.7948 with MSE metric 18945.2821\n",
      "Epoch 216 batch 200 train Loss 51.3837 test Loss 23.7914 with MSE metric 18943.2057\n",
      "Epoch 216 batch 210 train Loss 51.3752 test Loss 23.7880 with MSE metric 18941.1360\n",
      "Epoch 216 batch 220 train Loss 51.3667 test Loss 23.7847 with MSE metric 18938.9998\n",
      "Epoch 216 batch 230 train Loss 51.3582 test Loss 23.7813 with MSE metric 18936.9005\n",
      "Epoch 216 batch 240 train Loss 51.3497 test Loss 23.7779 with MSE metric 18934.8130\n",
      "Time taken for 1 epoch: 24.941506147384644 secs\n",
      "\n",
      "Epoch 217 batch 0 train Loss 51.3412 test Loss 23.7745 with MSE metric 18932.7600\n",
      "Epoch 217 batch 10 train Loss 51.3327 test Loss 23.7711 with MSE metric 18930.7436\n",
      "Epoch 217 batch 20 train Loss 51.3242 test Loss 23.7677 with MSE metric 18928.6948\n",
      "Epoch 217 batch 30 train Loss 51.3158 test Loss 23.7643 with MSE metric 18926.6561\n",
      "Epoch 217 batch 40 train Loss 51.3073 test Loss 23.7609 with MSE metric 18924.5654\n",
      "Epoch 217 batch 50 train Loss 51.2988 test Loss 23.7576 with MSE metric 18922.4926\n",
      "Epoch 217 batch 60 train Loss 51.2904 test Loss 23.7542 with MSE metric 18920.4099\n",
      "Epoch 217 batch 70 train Loss 51.2819 test Loss 23.7508 with MSE metric 18918.3918\n",
      "Epoch 217 batch 80 train Loss 51.2735 test Loss 23.7474 with MSE metric 18916.3357\n",
      "Epoch 217 batch 90 train Loss 51.2650 test Loss 23.7440 with MSE metric 18914.3205\n",
      "Epoch 217 batch 100 train Loss 51.2565 test Loss 23.7406 with MSE metric 18912.2388\n",
      "Epoch 217 batch 110 train Loss 51.2481 test Loss 23.7372 with MSE metric 18910.0549\n",
      "Epoch 217 batch 120 train Loss 51.2396 test Loss 23.7338 with MSE metric 18908.0204\n",
      "Epoch 217 batch 130 train Loss 51.2312 test Loss 23.7305 with MSE metric 18905.9776\n",
      "Epoch 217 batch 140 train Loss 51.2228 test Loss 23.7271 with MSE metric 18903.9097\n",
      "Epoch 217 batch 150 train Loss 51.2143 test Loss 23.7237 with MSE metric 18901.8769\n",
      "Epoch 217 batch 160 train Loss 51.2059 test Loss 23.7203 with MSE metric 18899.7832\n",
      "Epoch 217 batch 170 train Loss 51.1974 test Loss 23.7170 with MSE metric 18897.6905\n",
      "Epoch 217 batch 180 train Loss 51.1890 test Loss 23.7136 with MSE metric 18895.5770\n",
      "Epoch 217 batch 190 train Loss 51.1806 test Loss 23.7102 with MSE metric 18893.5802\n",
      "Epoch 217 batch 200 train Loss 51.1722 test Loss 23.7069 with MSE metric 18891.5131\n",
      "Epoch 217 batch 210 train Loss 51.1637 test Loss 23.7035 with MSE metric 18889.4319\n",
      "Epoch 217 batch 220 train Loss 51.1553 test Loss 23.7001 with MSE metric 18887.3747\n",
      "Epoch 217 batch 230 train Loss 51.1469 test Loss 23.6968 with MSE metric 18885.3672\n",
      "Epoch 217 batch 240 train Loss 51.1385 test Loss 23.6934 with MSE metric 18883.2937\n",
      "Time taken for 1 epoch: 26.831255674362183 secs\n",
      "\n",
      "Epoch 218 batch 0 train Loss 51.1301 test Loss 23.6901 with MSE metric 18881.1787\n",
      "Epoch 218 batch 10 train Loss 51.1217 test Loss 23.6867 with MSE metric 18879.0408\n",
      "Epoch 218 batch 20 train Loss 51.1133 test Loss 23.6833 with MSE metric 18876.9435\n",
      "Epoch 218 batch 30 train Loss 51.1049 test Loss 23.6800 with MSE metric 18874.9138\n",
      "Epoch 218 batch 40 train Loss 51.0965 test Loss 23.6766 with MSE metric 18872.8806\n",
      "Epoch 218 batch 50 train Loss 51.0881 test Loss 23.6733 with MSE metric 18870.8325\n",
      "Epoch 218 batch 60 train Loss 51.0797 test Loss 23.6699 with MSE metric 18868.7592\n",
      "Epoch 218 batch 70 train Loss 51.0713 test Loss 23.6666 with MSE metric 18866.6476\n",
      "Epoch 218 batch 80 train Loss 51.0630 test Loss 23.6632 with MSE metric 18864.6601\n",
      "Epoch 218 batch 90 train Loss 51.0546 test Loss 23.6599 with MSE metric 18862.5822\n",
      "Epoch 218 batch 100 train Loss 51.0462 test Loss 23.6566 with MSE metric 18860.4807\n",
      "Epoch 218 batch 110 train Loss 51.0378 test Loss 23.6532 with MSE metric 18858.3871\n",
      "Epoch 218 batch 120 train Loss 51.0294 test Loss 23.6499 with MSE metric 18856.3366\n",
      "Epoch 218 batch 130 train Loss 51.0211 test Loss 23.6465 with MSE metric 18854.2133\n",
      "Epoch 218 batch 140 train Loss 51.0127 test Loss 23.6431 with MSE metric 18852.1590\n",
      "Epoch 218 batch 150 train Loss 51.0043 test Loss 23.6398 with MSE metric 18850.0211\n",
      "Epoch 218 batch 160 train Loss 50.9960 test Loss 23.6365 with MSE metric 18847.9512\n",
      "Epoch 218 batch 170 train Loss 50.9876 test Loss 23.6331 with MSE metric 18845.9224\n",
      "Epoch 218 batch 180 train Loss 50.9793 test Loss 23.6298 with MSE metric 18843.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218 batch 190 train Loss 50.9709 test Loss 23.6265 with MSE metric 18841.7145\n",
      "Epoch 218 batch 200 train Loss 50.9626 test Loss 23.6231 with MSE metric 18839.6726\n",
      "Epoch 218 batch 210 train Loss 50.9542 test Loss 23.6198 with MSE metric 18837.6689\n",
      "Epoch 218 batch 220 train Loss 50.9459 test Loss 23.6165 with MSE metric 18835.6531\n",
      "Epoch 218 batch 230 train Loss 50.9376 test Loss 23.6131 with MSE metric 18833.6200\n",
      "Epoch 218 batch 240 train Loss 50.9292 test Loss 23.6098 with MSE metric 18831.5739\n",
      "Time taken for 1 epoch: 24.92932677268982 secs\n",
      "\n",
      "Epoch 219 batch 0 train Loss 50.9209 test Loss 23.6065 with MSE metric 18829.5741\n",
      "Epoch 219 batch 10 train Loss 50.9126 test Loss 23.6031 with MSE metric 18827.4662\n",
      "Epoch 219 batch 20 train Loss 50.9042 test Loss 23.5998 with MSE metric 18825.3941\n",
      "Epoch 219 batch 30 train Loss 50.8959 test Loss 23.5965 with MSE metric 18823.3353\n",
      "Epoch 219 batch 40 train Loss 50.8876 test Loss 23.5932 with MSE metric 18821.2770\n",
      "Epoch 219 batch 50 train Loss 50.8793 test Loss 23.5898 with MSE metric 18819.2195\n",
      "Epoch 219 batch 60 train Loss 50.8710 test Loss 23.5865 with MSE metric 18817.2118\n",
      "Epoch 219 batch 70 train Loss 50.8627 test Loss 23.5832 with MSE metric 18815.1725\n",
      "Epoch 219 batch 80 train Loss 50.8543 test Loss 23.5799 with MSE metric 18813.0959\n",
      "Epoch 219 batch 90 train Loss 50.8460 test Loss 23.5765 with MSE metric 18811.0087\n",
      "Epoch 219 batch 100 train Loss 50.8377 test Loss 23.5732 with MSE metric 18808.9047\n",
      "Epoch 219 batch 110 train Loss 50.8294 test Loss 23.5699 with MSE metric 18806.8897\n",
      "Epoch 219 batch 120 train Loss 50.8211 test Loss 23.5666 with MSE metric 18804.8802\n",
      "Epoch 219 batch 130 train Loss 50.8128 test Loss 23.5633 with MSE metric 18802.8654\n",
      "Epoch 219 batch 140 train Loss 50.8046 test Loss 23.5599 with MSE metric 18800.7989\n",
      "Epoch 219 batch 150 train Loss 50.7963 test Loss 23.5566 with MSE metric 18798.8463\n",
      "Epoch 219 batch 160 train Loss 50.7880 test Loss 23.5533 with MSE metric 18796.7512\n",
      "Epoch 219 batch 170 train Loss 50.7797 test Loss 23.5500 with MSE metric 18794.7080\n",
      "Epoch 219 batch 180 train Loss 50.7714 test Loss 23.5467 with MSE metric 18792.6935\n",
      "Epoch 219 batch 190 train Loss 50.7632 test Loss 23.5434 with MSE metric 18790.5921\n",
      "Epoch 219 batch 200 train Loss 50.7549 test Loss 23.5401 with MSE metric 18788.5771\n",
      "Epoch 219 batch 210 train Loss 50.7466 test Loss 23.5368 with MSE metric 18786.5078\n",
      "Epoch 219 batch 220 train Loss 50.7384 test Loss 23.5335 with MSE metric 18784.4691\n",
      "Epoch 219 batch 230 train Loss 50.7301 test Loss 23.5303 with MSE metric 18782.4255\n",
      "Epoch 219 batch 240 train Loss 50.7218 test Loss 23.5270 with MSE metric 18780.4171\n",
      "Time taken for 1 epoch: 24.427538871765137 secs\n",
      "\n",
      "Epoch 220 batch 0 train Loss 50.7136 test Loss 23.5237 with MSE metric 18778.4518\n",
      "Epoch 220 batch 10 train Loss 50.7053 test Loss 23.5204 with MSE metric 18776.4356\n",
      "Epoch 220 batch 20 train Loss 50.6971 test Loss 23.5171 with MSE metric 18774.4007\n",
      "Epoch 220 batch 30 train Loss 50.6889 test Loss 23.5138 with MSE metric 18772.3409\n",
      "Epoch 220 batch 40 train Loss 50.6806 test Loss 23.5105 with MSE metric 18770.3392\n",
      "Epoch 220 batch 50 train Loss 50.6724 test Loss 23.5072 with MSE metric 18768.3217\n",
      "Epoch 220 batch 60 train Loss 50.6641 test Loss 23.5039 with MSE metric 18766.3007\n",
      "Epoch 220 batch 70 train Loss 50.6559 test Loss 23.5006 with MSE metric 18764.2859\n",
      "Epoch 220 batch 80 train Loss 50.6477 test Loss 23.4973 with MSE metric 18762.2471\n",
      "Epoch 220 batch 90 train Loss 50.6394 test Loss 23.4940 with MSE metric 18760.2728\n",
      "Epoch 220 batch 100 train Loss 50.6312 test Loss 23.4908 with MSE metric 18758.2446\n",
      "Epoch 220 batch 110 train Loss 50.6230 test Loss 23.4875 with MSE metric 18756.2623\n",
      "Epoch 220 batch 120 train Loss 50.6148 test Loss 23.4842 with MSE metric 18754.2894\n",
      "Epoch 220 batch 130 train Loss 50.6066 test Loss 23.4809 with MSE metric 18752.2104\n",
      "Epoch 220 batch 140 train Loss 50.5983 test Loss 23.4776 with MSE metric 18750.1597\n",
      "Epoch 220 batch 150 train Loss 50.5901 test Loss 23.4743 with MSE metric 18748.1506\n",
      "Epoch 220 batch 160 train Loss 50.5819 test Loss 23.4710 with MSE metric 18746.1209\n",
      "Epoch 220 batch 170 train Loss 50.5737 test Loss 23.4678 with MSE metric 18744.0705\n",
      "Epoch 220 batch 180 train Loss 50.5655 test Loss 23.4645 with MSE metric 18742.0223\n",
      "Epoch 220 batch 190 train Loss 50.5573 test Loss 23.4612 with MSE metric 18740.0380\n",
      "Epoch 220 batch 200 train Loss 50.5491 test Loss 23.4579 with MSE metric 18737.9855\n",
      "Epoch 220 batch 210 train Loss 50.5409 test Loss 23.4547 with MSE metric 18735.9815\n",
      "Epoch 220 batch 220 train Loss 50.5327 test Loss 23.4514 with MSE metric 18733.9326\n",
      "Epoch 220 batch 230 train Loss 50.5245 test Loss 23.4481 with MSE metric 18731.8993\n",
      "Epoch 220 batch 240 train Loss 50.5163 test Loss 23.4449 with MSE metric 18729.8580\n",
      "Time taken for 1 epoch: 24.271629810333252 secs\n",
      "\n",
      "Epoch 221 batch 0 train Loss 50.5082 test Loss 23.4416 with MSE metric 18727.8686\n",
      "Epoch 221 batch 10 train Loss 50.5000 test Loss 23.4383 with MSE metric 18725.8046\n",
      "Epoch 221 batch 20 train Loss 50.4918 test Loss 23.4350 with MSE metric 18723.8283\n",
      "Epoch 221 batch 30 train Loss 50.4836 test Loss 23.4318 with MSE metric 18721.8719\n",
      "Epoch 221 batch 40 train Loss 50.4755 test Loss 23.4285 with MSE metric 18719.8452\n",
      "Epoch 221 batch 50 train Loss 50.4673 test Loss 23.4252 with MSE metric 18717.8840\n",
      "Epoch 221 batch 60 train Loss 50.4592 test Loss 23.4220 with MSE metric 18715.8058\n",
      "Epoch 221 batch 70 train Loss 50.4510 test Loss 23.4187 with MSE metric 18713.8798\n",
      "Epoch 221 batch 80 train Loss 50.4428 test Loss 23.4155 with MSE metric 18711.8855\n",
      "Epoch 221 batch 90 train Loss 50.4347 test Loss 23.4122 with MSE metric 18709.8787\n",
      "Epoch 221 batch 100 train Loss 50.4265 test Loss 23.4090 with MSE metric 18707.8578\n",
      "Epoch 221 batch 110 train Loss 50.4184 test Loss 23.4057 with MSE metric 18705.8911\n",
      "Epoch 221 batch 120 train Loss 50.4102 test Loss 23.4025 with MSE metric 18703.8591\n",
      "Epoch 221 batch 130 train Loss 50.4021 test Loss 23.3992 with MSE metric 18701.8553\n",
      "Epoch 221 batch 140 train Loss 50.3940 test Loss 23.3959 with MSE metric 18699.8417\n",
      "Epoch 221 batch 150 train Loss 50.3858 test Loss 23.3927 with MSE metric 18697.8525\n",
      "Epoch 221 batch 160 train Loss 50.3777 test Loss 23.3894 with MSE metric 18695.8250\n",
      "Epoch 221 batch 170 train Loss 50.3695 test Loss 23.3862 with MSE metric 18693.7656\n",
      "Epoch 221 batch 180 train Loss 50.3614 test Loss 23.3830 with MSE metric 18691.7620\n",
      "Epoch 221 batch 190 train Loss 50.3533 test Loss 23.3797 with MSE metric 18689.8013\n",
      "Epoch 221 batch 200 train Loss 50.3452 test Loss 23.3765 with MSE metric 18687.7426\n",
      "Epoch 221 batch 210 train Loss 50.3371 test Loss 23.3732 with MSE metric 18685.7214\n",
      "Epoch 221 batch 220 train Loss 50.3289 test Loss 23.3700 with MSE metric 18683.7548\n",
      "Epoch 221 batch 230 train Loss 50.3208 test Loss 23.3667 with MSE metric 18681.7444\n",
      "Epoch 221 batch 240 train Loss 50.3127 test Loss 23.3635 with MSE metric 18679.6955\n",
      "Time taken for 1 epoch: 24.446473121643066 secs\n",
      "\n",
      "Epoch 222 batch 0 train Loss 50.3046 test Loss 23.3603 with MSE metric 18677.7221\n",
      "Epoch 222 batch 10 train Loss 50.2965 test Loss 23.3570 with MSE metric 18675.7141\n",
      "Epoch 222 batch 20 train Loss 50.2884 test Loss 23.3538 with MSE metric 18673.7367\n",
      "Epoch 222 batch 30 train Loss 50.2803 test Loss 23.3506 with MSE metric 18671.7600\n",
      "Epoch 222 batch 40 train Loss 50.2722 test Loss 23.3473 with MSE metric 18669.7986\n",
      "Epoch 222 batch 50 train Loss 50.2641 test Loss 23.3441 with MSE metric 18667.8031\n",
      "Epoch 222 batch 60 train Loss 50.2560 test Loss 23.3408 with MSE metric 18665.7920\n",
      "Epoch 222 batch 70 train Loss 50.2479 test Loss 23.3376 with MSE metric 18663.8093\n",
      "Epoch 222 batch 80 train Loss 50.2398 test Loss 23.3344 with MSE metric 18661.7626\n",
      "Epoch 222 batch 90 train Loss 50.2318 test Loss 23.3312 with MSE metric 18659.7519\n",
      "Epoch 222 batch 100 train Loss 50.2237 test Loss 23.3280 with MSE metric 18657.7396\n",
      "Epoch 222 batch 110 train Loss 50.2156 test Loss 23.3248 with MSE metric 18655.6810\n",
      "Epoch 222 batch 120 train Loss 50.2075 test Loss 23.3215 with MSE metric 18653.7059\n",
      "Epoch 222 batch 130 train Loss 50.1995 test Loss 23.3183 with MSE metric 18651.7934\n",
      "Epoch 222 batch 140 train Loss 50.1914 test Loss 23.3151 with MSE metric 18649.8305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222 batch 150 train Loss 50.1833 test Loss 23.3119 with MSE metric 18647.8743\n",
      "Epoch 222 batch 160 train Loss 50.1753 test Loss 23.3087 with MSE metric 18645.8876\n",
      "Epoch 222 batch 170 train Loss 50.1672 test Loss 23.3054 with MSE metric 18643.8782\n",
      "Epoch 222 batch 180 train Loss 50.1592 test Loss 23.3022 with MSE metric 18641.8681\n",
      "Epoch 222 batch 190 train Loss 50.1511 test Loss 23.2990 with MSE metric 18639.9615\n",
      "Epoch 222 batch 200 train Loss 50.1431 test Loss 23.2958 with MSE metric 18637.9689\n",
      "Epoch 222 batch 210 train Loss 50.1350 test Loss 23.2926 with MSE metric 18635.9690\n",
      "Epoch 222 batch 220 train Loss 50.1270 test Loss 23.2893 with MSE metric 18633.9877\n",
      "Epoch 222 batch 230 train Loss 50.1189 test Loss 23.2861 with MSE metric 18632.0284\n",
      "Epoch 222 batch 240 train Loss 50.1109 test Loss 23.2829 with MSE metric 18630.0406\n",
      "Time taken for 1 epoch: 24.86866283416748 secs\n",
      "\n",
      "Epoch 223 batch 0 train Loss 50.1028 test Loss 23.2797 with MSE metric 18628.0436\n",
      "Epoch 223 batch 10 train Loss 50.0948 test Loss 23.2765 with MSE metric 18626.0339\n",
      "Epoch 223 batch 20 train Loss 50.0868 test Loss 23.2733 with MSE metric 18624.0167\n",
      "Epoch 223 batch 30 train Loss 50.0788 test Loss 23.2701 with MSE metric 18622.0786\n",
      "Epoch 223 batch 40 train Loss 50.0707 test Loss 23.2669 with MSE metric 18620.1506\n",
      "Epoch 223 batch 50 train Loss 50.0627 test Loss 23.2637 with MSE metric 18618.2416\n",
      "Epoch 223 batch 60 train Loss 50.0547 test Loss 23.2604 with MSE metric 18616.2756\n",
      "Epoch 223 batch 70 train Loss 50.0467 test Loss 23.2572 with MSE metric 18614.3274\n",
      "Epoch 223 batch 80 train Loss 50.0387 test Loss 23.2540 with MSE metric 18612.3766\n",
      "Epoch 223 batch 90 train Loss 50.0307 test Loss 23.2508 with MSE metric 18610.4302\n",
      "Epoch 223 batch 100 train Loss 50.0227 test Loss 23.2476 with MSE metric 18608.4354\n",
      "Epoch 223 batch 110 train Loss 50.0147 test Loss 23.2444 with MSE metric 18606.4920\n",
      "Epoch 223 batch 120 train Loss 50.0067 test Loss 23.2412 with MSE metric 18604.6069\n",
      "Epoch 223 batch 130 train Loss 49.9987 test Loss 23.2380 with MSE metric 18602.6777\n",
      "Epoch 223 batch 140 train Loss 49.9907 test Loss 23.2348 with MSE metric 18600.7396\n",
      "Epoch 223 batch 150 train Loss 49.9827 test Loss 23.2316 with MSE metric 18598.7593\n",
      "Epoch 223 batch 160 train Loss 49.9747 test Loss 23.2284 with MSE metric 18596.7805\n",
      "Epoch 223 batch 170 train Loss 49.9667 test Loss 23.2253 with MSE metric 18594.7934\n",
      "Epoch 223 batch 180 train Loss 49.9587 test Loss 23.2221 with MSE metric 18592.7955\n",
      "Epoch 223 batch 190 train Loss 49.9507 test Loss 23.2189 with MSE metric 18590.8283\n",
      "Epoch 223 batch 200 train Loss 49.9428 test Loss 23.2157 with MSE metric 18588.8677\n",
      "Epoch 223 batch 210 train Loss 49.9348 test Loss 23.2125 with MSE metric 18586.9619\n",
      "Epoch 223 batch 220 train Loss 49.9268 test Loss 23.2093 with MSE metric 18585.0609\n",
      "Epoch 223 batch 230 train Loss 49.9189 test Loss 23.2061 with MSE metric 18583.1087\n",
      "Epoch 223 batch 240 train Loss 49.9109 test Loss 23.2029 with MSE metric 18581.1761\n",
      "Time taken for 1 epoch: 24.868847846984863 secs\n",
      "\n",
      "Epoch 224 batch 0 train Loss 49.9029 test Loss 23.1997 with MSE metric 18579.2661\n",
      "Epoch 224 batch 10 train Loss 49.8950 test Loss 23.1966 with MSE metric 18577.3238\n",
      "Epoch 224 batch 20 train Loss 49.8870 test Loss 23.1934 with MSE metric 18575.4301\n",
      "Epoch 224 batch 30 train Loss 49.8791 test Loss 23.1902 with MSE metric 18573.3702\n",
      "Epoch 224 batch 40 train Loss 49.8711 test Loss 23.1870 with MSE metric 18571.3659\n",
      "Epoch 224 batch 50 train Loss 49.8632 test Loss 23.1838 with MSE metric 18569.4104\n",
      "Epoch 224 batch 60 train Loss 49.8552 test Loss 23.1806 with MSE metric 18567.4793\n",
      "Epoch 224 batch 70 train Loss 49.8473 test Loss 23.1775 with MSE metric 18565.5483\n",
      "Epoch 224 batch 80 train Loss 49.8393 test Loss 23.1743 with MSE metric 18563.5792\n",
      "Epoch 224 batch 90 train Loss 49.8314 test Loss 23.1711 with MSE metric 18561.6330\n",
      "Epoch 224 batch 100 train Loss 49.8235 test Loss 23.1680 with MSE metric 18559.7039\n",
      "Epoch 224 batch 110 train Loss 49.8155 test Loss 23.1648 with MSE metric 18557.7302\n",
      "Epoch 224 batch 120 train Loss 49.8076 test Loss 23.1616 with MSE metric 18555.7190\n",
      "Epoch 224 batch 130 train Loss 49.7997 test Loss 23.1584 with MSE metric 18553.7615\n",
      "Epoch 224 batch 140 train Loss 49.7917 test Loss 23.1553 with MSE metric 18551.8347\n",
      "Epoch 224 batch 150 train Loss 49.7838 test Loss 23.1521 with MSE metric 18549.9076\n",
      "Epoch 224 batch 160 train Loss 49.7759 test Loss 23.1489 with MSE metric 18547.9429\n",
      "Epoch 224 batch 170 train Loss 49.7680 test Loss 23.1458 with MSE metric 18545.9394\n",
      "Epoch 224 batch 180 train Loss 49.7601 test Loss 23.1426 with MSE metric 18543.9955\n",
      "Epoch 224 batch 190 train Loss 49.7521 test Loss 23.1395 with MSE metric 18541.9575\n",
      "Epoch 224 batch 200 train Loss 49.7442 test Loss 23.1363 with MSE metric 18539.9987\n",
      "Epoch 224 batch 210 train Loss 49.7363 test Loss 23.1331 with MSE metric 18538.0557\n",
      "Epoch 224 batch 220 train Loss 49.7284 test Loss 23.1300 with MSE metric 18536.0648\n",
      "Epoch 224 batch 230 train Loss 49.7205 test Loss 23.1268 with MSE metric 18534.1466\n",
      "Epoch 224 batch 240 train Loss 49.7126 test Loss 23.1237 with MSE metric 18532.1650\n",
      "Time taken for 1 epoch: 24.86346697807312 secs\n",
      "\n",
      "Epoch 225 batch 0 train Loss 49.7047 test Loss 23.1205 with MSE metric 18530.2019\n",
      "Epoch 225 batch 10 train Loss 49.6968 test Loss 23.1174 with MSE metric 18528.2969\n",
      "Epoch 225 batch 20 train Loss 49.6890 test Loss 23.1142 with MSE metric 18526.3775\n",
      "Epoch 225 batch 30 train Loss 49.6811 test Loss 23.1111 with MSE metric 18524.4343\n",
      "Epoch 225 batch 40 train Loss 49.6732 test Loss 23.1080 with MSE metric 18522.5606\n",
      "Epoch 225 batch 50 train Loss 49.6653 test Loss 23.1048 with MSE metric 18520.6270\n",
      "Epoch 225 batch 60 train Loss 49.6574 test Loss 23.1017 with MSE metric 18518.7198\n",
      "Epoch 225 batch 70 train Loss 49.6496 test Loss 23.0985 with MSE metric 18516.7613\n",
      "Epoch 225 batch 80 train Loss 49.6417 test Loss 23.0954 with MSE metric 18514.8520\n",
      "Epoch 225 batch 90 train Loss 49.6338 test Loss 23.0922 with MSE metric 18512.9908\n",
      "Epoch 225 batch 100 train Loss 49.6260 test Loss 23.0891 with MSE metric 18511.0654\n",
      "Epoch 225 batch 110 train Loss 49.6181 test Loss 23.0859 with MSE metric 18509.1549\n",
      "Epoch 225 batch 120 train Loss 49.6103 test Loss 23.0828 with MSE metric 18507.2484\n",
      "Epoch 225 batch 130 train Loss 49.6024 test Loss 23.0796 with MSE metric 18505.3477\n",
      "Epoch 225 batch 140 train Loss 49.5945 test Loss 23.0765 with MSE metric 18503.3975\n",
      "Epoch 225 batch 150 train Loss 49.5867 test Loss 23.0734 with MSE metric 18501.4631\n",
      "Epoch 225 batch 160 train Loss 49.5788 test Loss 23.0703 with MSE metric 18499.5635\n",
      "Epoch 225 batch 170 train Loss 49.5710 test Loss 23.0671 with MSE metric 18497.6551\n",
      "Epoch 225 batch 180 train Loss 49.5632 test Loss 23.0640 with MSE metric 18495.7181\n",
      "Epoch 225 batch 190 train Loss 49.5553 test Loss 23.0608 with MSE metric 18493.7888\n",
      "Epoch 225 batch 200 train Loss 49.5475 test Loss 23.0577 with MSE metric 18491.8025\n",
      "Epoch 225 batch 210 train Loss 49.5396 test Loss 23.0546 with MSE metric 18489.8479\n",
      "Epoch 225 batch 220 train Loss 49.5318 test Loss 23.0514 with MSE metric 18487.9524\n",
      "Epoch 225 batch 230 train Loss 49.5240 test Loss 23.0483 with MSE metric 18485.9569\n",
      "Epoch 225 batch 240 train Loss 49.5162 test Loss 23.0452 with MSE metric 18484.0316\n",
      "Time taken for 1 epoch: 24.348795890808105 secs\n",
      "\n",
      "Epoch 226 batch 0 train Loss 49.5083 test Loss 23.0421 with MSE metric 18482.0409\n",
      "Epoch 226 batch 10 train Loss 49.5005 test Loss 23.0389 with MSE metric 18480.0939\n",
      "Epoch 226 batch 20 train Loss 49.4927 test Loss 23.0358 with MSE metric 18478.1638\n",
      "Epoch 226 batch 30 train Loss 49.4849 test Loss 23.0327 with MSE metric 18476.2523\n",
      "Epoch 226 batch 40 train Loss 49.4771 test Loss 23.0296 with MSE metric 18474.3338\n",
      "Epoch 226 batch 50 train Loss 49.4693 test Loss 23.0265 with MSE metric 18472.4075\n",
      "Epoch 226 batch 60 train Loss 49.4614 test Loss 23.0234 with MSE metric 18470.4520\n",
      "Epoch 226 batch 70 train Loss 49.4536 test Loss 23.0203 with MSE metric 18468.5034\n",
      "Epoch 226 batch 80 train Loss 49.4458 test Loss 23.0171 with MSE metric 18466.5078\n",
      "Epoch 226 batch 90 train Loss 49.4380 test Loss 23.0140 with MSE metric 18464.6112\n",
      "Epoch 226 batch 100 train Loss 49.4302 test Loss 23.0109 with MSE metric 18462.6612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226 batch 110 train Loss 49.4224 test Loss 23.0078 with MSE metric 18460.7700\n",
      "Epoch 226 batch 120 train Loss 49.4147 test Loss 23.0047 with MSE metric 18458.8159\n",
      "Epoch 226 batch 130 train Loss 49.4069 test Loss 23.0016 with MSE metric 18456.9075\n",
      "Epoch 226 batch 140 train Loss 49.3991 test Loss 22.9985 with MSE metric 18455.0264\n",
      "Epoch 226 batch 150 train Loss 49.3913 test Loss 22.9954 with MSE metric 18453.1644\n",
      "Epoch 226 batch 160 train Loss 49.3835 test Loss 22.9923 with MSE metric 18451.2706\n",
      "Epoch 226 batch 170 train Loss 49.3758 test Loss 22.9891 with MSE metric 18449.3714\n",
      "Epoch 226 batch 180 train Loss 49.3680 test Loss 22.9860 with MSE metric 18447.4469\n",
      "Epoch 226 batch 190 train Loss 49.3602 test Loss 22.9829 with MSE metric 18445.5378\n",
      "Epoch 226 batch 200 train Loss 49.3524 test Loss 22.9798 with MSE metric 18443.6312\n",
      "Epoch 226 batch 210 train Loss 49.3447 test Loss 22.9767 with MSE metric 18441.6976\n",
      "Epoch 226 batch 220 train Loss 49.3369 test Loss 22.9736 with MSE metric 18439.7475\n",
      "Epoch 226 batch 230 train Loss 49.3291 test Loss 22.9705 with MSE metric 18437.8302\n",
      "Epoch 226 batch 240 train Loss 49.3214 test Loss 22.9674 with MSE metric 18435.8689\n",
      "Time taken for 1 epoch: 24.54081678390503 secs\n",
      "\n",
      "Epoch 227 batch 0 train Loss 49.3136 test Loss 22.9643 with MSE metric 18433.9501\n",
      "Epoch 227 batch 10 train Loss 49.3059 test Loss 22.9612 with MSE metric 18432.1016\n",
      "Epoch 227 batch 20 train Loss 49.2981 test Loss 22.9581 with MSE metric 18430.2173\n",
      "Epoch 227 batch 30 train Loss 49.2904 test Loss 22.9550 with MSE metric 18428.3444\n",
      "Epoch 227 batch 40 train Loss 49.2826 test Loss 22.9519 with MSE metric 18426.4146\n",
      "Epoch 227 batch 50 train Loss 49.2749 test Loss 22.9488 with MSE metric 18424.5395\n",
      "Epoch 227 batch 60 train Loss 49.2672 test Loss 22.9457 with MSE metric 18422.6221\n",
      "Epoch 227 batch 70 train Loss 49.2594 test Loss 22.9426 with MSE metric 18420.6627\n",
      "Epoch 227 batch 80 train Loss 49.2517 test Loss 22.9395 with MSE metric 18418.7878\n",
      "Epoch 227 batch 90 train Loss 49.2440 test Loss 22.9364 with MSE metric 18416.8550\n",
      "Epoch 227 batch 100 train Loss 49.2362 test Loss 22.9333 with MSE metric 18414.9806\n",
      "Epoch 227 batch 110 train Loss 49.2285 test Loss 22.9303 with MSE metric 18413.0944\n",
      "Epoch 227 batch 120 train Loss 49.2208 test Loss 22.9272 with MSE metric 18411.2233\n",
      "Epoch 227 batch 130 train Loss 49.2131 test Loss 22.9241 with MSE metric 18409.3525\n",
      "Epoch 227 batch 140 train Loss 49.2054 test Loss 22.9210 with MSE metric 18407.4078\n",
      "Epoch 227 batch 150 train Loss 49.1977 test Loss 22.9179 with MSE metric 18405.4978\n",
      "Epoch 227 batch 160 train Loss 49.1899 test Loss 22.9148 with MSE metric 18403.6113\n",
      "Epoch 227 batch 170 train Loss 49.1822 test Loss 22.9118 with MSE metric 18401.6843\n",
      "Epoch 227 batch 180 train Loss 49.1745 test Loss 22.9087 with MSE metric 18399.7768\n",
      "Epoch 227 batch 190 train Loss 49.1668 test Loss 22.9056 with MSE metric 18397.8708\n",
      "Epoch 227 batch 200 train Loss 49.1591 test Loss 22.9025 with MSE metric 18395.9815\n",
      "Epoch 227 batch 210 train Loss 49.1514 test Loss 22.8995 with MSE metric 18394.0810\n",
      "Epoch 227 batch 220 train Loss 49.1437 test Loss 22.8964 with MSE metric 18392.1768\n",
      "Epoch 227 batch 230 train Loss 49.1360 test Loss 22.8933 with MSE metric 18390.3314\n",
      "Epoch 227 batch 240 train Loss 49.1284 test Loss 22.8902 with MSE metric 18388.4389\n",
      "Time taken for 1 epoch: 24.942822217941284 secs\n",
      "\n",
      "Epoch 228 batch 0 train Loss 49.1207 test Loss 22.8871 with MSE metric 18386.5245\n",
      "Epoch 228 batch 10 train Loss 49.1130 test Loss 22.8841 with MSE metric 18384.6766\n",
      "Epoch 228 batch 20 train Loss 49.1053 test Loss 22.8810 with MSE metric 18382.8168\n",
      "Epoch 228 batch 30 train Loss 49.0976 test Loss 22.8779 with MSE metric 18381.0004\n",
      "Epoch 228 batch 40 train Loss 49.0900 test Loss 22.8748 with MSE metric 18379.1360\n",
      "Epoch 228 batch 50 train Loss 49.0823 test Loss 22.8718 with MSE metric 18377.2606\n",
      "Epoch 228 batch 60 train Loss 49.0746 test Loss 22.8687 with MSE metric 18375.3777\n",
      "Epoch 228 batch 70 train Loss 49.0669 test Loss 22.8657 with MSE metric 18373.4784\n",
      "Epoch 228 batch 80 train Loss 49.0593 test Loss 22.8626 with MSE metric 18371.5731\n",
      "Epoch 228 batch 90 train Loss 49.0516 test Loss 22.8595 with MSE metric 18369.6992\n",
      "Epoch 228 batch 100 train Loss 49.0440 test Loss 22.8564 with MSE metric 18367.8582\n",
      "Epoch 228 batch 110 train Loss 49.0363 test Loss 22.8534 with MSE metric 18365.9724\n",
      "Epoch 228 batch 120 train Loss 49.0286 test Loss 22.8503 with MSE metric 18364.0542\n",
      "Epoch 228 batch 130 train Loss 49.0210 test Loss 22.8473 with MSE metric 18362.1344\n",
      "Epoch 228 batch 140 train Loss 49.0133 test Loss 22.8442 with MSE metric 18360.2064\n",
      "Epoch 228 batch 150 train Loss 49.0057 test Loss 22.8412 with MSE metric 18358.3111\n",
      "Epoch 228 batch 160 train Loss 48.9981 test Loss 22.8381 with MSE metric 18356.4672\n",
      "Epoch 228 batch 170 train Loss 48.9904 test Loss 22.8351 with MSE metric 18354.5887\n",
      "Epoch 228 batch 180 train Loss 48.9828 test Loss 22.8320 with MSE metric 18352.7034\n",
      "Epoch 228 batch 190 train Loss 48.9751 test Loss 22.8290 with MSE metric 18350.8349\n",
      "Epoch 228 batch 200 train Loss 48.9675 test Loss 22.8259 with MSE metric 18348.9449\n",
      "Epoch 228 batch 210 train Loss 48.9599 test Loss 22.8229 with MSE metric 18347.0531\n",
      "Epoch 228 batch 220 train Loss 48.9522 test Loss 22.8198 with MSE metric 18345.1429\n",
      "Epoch 228 batch 230 train Loss 48.9446 test Loss 22.8168 with MSE metric 18343.3150\n",
      "Epoch 228 batch 240 train Loss 48.9370 test Loss 22.8137 with MSE metric 18341.4584\n",
      "Time taken for 1 epoch: 23.965537071228027 secs\n",
      "\n",
      "Epoch 229 batch 0 train Loss 48.9294 test Loss 22.8107 with MSE metric 18339.5765\n",
      "Epoch 229 batch 10 train Loss 48.9218 test Loss 22.8077 with MSE metric 18337.7313\n",
      "Epoch 229 batch 20 train Loss 48.9142 test Loss 22.8046 with MSE metric 18335.8389\n",
      "Epoch 229 batch 30 train Loss 48.9065 test Loss 22.8015 with MSE metric 18333.9444\n",
      "Epoch 229 batch 40 train Loss 48.8989 test Loss 22.7985 with MSE metric 18332.0544\n",
      "Epoch 229 batch 50 train Loss 48.8913 test Loss 22.7955 with MSE metric 18330.1917\n",
      "Epoch 229 batch 60 train Loss 48.8837 test Loss 22.7925 with MSE metric 18328.3268\n",
      "Epoch 229 batch 70 train Loss 48.8761 test Loss 22.7894 with MSE metric 18326.4688\n",
      "Epoch 229 batch 80 train Loss 48.8685 test Loss 22.7864 with MSE metric 18324.6116\n",
      "Epoch 229 batch 90 train Loss 48.8609 test Loss 22.7833 with MSE metric 18322.7287\n",
      "Epoch 229 batch 100 train Loss 48.8533 test Loss 22.7803 with MSE metric 18320.8755\n",
      "Epoch 229 batch 110 train Loss 48.8457 test Loss 22.7773 with MSE metric 18318.9507\n",
      "Epoch 229 batch 120 train Loss 48.8381 test Loss 22.7742 with MSE metric 18317.0896\n",
      "Epoch 229 batch 130 train Loss 48.8306 test Loss 22.7712 with MSE metric 18315.2489\n",
      "Epoch 229 batch 140 train Loss 48.8230 test Loss 22.7682 with MSE metric 18313.4142\n",
      "Epoch 229 batch 150 train Loss 48.8154 test Loss 22.7651 with MSE metric 18311.5063\n",
      "Epoch 229 batch 160 train Loss 48.8078 test Loss 22.7621 with MSE metric 18309.6315\n",
      "Epoch 229 batch 170 train Loss 48.8003 test Loss 22.7591 with MSE metric 18307.7600\n",
      "Epoch 229 batch 180 train Loss 48.7927 test Loss 22.7561 with MSE metric 18305.8244\n",
      "Epoch 229 batch 190 train Loss 48.7851 test Loss 22.7530 with MSE metric 18303.9986\n",
      "Epoch 229 batch 200 train Loss 48.7775 test Loss 22.7500 with MSE metric 18302.1532\n",
      "Epoch 229 batch 210 train Loss 48.7700 test Loss 22.7470 with MSE metric 18300.2932\n",
      "Epoch 229 batch 220 train Loss 48.7624 test Loss 22.7440 with MSE metric 18298.3825\n",
      "Epoch 229 batch 230 train Loss 48.7549 test Loss 22.7409 with MSE metric 18296.5152\n",
      "Epoch 229 batch 240 train Loss 48.7473 test Loss 22.7380 with MSE metric 18294.6253\n",
      "Time taken for 1 epoch: 24.206366062164307 secs\n",
      "\n",
      "Epoch 230 batch 0 train Loss 48.7397 test Loss 22.7349 with MSE metric 18292.7504\n",
      "Epoch 230 batch 10 train Loss 48.7322 test Loss 22.7319 with MSE metric 18290.9067\n",
      "Epoch 230 batch 20 train Loss 48.7246 test Loss 22.7289 with MSE metric 18289.0541\n",
      "Epoch 230 batch 30 train Loss 48.7171 test Loss 22.7259 with MSE metric 18287.1929\n",
      "Epoch 230 batch 40 train Loss 48.7095 test Loss 22.7229 with MSE metric 18285.3706\n",
      "Epoch 230 batch 50 train Loss 48.7020 test Loss 22.7198 with MSE metric 18283.5184\n",
      "Epoch 230 batch 60 train Loss 48.6945 test Loss 22.7168 with MSE metric 18281.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230 batch 70 train Loss 48.6869 test Loss 22.7138 with MSE metric 18279.7845\n",
      "Epoch 230 batch 80 train Loss 48.6794 test Loss 22.7108 with MSE metric 18277.9849\n",
      "Epoch 230 batch 90 train Loss 48.6719 test Loss 22.7078 with MSE metric 18276.1036\n",
      "Epoch 230 batch 100 train Loss 48.6643 test Loss 22.7048 with MSE metric 18274.2226\n",
      "Epoch 230 batch 110 train Loss 48.6568 test Loss 22.7018 with MSE metric 18272.3394\n",
      "Epoch 230 batch 120 train Loss 48.6493 test Loss 22.6988 with MSE metric 18270.4974\n",
      "Epoch 230 batch 130 train Loss 48.6418 test Loss 22.6957 with MSE metric 18268.6970\n",
      "Epoch 230 batch 140 train Loss 48.6343 test Loss 22.6928 with MSE metric 18266.8527\n",
      "Epoch 230 batch 150 train Loss 48.6267 test Loss 22.6897 with MSE metric 18265.0717\n",
      "Epoch 230 batch 160 train Loss 48.6192 test Loss 22.6867 with MSE metric 18263.2597\n",
      "Epoch 230 batch 170 train Loss 48.6117 test Loss 22.6837 with MSE metric 18261.3579\n",
      "Epoch 230 batch 180 train Loss 48.6042 test Loss 22.6807 with MSE metric 18259.4925\n",
      "Epoch 230 batch 190 train Loss 48.5967 test Loss 22.6777 with MSE metric 18257.6761\n",
      "Epoch 230 batch 200 train Loss 48.5892 test Loss 22.6748 with MSE metric 18255.8178\n",
      "Epoch 230 batch 210 train Loss 48.5817 test Loss 22.6718 with MSE metric 18253.9610\n",
      "Epoch 230 batch 220 train Loss 48.5742 test Loss 22.6688 with MSE metric 18252.1552\n",
      "Epoch 230 batch 230 train Loss 48.5667 test Loss 22.6658 with MSE metric 18250.2991\n",
      "Epoch 230 batch 240 train Loss 48.5592 test Loss 22.6628 with MSE metric 18248.4618\n",
      "Time taken for 1 epoch: 24.427266836166382 secs\n",
      "\n",
      "Epoch 231 batch 0 train Loss 48.5518 test Loss 22.6598 with MSE metric 18246.6751\n",
      "Epoch 231 batch 10 train Loss 48.5443 test Loss 22.6568 with MSE metric 18244.8345\n",
      "Epoch 231 batch 20 train Loss 48.5368 test Loss 22.6538 with MSE metric 18243.0008\n",
      "Epoch 231 batch 30 train Loss 48.5293 test Loss 22.6508 with MSE metric 18241.1576\n",
      "Epoch 231 batch 40 train Loss 48.5218 test Loss 22.6479 with MSE metric 18239.2866\n",
      "Epoch 231 batch 50 train Loss 48.5144 test Loss 22.6449 with MSE metric 18237.4668\n",
      "Epoch 231 batch 60 train Loss 48.5069 test Loss 22.6419 with MSE metric 18235.7220\n",
      "Epoch 231 batch 70 train Loss 48.4994 test Loss 22.6389 with MSE metric 18233.8186\n",
      "Epoch 231 batch 80 train Loss 48.4920 test Loss 22.6359 with MSE metric 18231.9514\n",
      "Epoch 231 batch 90 train Loss 48.4845 test Loss 22.6329 with MSE metric 18230.1337\n",
      "Epoch 231 batch 100 train Loss 48.4770 test Loss 22.6300 with MSE metric 18228.3072\n",
      "Epoch 231 batch 110 train Loss 48.4696 test Loss 22.6270 with MSE metric 18226.4618\n",
      "Epoch 231 batch 120 train Loss 48.4621 test Loss 22.6240 with MSE metric 18224.6189\n",
      "Epoch 231 batch 130 train Loss 48.4547 test Loss 22.6210 with MSE metric 18222.7622\n",
      "Epoch 231 batch 140 train Loss 48.4472 test Loss 22.6180 with MSE metric 18220.9144\n",
      "Epoch 231 batch 150 train Loss 48.4397 test Loss 22.6151 with MSE metric 18219.0974\n",
      "Epoch 231 batch 160 train Loss 48.4323 test Loss 22.6121 with MSE metric 18217.3227\n",
      "Epoch 231 batch 170 train Loss 48.4249 test Loss 22.6091 with MSE metric 18215.4387\n",
      "Epoch 231 batch 180 train Loss 48.4174 test Loss 22.6062 with MSE metric 18213.5706\n",
      "Epoch 231 batch 190 train Loss 48.4100 test Loss 22.6032 with MSE metric 18211.7757\n",
      "Epoch 231 batch 200 train Loss 48.4025 test Loss 22.6002 with MSE metric 18210.0025\n",
      "Epoch 231 batch 210 train Loss 48.3951 test Loss 22.5973 with MSE metric 18208.1362\n",
      "Epoch 231 batch 220 train Loss 48.3877 test Loss 22.5943 with MSE metric 18206.2985\n",
      "Epoch 231 batch 230 train Loss 48.3802 test Loss 22.5914 with MSE metric 18204.5036\n",
      "Epoch 231 batch 240 train Loss 48.3728 test Loss 22.5884 with MSE metric 18202.6382\n",
      "Time taken for 1 epoch: 27.46484398841858 secs\n",
      "\n",
      "Epoch 232 batch 0 train Loss 48.3654 test Loss 22.5854 with MSE metric 18200.8637\n",
      "Epoch 232 batch 10 train Loss 48.3580 test Loss 22.5824 with MSE metric 18199.0905\n",
      "Epoch 232 batch 20 train Loss 48.3506 test Loss 22.5795 with MSE metric 18197.2615\n",
      "Epoch 232 batch 30 train Loss 48.3431 test Loss 22.5765 with MSE metric 18195.4550\n",
      "Epoch 232 batch 40 train Loss 48.3357 test Loss 22.5736 with MSE metric 18193.5605\n",
      "Epoch 232 batch 50 train Loss 48.3283 test Loss 22.5706 with MSE metric 18191.7071\n",
      "Epoch 232 batch 60 train Loss 48.3209 test Loss 22.5677 with MSE metric 18189.9187\n",
      "Epoch 232 batch 70 train Loss 48.3135 test Loss 22.5647 with MSE metric 18188.0611\n",
      "Epoch 232 batch 80 train Loss 48.3061 test Loss 22.5617 with MSE metric 18186.2304\n",
      "Epoch 232 batch 90 train Loss 48.2987 test Loss 22.5588 with MSE metric 18184.4003\n",
      "Epoch 232 batch 100 train Loss 48.2913 test Loss 22.5558 with MSE metric 18182.5817\n",
      "Epoch 232 batch 110 train Loss 48.2839 test Loss 22.5529 with MSE metric 18180.7032\n",
      "Epoch 232 batch 120 train Loss 48.2765 test Loss 22.5499 with MSE metric 18178.8556\n",
      "Epoch 232 batch 130 train Loss 48.2691 test Loss 22.5470 with MSE metric 18177.0272\n",
      "Epoch 232 batch 140 train Loss 48.2617 test Loss 22.5440 with MSE metric 18175.2061\n",
      "Epoch 232 batch 150 train Loss 48.2543 test Loss 22.5411 with MSE metric 18173.3965\n",
      "Epoch 232 batch 160 train Loss 48.2470 test Loss 22.5381 with MSE metric 18171.5666\n",
      "Epoch 232 batch 170 train Loss 48.2396 test Loss 22.5352 with MSE metric 18169.7592\n",
      "Epoch 232 batch 180 train Loss 48.2322 test Loss 22.5322 with MSE metric 18167.9187\n",
      "Epoch 232 batch 190 train Loss 48.2248 test Loss 22.5293 with MSE metric 18166.1124\n",
      "Epoch 232 batch 200 train Loss 48.2174 test Loss 22.5264 with MSE metric 18164.2915\n",
      "Epoch 232 batch 210 train Loss 48.2101 test Loss 22.5234 with MSE metric 18162.4785\n",
      "Epoch 232 batch 220 train Loss 48.2027 test Loss 22.5205 with MSE metric 18160.7099\n",
      "Epoch 232 batch 230 train Loss 48.1953 test Loss 22.5175 with MSE metric 18158.8645\n",
      "Epoch 232 batch 240 train Loss 48.1880 test Loss 22.5146 with MSE metric 18157.0387\n",
      "Time taken for 1 epoch: 28.858561992645264 secs\n",
      "\n",
      "Epoch 233 batch 0 train Loss 48.1806 test Loss 22.5116 with MSE metric 18155.2197\n",
      "Epoch 233 batch 10 train Loss 48.1733 test Loss 22.5087 with MSE metric 18153.4238\n",
      "Epoch 233 batch 20 train Loss 48.1659 test Loss 22.5058 with MSE metric 18151.5688\n",
      "Epoch 233 batch 30 train Loss 48.1586 test Loss 22.5028 with MSE metric 18149.8229\n",
      "Epoch 233 batch 40 train Loss 48.1512 test Loss 22.4999 with MSE metric 18148.0239\n",
      "Epoch 233 batch 50 train Loss 48.1439 test Loss 22.4969 with MSE metric 18146.1965\n",
      "Epoch 233 batch 60 train Loss 48.1365 test Loss 22.4940 with MSE metric 18144.3393\n",
      "Epoch 233 batch 70 train Loss 48.1292 test Loss 22.4911 with MSE metric 18142.5481\n",
      "Epoch 233 batch 80 train Loss 48.1218 test Loss 22.4881 with MSE metric 18140.7961\n",
      "Epoch 233 batch 90 train Loss 48.1145 test Loss 22.4852 with MSE metric 18138.9967\n",
      "Epoch 233 batch 100 train Loss 48.1072 test Loss 22.4823 with MSE metric 18137.2123\n",
      "Epoch 233 batch 110 train Loss 48.0998 test Loss 22.4793 with MSE metric 18135.4133\n",
      "Epoch 233 batch 120 train Loss 48.0925 test Loss 22.4764 with MSE metric 18133.6145\n",
      "Epoch 233 batch 130 train Loss 48.0852 test Loss 22.4735 with MSE metric 18131.7939\n",
      "Epoch 233 batch 140 train Loss 48.0779 test Loss 22.4706 with MSE metric 18130.0357\n",
      "Epoch 233 batch 150 train Loss 48.0705 test Loss 22.4677 with MSE metric 18128.2267\n",
      "Epoch 233 batch 160 train Loss 48.0632 test Loss 22.4647 with MSE metric 18126.4349\n",
      "Epoch 233 batch 170 train Loss 48.0559 test Loss 22.4618 with MSE metric 18124.6398\n",
      "Epoch 233 batch 180 train Loss 48.0486 test Loss 22.4589 with MSE metric 18122.9028\n",
      "Epoch 233 batch 190 train Loss 48.0413 test Loss 22.4560 with MSE metric 18121.0712\n",
      "Epoch 233 batch 200 train Loss 48.0340 test Loss 22.4530 with MSE metric 18119.3174\n",
      "Epoch 233 batch 210 train Loss 48.0266 test Loss 22.4501 with MSE metric 18117.4635\n",
      "Epoch 233 batch 220 train Loss 48.0193 test Loss 22.4472 with MSE metric 18115.6934\n",
      "Epoch 233 batch 230 train Loss 48.0120 test Loss 22.4443 with MSE metric 18113.8962\n",
      "Epoch 233 batch 240 train Loss 48.0047 test Loss 22.4414 with MSE metric 18112.0526\n",
      "Time taken for 1 epoch: 29.10159397125244 secs\n",
      "\n",
      "Epoch 234 batch 0 train Loss 47.9974 test Loss 22.4384 with MSE metric 18110.2985\n",
      "Epoch 234 batch 10 train Loss 47.9901 test Loss 22.4355 with MSE metric 18108.4738\n",
      "Epoch 234 batch 20 train Loss 47.9829 test Loss 22.4326 with MSE metric 18106.6423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234 batch 30 train Loss 47.9756 test Loss 22.4297 with MSE metric 18104.8516\n",
      "Epoch 234 batch 40 train Loss 47.9683 test Loss 22.4268 with MSE metric 18103.0342\n",
      "Epoch 234 batch 50 train Loss 47.9610 test Loss 22.4239 with MSE metric 18101.3285\n",
      "Epoch 234 batch 60 train Loss 47.9537 test Loss 22.4210 with MSE metric 18099.5609\n",
      "Epoch 234 batch 70 train Loss 47.9464 test Loss 22.4181 with MSE metric 18097.7139\n",
      "Epoch 234 batch 80 train Loss 47.9392 test Loss 22.4152 with MSE metric 18095.9320\n",
      "Epoch 234 batch 90 train Loss 47.9319 test Loss 22.4123 with MSE metric 18094.1762\n",
      "Epoch 234 batch 100 train Loss 47.9246 test Loss 22.4094 with MSE metric 18092.3733\n",
      "Epoch 234 batch 110 train Loss 47.9173 test Loss 22.4065 with MSE metric 18090.5772\n",
      "Epoch 234 batch 120 train Loss 47.9101 test Loss 22.4036 with MSE metric 18088.8113\n",
      "Epoch 234 batch 130 train Loss 47.9028 test Loss 22.4006 with MSE metric 18086.9855\n",
      "Epoch 234 batch 140 train Loss 47.8955 test Loss 22.3977 with MSE metric 18085.1560\n",
      "Epoch 234 batch 150 train Loss 47.8883 test Loss 22.3948 with MSE metric 18083.3658\n",
      "Epoch 234 batch 160 train Loss 47.8810 test Loss 22.3919 with MSE metric 18081.5703\n",
      "Epoch 234 batch 170 train Loss 47.8738 test Loss 22.3891 with MSE metric 18079.8342\n",
      "Epoch 234 batch 180 train Loss 47.8665 test Loss 22.3862 with MSE metric 18078.0593\n",
      "Epoch 234 batch 190 train Loss 47.8593 test Loss 22.3833 with MSE metric 18076.3289\n",
      "Epoch 234 batch 200 train Loss 47.8520 test Loss 22.3804 with MSE metric 18074.5525\n",
      "Epoch 234 batch 210 train Loss 47.8448 test Loss 22.3775 with MSE metric 18072.7681\n",
      "Epoch 234 batch 220 train Loss 47.8375 test Loss 22.3746 with MSE metric 18070.9718\n",
      "Epoch 234 batch 230 train Loss 47.8303 test Loss 22.3717 with MSE metric 18069.1835\n",
      "Epoch 234 batch 240 train Loss 47.8231 test Loss 22.3688 with MSE metric 18067.4500\n",
      "Time taken for 1 epoch: 28.96806001663208 secs\n",
      "\n",
      "Epoch 235 batch 0 train Loss 47.8158 test Loss 22.3659 with MSE metric 18065.6807\n",
      "Epoch 235 batch 10 train Loss 47.8086 test Loss 22.3630 with MSE metric 18063.9568\n",
      "Epoch 235 batch 20 train Loss 47.8014 test Loss 22.3601 with MSE metric 18062.1892\n",
      "Epoch 235 batch 30 train Loss 47.7941 test Loss 22.3572 with MSE metric 18060.4199\n",
      "Epoch 235 batch 40 train Loss 47.7869 test Loss 22.3544 with MSE metric 18058.6333\n",
      "Epoch 235 batch 50 train Loss 47.7797 test Loss 22.3515 with MSE metric 18056.8240\n",
      "Epoch 235 batch 60 train Loss 47.7725 test Loss 22.3486 with MSE metric 18055.1154\n",
      "Epoch 235 batch 70 train Loss 47.7652 test Loss 22.3457 with MSE metric 18053.3329\n",
      "Epoch 235 batch 80 train Loss 47.7580 test Loss 22.3429 with MSE metric 18051.5758\n",
      "Epoch 235 batch 90 train Loss 47.7508 test Loss 22.3400 with MSE metric 18049.7848\n",
      "Epoch 235 batch 100 train Loss 47.7436 test Loss 22.3371 with MSE metric 18047.9515\n",
      "Epoch 235 batch 110 train Loss 47.7364 test Loss 22.3342 with MSE metric 18046.1286\n",
      "Epoch 235 batch 120 train Loss 47.7292 test Loss 22.3313 with MSE metric 18044.3723\n",
      "Epoch 235 batch 130 train Loss 47.7220 test Loss 22.3284 with MSE metric 18042.6710\n",
      "Epoch 235 batch 140 train Loss 47.7148 test Loss 22.3256 with MSE metric 18040.9189\n",
      "Epoch 235 batch 150 train Loss 47.7076 test Loss 22.3227 with MSE metric 18039.1125\n",
      "Epoch 235 batch 160 train Loss 47.7004 test Loss 22.3198 with MSE metric 18037.3060\n",
      "Epoch 235 batch 170 train Loss 47.6932 test Loss 22.3169 with MSE metric 18035.4398\n",
      "Epoch 235 batch 180 train Loss 47.6860 test Loss 22.3141 with MSE metric 18033.5952\n",
      "Epoch 235 batch 190 train Loss 47.6788 test Loss 22.3112 with MSE metric 18031.8547\n",
      "Epoch 235 batch 200 train Loss 47.6716 test Loss 22.3083 with MSE metric 18030.0864\n",
      "Epoch 235 batch 210 train Loss 47.6644 test Loss 22.3055 with MSE metric 18028.2811\n",
      "Epoch 235 batch 220 train Loss 47.6572 test Loss 22.3026 with MSE metric 18026.4936\n",
      "Epoch 235 batch 230 train Loss 47.6501 test Loss 22.2997 with MSE metric 18024.7196\n",
      "Epoch 235 batch 240 train Loss 47.6429 test Loss 22.2969 with MSE metric 18022.9415\n",
      "Time taken for 1 epoch: 32.15312933921814 secs\n",
      "\n",
      "Epoch 236 batch 0 train Loss 47.6357 test Loss 22.2940 with MSE metric 18021.1702\n",
      "Epoch 236 batch 10 train Loss 47.6285 test Loss 22.2912 with MSE metric 18019.3922\n",
      "Epoch 236 batch 20 train Loss 47.6214 test Loss 22.2883 with MSE metric 18017.6506\n",
      "Epoch 236 batch 30 train Loss 47.6142 test Loss 22.2854 with MSE metric 18015.8604\n",
      "Epoch 236 batch 40 train Loss 47.6070 test Loss 22.2826 with MSE metric 18014.0627\n",
      "Epoch 236 batch 50 train Loss 47.5999 test Loss 22.2797 with MSE metric 18012.3159\n",
      "Epoch 236 batch 60 train Loss 47.5927 test Loss 22.2769 with MSE metric 18010.6138\n",
      "Epoch 236 batch 70 train Loss 47.5856 test Loss 22.2740 with MSE metric 18008.8731\n",
      "Epoch 236 batch 80 train Loss 47.5784 test Loss 22.2711 with MSE metric 18007.1389\n",
      "Epoch 236 batch 90 train Loss 47.5713 test Loss 22.2683 with MSE metric 18005.3765\n",
      "Epoch 236 batch 100 train Loss 47.5641 test Loss 22.2654 with MSE metric 18003.5606\n",
      "Epoch 236 batch 110 train Loss 47.5570 test Loss 22.2626 with MSE metric 18001.8565\n",
      "Epoch 236 batch 120 train Loss 47.5498 test Loss 22.2597 with MSE metric 18000.1091\n",
      "Epoch 236 batch 130 train Loss 47.5427 test Loss 22.2569 with MSE metric 17998.3649\n",
      "Epoch 236 batch 140 train Loss 47.5355 test Loss 22.2541 with MSE metric 17996.5738\n",
      "Epoch 236 batch 150 train Loss 47.5284 test Loss 22.2512 with MSE metric 17994.9160\n",
      "Epoch 236 batch 160 train Loss 47.5213 test Loss 22.2484 with MSE metric 17993.1724\n",
      "Epoch 236 batch 170 train Loss 47.5141 test Loss 22.2455 with MSE metric 17991.4523\n",
      "Epoch 236 batch 180 train Loss 47.5070 test Loss 22.2427 with MSE metric 17989.7531\n",
      "Epoch 236 batch 190 train Loss 47.4999 test Loss 22.2398 with MSE metric 17988.0156\n",
      "Epoch 236 batch 200 train Loss 47.4927 test Loss 22.2370 with MSE metric 17986.2960\n",
      "Epoch 236 batch 210 train Loss 47.4856 test Loss 22.2341 with MSE metric 17984.5490\n",
      "Epoch 236 batch 220 train Loss 47.4785 test Loss 22.2313 with MSE metric 17982.8054\n",
      "Epoch 236 batch 230 train Loss 47.4714 test Loss 22.2284 with MSE metric 17981.0641\n",
      "Epoch 236 batch 240 train Loss 47.4643 test Loss 22.2256 with MSE metric 17979.3662\n",
      "Time taken for 1 epoch: 29.30556082725525 secs\n",
      "\n",
      "Epoch 237 batch 0 train Loss 47.4572 test Loss 22.2228 with MSE metric 17977.6136\n",
      "Epoch 237 batch 10 train Loss 47.4501 test Loss 22.2199 with MSE metric 17975.8657\n",
      "Epoch 237 batch 20 train Loss 47.4429 test Loss 22.2171 with MSE metric 17974.1298\n",
      "Epoch 237 batch 30 train Loss 47.4358 test Loss 22.2143 with MSE metric 17972.3546\n",
      "Epoch 237 batch 40 train Loss 47.4287 test Loss 22.2114 with MSE metric 17970.6372\n",
      "Epoch 237 batch 50 train Loss 47.4216 test Loss 22.2086 with MSE metric 17968.9007\n",
      "Epoch 237 batch 60 train Loss 47.4145 test Loss 22.2057 with MSE metric 17967.0873\n",
      "Epoch 237 batch 70 train Loss 47.4074 test Loss 22.2029 with MSE metric 17965.3551\n",
      "Epoch 237 batch 80 train Loss 47.4003 test Loss 22.2000 with MSE metric 17963.6396\n",
      "Epoch 237 batch 90 train Loss 47.3932 test Loss 22.1972 with MSE metric 17961.8553\n",
      "Epoch 237 batch 100 train Loss 47.3862 test Loss 22.1944 with MSE metric 17960.1039\n",
      "Epoch 237 batch 110 train Loss 47.3791 test Loss 22.1915 with MSE metric 17958.3967\n",
      "Epoch 237 batch 120 train Loss 47.3720 test Loss 22.1887 with MSE metric 17956.6949\n",
      "Epoch 237 batch 130 train Loss 47.3649 test Loss 22.1859 with MSE metric 17954.9360\n",
      "Epoch 237 batch 140 train Loss 47.3578 test Loss 22.1830 with MSE metric 17953.2733\n",
      "Epoch 237 batch 150 train Loss 47.3508 test Loss 22.1802 with MSE metric 17951.5570\n",
      "Epoch 237 batch 160 train Loss 47.3437 test Loss 22.1774 with MSE metric 17949.8479\n",
      "Epoch 237 batch 170 train Loss 47.3366 test Loss 22.1746 with MSE metric 17948.1202\n",
      "Epoch 237 batch 180 train Loss 47.3295 test Loss 22.1717 with MSE metric 17946.3372\n",
      "Epoch 237 batch 190 train Loss 47.3225 test Loss 22.1689 with MSE metric 17944.6090\n",
      "Epoch 237 batch 200 train Loss 47.3154 test Loss 22.1661 with MSE metric 17942.8472\n",
      "Epoch 237 batch 210 train Loss 47.3083 test Loss 22.1633 with MSE metric 17941.0748\n",
      "Epoch 237 batch 220 train Loss 47.3013 test Loss 22.1605 with MSE metric 17939.3558\n",
      "Epoch 237 batch 230 train Loss 47.2942 test Loss 22.1576 with MSE metric 17937.6696\n",
      "Epoch 237 batch 240 train Loss 47.2872 test Loss 22.1548 with MSE metric 17935.9421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 30.929280996322632 secs\n",
      "\n",
      "Epoch 238 batch 0 train Loss 47.2801 test Loss 22.1520 with MSE metric 17934.2647\n",
      "Epoch 238 batch 10 train Loss 47.2731 test Loss 22.1492 with MSE metric 17932.4923\n",
      "Epoch 238 batch 20 train Loss 47.2660 test Loss 22.1464 with MSE metric 17930.7743\n",
      "Epoch 238 batch 30 train Loss 47.2590 test Loss 22.1435 with MSE metric 17929.0491\n",
      "Epoch 238 batch 40 train Loss 47.2519 test Loss 22.1407 with MSE metric 17927.3859\n",
      "Epoch 238 batch 50 train Loss 47.2449 test Loss 22.1379 with MSE metric 17925.6194\n",
      "Epoch 238 batch 60 train Loss 47.2378 test Loss 22.1351 with MSE metric 17923.8536\n",
      "Epoch 238 batch 70 train Loss 47.2308 test Loss 22.1323 with MSE metric 17922.2029\n",
      "Epoch 238 batch 80 train Loss 47.2238 test Loss 22.1295 with MSE metric 17920.5072\n",
      "Epoch 238 batch 90 train Loss 47.2167 test Loss 22.1267 with MSE metric 17918.7359\n",
      "Epoch 238 batch 100 train Loss 47.2097 test Loss 22.1239 with MSE metric 17916.9645\n",
      "Epoch 238 batch 110 train Loss 47.2027 test Loss 22.1211 with MSE metric 17915.2268\n",
      "Epoch 238 batch 120 train Loss 47.1956 test Loss 22.1183 with MSE metric 17913.5332\n",
      "Epoch 238 batch 130 train Loss 47.1886 test Loss 22.1155 with MSE metric 17911.8074\n",
      "Epoch 238 batch 140 train Loss 47.1816 test Loss 22.1127 with MSE metric 17910.0438\n",
      "Epoch 238 batch 150 train Loss 47.1746 test Loss 22.1099 with MSE metric 17908.2969\n",
      "Epoch 238 batch 160 train Loss 47.1675 test Loss 22.1071 with MSE metric 17906.5183\n",
      "Epoch 238 batch 170 train Loss 47.1605 test Loss 22.1043 with MSE metric 17904.8603\n",
      "Epoch 238 batch 180 train Loss 47.1535 test Loss 22.1015 with MSE metric 17903.1030\n",
      "Epoch 238 batch 190 train Loss 47.1465 test Loss 22.0987 with MSE metric 17901.3492\n",
      "Epoch 238 batch 200 train Loss 47.1395 test Loss 22.0959 with MSE metric 17899.6479\n",
      "Epoch 238 batch 210 train Loss 47.1325 test Loss 22.0931 with MSE metric 17897.9060\n",
      "Epoch 238 batch 220 train Loss 47.1255 test Loss 22.0904 with MSE metric 17896.1039\n",
      "Epoch 238 batch 230 train Loss 47.1185 test Loss 22.0876 with MSE metric 17894.3620\n",
      "Epoch 238 batch 240 train Loss 47.1115 test Loss 22.0848 with MSE metric 17892.6272\n",
      "Time taken for 1 epoch: 30.410068035125732 secs\n",
      "\n",
      "Epoch 239 batch 0 train Loss 47.1045 test Loss 22.0820 with MSE metric 17890.9234\n",
      "Epoch 239 batch 10 train Loss 47.0975 test Loss 22.0793 with MSE metric 17889.1916\n",
      "Epoch 239 batch 20 train Loss 47.0905 test Loss 22.0764 with MSE metric 17887.4026\n",
      "Epoch 239 batch 30 train Loss 47.0835 test Loss 22.0737 with MSE metric 17885.6399\n",
      "Epoch 239 batch 40 train Loss 47.0765 test Loss 22.0709 with MSE metric 17883.9116\n",
      "Epoch 239 batch 50 train Loss 47.0695 test Loss 22.0681 with MSE metric 17882.1781\n",
      "Epoch 239 batch 60 train Loss 47.0626 test Loss 22.0653 with MSE metric 17880.4260\n",
      "Epoch 239 batch 70 train Loss 47.0556 test Loss 22.0625 with MSE metric 17878.7019\n",
      "Epoch 239 batch 80 train Loss 47.0486 test Loss 22.0597 with MSE metric 17876.9508\n",
      "Epoch 239 batch 90 train Loss 47.0416 test Loss 22.0569 with MSE metric 17875.2151\n",
      "Epoch 239 batch 100 train Loss 47.0347 test Loss 22.0541 with MSE metric 17873.5121\n",
      "Epoch 239 batch 110 train Loss 47.0277 test Loss 22.0513 with MSE metric 17871.7911\n",
      "Epoch 239 batch 120 train Loss 47.0207 test Loss 22.0486 with MSE metric 17870.1277\n",
      "Epoch 239 batch 130 train Loss 47.0138 test Loss 22.0458 with MSE metric 17868.4012\n",
      "Epoch 239 batch 140 train Loss 47.0068 test Loss 22.0430 with MSE metric 17866.6987\n",
      "Epoch 239 batch 150 train Loss 46.9998 test Loss 22.0402 with MSE metric 17864.9670\n",
      "Epoch 239 batch 160 train Loss 46.9929 test Loss 22.0374 with MSE metric 17863.2903\n",
      "Epoch 239 batch 170 train Loss 46.9859 test Loss 22.0346 with MSE metric 17861.5941\n",
      "Epoch 239 batch 180 train Loss 46.9790 test Loss 22.0319 with MSE metric 17859.9305\n",
      "Epoch 239 batch 190 train Loss 46.9720 test Loss 22.0291 with MSE metric 17858.2464\n",
      "Epoch 239 batch 200 train Loss 46.9651 test Loss 22.0263 with MSE metric 17856.4967\n",
      "Epoch 239 batch 210 train Loss 46.9581 test Loss 22.0235 with MSE metric 17854.7784\n",
      "Epoch 239 batch 220 train Loss 46.9512 test Loss 22.0208 with MSE metric 17853.1517\n",
      "Epoch 239 batch 230 train Loss 46.9442 test Loss 22.0180 with MSE metric 17851.3948\n",
      "Epoch 239 batch 240 train Loss 46.9373 test Loss 22.0153 with MSE metric 17849.7317\n",
      "Time taken for 1 epoch: 29.774513006210327 secs\n",
      "\n",
      "Epoch 240 batch 0 train Loss 46.9304 test Loss 22.0125 with MSE metric 17848.0426\n",
      "Epoch 240 batch 10 train Loss 46.9234 test Loss 22.0097 with MSE metric 17846.3487\n",
      "Epoch 240 batch 20 train Loss 46.9165 test Loss 22.0069 with MSE metric 17844.6401\n",
      "Epoch 240 batch 30 train Loss 46.9096 test Loss 22.0042 with MSE metric 17842.9185\n",
      "Epoch 240 batch 40 train Loss 46.9026 test Loss 22.0014 with MSE metric 17841.1957\n",
      "Epoch 240 batch 50 train Loss 46.8957 test Loss 21.9986 with MSE metric 17839.4483\n",
      "Epoch 240 batch 60 train Loss 46.8888 test Loss 21.9959 with MSE metric 17837.7462\n",
      "Epoch 240 batch 70 train Loss 46.8819 test Loss 21.9931 with MSE metric 17836.0681\n",
      "Epoch 240 batch 80 train Loss 46.8750 test Loss 21.9903 with MSE metric 17834.3944\n",
      "Epoch 240 batch 90 train Loss 46.8680 test Loss 21.9876 with MSE metric 17832.7135\n",
      "Epoch 240 batch 100 train Loss 46.8611 test Loss 21.9848 with MSE metric 17831.0230\n",
      "Epoch 240 batch 110 train Loss 46.8542 test Loss 21.9821 with MSE metric 17829.3121\n",
      "Epoch 240 batch 120 train Loss 46.8473 test Loss 21.9793 with MSE metric 17827.6024\n",
      "Epoch 240 batch 130 train Loss 46.8404 test Loss 21.9766 with MSE metric 17825.8563\n",
      "Epoch 240 batch 140 train Loss 46.8335 test Loss 21.9738 with MSE metric 17824.1259\n",
      "Epoch 240 batch 150 train Loss 46.8266 test Loss 21.9711 with MSE metric 17822.4008\n",
      "Epoch 240 batch 160 train Loss 46.8197 test Loss 21.9683 with MSE metric 17820.6823\n",
      "Epoch 240 batch 170 train Loss 46.8128 test Loss 21.9655 with MSE metric 17818.9960\n",
      "Epoch 240 batch 180 train Loss 46.8059 test Loss 21.9628 with MSE metric 17817.3374\n",
      "Epoch 240 batch 190 train Loss 46.7990 test Loss 21.9600 with MSE metric 17815.7029\n",
      "Epoch 240 batch 200 train Loss 46.7921 test Loss 21.9573 with MSE metric 17813.9542\n",
      "Epoch 240 batch 210 train Loss 46.7852 test Loss 21.9545 with MSE metric 17812.3214\n",
      "Epoch 240 batch 220 train Loss 46.7783 test Loss 21.9518 with MSE metric 17810.5927\n",
      "Epoch 240 batch 230 train Loss 46.7714 test Loss 21.9491 with MSE metric 17808.8726\n",
      "Epoch 240 batch 240 train Loss 46.7646 test Loss 21.9463 with MSE metric 17807.1691\n",
      "Time taken for 1 epoch: 29.432066917419434 secs\n",
      "\n",
      "Epoch 241 batch 0 train Loss 46.7577 test Loss 21.9435 with MSE metric 17805.4839\n",
      "Epoch 241 batch 10 train Loss 46.7508 test Loss 21.9408 with MSE metric 17803.7418\n",
      "Epoch 241 batch 20 train Loss 46.7439 test Loss 21.9380 with MSE metric 17802.0509\n",
      "Epoch 241 batch 30 train Loss 46.7371 test Loss 21.9353 with MSE metric 17800.3315\n",
      "Epoch 241 batch 40 train Loss 46.7302 test Loss 21.9326 with MSE metric 17798.6038\n",
      "Epoch 241 batch 50 train Loss 46.7233 test Loss 21.9298 with MSE metric 17796.9566\n",
      "Epoch 241 batch 60 train Loss 46.7165 test Loss 21.9271 with MSE metric 17795.2292\n",
      "Epoch 241 batch 70 train Loss 46.7096 test Loss 21.9243 with MSE metric 17793.5239\n",
      "Epoch 241 batch 80 train Loss 46.7027 test Loss 21.9216 with MSE metric 17791.8281\n",
      "Epoch 241 batch 90 train Loss 46.6959 test Loss 21.9188 with MSE metric 17790.1213\n",
      "Epoch 241 batch 100 train Loss 46.6890 test Loss 21.9161 with MSE metric 17788.4921\n",
      "Epoch 241 batch 110 train Loss 46.6822 test Loss 21.9133 with MSE metric 17786.8011\n",
      "Epoch 241 batch 120 train Loss 46.6753 test Loss 21.9106 with MSE metric 17785.0807\n",
      "Epoch 241 batch 130 train Loss 46.6684 test Loss 21.9079 with MSE metric 17783.3895\n",
      "Epoch 241 batch 140 train Loss 46.6616 test Loss 21.9051 with MSE metric 17781.7586\n",
      "Epoch 241 batch 150 train Loss 46.6548 test Loss 21.9024 with MSE metric 17780.1611\n",
      "Epoch 241 batch 160 train Loss 46.6479 test Loss 21.8997 with MSE metric 17778.5119\n",
      "Epoch 241 batch 170 train Loss 46.6411 test Loss 21.8970 with MSE metric 17776.8038\n",
      "Epoch 241 batch 180 train Loss 46.6342 test Loss 21.8942 with MSE metric 17775.1326\n",
      "Epoch 241 batch 190 train Loss 46.6274 test Loss 21.8915 with MSE metric 17773.4585\n",
      "Epoch 241 batch 200 train Loss 46.6206 test Loss 21.8888 with MSE metric 17771.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241 batch 210 train Loss 46.6137 test Loss 21.8860 with MSE metric 17770.1446\n",
      "Epoch 241 batch 220 train Loss 46.6069 test Loss 21.8833 with MSE metric 17768.4386\n",
      "Epoch 241 batch 230 train Loss 46.6001 test Loss 21.8806 with MSE metric 17766.7777\n",
      "Epoch 241 batch 240 train Loss 46.5933 test Loss 21.8778 with MSE metric 17765.0795\n",
      "Time taken for 1 epoch: 29.473978281021118 secs\n",
      "\n",
      "Epoch 242 batch 0 train Loss 46.5864 test Loss 21.8751 with MSE metric 17763.3826\n",
      "Epoch 242 batch 10 train Loss 46.5796 test Loss 21.8724 with MSE metric 17761.7321\n",
      "Epoch 242 batch 20 train Loss 46.5728 test Loss 21.8696 with MSE metric 17760.0578\n",
      "Epoch 242 batch 30 train Loss 46.5660 test Loss 21.8669 with MSE metric 17758.3264\n",
      "Epoch 242 batch 40 train Loss 46.5592 test Loss 21.8642 with MSE metric 17756.6753\n",
      "Epoch 242 batch 50 train Loss 46.5523 test Loss 21.8615 with MSE metric 17754.9663\n",
      "Epoch 242 batch 60 train Loss 46.5455 test Loss 21.8588 with MSE metric 17753.3582\n",
      "Epoch 242 batch 70 train Loss 46.5387 test Loss 21.8561 with MSE metric 17751.6856\n",
      "Epoch 242 batch 80 train Loss 46.5319 test Loss 21.8533 with MSE metric 17750.0170\n",
      "Epoch 242 batch 90 train Loss 46.5251 test Loss 21.8506 with MSE metric 17748.3300\n",
      "Epoch 242 batch 100 train Loss 46.5183 test Loss 21.8479 with MSE metric 17746.6366\n",
      "Epoch 242 batch 110 train Loss 46.5115 test Loss 21.8452 with MSE metric 17744.9532\n",
      "Epoch 242 batch 120 train Loss 46.5047 test Loss 21.8424 with MSE metric 17743.2422\n",
      "Epoch 242 batch 130 train Loss 46.4979 test Loss 21.8397 with MSE metric 17741.5797\n",
      "Epoch 242 batch 140 train Loss 46.4911 test Loss 21.8370 with MSE metric 17739.8989\n",
      "Epoch 242 batch 150 train Loss 46.4843 test Loss 21.8343 with MSE metric 17738.2274\n",
      "Epoch 242 batch 160 train Loss 46.4776 test Loss 21.8316 with MSE metric 17736.5163\n",
      "Epoch 242 batch 170 train Loss 46.4708 test Loss 21.8289 with MSE metric 17734.8120\n",
      "Epoch 242 batch 180 train Loss 46.4640 test Loss 21.8262 with MSE metric 17733.1655\n",
      "Epoch 242 batch 190 train Loss 46.4572 test Loss 21.8235 with MSE metric 17731.5159\n",
      "Epoch 242 batch 200 train Loss 46.4504 test Loss 21.8208 with MSE metric 17729.8696\n",
      "Epoch 242 batch 210 train Loss 46.4437 test Loss 21.8181 with MSE metric 17728.1772\n",
      "Epoch 242 batch 220 train Loss 46.4369 test Loss 21.8154 with MSE metric 17726.5079\n",
      "Epoch 242 batch 230 train Loss 46.4301 test Loss 21.8127 with MSE metric 17724.8737\n",
      "Epoch 242 batch 240 train Loss 46.4233 test Loss 21.8100 with MSE metric 17723.2963\n",
      "Time taken for 1 epoch: 29.88199806213379 secs\n",
      "\n",
      "Epoch 243 batch 0 train Loss 46.4166 test Loss 21.8073 with MSE metric 17721.6055\n",
      "Epoch 243 batch 10 train Loss 46.4098 test Loss 21.8046 with MSE metric 17719.8910\n",
      "Epoch 243 batch 20 train Loss 46.4030 test Loss 21.8019 with MSE metric 17718.1863\n",
      "Epoch 243 batch 30 train Loss 46.3963 test Loss 21.7992 with MSE metric 17716.5584\n",
      "Epoch 243 batch 40 train Loss 46.3895 test Loss 21.7965 with MSE metric 17714.8795\n",
      "Epoch 243 batch 50 train Loss 46.3828 test Loss 21.7938 with MSE metric 17713.2151\n",
      "Epoch 243 batch 60 train Loss 46.3760 test Loss 21.7911 with MSE metric 17711.5849\n",
      "Epoch 243 batch 70 train Loss 46.3693 test Loss 21.7884 with MSE metric 17709.9843\n",
      "Epoch 243 batch 80 train Loss 46.3625 test Loss 21.7857 with MSE metric 17708.3418\n",
      "Epoch 243 batch 90 train Loss 46.3558 test Loss 21.7829 with MSE metric 17706.7075\n",
      "Epoch 243 batch 100 train Loss 46.3490 test Loss 21.7803 with MSE metric 17705.0148\n",
      "Epoch 243 batch 110 train Loss 46.3423 test Loss 21.7776 with MSE metric 17703.3568\n",
      "Epoch 243 batch 120 train Loss 46.3355 test Loss 21.7749 with MSE metric 17701.6847\n",
      "Epoch 243 batch 130 train Loss 46.3288 test Loss 21.7722 with MSE metric 17699.9710\n",
      "Epoch 243 batch 140 train Loss 46.3221 test Loss 21.7695 with MSE metric 17698.3170\n",
      "Epoch 243 batch 150 train Loss 46.3153 test Loss 21.7668 with MSE metric 17696.6687\n",
      "Epoch 243 batch 160 train Loss 46.3086 test Loss 21.7641 with MSE metric 17695.0149\n",
      "Epoch 243 batch 170 train Loss 46.3019 test Loss 21.7615 with MSE metric 17693.3584\n",
      "Epoch 243 batch 180 train Loss 46.2952 test Loss 21.7587 with MSE metric 17691.6958\n",
      "Epoch 243 batch 190 train Loss 46.2884 test Loss 21.7561 with MSE metric 17690.0287\n",
      "Epoch 243 batch 200 train Loss 46.2817 test Loss 21.7534 with MSE metric 17688.3954\n",
      "Epoch 243 batch 210 train Loss 46.2750 test Loss 21.7507 with MSE metric 17686.8065\n",
      "Epoch 243 batch 220 train Loss 46.2683 test Loss 21.7480 with MSE metric 17685.1988\n",
      "Epoch 243 batch 230 train Loss 46.2616 test Loss 21.7453 with MSE metric 17683.4684\n",
      "Epoch 243 batch 240 train Loss 46.2548 test Loss 21.7427 with MSE metric 17681.8313\n",
      "Time taken for 1 epoch: 29.180109977722168 secs\n",
      "\n",
      "Epoch 244 batch 0 train Loss 46.2481 test Loss 21.7400 with MSE metric 17680.2208\n",
      "Epoch 244 batch 10 train Loss 46.2414 test Loss 21.7373 with MSE metric 17678.6180\n",
      "Epoch 244 batch 20 train Loss 46.2347 test Loss 21.7346 with MSE metric 17676.9719\n",
      "Epoch 244 batch 30 train Loss 46.2280 test Loss 21.7319 with MSE metric 17675.3092\n",
      "Epoch 244 batch 40 train Loss 46.2213 test Loss 21.7293 with MSE metric 17673.6446\n",
      "Epoch 244 batch 50 train Loss 46.2146 test Loss 21.7266 with MSE metric 17671.9654\n",
      "Epoch 244 batch 60 train Loss 46.2079 test Loss 21.7239 with MSE metric 17670.2670\n",
      "Epoch 244 batch 70 train Loss 46.2012 test Loss 21.7213 with MSE metric 17668.6128\n",
      "Epoch 244 batch 80 train Loss 46.1945 test Loss 21.7186 with MSE metric 17666.9766\n",
      "Epoch 244 batch 90 train Loss 46.1878 test Loss 21.7159 with MSE metric 17665.3193\n",
      "Epoch 244 batch 100 train Loss 46.1811 test Loss 21.7133 with MSE metric 17663.6314\n",
      "Epoch 244 batch 110 train Loss 46.1744 test Loss 21.7106 with MSE metric 17662.0047\n",
      "Epoch 244 batch 120 train Loss 46.1678 test Loss 21.7079 with MSE metric 17660.3424\n",
      "Epoch 244 batch 130 train Loss 46.1611 test Loss 21.7052 with MSE metric 17658.6807\n",
      "Epoch 244 batch 140 train Loss 46.1544 test Loss 21.7026 with MSE metric 17657.0453\n",
      "Epoch 244 batch 150 train Loss 46.1477 test Loss 21.6999 with MSE metric 17655.4622\n",
      "Epoch 244 batch 160 train Loss 46.1410 test Loss 21.6972 with MSE metric 17653.7918\n",
      "Epoch 244 batch 170 train Loss 46.1344 test Loss 21.6946 with MSE metric 17652.1583\n",
      "Epoch 244 batch 180 train Loss 46.1277 test Loss 21.6919 with MSE metric 17650.4664\n",
      "Epoch 244 batch 190 train Loss 46.1210 test Loss 21.6892 with MSE metric 17648.8077\n",
      "Epoch 244 batch 200 train Loss 46.1144 test Loss 21.6866 with MSE metric 17647.2067\n",
      "Epoch 244 batch 210 train Loss 46.1077 test Loss 21.6839 with MSE metric 17645.5156\n",
      "Epoch 244 batch 220 train Loss 46.1010 test Loss 21.6813 with MSE metric 17643.8363\n",
      "Epoch 244 batch 230 train Loss 46.0944 test Loss 21.6786 with MSE metric 17642.1996\n",
      "Epoch 244 batch 240 train Loss 46.0877 test Loss 21.6759 with MSE metric 17640.5272\n",
      "Time taken for 1 epoch: 28.765380859375 secs\n",
      "\n",
      "Epoch 245 batch 0 train Loss 46.0810 test Loss 21.6733 with MSE metric 17638.9035\n",
      "Epoch 245 batch 10 train Loss 46.0744 test Loss 21.6706 with MSE metric 17637.2235\n",
      "Epoch 245 batch 20 train Loss 46.0677 test Loss 21.6680 with MSE metric 17635.6661\n",
      "Epoch 245 batch 30 train Loss 46.0611 test Loss 21.6653 with MSE metric 17634.0161\n",
      "Epoch 245 batch 40 train Loss 46.0544 test Loss 21.6626 with MSE metric 17632.4084\n",
      "Epoch 245 batch 50 train Loss 46.0478 test Loss 21.6600 with MSE metric 17630.7811\n",
      "Epoch 245 batch 60 train Loss 46.0412 test Loss 21.6573 with MSE metric 17629.1533\n",
      "Epoch 245 batch 70 train Loss 46.0345 test Loss 21.6547 with MSE metric 17627.5144\n",
      "Epoch 245 batch 80 train Loss 46.0279 test Loss 21.6520 with MSE metric 17625.8213\n",
      "Epoch 245 batch 90 train Loss 46.0212 test Loss 21.6494 with MSE metric 17624.2395\n",
      "Epoch 245 batch 100 train Loss 46.0146 test Loss 21.6467 with MSE metric 17622.6325\n",
      "Epoch 245 batch 110 train Loss 46.0080 test Loss 21.6441 with MSE metric 17621.0034\n",
      "Epoch 245 batch 120 train Loss 46.0013 test Loss 21.6414 with MSE metric 17619.3060\n",
      "Epoch 245 batch 130 train Loss 45.9947 test Loss 21.6388 with MSE metric 17617.7021\n",
      "Epoch 245 batch 140 train Loss 45.9881 test Loss 21.6361 with MSE metric 17616.0201\n",
      "Epoch 245 batch 150 train Loss 45.9815 test Loss 21.6335 with MSE metric 17614.4458\n",
      "Epoch 245 batch 160 train Loss 45.9748 test Loss 21.6308 with MSE metric 17612.8203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245 batch 170 train Loss 45.9682 test Loss 21.6282 with MSE metric 17611.1689\n",
      "Epoch 245 batch 180 train Loss 45.9616 test Loss 21.6255 with MSE metric 17609.5246\n",
      "Epoch 245 batch 190 train Loss 45.9550 test Loss 21.6229 with MSE metric 17607.9538\n",
      "Epoch 245 batch 200 train Loss 45.9484 test Loss 21.6202 with MSE metric 17606.3292\n",
      "Epoch 245 batch 210 train Loss 45.9418 test Loss 21.6176 with MSE metric 17604.7086\n",
      "Epoch 245 batch 220 train Loss 45.9352 test Loss 21.6150 with MSE metric 17603.0676\n",
      "Epoch 245 batch 230 train Loss 45.9285 test Loss 21.6124 with MSE metric 17601.3933\n",
      "Epoch 245 batch 240 train Loss 45.9219 test Loss 21.6097 with MSE metric 17599.7641\n",
      "Time taken for 1 epoch: 27.911991119384766 secs\n",
      "\n",
      "Epoch 246 batch 0 train Loss 45.9153 test Loss 21.6071 with MSE metric 17598.1434\n",
      "Epoch 246 batch 10 train Loss 45.9087 test Loss 21.6044 with MSE metric 17596.4817\n",
      "Epoch 246 batch 20 train Loss 45.9021 test Loss 21.6018 with MSE metric 17594.8499\n",
      "Epoch 246 batch 30 train Loss 45.8955 test Loss 21.5991 with MSE metric 17593.2518\n",
      "Epoch 246 batch 40 train Loss 45.8889 test Loss 21.5965 with MSE metric 17591.5719\n",
      "Epoch 246 batch 50 train Loss 45.8823 test Loss 21.5939 with MSE metric 17589.8997\n",
      "Epoch 246 batch 60 train Loss 45.8757 test Loss 21.5913 with MSE metric 17588.2497\n",
      "Epoch 246 batch 70 train Loss 45.8692 test Loss 21.5886 with MSE metric 17586.5707\n",
      "Epoch 246 batch 80 train Loss 45.8626 test Loss 21.5860 with MSE metric 17584.9439\n",
      "Epoch 246 batch 90 train Loss 45.8560 test Loss 21.5834 with MSE metric 17583.3208\n",
      "Epoch 246 batch 100 train Loss 45.8494 test Loss 21.5807 with MSE metric 17581.7492\n",
      "Epoch 246 batch 110 train Loss 45.8428 test Loss 21.5781 with MSE metric 17580.1391\n",
      "Epoch 246 batch 120 train Loss 45.8363 test Loss 21.5755 with MSE metric 17578.5362\n",
      "Epoch 246 batch 130 train Loss 45.8297 test Loss 21.5728 with MSE metric 17576.8728\n",
      "Epoch 246 batch 140 train Loss 45.8231 test Loss 21.5702 with MSE metric 17575.2819\n",
      "Epoch 246 batch 150 train Loss 45.8165 test Loss 21.5676 with MSE metric 17573.6831\n",
      "Epoch 246 batch 160 train Loss 45.8100 test Loss 21.5649 with MSE metric 17572.1074\n",
      "Epoch 246 batch 170 train Loss 45.8034 test Loss 21.5623 with MSE metric 17570.4400\n",
      "Epoch 246 batch 180 train Loss 45.7968 test Loss 21.5597 with MSE metric 17568.8442\n",
      "Epoch 246 batch 190 train Loss 45.7903 test Loss 21.5571 with MSE metric 17567.2775\n",
      "Epoch 246 batch 200 train Loss 45.7837 test Loss 21.5544 with MSE metric 17565.6719\n",
      "Epoch 246 batch 210 train Loss 45.7772 test Loss 21.5518 with MSE metric 17564.1058\n",
      "Epoch 246 batch 220 train Loss 45.7706 test Loss 21.5492 with MSE metric 17562.4097\n",
      "Epoch 246 batch 230 train Loss 45.7641 test Loss 21.5466 with MSE metric 17560.7611\n",
      "Epoch 246 batch 240 train Loss 45.7575 test Loss 21.5440 with MSE metric 17559.1359\n",
      "Time taken for 1 epoch: 31.217116832733154 secs\n",
      "\n",
      "Epoch 247 batch 0 train Loss 45.7510 test Loss 21.5413 with MSE metric 17557.5556\n",
      "Epoch 247 batch 10 train Loss 45.7444 test Loss 21.5387 with MSE metric 17555.9909\n",
      "Epoch 247 batch 20 train Loss 45.7379 test Loss 21.5361 with MSE metric 17554.4283\n",
      "Epoch 247 batch 30 train Loss 45.7313 test Loss 21.5335 with MSE metric 17552.7935\n",
      "Epoch 247 batch 40 train Loss 45.7248 test Loss 21.5309 with MSE metric 17551.1759\n",
      "Epoch 247 batch 50 train Loss 45.7182 test Loss 21.5283 with MSE metric 17549.5182\n",
      "Epoch 247 batch 60 train Loss 45.7117 test Loss 21.5257 with MSE metric 17547.9469\n",
      "Epoch 247 batch 70 train Loss 45.7052 test Loss 21.5231 with MSE metric 17546.3413\n",
      "Epoch 247 batch 80 train Loss 45.6986 test Loss 21.5204 with MSE metric 17544.7244\n",
      "Epoch 247 batch 90 train Loss 45.6921 test Loss 21.5178 with MSE metric 17543.0906\n",
      "Epoch 247 batch 100 train Loss 45.6856 test Loss 21.5152 with MSE metric 17541.5450\n",
      "Epoch 247 batch 110 train Loss 45.6791 test Loss 21.5126 with MSE metric 17539.9998\n",
      "Epoch 247 batch 120 train Loss 45.6726 test Loss 21.5100 with MSE metric 17538.4223\n",
      "Epoch 247 batch 130 train Loss 45.6660 test Loss 21.5074 with MSE metric 17536.8457\n",
      "Epoch 247 batch 140 train Loss 45.6595 test Loss 21.5048 with MSE metric 17535.2706\n",
      "Epoch 247 batch 150 train Loss 45.6530 test Loss 21.5022 with MSE metric 17533.6823\n",
      "Epoch 247 batch 160 train Loss 45.6465 test Loss 21.4995 with MSE metric 17532.1074\n",
      "Epoch 247 batch 170 train Loss 45.6400 test Loss 21.4969 with MSE metric 17530.5202\n",
      "Epoch 247 batch 180 train Loss 45.6335 test Loss 21.4943 with MSE metric 17528.8960\n",
      "Epoch 247 batch 190 train Loss 45.6270 test Loss 21.4917 with MSE metric 17527.3048\n",
      "Epoch 247 batch 200 train Loss 45.6204 test Loss 21.4891 with MSE metric 17525.7189\n",
      "Epoch 247 batch 210 train Loss 45.6139 test Loss 21.4865 with MSE metric 17524.1338\n",
      "Epoch 247 batch 220 train Loss 45.6074 test Loss 21.4839 with MSE metric 17522.5663\n",
      "Epoch 247 batch 230 train Loss 45.6009 test Loss 21.4813 with MSE metric 17520.9456\n",
      "Epoch 247 batch 240 train Loss 45.5944 test Loss 21.4788 with MSE metric 17519.2674\n",
      "Time taken for 1 epoch: 29.949848175048828 secs\n",
      "\n",
      "Epoch 248 batch 0 train Loss 45.5879 test Loss 21.4762 with MSE metric 17517.6594\n",
      "Epoch 248 batch 10 train Loss 45.5814 test Loss 21.4736 with MSE metric 17516.0640\n",
      "Epoch 248 batch 20 train Loss 45.5749 test Loss 21.4710 with MSE metric 17514.4883\n",
      "Epoch 248 batch 30 train Loss 45.5685 test Loss 21.4684 with MSE metric 17512.9008\n",
      "Epoch 248 batch 40 train Loss 45.5620 test Loss 21.4658 with MSE metric 17511.3269\n",
      "Epoch 248 batch 50 train Loss 45.5555 test Loss 21.4632 with MSE metric 17509.7058\n",
      "Epoch 248 batch 60 train Loss 45.5490 test Loss 21.4606 with MSE metric 17508.1071\n",
      "Epoch 248 batch 70 train Loss 45.5425 test Loss 21.4580 with MSE metric 17506.5407\n",
      "Epoch 248 batch 80 train Loss 45.5360 test Loss 21.4554 with MSE metric 17504.9658\n",
      "Epoch 248 batch 90 train Loss 45.5296 test Loss 21.4528 with MSE metric 17503.4175\n",
      "Epoch 248 batch 100 train Loss 45.5231 test Loss 21.4502 with MSE metric 17501.8517\n",
      "Epoch 248 batch 110 train Loss 45.5166 test Loss 21.4476 with MSE metric 17500.2695\n",
      "Epoch 248 batch 120 train Loss 45.5101 test Loss 21.4451 with MSE metric 17498.6999\n",
      "Epoch 248 batch 130 train Loss 45.5037 test Loss 21.4425 with MSE metric 17497.1227\n",
      "Epoch 248 batch 140 train Loss 45.4972 test Loss 21.4399 with MSE metric 17495.4967\n",
      "Epoch 248 batch 150 train Loss 45.4907 test Loss 21.4373 with MSE metric 17493.8874\n",
      "Epoch 248 batch 160 train Loss 45.4843 test Loss 21.4347 with MSE metric 17492.3819\n",
      "Epoch 248 batch 170 train Loss 45.4778 test Loss 21.4321 with MSE metric 17490.8015\n",
      "Epoch 248 batch 180 train Loss 45.4714 test Loss 21.4296 with MSE metric 17489.1677\n",
      "Epoch 248 batch 190 train Loss 45.4649 test Loss 21.4270 with MSE metric 17487.5851\n",
      "Epoch 248 batch 200 train Loss 45.4584 test Loss 21.4244 with MSE metric 17486.0530\n",
      "Epoch 248 batch 210 train Loss 45.4520 test Loss 21.4218 with MSE metric 17484.4749\n",
      "Epoch 248 batch 220 train Loss 45.4455 test Loss 21.4193 with MSE metric 17482.8679\n",
      "Epoch 248 batch 230 train Loss 45.4391 test Loss 21.4167 with MSE metric 17481.2893\n",
      "Epoch 248 batch 240 train Loss 45.4326 test Loss 21.4141 with MSE metric 17479.7098\n",
      "Time taken for 1 epoch: 27.687521934509277 secs\n",
      "\n",
      "Epoch 249 batch 0 train Loss 45.4262 test Loss 21.4115 with MSE metric 17478.1159\n",
      "Epoch 249 batch 10 train Loss 45.4198 test Loss 21.4089 with MSE metric 17476.5294\n",
      "Epoch 249 batch 20 train Loss 45.4133 test Loss 21.4064 with MSE metric 17474.9329\n",
      "Epoch 249 batch 30 train Loss 45.4069 test Loss 21.4038 with MSE metric 17473.3775\n",
      "Epoch 249 batch 40 train Loss 45.4004 test Loss 21.4012 with MSE metric 17471.8257\n",
      "Epoch 249 batch 50 train Loss 45.3940 test Loss 21.3987 with MSE metric 17470.0951\n",
      "Epoch 249 batch 60 train Loss 45.3876 test Loss 21.3961 with MSE metric 17468.5308\n",
      "Epoch 249 batch 70 train Loss 45.3811 test Loss 21.3935 with MSE metric 17466.9706\n",
      "Epoch 249 batch 80 train Loss 45.3747 test Loss 21.3910 with MSE metric 17465.4119\n",
      "Epoch 249 batch 90 train Loss 45.3683 test Loss 21.3884 with MSE metric 17463.8469\n",
      "Epoch 249 batch 100 train Loss 45.3619 test Loss 21.3858 with MSE metric 17462.2418\n",
      "Epoch 249 batch 110 train Loss 45.3554 test Loss 21.3832 with MSE metric 17460.6834\n",
      "Epoch 249 batch 120 train Loss 45.3490 test Loss 21.3807 with MSE metric 17459.0609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249 batch 130 train Loss 45.3426 test Loss 21.3781 with MSE metric 17457.5049\n",
      "Epoch 249 batch 140 train Loss 45.3362 test Loss 21.3756 with MSE metric 17455.9776\n",
      "Epoch 249 batch 150 train Loss 45.3298 test Loss 21.3730 with MSE metric 17454.3847\n",
      "Epoch 249 batch 160 train Loss 45.3234 test Loss 21.3705 with MSE metric 17452.7957\n",
      "Epoch 249 batch 170 train Loss 45.3169 test Loss 21.3679 with MSE metric 17451.1789\n",
      "Epoch 249 batch 180 train Loss 45.3105 test Loss 21.3653 with MSE metric 17449.6354\n",
      "Epoch 249 batch 190 train Loss 45.3041 test Loss 21.3628 with MSE metric 17448.0767\n",
      "Epoch 249 batch 200 train Loss 45.2977 test Loss 21.3602 with MSE metric 17446.4880\n",
      "Epoch 249 batch 210 train Loss 45.2913 test Loss 21.3576 with MSE metric 17444.8587\n",
      "Epoch 249 batch 220 train Loss 45.2849 test Loss 21.3551 with MSE metric 17443.2754\n",
      "Epoch 249 batch 230 train Loss 45.2785 test Loss 21.3525 with MSE metric 17441.7247\n",
      "Epoch 249 batch 240 train Loss 45.2721 test Loss 21.3500 with MSE metric 17440.1750\n",
      "Time taken for 1 epoch: 27.508673191070557 secs\n",
      "\n",
      "Epoch 250 batch 0 train Loss 45.2657 test Loss 21.3474 with MSE metric 17438.5717\n",
      "Epoch 250 batch 10 train Loss 45.2593 test Loss 21.3448 with MSE metric 17436.9903\n",
      "Epoch 250 batch 20 train Loss 45.2530 test Loss 21.3423 with MSE metric 17435.4322\n",
      "Epoch 250 batch 30 train Loss 45.2466 test Loss 21.3397 with MSE metric 17433.8559\n",
      "Epoch 250 batch 40 train Loss 45.2402 test Loss 21.3372 with MSE metric 17432.2460\n",
      "Epoch 250 batch 50 train Loss 45.2338 test Loss 21.3346 with MSE metric 17430.6571\n",
      "Epoch 250 batch 60 train Loss 45.2274 test Loss 21.3321 with MSE metric 17429.1863\n",
      "Epoch 250 batch 70 train Loss 45.2210 test Loss 21.3295 with MSE metric 17427.6208\n",
      "Epoch 250 batch 80 train Loss 45.2147 test Loss 21.3270 with MSE metric 17426.0620\n",
      "Epoch 250 batch 90 train Loss 45.2083 test Loss 21.3245 with MSE metric 17424.5064\n",
      "Epoch 250 batch 100 train Loss 45.2019 test Loss 21.3219 with MSE metric 17422.9819\n",
      "Epoch 250 batch 110 train Loss 45.1956 test Loss 21.3193 with MSE metric 17421.3970\n",
      "Epoch 250 batch 120 train Loss 45.1892 test Loss 21.3168 with MSE metric 17419.7856\n",
      "Epoch 250 batch 130 train Loss 45.1828 test Loss 21.3143 with MSE metric 17418.2539\n",
      "Epoch 250 batch 140 train Loss 45.1765 test Loss 21.3117 with MSE metric 17416.6733\n",
      "Epoch 250 batch 150 train Loss 45.1701 test Loss 21.3092 with MSE metric 17415.1002\n",
      "Epoch 250 batch 160 train Loss 45.1637 test Loss 21.3066 with MSE metric 17413.5217\n",
      "Epoch 250 batch 170 train Loss 45.1574 test Loss 21.3041 with MSE metric 17411.9224\n",
      "Epoch 250 batch 180 train Loss 45.1510 test Loss 21.3016 with MSE metric 17410.3327\n",
      "Epoch 250 batch 190 train Loss 45.1447 test Loss 21.2990 with MSE metric 17408.8085\n",
      "Epoch 250 batch 200 train Loss 45.1383 test Loss 21.2965 with MSE metric 17407.2147\n",
      "Epoch 250 batch 210 train Loss 45.1320 test Loss 21.2939 with MSE metric 17405.6520\n",
      "Epoch 250 batch 220 train Loss 45.1256 test Loss 21.2914 with MSE metric 17404.1267\n",
      "Epoch 250 batch 230 train Loss 45.1193 test Loss 21.2888 with MSE metric 17402.5718\n",
      "Epoch 250 batch 240 train Loss 45.1129 test Loss 21.2863 with MSE metric 17400.9836\n",
      "Time taken for 1 epoch: 31.14138913154602 secs\n",
      "\n",
      "Epoch 251 batch 0 train Loss 45.1066 test Loss 21.2838 with MSE metric 17399.4248\n",
      "Epoch 251 batch 10 train Loss 45.1002 test Loss 21.2813 with MSE metric 17397.7744\n",
      "Epoch 251 batch 20 train Loss 45.0939 test Loss 21.2787 with MSE metric 17396.2303\n",
      "Epoch 251 batch 30 train Loss 45.0876 test Loss 21.2762 with MSE metric 17394.6667\n",
      "Epoch 251 batch 40 train Loss 45.0812 test Loss 21.2737 with MSE metric 17393.0661\n",
      "Epoch 251 batch 50 train Loss 45.0749 test Loss 21.2711 with MSE metric 17391.5867\n",
      "Epoch 251 batch 60 train Loss 45.0686 test Loss 21.2686 with MSE metric 17390.0549\n",
      "Epoch 251 batch 70 train Loss 45.0622 test Loss 21.2661 with MSE metric 17388.5290\n",
      "Epoch 251 batch 80 train Loss 45.0559 test Loss 21.2635 with MSE metric 17386.9911\n",
      "Epoch 251 batch 90 train Loss 45.0496 test Loss 21.2610 with MSE metric 17385.4193\n",
      "Epoch 251 batch 100 train Loss 45.0433 test Loss 21.2584 with MSE metric 17383.8843\n",
      "Epoch 251 batch 110 train Loss 45.0370 test Loss 21.2559 with MSE metric 17382.3448\n",
      "Epoch 251 batch 120 train Loss 45.0306 test Loss 21.2534 with MSE metric 17380.7328\n",
      "Epoch 251 batch 130 train Loss 45.0243 test Loss 21.2509 with MSE metric 17379.1757\n",
      "Epoch 251 batch 140 train Loss 45.0180 test Loss 21.2484 with MSE metric 17377.6717\n",
      "Epoch 251 batch 150 train Loss 45.0117 test Loss 21.2459 with MSE metric 17376.1686\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s=32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 10 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s= 32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
