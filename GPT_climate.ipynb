{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, data_combine, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, t, token = data_combine.climate_data_to_model_input('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Initializing from scratch.\n",
      "Epoch 0 batch 0 train Loss 276635.7271 test Loss 92122.0270 with MSE metric 45551.5664\n",
      "Epoch 0 batch 10 train Loss 147707.3831 test Loss 46122.4647 with MSE metric 46528.5883\n",
      "Epoch 0 batch 20 train Loss 88084.5578 test Loss 30754.6986 with MSE metric 46206.7365\n",
      "Epoch 0 batch 30 train Loss 62429.0003 test Loss 23068.2816 with MSE metric 45921.8479\n",
      "Epoch 0 batch 40 train Loss 48274.3682 test Loss 18456.1269 with MSE metric 46211.5025\n",
      "Epoch 0 batch 50 train Loss 39442.4337 test Loss 15381.2496 with MSE metric 46455.9027\n",
      "Epoch 0 batch 60 train Loss 33508.3575 test Loss 13184.8707 with MSE metric 46346.1084\n",
      "Epoch 0 batch 70 train Loss 29167.5388 test Loss 11537.5780 with MSE metric 46335.6242\n",
      "Epoch 0 batch 80 train Loss 25797.9345 test Loss 10256.3357 with MSE metric 46273.0657\n",
      "Epoch 0 batch 90 train Loss 23146.2318 test Loss 9231.3473 with MSE metric 46227.9679\n",
      "Epoch 0 batch 100 train Loss 21060.0951 test Loss 8392.7229 with MSE metric 46165.1620\n",
      "Epoch 0 batch 110 train Loss 19309.1080 test Loss 7693.8508 with MSE metric 46050.8190\n",
      "Epoch 0 batch 120 train Loss 17813.1833 test Loss 7102.5077 with MSE metric 46037.1066\n",
      "Epoch 0 batch 130 train Loss 16545.6666 test Loss 6595.6374 with MSE metric 45987.3798\n",
      "Epoch 0 batch 140 train Loss 15436.1051 test Loss 6156.3540 with MSE metric 45965.9638\n",
      "Epoch 0 batch 150 train Loss 14490.9189 test Loss 5771.9818 with MSE metric 45837.2313\n",
      "Epoch 0 batch 160 train Loss 13648.2320 test Loss 5432.8268 with MSE metric 45738.6189\n",
      "Epoch 0 batch 170 train Loss 12894.9693 test Loss 5131.3574 with MSE metric 45722.8218\n",
      "Epoch 0 batch 180 train Loss 12231.2676 test Loss 4861.6208 with MSE metric 45704.8512\n",
      "Epoch 0 batch 190 train Loss 11666.5050 test Loss 4618.8589 with MSE metric 45681.4122\n",
      "Epoch 0 batch 200 train Loss 11118.5316 test Loss 4399.2165 with MSE metric 45638.4006\n",
      "Epoch 0 batch 210 train Loss 10623.2994 test Loss 4199.5465 with MSE metric 45616.0264\n",
      "Epoch 0 batch 220 train Loss 10173.6591 test Loss 4017.2419 with MSE metric 45623.0627\n",
      "Epoch 0 batch 230 train Loss 9873.5946 test Loss 3850.1275 with MSE metric 45636.4663\n",
      "Epoch 0 batch 240 train Loss 9480.7455 test Loss 3696.3865 with MSE metric 45682.9595\n",
      "Time taken for 1 epoch: 29.80279779434204 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 9124.2280 test Loss 3554.4753 with MSE metric 45660.4712\n",
      "Epoch 1 batch 10 train Loss 8791.8516 test Loss 3423.0775 with MSE metric 45631.3107\n",
      "Epoch 1 batch 20 train Loss 8483.4931 test Loss 3301.0655 with MSE metric 45624.8822\n",
      "Epoch 1 batch 30 train Loss 8197.5235 test Loss 3187.4693 with MSE metric 45595.5737\n",
      "Epoch 1 batch 40 train Loss 7926.3725 test Loss 3081.4480 with MSE metric 45558.1109\n",
      "Epoch 1 batch 50 train Loss 7672.9370 test Loss 2982.2665 with MSE metric 45564.8212\n",
      "Epoch 1 batch 60 train Loss 7435.2876 test Loss 2889.2848 with MSE metric 45548.5692\n",
      "Epoch 1 batch 70 train Loss 7216.6531 test Loss 2801.9395 with MSE metric 45565.6623\n",
      "Epoch 1 batch 80 train Loss 7013.3297 test Loss 2719.7330 with MSE metric 45543.4595\n",
      "Epoch 1 batch 90 train Loss 6815.8027 test Loss 2642.2251 with MSE metric 45544.6597\n",
      "Epoch 1 batch 100 train Loss 6630.0385 test Loss 2569.0271 with MSE metric 45511.0475\n",
      "Epoch 1 batch 110 train Loss 6454.1176 test Loss 2499.7850 with MSE metric 45552.1871\n",
      "Epoch 1 batch 120 train Loss 6286.4269 test Loss 2434.1891 with MSE metric 45541.7238\n",
      "Epoch 1 batch 130 train Loss 6127.5972 test Loss 2371.9559 with MSE metric 45516.7344\n",
      "Epoch 1 batch 140 train Loss 5977.9277 test Loss 2312.8366 with MSE metric 45525.2810\n",
      "Epoch 1 batch 150 train Loss 5834.2226 test Loss 2256.6018 with MSE metric 45501.9900\n",
      "Epoch 1 batch 160 train Loss 5696.9706 test Loss 2203.0444 with MSE metric 45504.5150\n",
      "Epoch 1 batch 170 train Loss 5567.8414 test Loss 2151.9788 with MSE metric 45511.1050\n",
      "Epoch 1 batch 180 train Loss 5444.3873 test Loss 2103.2348 with MSE metric 45505.7568\n",
      "Epoch 1 batch 190 train Loss 5327.1111 test Loss 2056.6562 with MSE metric 45486.8465\n",
      "Epoch 1 batch 200 train Loss 5215.3255 test Loss 2012.1051 with MSE metric 45456.5968\n",
      "Epoch 1 batch 210 train Loss 5107.2656 test Loss 1969.4498 with MSE metric 45438.7616\n",
      "Epoch 1 batch 220 train Loss 5004.6703 test Loss 1928.5730 with MSE metric 45416.6755\n",
      "Epoch 1 batch 230 train Loss 4904.7037 test Loss 1889.3640 with MSE metric 45411.7511\n",
      "Epoch 1 batch 240 train Loss 4808.9931 test Loss 1851.7242 with MSE metric 45396.3384\n",
      "Time taken for 1 epoch: 30.204960107803345 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 4715.8155 test Loss 1815.5617 with MSE metric 45372.3463\n",
      "Epoch 2 batch 10 train Loss 4626.4286 test Loss 1780.7894 with MSE metric 45407.1597\n",
      "Epoch 2 batch 20 train Loss 4540.6385 test Loss 1747.3304 with MSE metric 45395.9634\n",
      "Epoch 2 batch 30 train Loss 4458.9864 test Loss 1715.1112 with MSE metric 45368.7647\n",
      "Epoch 2 batch 40 train Loss 4378.9445 test Loss 1684.0633 with MSE metric 45358.9707\n",
      "Epoch 2 batch 50 train Loss 4301.6845 test Loss 1654.1259 with MSE metric 45358.0889\n",
      "Epoch 2 batch 60 train Loss 4226.7513 test Loss 1625.2389 with MSE metric 45356.1640\n",
      "Epoch 2 batch 70 train Loss 4155.5887 test Loss 1597.3483 with MSE metric 45337.4026\n",
      "Epoch 2 batch 80 train Loss 4085.8513 test Loss 1570.4030 with MSE metric 45320.4106\n",
      "Epoch 2 batch 90 train Loss 4018.8824 test Loss 1544.3560 with MSE metric 45301.5502\n",
      "Epoch 2 batch 100 train Loss 3953.5983 test Loss 1519.1634 with MSE metric 45312.4614\n",
      "Epoch 2 batch 110 train Loss 3907.1685 test Loss 1494.7844 with MSE metric 45320.0233\n",
      "Epoch 2 batch 120 train Loss 3846.0601 test Loss 1471.1793 with MSE metric 45321.6546\n",
      "Epoch 2 batch 130 train Loss 3786.8848 test Loss 1448.3114 with MSE metric 45310.0291\n",
      "Epoch 2 batch 140 train Loss 3730.0600 test Loss 1426.1475 with MSE metric 45307.0465\n",
      "Epoch 2 batch 150 train Loss 3674.7688 test Loss 1404.6558 with MSE metric 45301.0975\n",
      "Epoch 2 batch 160 train Loss 3622.3668 test Loss 1383.8053 with MSE metric 45284.5422\n",
      "Epoch 2 batch 170 train Loss 3570.4887 test Loss 1363.5687 with MSE metric 45264.1534\n",
      "Epoch 2 batch 180 train Loss 3519.4906 test Loss 1343.9183 with MSE metric 45255.2268\n",
      "Epoch 2 batch 190 train Loss 3469.9788 test Loss 1324.8305 with MSE metric 45242.3446\n",
      "Epoch 2 batch 200 train Loss 3421.7977 test Loss 1306.2798 with MSE metric 45234.8210\n",
      "Epoch 2 batch 210 train Loss 3374.9198 test Loss 1288.2458 with MSE metric 45216.6375\n",
      "Epoch 2 batch 220 train Loss 3329.2466 test Loss 1270.7057 with MSE metric 45224.7367\n",
      "Epoch 2 batch 230 train Loss 3290.2829 test Loss 1253.6399 with MSE metric 45212.4944\n",
      "Epoch 2 batch 240 train Loss 3247.0155 test Loss 1237.0290 with MSE metric 45213.0627\n",
      "Time taken for 1 epoch: 31.725919008255005 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 3204.6864 test Loss 1220.8559 with MSE metric 45215.3255\n",
      "Epoch 3 batch 10 train Loss 3164.4331 test Loss 1205.1031 with MSE metric 45203.2527\n",
      "Epoch 3 batch 20 train Loss 3124.5183 test Loss 1189.7540 with MSE metric 45187.2403\n",
      "Epoch 3 batch 30 train Loss 3085.7194 test Loss 1174.7932 with MSE metric 45186.5168\n",
      "Epoch 3 batch 40 train Loss 3047.8729 test Loss 1160.2082 with MSE metric 45193.8979\n",
      "Epoch 3 batch 50 train Loss 3010.8848 test Loss 1145.9832 with MSE metric 45201.4190\n",
      "Epoch 3 batch 60 train Loss 2974.5083 test Loss 1132.1055 with MSE metric 45197.5738\n",
      "Epoch 3 batch 70 train Loss 2939.4955 test Loss 1118.5618 with MSE metric 45179.5291\n",
      "Epoch 3 batch 80 train Loss 2904.8209 test Loss 1105.3416 with MSE metric 45177.5222\n",
      "Epoch 3 batch 90 train Loss 2871.3609 test Loss 1092.4312 with MSE metric 45154.4694\n",
      "Epoch 3 batch 100 train Loss 2838.4304 test Loss 1079.8221 with MSE metric 45135.9870\n",
      "Epoch 3 batch 110 train Loss 2806.0842 test Loss 1067.5027 with MSE metric 45127.8282\n",
      "Epoch 3 batch 120 train Loss 2774.9793 test Loss 1055.4634 with MSE metric 45137.2418\n",
      "Epoch 3 batch 130 train Loss 2744.1656 test Loss 1043.6953 with MSE metric 45140.2590\n",
      "Epoch 3 batch 140 train Loss 2714.0698 test Loss 1032.1884 with MSE metric 45125.9614\n",
      "Epoch 3 batch 150 train Loss 2684.6101 test Loss 1020.9345 with MSE metric 45127.0517\n",
      "Epoch 3 batch 160 train Loss 2655.7088 test Loss 1009.9257 with MSE metric 45106.0034\n",
      "Epoch 3 batch 170 train Loss 2627.6455 test Loss 999.1540 with MSE metric 45100.1974\n",
      "Epoch 3 batch 180 train Loss 2600.1196 test Loss 988.6107 with MSE metric 45096.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 batch 190 train Loss 2573.7582 test Loss 978.2907 with MSE metric 45097.1019\n",
      "Epoch 3 batch 200 train Loss 2547.3261 test Loss 968.1856 with MSE metric 45086.2424\n",
      "Epoch 3 batch 210 train Loss 2521.5919 test Loss 958.2892 with MSE metric 45083.5910\n",
      "Epoch 3 batch 220 train Loss 2496.2625 test Loss 948.5948 with MSE metric 45077.8469\n",
      "Epoch 3 batch 230 train Loss 2471.7906 test Loss 939.0961 with MSE metric 45075.7935\n",
      "Epoch 3 batch 240 train Loss 2447.3417 test Loss 929.7878 with MSE metric 45074.1080\n",
      "Time taken for 1 epoch: 30.357666015625 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 2423.4229 test Loss 920.6635 with MSE metric 45070.6918\n",
      "Epoch 4 batch 10 train Loss 2399.9280 test Loss 911.7182 with MSE metric 45057.1591\n",
      "Epoch 4 batch 20 train Loss 2376.9605 test Loss 902.9480 with MSE metric 45060.1760\n",
      "Epoch 4 batch 30 train Loss 2354.4834 test Loss 894.3457 with MSE metric 45053.5382\n",
      "Epoch 4 batch 40 train Loss 2332.3808 test Loss 885.9081 with MSE metric 45056.1516\n",
      "Epoch 4 batch 50 train Loss 2310.8005 test Loss 877.6294 with MSE metric 45048.3208\n",
      "Epoch 4 batch 60 train Loss 2289.5329 test Loss 869.5059 with MSE metric 45050.3329\n",
      "Epoch 4 batch 70 train Loss 2268.5803 test Loss 861.5328 with MSE metric 45049.0500\n",
      "Epoch 4 batch 80 train Loss 2248.0512 test Loss 853.7055 with MSE metric 45050.0251\n",
      "Epoch 4 batch 90 train Loss 2228.1946 test Loss 846.0206 with MSE metric 45040.2955\n",
      "Epoch 4 batch 100 train Loss 2208.3677 test Loss 838.4739 with MSE metric 45038.7982\n",
      "Epoch 4 batch 110 train Loss 2189.0255 test Loss 831.0628 with MSE metric 45039.2967\n",
      "Epoch 4 batch 120 train Loss 2169.8542 test Loss 823.7827 with MSE metric 45040.0957\n",
      "Epoch 4 batch 130 train Loss 2151.1224 test Loss 816.6308 with MSE metric 45031.2315\n",
      "Epoch 4 batch 140 train Loss 2132.6633 test Loss 809.6035 with MSE metric 45028.1465\n",
      "Epoch 4 batch 150 train Loss 2114.6679 test Loss 802.6972 with MSE metric 45029.7922\n",
      "Epoch 4 batch 160 train Loss 2096.8272 test Loss 795.9090 with MSE metric 45009.7464\n",
      "Epoch 4 batch 170 train Loss 2079.2895 test Loss 789.2361 with MSE metric 45003.7349\n",
      "Epoch 4 batch 180 train Loss 2062.1005 test Loss 782.6755 with MSE metric 45006.3125\n",
      "Epoch 4 batch 190 train Loss 2045.0445 test Loss 776.2245 with MSE metric 45009.7786\n",
      "Epoch 4 batch 200 train Loss 2028.3304 test Loss 769.8799 with MSE metric 45013.1025\n",
      "Epoch 4 batch 210 train Loss 2012.2038 test Loss 763.6393 with MSE metric 45015.9939\n",
      "Epoch 4 batch 220 train Loss 1996.0434 test Loss 757.4997 with MSE metric 45015.8314\n",
      "Epoch 4 batch 230 train Loss 1980.1839 test Loss 751.4598 with MSE metric 45021.4358\n",
      "Epoch 4 batch 240 train Loss 1964.5745 test Loss 745.5168 with MSE metric 45019.8027\n",
      "Time taken for 1 epoch: 26.902130365371704 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 1949.3237 test Loss 739.6682 with MSE metric 45015.0101\n",
      "Epoch 5 batch 10 train Loss 1934.1405 test Loss 733.9112 with MSE metric 45005.8931\n",
      "Epoch 5 batch 20 train Loss 1919.2996 test Loss 728.2447 with MSE metric 44992.6769\n",
      "Epoch 5 batch 30 train Loss 1904.5790 test Loss 722.6661 with MSE metric 44985.9718\n",
      "Epoch 5 batch 40 train Loss 1890.1015 test Loss 717.1732 with MSE metric 44980.7272\n",
      "Epoch 5 batch 50 train Loss 1875.8885 test Loss 711.7641 with MSE metric 44976.6084\n",
      "Epoch 5 batch 60 train Loss 1861.7850 test Loss 706.4370 with MSE metric 44972.1892\n",
      "Epoch 5 batch 70 train Loss 1848.0188 test Loss 701.1902 with MSE metric 44966.2768\n",
      "Epoch 5 batch 80 train Loss 1834.3533 test Loss 696.0219 with MSE metric 44961.8813\n",
      "Epoch 5 batch 90 train Loss 1820.9810 test Loss 690.9305 with MSE metric 44958.3249\n",
      "Epoch 5 batch 100 train Loss 1807.7856 test Loss 685.9142 with MSE metric 44957.5026\n",
      "Epoch 5 batch 110 train Loss 1794.7100 test Loss 680.9711 with MSE metric 44954.3753\n",
      "Epoch 5 batch 120 train Loss 1781.8545 test Loss 676.0997 with MSE metric 44950.0262\n",
      "Epoch 5 batch 130 train Loss 1769.5992 test Loss 671.2974 with MSE metric 44953.6941\n",
      "Epoch 5 batch 140 train Loss 1757.0611 test Loss 666.5647 with MSE metric 44950.4413\n",
      "Epoch 5 batch 150 train Loss 1744.7342 test Loss 661.8990 with MSE metric 44945.1746\n",
      "Epoch 5 batch 160 train Loss 1732.5453 test Loss 657.2992 with MSE metric 44942.7267\n",
      "Epoch 5 batch 170 train Loss 1720.5524 test Loss 652.7641 with MSE metric 44939.7389\n",
      "Epoch 5 batch 180 train Loss 1708.8043 test Loss 648.2915 with MSE metric 44932.7510\n",
      "Epoch 5 batch 190 train Loss 1697.1367 test Loss 643.8807 with MSE metric 44930.0002\n",
      "Epoch 5 batch 200 train Loss 1685.6918 test Loss 639.5310 with MSE metric 44923.7291\n",
      "Epoch 5 batch 210 train Loss 1674.3916 test Loss 635.2398 with MSE metric 44923.1230\n",
      "Epoch 5 batch 220 train Loss 1663.1726 test Loss 631.0065 with MSE metric 44920.7822\n",
      "Epoch 5 batch 230 train Loss 1652.1218 test Loss 626.8302 with MSE metric 44918.4070\n",
      "Epoch 5 batch 240 train Loss 1641.2626 test Loss 622.7098 with MSE metric 44907.1517\n",
      "Time taken for 1 epoch: 26.943596124649048 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 1630.5116 test Loss 618.6443 with MSE metric 44908.3888\n",
      "Epoch 6 batch 10 train Loss 1619.9084 test Loss 614.6322 with MSE metric 44907.9875\n",
      "Epoch 6 batch 20 train Loss 1609.4370 test Loss 610.6725 with MSE metric 44904.9789\n",
      "Epoch 6 batch 30 train Loss 1599.1753 test Loss 606.7645 with MSE metric 44898.2220\n",
      "Epoch 6 batch 40 train Loss 1589.4027 test Loss 602.9062 with MSE metric 44898.4453\n",
      "Epoch 6 batch 50 train Loss 1579.8025 test Loss 599.0980 with MSE metric 44895.3265\n",
      "Epoch 6 batch 60 train Loss 1570.1713 test Loss 595.3385 with MSE metric 44895.1295\n",
      "Epoch 6 batch 70 train Loss 1560.3152 test Loss 591.6264 with MSE metric 44889.9835\n",
      "Epoch 6 batch 80 train Loss 1550.6231 test Loss 587.9608 with MSE metric 44877.7429\n",
      "Epoch 6 batch 90 train Loss 1540.9968 test Loss 584.3415 with MSE metric 44868.1489\n",
      "Epoch 6 batch 100 train Loss 1531.5152 test Loss 580.7673 with MSE metric 44863.2595\n",
      "Epoch 6 batch 110 train Loss 1522.2312 test Loss 577.2369 with MSE metric 44860.4943\n",
      "Epoch 6 batch 120 train Loss 1512.9760 test Loss 573.7503 with MSE metric 44855.4648\n",
      "Epoch 6 batch 130 train Loss 1503.8501 test Loss 570.3058 with MSE metric 44853.5367\n",
      "Epoch 6 batch 140 train Loss 1494.8289 test Loss 566.9034 with MSE metric 44852.7956\n",
      "Epoch 6 batch 150 train Loss 1485.9243 test Loss 563.5417 with MSE metric 44845.7155\n",
      "Epoch 6 batch 160 train Loss 1477.0828 test Loss 560.2209 with MSE metric 44849.0325\n",
      "Epoch 6 batch 170 train Loss 1468.3799 test Loss 556.9394 with MSE metric 44841.9069\n",
      "Epoch 6 batch 180 train Loss 1459.7514 test Loss 553.6971 with MSE metric 44841.4974\n",
      "Epoch 6 batch 190 train Loss 1451.2333 test Loss 550.4925 with MSE metric 44835.1646\n",
      "Epoch 6 batch 200 train Loss 1442.8199 test Loss 547.3256 with MSE metric 44837.8007\n",
      "Epoch 6 batch 210 train Loss 1434.5088 test Loss 544.1954 with MSE metric 44837.0209\n",
      "Epoch 6 batch 220 train Loss 1426.3004 test Loss 541.1014 with MSE metric 44837.4252\n",
      "Epoch 6 batch 230 train Loss 1418.2052 test Loss 538.0432 with MSE metric 44834.3945\n",
      "Epoch 6 batch 240 train Loss 1410.2322 test Loss 535.0199 with MSE metric 44837.7895\n",
      "Time taken for 1 epoch: 28.42843222618103 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 1402.3128 test Loss 532.0310 with MSE metric 44838.7347\n",
      "Epoch 7 batch 10 train Loss 1394.8938 test Loss 529.0759 with MSE metric 44837.8097\n",
      "Epoch 7 batch 20 train Loss 1387.1896 test Loss 526.1541 with MSE metric 44836.9873\n",
      "Epoch 7 batch 30 train Loss 1379.5406 test Loss 523.2653 with MSE metric 44838.1247\n",
      "Epoch 7 batch 40 train Loss 1371.9384 test Loss 520.4084 with MSE metric 44842.7618\n",
      "Epoch 7 batch 50 train Loss 1364.4487 test Loss 517.5830 with MSE metric 44842.7576\n",
      "Epoch 7 batch 60 train Loss 1357.0505 test Loss 514.7889 with MSE metric 44840.7958\n",
      "Epoch 7 batch 70 train Loss 1349.6931 test Loss 512.0254 with MSE metric 44841.6104\n",
      "Epoch 7 batch 80 train Loss 1342.4208 test Loss 509.2917 with MSE metric 44838.3105\n",
      "Epoch 7 batch 90 train Loss 1335.2635 test Loss 506.5879 with MSE metric 44837.1282\n",
      "Epoch 7 batch 100 train Loss 1328.2923 test Loss 503.9134 with MSE metric 44828.0234\n",
      "Epoch 7 batch 110 train Loss 1321.3661 test Loss 501.2670 with MSE metric 44822.9268\n",
      "Epoch 7 batch 120 train Loss 1314.4068 test Loss 498.6491 with MSE metric 44821.2390\n",
      "Epoch 7 batch 130 train Loss 1307.5278 test Loss 496.0590 with MSE metric 44819.3020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 batch 140 train Loss 1300.7095 test Loss 493.4961 with MSE metric 44818.2943\n",
      "Epoch 7 batch 150 train Loss 1294.0040 test Loss 490.9602 with MSE metric 44817.6849\n",
      "Epoch 7 batch 160 train Loss 1287.3511 test Loss 488.4506 with MSE metric 44813.3637\n",
      "Epoch 7 batch 170 train Loss 1280.7618 test Loss 485.9669 with MSE metric 44810.6646\n",
      "Epoch 7 batch 180 train Loss 1274.3010 test Loss 483.5091 with MSE metric 44811.2059\n",
      "Epoch 7 batch 190 train Loss 1267.8261 test Loss 481.0765 with MSE metric 44810.5494\n",
      "Epoch 7 batch 200 train Loss 1261.4127 test Loss 478.6689 with MSE metric 44809.8589\n",
      "Epoch 7 batch 210 train Loss 1255.0965 test Loss 476.2854 with MSE metric 44813.5021\n",
      "Epoch 7 batch 220 train Loss 1248.8292 test Loss 473.9266 with MSE metric 44812.7825\n",
      "Epoch 7 batch 230 train Loss 1242.6034 test Loss 471.5913 with MSE metric 44808.9969\n",
      "Epoch 7 batch 240 train Loss 1236.4422 test Loss 469.2792 with MSE metric 44808.6132\n",
      "Time taken for 1 epoch: 29.421195030212402 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 1230.3906 test Loss 466.9904 with MSE metric 44808.7149\n",
      "Epoch 8 batch 10 train Loss 1224.4285 test Loss 464.7240 with MSE metric 44806.7802\n",
      "Epoch 8 batch 20 train Loss 1218.6171 test Loss 462.4802 with MSE metric 44807.5621\n",
      "Epoch 8 batch 30 train Loss 1212.7419 test Loss 460.2585 with MSE metric 44804.1905\n",
      "Epoch 8 batch 40 train Loss 1206.9594 test Loss 458.0585 with MSE metric 44804.8762\n",
      "Epoch 8 batch 50 train Loss 1201.1744 test Loss 455.8799 with MSE metric 44804.1438\n",
      "Epoch 8 batch 60 train Loss 1195.4953 test Loss 453.7222 with MSE metric 44800.6782\n",
      "Epoch 8 batch 70 train Loss 1189.8997 test Loss 451.5857 with MSE metric 44800.6376\n",
      "Epoch 8 batch 80 train Loss 1184.2786 test Loss 449.4694 with MSE metric 44794.7038\n",
      "Epoch 8 batch 90 train Loss 1178.7679 test Loss 447.3732 with MSE metric 44790.2379\n",
      "Epoch 8 batch 100 train Loss 1173.2369 test Loss 445.2970 with MSE metric 44784.2181\n",
      "Epoch 8 batch 110 train Loss 1167.7877 test Loss 443.2406 with MSE metric 44779.5893\n",
      "Epoch 8 batch 120 train Loss 1162.3806 test Loss 441.2032 with MSE metric 44783.8443\n",
      "Epoch 8 batch 130 train Loss 1156.9951 test Loss 439.1849 with MSE metric 44780.4207\n",
      "Epoch 8 batch 140 train Loss 1151.6675 test Loss 437.1854 with MSE metric 44780.1581\n",
      "Epoch 8 batch 150 train Loss 1146.3919 test Loss 435.2046 with MSE metric 44781.6780\n",
      "Epoch 8 batch 160 train Loss 1141.1671 test Loss 433.2420 with MSE metric 44780.8104\n",
      "Epoch 8 batch 170 train Loss 1135.9960 test Loss 431.2975 with MSE metric 44782.0080\n",
      "Epoch 8 batch 180 train Loss 1130.8632 test Loss 429.3708 with MSE metric 44782.8237\n",
      "Epoch 8 batch 190 train Loss 1125.7662 test Loss 427.4617 with MSE metric 44784.3824\n",
      "Epoch 8 batch 200 train Loss 1120.7611 test Loss 425.5697 with MSE metric 44780.5057\n",
      "Epoch 8 batch 210 train Loss 1115.8185 test Loss 423.6950 with MSE metric 44778.1294\n",
      "Epoch 8 batch 220 train Loss 1110.8698 test Loss 421.8371 with MSE metric 44776.9149\n",
      "Epoch 8 batch 230 train Loss 1105.9861 test Loss 419.9959 with MSE metric 44780.6554\n",
      "Epoch 8 batch 240 train Loss 1101.1204 test Loss 418.1710 with MSE metric 44781.0386\n",
      "Time taken for 1 epoch: 28.709784269332886 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 1096.3074 test Loss 416.3621 with MSE metric 44782.9400\n",
      "Epoch 9 batch 10 train Loss 1091.5239 test Loss 414.5694 with MSE metric 44785.1685\n",
      "Epoch 9 batch 20 train Loss 1086.7956 test Loss 412.7924 with MSE metric 44780.8820\n",
      "Epoch 9 batch 30 train Loss 1082.1020 test Loss 411.0307 with MSE metric 44778.6129\n",
      "Epoch 9 batch 40 train Loss 1077.4411 test Loss 409.2844 with MSE metric 44776.2438\n",
      "Epoch 9 batch 50 train Loss 1072.8193 test Loss 407.5533 with MSE metric 44781.0837\n",
      "Epoch 9 batch 60 train Loss 1068.2353 test Loss 405.8372 with MSE metric 44782.0508\n",
      "Epoch 9 batch 70 train Loss 1063.6886 test Loss 404.1360 with MSE metric 44780.8341\n",
      "Epoch 9 batch 80 train Loss 1059.1905 test Loss 402.4494 with MSE metric 44781.4039\n",
      "Epoch 9 batch 90 train Loss 1054.7699 test Loss 400.7771 with MSE metric 44787.7362\n",
      "Epoch 9 batch 100 train Loss 1050.3931 test Loss 399.1191 with MSE metric 44785.3247\n",
      "Epoch 9 batch 110 train Loss 1046.0192 test Loss 397.4750 with MSE metric 44784.5071\n",
      "Epoch 9 batch 120 train Loss 1041.6685 test Loss 395.8446 with MSE metric 44786.4044\n",
      "Epoch 9 batch 130 train Loss 1037.3602 test Loss 394.2278 with MSE metric 44781.6735\n",
      "Epoch 9 batch 140 train Loss 1033.0749 test Loss 392.6245 with MSE metric 44776.6956\n",
      "Epoch 9 batch 150 train Loss 1028.8282 test Loss 391.0346 with MSE metric 44776.2279\n",
      "Epoch 9 batch 160 train Loss 1024.6156 test Loss 389.4578 with MSE metric 44767.4653\n",
      "Epoch 9 batch 170 train Loss 1020.4697 test Loss 387.8941 with MSE metric 44768.6716\n",
      "Epoch 9 batch 180 train Loss 1016.3315 test Loss 386.3434 with MSE metric 44766.8421\n",
      "Epoch 9 batch 190 train Loss 1012.2648 test Loss 384.8051 with MSE metric 44765.5127\n",
      "Epoch 9 batch 200 train Loss 1008.1914 test Loss 383.2797 with MSE metric 44766.9645\n",
      "Epoch 9 batch 210 train Loss 1004.1462 test Loss 381.7665 with MSE metric 44762.0403\n",
      "Epoch 9 batch 220 train Loss 1000.1362 test Loss 380.2652 with MSE metric 44756.9332\n",
      "Epoch 9 batch 230 train Loss 996.1593 test Loss 378.7764 with MSE metric 44755.8938\n",
      "Epoch 9 batch 240 train Loss 992.2195 test Loss 377.2995 with MSE metric 44757.9395\n",
      "Time taken for 1 epoch: 26.26639413833618 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 988.3106 test Loss 375.8343 with MSE metric 44752.6474\n",
      "Epoch 10 batch 10 train Loss 984.4457 test Loss 374.3808 with MSE metric 44752.1910\n",
      "Epoch 10 batch 20 train Loss 980.5911 test Loss 372.9387 with MSE metric 44749.4991\n",
      "Epoch 10 batch 30 train Loss 976.7615 test Loss 371.5080 with MSE metric 44745.4841\n",
      "Epoch 10 batch 40 train Loss 972.9814 test Loss 370.0887 with MSE metric 44743.7243\n",
      "Epoch 10 batch 50 train Loss 969.2190 test Loss 368.6804 with MSE metric 44738.4643\n",
      "Epoch 10 batch 60 train Loss 965.4848 test Loss 367.2831 with MSE metric 44735.0319\n",
      "Epoch 10 batch 70 train Loss 961.7912 test Loss 365.8969 with MSE metric 44737.4210\n",
      "Epoch 10 batch 80 train Loss 958.1134 test Loss 364.5213 with MSE metric 44735.4580\n",
      "Epoch 10 batch 90 train Loss 954.4746 test Loss 363.1560 with MSE metric 44734.0153\n",
      "Epoch 10 batch 100 train Loss 951.0032 test Loss 361.8011 with MSE metric 44734.3537\n",
      "Epoch 10 batch 110 train Loss 947.4074 test Loss 360.4568 with MSE metric 44735.1766\n",
      "Epoch 10 batch 120 train Loss 943.8441 test Loss 359.1229 with MSE metric 44730.9161\n",
      "Epoch 10 batch 130 train Loss 940.3111 test Loss 357.7991 with MSE metric 44727.8943\n",
      "Epoch 10 batch 140 train Loss 936.8113 test Loss 356.4852 with MSE metric 44727.6135\n",
      "Epoch 10 batch 150 train Loss 933.3255 test Loss 355.1812 with MSE metric 44725.6370\n",
      "Epoch 10 batch 160 train Loss 929.8681 test Loss 353.8870 with MSE metric 44724.3893\n",
      "Epoch 10 batch 170 train Loss 926.4301 test Loss 352.6024 with MSE metric 44724.1716\n",
      "Epoch 10 batch 180 train Loss 923.0297 test Loss 351.3273 with MSE metric 44721.5506\n",
      "Epoch 10 batch 190 train Loss 919.7079 test Loss 350.0621 with MSE metric 44718.6745\n",
      "Epoch 10 batch 200 train Loss 916.3466 test Loss 348.8059 with MSE metric 44717.5225\n",
      "Epoch 10 batch 210 train Loss 913.0072 test Loss 347.5592 with MSE metric 44715.3538\n",
      "Epoch 10 batch 220 train Loss 909.6953 test Loss 346.3217 with MSE metric 44713.4314\n",
      "Epoch 10 batch 230 train Loss 906.4107 test Loss 345.0931 with MSE metric 44709.1637\n",
      "Epoch 10 batch 240 train Loss 903.1823 test Loss 343.8733 with MSE metric 44708.9765\n",
      "Time taken for 1 epoch: 28.26937174797058 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 899.9468 test Loss 342.6624 with MSE metric 44709.2745\n",
      "Epoch 11 batch 10 train Loss 896.7564 test Loss 341.4603 with MSE metric 44710.2116\n",
      "Epoch 11 batch 20 train Loss 893.5612 test Loss 340.2668 with MSE metric 44711.2036\n",
      "Epoch 11 batch 30 train Loss 890.3962 test Loss 339.0820 with MSE metric 44711.3188\n",
      "Epoch 11 batch 40 train Loss 887.2443 test Loss 337.9054 with MSE metric 44713.8833\n",
      "Epoch 11 batch 50 train Loss 884.1199 test Loss 336.7374 with MSE metric 44711.1275\n",
      "Epoch 11 batch 60 train Loss 881.0252 test Loss 335.5779 with MSE metric 44708.5708\n",
      "Epoch 11 batch 70 train Loss 877.9422 test Loss 334.4263 with MSE metric 44707.4125\n",
      "Epoch 11 batch 80 train Loss 874.8821 test Loss 333.2829 with MSE metric 44706.8448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 batch 90 train Loss 871.8436 test Loss 332.1477 with MSE metric 44707.9802\n",
      "Epoch 11 batch 100 train Loss 868.8273 test Loss 331.0205 with MSE metric 44706.4711\n",
      "Epoch 11 batch 110 train Loss 865.8288 test Loss 329.9011 with MSE metric 44702.7393\n",
      "Epoch 11 batch 120 train Loss 862.8777 test Loss 328.7893 with MSE metric 44698.5043\n",
      "Epoch 11 batch 130 train Loss 859.9492 test Loss 327.6854 with MSE metric 44698.4619\n",
      "Epoch 11 batch 140 train Loss 857.0172 test Loss 326.5892 with MSE metric 44696.4532\n",
      "Epoch 11 batch 150 train Loss 854.1239 test Loss 325.5003 with MSE metric 44692.4952\n",
      "Epoch 11 batch 160 train Loss 851.2339 test Loss 324.4191 with MSE metric 44690.6223\n",
      "Epoch 11 batch 170 train Loss 848.3575 test Loss 323.3452 with MSE metric 44689.6701\n",
      "Epoch 11 batch 180 train Loss 845.5189 test Loss 322.2788 with MSE metric 44688.6152\n",
      "Epoch 11 batch 190 train Loss 842.7060 test Loss 321.2197 with MSE metric 44688.2442\n",
      "Epoch 11 batch 200 train Loss 839.8990 test Loss 320.1676 with MSE metric 44689.1026\n",
      "Epoch 11 batch 210 train Loss 837.1020 test Loss 319.1224 with MSE metric 44685.8370\n",
      "Epoch 11 batch 220 train Loss 834.3277 test Loss 318.0846 with MSE metric 44685.0183\n",
      "Epoch 11 batch 230 train Loss 831.5774 test Loss 317.0537 with MSE metric 44688.6831\n",
      "Epoch 11 batch 240 train Loss 828.8410 test Loss 316.0297 with MSE metric 44687.6819\n",
      "Time taken for 1 epoch: 27.862637758255005 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 826.1139 test Loss 315.0125 with MSE metric 44686.7473\n",
      "Epoch 12 batch 10 train Loss 823.4161 test Loss 314.0018 with MSE metric 44686.8018\n",
      "Epoch 12 batch 20 train Loss 820.7248 test Loss 312.9979 with MSE metric 44684.6282\n",
      "Epoch 12 batch 30 train Loss 818.0516 test Loss 312.0006 with MSE metric 44682.8346\n",
      "Epoch 12 batch 40 train Loss 815.3965 test Loss 311.0100 with MSE metric 44678.7482\n",
      "Epoch 12 batch 50 train Loss 812.7663 test Loss 310.0258 with MSE metric 44680.0629\n",
      "Epoch 12 batch 60 train Loss 810.1732 test Loss 309.0481 with MSE metric 44682.5152\n",
      "Epoch 12 batch 70 train Loss 807.5722 test Loss 308.0768 with MSE metric 44683.3552\n",
      "Epoch 12 batch 80 train Loss 804.9882 test Loss 307.1117 with MSE metric 44681.0805\n",
      "Epoch 12 batch 90 train Loss 802.4322 test Loss 306.1529 with MSE metric 44679.0264\n",
      "Epoch 12 batch 100 train Loss 799.8885 test Loss 305.2003 with MSE metric 44684.1487\n",
      "Epoch 12 batch 110 train Loss 797.3700 test Loss 304.2537 with MSE metric 44682.2568\n",
      "Epoch 12 batch 120 train Loss 794.8503 test Loss 303.3133 with MSE metric 44679.5334\n",
      "Epoch 12 batch 130 train Loss 792.3456 test Loss 302.3790 with MSE metric 44675.8627\n",
      "Epoch 12 batch 140 train Loss 789.8730 test Loss 301.4506 with MSE metric 44672.1712\n",
      "Epoch 12 batch 150 train Loss 787.4012 test Loss 300.5279 with MSE metric 44672.5554\n",
      "Epoch 12 batch 160 train Loss 784.9648 test Loss 299.6110 with MSE metric 44675.9904\n",
      "Epoch 12 batch 170 train Loss 782.5254 test Loss 298.7000 with MSE metric 44681.6546\n",
      "Epoch 12 batch 180 train Loss 780.1048 test Loss 297.7949 with MSE metric 44680.5267\n",
      "Epoch 12 batch 190 train Loss 777.6986 test Loss 296.8954 with MSE metric 44677.7837\n",
      "Epoch 12 batch 200 train Loss 775.3045 test Loss 296.0015 with MSE metric 44678.0041\n",
      "Epoch 12 batch 210 train Loss 772.9262 test Loss 295.1132 with MSE metric 44675.5806\n",
      "Epoch 12 batch 220 train Loss 770.5724 test Loss 294.2304 with MSE metric 44677.7565\n",
      "Epoch 12 batch 230 train Loss 768.2180 test Loss 293.3529 with MSE metric 44679.8394\n",
      "Epoch 12 batch 240 train Loss 765.8847 test Loss 292.4810 with MSE metric 44678.7118\n",
      "Time taken for 1 epoch: 28.55362367630005 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 763.5676 test Loss 291.6143 with MSE metric 44672.9973\n",
      "Epoch 13 batch 10 train Loss 761.3746 test Loss 290.7530 with MSE metric 44671.4468\n",
      "Epoch 13 batch 20 train Loss 759.0766 test Loss 289.8969 with MSE metric 44671.0782\n",
      "Epoch 13 batch 30 train Loss 756.7954 test Loss 289.0460 with MSE metric 44669.3588\n",
      "Epoch 13 batch 40 train Loss 754.5238 test Loss 288.2005 with MSE metric 44671.2155\n",
      "Epoch 13 batch 50 train Loss 752.2770 test Loss 287.3601 with MSE metric 44674.3512\n",
      "Epoch 13 batch 60 train Loss 750.0392 test Loss 286.5248 with MSE metric 44676.4519\n",
      "Epoch 13 batch 70 train Loss 747.8237 test Loss 285.6944 with MSE metric 44676.9571\n",
      "Epoch 13 batch 80 train Loss 745.6089 test Loss 284.8692 with MSE metric 44676.2174\n",
      "Epoch 13 batch 90 train Loss 743.4107 test Loss 284.0488 with MSE metric 44676.2114\n",
      "Epoch 13 batch 100 train Loss 741.2214 test Loss 283.2334 with MSE metric 44674.6699\n",
      "Epoch 13 batch 110 train Loss 739.0465 test Loss 282.4228 with MSE metric 44671.0623\n",
      "Epoch 13 batch 120 train Loss 736.8860 test Loss 281.6169 with MSE metric 44669.2436\n",
      "Epoch 13 batch 130 train Loss 734.7384 test Loss 280.8158 with MSE metric 44667.1430\n",
      "Epoch 13 batch 140 train Loss 732.6059 test Loss 280.0193 with MSE metric 44662.0977\n",
      "Epoch 13 batch 150 train Loss 730.4834 test Loss 279.2276 with MSE metric 44665.6378\n",
      "Epoch 13 batch 160 train Loss 728.3706 test Loss 278.4405 with MSE metric 44665.4503\n",
      "Epoch 13 batch 170 train Loss 726.3468 test Loss 277.6581 with MSE metric 44665.1478\n",
      "Epoch 13 batch 180 train Loss 724.2588 test Loss 276.8801 with MSE metric 44661.0026\n",
      "Epoch 13 batch 190 train Loss 722.1830 test Loss 276.1065 with MSE metric 44664.2451\n",
      "Epoch 13 batch 200 train Loss 720.1283 test Loss 275.3376 with MSE metric 44665.0625\n",
      "Epoch 13 batch 210 train Loss 718.0773 test Loss 274.5731 with MSE metric 44665.2590\n",
      "Epoch 13 batch 220 train Loss 716.0477 test Loss 273.8132 with MSE metric 44666.0939\n",
      "Epoch 13 batch 230 train Loss 714.0193 test Loss 273.0575 with MSE metric 44669.2406\n",
      "Epoch 13 batch 240 train Loss 712.0108 test Loss 272.3061 with MSE metric 44670.3363\n",
      "Time taken for 1 epoch: 25.893537998199463 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 710.0040 test Loss 271.5590 with MSE metric 44667.5588\n",
      "Epoch 14 batch 10 train Loss 708.0099 test Loss 270.8161 with MSE metric 44664.6253\n",
      "Epoch 14 batch 20 train Loss 706.0275 test Loss 270.0775 with MSE metric 44659.6701\n",
      "Epoch 14 batch 30 train Loss 704.0543 test Loss 269.3430 with MSE metric 44658.5434\n",
      "Epoch 14 batch 40 train Loss 702.0951 test Loss 268.6127 with MSE metric 44659.1883\n",
      "Epoch 14 batch 50 train Loss 700.1510 test Loss 267.8866 with MSE metric 44656.3263\n",
      "Epoch 14 batch 60 train Loss 698.2208 test Loss 267.1644 with MSE metric 44656.3787\n",
      "Epoch 14 batch 70 train Loss 696.2933 test Loss 266.4465 with MSE metric 44655.9660\n",
      "Epoch 14 batch 80 train Loss 694.3830 test Loss 265.7326 with MSE metric 44656.6759\n",
      "Epoch 14 batch 90 train Loss 692.4755 test Loss 265.0223 with MSE metric 44658.0571\n",
      "Epoch 14 batch 100 train Loss 690.5795 test Loss 264.3162 with MSE metric 44656.7533\n",
      "Epoch 14 batch 110 train Loss 688.6938 test Loss 263.6140 with MSE metric 44659.1272\n",
      "Epoch 14 batch 120 train Loss 686.8213 test Loss 262.9157 with MSE metric 44659.9702\n",
      "Epoch 14 batch 130 train Loss 684.9554 test Loss 262.2213 with MSE metric 44656.1318\n",
      "Epoch 14 batch 140 train Loss 683.1003 test Loss 261.5305 with MSE metric 44652.9989\n",
      "Epoch 14 batch 150 train Loss 681.2574 test Loss 260.8438 with MSE metric 44651.5726\n",
      "Epoch 14 batch 160 train Loss 679.4220 test Loss 260.1608 with MSE metric 44650.6180\n",
      "Epoch 14 batch 170 train Loss 677.6114 test Loss 259.4814 with MSE metric 44650.9449\n",
      "Epoch 14 batch 180 train Loss 675.8012 test Loss 258.8057 with MSE metric 44646.6062\n",
      "Epoch 14 batch 190 train Loss 673.9959 test Loss 258.1337 with MSE metric 44644.9843\n",
      "Epoch 14 batch 200 train Loss 672.2022 test Loss 257.4655 with MSE metric 44643.9647\n",
      "Epoch 14 batch 210 train Loss 670.4183 test Loss 256.8008 with MSE metric 44645.5900\n",
      "Epoch 14 batch 220 train Loss 668.6418 test Loss 256.1397 with MSE metric 44644.3464\n",
      "Epoch 14 batch 230 train Loss 666.8784 test Loss 255.4822 with MSE metric 44644.5990\n",
      "Epoch 14 batch 240 train Loss 665.1224 test Loss 254.8280 with MSE metric 44645.2015\n",
      "Time taken for 1 epoch: 28.45941400527954 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 663.3777 test Loss 254.1773 with MSE metric 44646.0120\n",
      "Epoch 15 batch 10 train Loss 661.6410 test Loss 253.5301 with MSE metric 44645.7942\n",
      "Epoch 15 batch 20 train Loss 659.9102 test Loss 252.8864 with MSE metric 44646.8515\n",
      "Epoch 15 batch 30 train Loss 658.2015 test Loss 252.2461 with MSE metric 44648.8572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 batch 40 train Loss 656.4897 test Loss 251.6091 with MSE metric 44649.6013\n",
      "Epoch 15 batch 50 train Loss 654.7873 test Loss 250.9754 with MSE metric 44650.0012\n",
      "Epoch 15 batch 60 train Loss 653.1027 test Loss 250.3452 with MSE metric 44650.0421\n",
      "Epoch 15 batch 70 train Loss 651.4181 test Loss 249.7183 with MSE metric 44649.1121\n",
      "Epoch 15 batch 80 train Loss 649.7408 test Loss 249.0947 with MSE metric 44647.7067\n",
      "Epoch 15 batch 90 train Loss 648.0725 test Loss 248.4744 with MSE metric 44646.1725\n",
      "Epoch 15 batch 100 train Loss 646.4165 test Loss 247.8572 with MSE metric 44645.2851\n",
      "Epoch 15 batch 110 train Loss 644.7653 test Loss 247.2429 with MSE metric 44644.8591\n",
      "Epoch 15 batch 120 train Loss 643.1229 test Loss 246.6321 with MSE metric 44643.1733\n",
      "Epoch 15 batch 130 train Loss 641.4899 test Loss 246.0245 with MSE metric 44641.2036\n",
      "Epoch 15 batch 140 train Loss 639.8646 test Loss 245.4198 with MSE metric 44639.2454\n",
      "Epoch 15 batch 150 train Loss 638.2462 test Loss 244.8183 with MSE metric 44637.1294\n",
      "Epoch 15 batch 160 train Loss 636.6366 test Loss 244.2200 with MSE metric 44636.2056\n",
      "Epoch 15 batch 170 train Loss 635.0384 test Loss 243.6246 with MSE metric 44633.5415\n",
      "Epoch 15 batch 180 train Loss 633.4459 test Loss 243.0323 with MSE metric 44634.9891\n",
      "Epoch 15 batch 190 train Loss 631.8672 test Loss 242.4429 with MSE metric 44632.7174\n",
      "Epoch 15 batch 200 train Loss 630.2952 test Loss 241.8566 with MSE metric 44630.0086\n",
      "Epoch 15 batch 210 train Loss 628.7249 test Loss 241.2731 with MSE metric 44627.8200\n",
      "Epoch 15 batch 220 train Loss 627.1637 test Loss 240.6927 with MSE metric 44626.1303\n",
      "Epoch 15 batch 230 train Loss 625.6177 test Loss 240.1152 with MSE metric 44621.7435\n",
      "Epoch 15 batch 240 train Loss 624.0721 test Loss 239.5405 with MSE metric 44619.6470\n",
      "Time taken for 1 epoch: 28.90988326072693 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 622.5375 test Loss 238.9688 with MSE metric 44619.5518\n",
      "Epoch 16 batch 10 train Loss 621.0085 test Loss 238.3998 with MSE metric 44616.2723\n",
      "Epoch 16 batch 20 train Loss 619.4871 test Loss 237.8339 with MSE metric 44612.1733\n",
      "Epoch 16 batch 30 train Loss 617.9728 test Loss 237.2707 with MSE metric 44613.5428\n",
      "Epoch 16 batch 40 train Loss 616.4645 test Loss 236.7103 with MSE metric 44615.1343\n",
      "Epoch 16 batch 50 train Loss 614.9680 test Loss 236.1526 with MSE metric 44615.7748\n",
      "Epoch 16 batch 60 train Loss 613.4840 test Loss 235.5977 with MSE metric 44617.3193\n",
      "Epoch 16 batch 70 train Loss 611.9978 test Loss 235.0456 with MSE metric 44615.2659\n",
      "Epoch 16 batch 80 train Loss 610.5251 test Loss 234.4961 with MSE metric 44617.2815\n",
      "Epoch 16 batch 90 train Loss 609.0752 test Loss 233.9494 with MSE metric 44619.3489\n",
      "Epoch 16 batch 100 train Loss 607.6105 test Loss 233.4053 with MSE metric 44616.7754\n",
      "Epoch 16 batch 110 train Loss 606.1545 test Loss 232.8640 with MSE metric 44617.7027\n",
      "Epoch 16 batch 120 train Loss 604.7073 test Loss 232.3252 with MSE metric 44618.3692\n",
      "Epoch 16 batch 130 train Loss 603.2639 test Loss 231.7892 with MSE metric 44620.2422\n",
      "Epoch 16 batch 140 train Loss 601.8282 test Loss 231.2557 with MSE metric 44621.7492\n",
      "Epoch 16 batch 150 train Loss 600.3996 test Loss 230.7247 with MSE metric 44622.8676\n",
      "Epoch 16 batch 160 train Loss 598.9808 test Loss 230.1964 with MSE metric 44621.7593\n",
      "Epoch 16 batch 170 train Loss 597.5665 test Loss 229.6704 with MSE metric 44623.5806\n",
      "Epoch 16 batch 180 train Loss 596.1589 test Loss 229.1471 with MSE metric 44622.9014\n",
      "Epoch 16 batch 190 train Loss 594.7965 test Loss 228.6263 with MSE metric 44623.5280\n",
      "Epoch 16 batch 200 train Loss 593.4124 test Loss 228.1079 with MSE metric 44621.6565\n",
      "Epoch 16 batch 210 train Loss 592.0234 test Loss 227.5920 with MSE metric 44622.1929\n",
      "Epoch 16 batch 220 train Loss 590.6417 test Loss 227.0783 with MSE metric 44622.4778\n",
      "Epoch 16 batch 230 train Loss 589.2674 test Loss 226.5673 with MSE metric 44622.3663\n",
      "Epoch 16 batch 240 train Loss 587.9002 test Loss 226.0586 with MSE metric 44621.4502\n",
      "Time taken for 1 epoch: 28.559781074523926 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 586.5374 test Loss 225.5524 with MSE metric 44619.4639\n",
      "Epoch 17 batch 10 train Loss 585.1842 test Loss 225.0485 with MSE metric 44618.4106\n",
      "Epoch 17 batch 20 train Loss 583.8353 test Loss 224.5471 with MSE metric 44619.4588\n",
      "Epoch 17 batch 30 train Loss 582.4959 test Loss 224.0481 with MSE metric 44618.4556\n",
      "Epoch 17 batch 40 train Loss 581.1578 test Loss 223.5512 with MSE metric 44617.4340\n",
      "Epoch 17 batch 50 train Loss 579.8283 test Loss 223.0567 with MSE metric 44616.7769\n",
      "Epoch 17 batch 60 train Loss 578.5057 test Loss 222.5645 with MSE metric 44616.3809\n",
      "Epoch 17 batch 70 train Loss 577.1856 test Loss 222.0746 with MSE metric 44616.3480\n",
      "Epoch 17 batch 80 train Loss 575.8727 test Loss 221.5867 with MSE metric 44614.7490\n",
      "Epoch 17 batch 90 train Loss 574.5689 test Loss 221.1013 with MSE metric 44618.5417\n",
      "Epoch 17 batch 100 train Loss 573.2682 test Loss 220.6182 with MSE metric 44617.8802\n",
      "Epoch 17 batch 110 train Loss 571.9757 test Loss 220.1373 with MSE metric 44617.5010\n",
      "Epoch 17 batch 120 train Loss 570.6888 test Loss 219.6584 with MSE metric 44616.9643\n",
      "Epoch 17 batch 130 train Loss 569.4081 test Loss 219.1819 with MSE metric 44616.1957\n",
      "Epoch 17 batch 140 train Loss 568.1315 test Loss 218.7074 with MSE metric 44614.6499\n",
      "Epoch 17 batch 150 train Loss 566.8612 test Loss 218.2352 with MSE metric 44612.1384\n",
      "Epoch 17 batch 160 train Loss 565.5968 test Loss 217.7651 with MSE metric 44609.1376\n",
      "Epoch 17 batch 170 train Loss 564.3373 test Loss 217.2971 with MSE metric 44609.7966\n",
      "Epoch 17 batch 180 train Loss 563.0825 test Loss 216.8313 with MSE metric 44610.3200\n",
      "Epoch 17 batch 190 train Loss 561.8392 test Loss 216.3677 with MSE metric 44609.1782\n",
      "Epoch 17 batch 200 train Loss 560.5953 test Loss 215.9061 with MSE metric 44609.8612\n",
      "Epoch 17 batch 210 train Loss 559.3602 test Loss 215.4465 with MSE metric 44609.8614\n",
      "Epoch 17 batch 220 train Loss 558.1292 test Loss 214.9890 with MSE metric 44608.8580\n",
      "Epoch 17 batch 230 train Loss 556.9032 test Loss 214.5336 with MSE metric 44610.3394\n",
      "Epoch 17 batch 240 train Loss 555.6828 test Loss 214.0800 with MSE metric 44611.4293\n",
      "Time taken for 1 epoch: 29.99758791923523 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 554.4664 test Loss 213.6286 with MSE metric 44611.7451\n",
      "Epoch 18 batch 10 train Loss 553.2573 test Loss 213.1791 with MSE metric 44610.5758\n",
      "Epoch 18 batch 20 train Loss 552.0523 test Loss 212.7318 with MSE metric 44608.1104\n",
      "Epoch 18 batch 30 train Loss 550.8527 test Loss 212.2865 with MSE metric 44605.2050\n",
      "Epoch 18 batch 40 train Loss 549.6578 test Loss 211.8430 with MSE metric 44601.0230\n",
      "Epoch 18 batch 50 train Loss 548.4685 test Loss 211.4016 with MSE metric 44599.6690\n",
      "Epoch 18 batch 60 train Loss 547.2831 test Loss 210.9620 with MSE metric 44600.2956\n",
      "Epoch 18 batch 70 train Loss 546.1045 test Loss 210.5242 with MSE metric 44598.7428\n",
      "Epoch 18 batch 80 train Loss 544.9328 test Loss 210.0884 with MSE metric 44598.1984\n",
      "Epoch 18 batch 90 train Loss 543.7638 test Loss 209.6547 with MSE metric 44597.9782\n",
      "Epoch 18 batch 100 train Loss 542.5996 test Loss 209.2227 with MSE metric 44596.8771\n",
      "Epoch 18 batch 110 train Loss 541.4404 test Loss 208.7927 with MSE metric 44594.8061\n",
      "Epoch 18 batch 120 train Loss 540.2863 test Loss 208.3646 with MSE metric 44591.9276\n",
      "Epoch 18 batch 130 train Loss 539.1372 test Loss 207.9383 with MSE metric 44591.9650\n",
      "Epoch 18 batch 140 train Loss 537.9936 test Loss 207.5138 with MSE metric 44591.0258\n",
      "Epoch 18 batch 150 train Loss 536.8629 test Loss 207.0911 with MSE metric 44591.6491\n",
      "Epoch 18 batch 160 train Loss 535.7331 test Loss 206.6703 with MSE metric 44590.9638\n",
      "Epoch 18 batch 170 train Loss 534.6034 test Loss 206.2512 with MSE metric 44589.4598\n",
      "Epoch 18 batch 180 train Loss 533.4796 test Loss 205.8339 with MSE metric 44588.8982\n",
      "Epoch 18 batch 190 train Loss 532.3612 test Loss 205.4184 with MSE metric 44590.7852\n",
      "Epoch 18 batch 200 train Loss 531.2461 test Loss 205.0045 with MSE metric 44591.5490\n",
      "Epoch 18 batch 210 train Loss 530.1370 test Loss 204.5927 with MSE metric 44589.9326\n",
      "Epoch 18 batch 220 train Loss 529.0327 test Loss 204.1825 with MSE metric 44587.4440\n",
      "Epoch 18 batch 230 train Loss 527.9343 test Loss 203.7739 with MSE metric 44586.7431\n",
      "Epoch 18 batch 240 train Loss 526.8394 test Loss 203.3671 with MSE metric 44585.4931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 29.84224796295166 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 525.7473 test Loss 202.9620 with MSE metric 44585.1595\n",
      "Epoch 19 batch 10 train Loss 524.6610 test Loss 202.5586 with MSE metric 44583.3717\n",
      "Epoch 19 batch 20 train Loss 523.5810 test Loss 202.1569 with MSE metric 44583.4698\n",
      "Epoch 19 batch 30 train Loss 522.5030 test Loss 201.7569 with MSE metric 44582.7458\n",
      "Epoch 19 batch 40 train Loss 521.4332 test Loss 201.3587 with MSE metric 44581.1580\n",
      "Epoch 19 batch 50 train Loss 520.3638 test Loss 200.9622 with MSE metric 44579.6533\n",
      "Epoch 19 batch 60 train Loss 519.2989 test Loss 200.5672 with MSE metric 44577.5454\n",
      "Epoch 19 batch 70 train Loss 518.2389 test Loss 200.1739 with MSE metric 44574.7417\n",
      "Epoch 19 batch 80 train Loss 517.1837 test Loss 199.7822 with MSE metric 44573.0875\n",
      "Epoch 19 batch 90 train Loss 516.1344 test Loss 199.3921 with MSE metric 44571.1268\n",
      "Epoch 19 batch 100 train Loss 515.0874 test Loss 199.0036 with MSE metric 44570.0729\n",
      "Epoch 19 batch 110 train Loss 514.0465 test Loss 198.6167 with MSE metric 44570.3090\n",
      "Epoch 19 batch 120 train Loss 513.0080 test Loss 198.2315 with MSE metric 44567.8395\n",
      "Epoch 19 batch 130 train Loss 511.9804 test Loss 197.8478 with MSE metric 44565.7494\n",
      "Epoch 19 batch 140 train Loss 510.9505 test Loss 197.4656 with MSE metric 44564.7961\n",
      "Epoch 19 batch 150 train Loss 509.9253 test Loss 197.0852 with MSE metric 44566.4279\n",
      "Epoch 19 batch 160 train Loss 508.9041 test Loss 196.7063 with MSE metric 44564.1533\n",
      "Epoch 19 batch 170 train Loss 507.8868 test Loss 196.3288 with MSE metric 44562.3490\n",
      "Epoch 19 batch 180 train Loss 506.8751 test Loss 195.9528 with MSE metric 44562.4334\n",
      "Epoch 19 batch 190 train Loss 505.8698 test Loss 195.5784 with MSE metric 44564.2516\n",
      "Epoch 19 batch 200 train Loss 504.8644 test Loss 195.2056 with MSE metric 44563.3392\n",
      "Epoch 19 batch 210 train Loss 503.8771 test Loss 194.8343 with MSE metric 44563.3639\n",
      "Epoch 19 batch 220 train Loss 502.8801 test Loss 194.4645 with MSE metric 44562.4938\n",
      "Epoch 19 batch 230 train Loss 501.8873 test Loss 194.0961 with MSE metric 44560.2809\n",
      "Epoch 19 batch 240 train Loss 500.8983 test Loss 193.7293 with MSE metric 44559.0701\n",
      "Time taken for 1 epoch: 30.568185091018677 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 499.9135 test Loss 193.3640 with MSE metric 44559.5486\n",
      "Epoch 20 batch 10 train Loss 498.9328 test Loss 193.0001 with MSE metric 44559.9785\n",
      "Epoch 20 batch 20 train Loss 497.9550 test Loss 192.6377 with MSE metric 44560.1646\n",
      "Epoch 20 batch 30 train Loss 496.9813 test Loss 192.2765 with MSE metric 44557.6269\n",
      "Epoch 20 batch 40 train Loss 496.0113 test Loss 191.9170 with MSE metric 44555.8084\n",
      "Epoch 20 batch 50 train Loss 495.0456 test Loss 191.5588 with MSE metric 44556.9033\n",
      "Epoch 20 batch 60 train Loss 494.0829 test Loss 191.2020 with MSE metric 44556.1448\n",
      "Epoch 20 batch 70 train Loss 493.1243 test Loss 190.8467 with MSE metric 44554.2765\n",
      "Epoch 20 batch 80 train Loss 492.1821 test Loss 190.4927 with MSE metric 44553.2407\n",
      "Epoch 20 batch 90 train Loss 491.2308 test Loss 190.1401 with MSE metric 44552.6936\n",
      "Epoch 20 batch 100 train Loss 490.2838 test Loss 189.7890 with MSE metric 44552.7122\n",
      "Epoch 20 batch 110 train Loss 489.3421 test Loss 189.4391 with MSE metric 44552.6955\n",
      "Epoch 20 batch 120 train Loss 488.4034 test Loss 189.0906 with MSE metric 44551.7263\n",
      "Epoch 20 batch 130 train Loss 487.4682 test Loss 188.7435 with MSE metric 44550.7526\n",
      "Epoch 20 batch 140 train Loss 486.5357 test Loss 188.3978 with MSE metric 44549.9307\n",
      "Epoch 20 batch 150 train Loss 485.6121 test Loss 188.0535 with MSE metric 44547.1567\n",
      "Epoch 20 batch 160 train Loss 484.6869 test Loss 187.7105 with MSE metric 44545.5417\n",
      "Epoch 20 batch 170 train Loss 483.7676 test Loss 187.3688 with MSE metric 44545.3842\n",
      "Epoch 20 batch 180 train Loss 482.8502 test Loss 187.0285 with MSE metric 44544.4560\n",
      "Epoch 20 batch 190 train Loss 481.9357 test Loss 186.6895 with MSE metric 44544.5368\n",
      "Epoch 20 batch 200 train Loss 481.0239 test Loss 186.3518 with MSE metric 44544.0892\n",
      "Epoch 20 batch 210 train Loss 480.1160 test Loss 186.0154 with MSE metric 44543.4862\n",
      "Epoch 20 batch 220 train Loss 479.2114 test Loss 185.6802 with MSE metric 44541.3666\n",
      "Epoch 20 batch 230 train Loss 478.3118 test Loss 185.3463 with MSE metric 44539.3565\n",
      "Epoch 20 batch 240 train Loss 477.4146 test Loss 185.0136 with MSE metric 44538.8495\n",
      "Time taken for 1 epoch: 30.16786217689514 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 476.5206 test Loss 184.6823 with MSE metric 44541.2643\n",
      "Epoch 21 batch 10 train Loss 475.6377 test Loss 184.3524 with MSE metric 44540.3382\n",
      "Epoch 21 batch 20 train Loss 474.7501 test Loss 184.0235 with MSE metric 44540.6327\n",
      "Epoch 21 batch 30 train Loss 473.8670 test Loss 183.6959 with MSE metric 44539.9980\n",
      "Epoch 21 batch 40 train Loss 472.9864 test Loss 183.3695 with MSE metric 44539.7680\n",
      "Epoch 21 batch 50 train Loss 472.1109 test Loss 183.0446 with MSE metric 44538.6627\n",
      "Epoch 21 batch 60 train Loss 471.2366 test Loss 182.7208 with MSE metric 44539.1137\n",
      "Epoch 21 batch 70 train Loss 470.3668 test Loss 182.3981 with MSE metric 44538.3571\n",
      "Epoch 21 batch 80 train Loss 469.5000 test Loss 182.0767 with MSE metric 44539.5117\n",
      "Epoch 21 batch 90 train Loss 468.6358 test Loss 181.7566 with MSE metric 44537.4115\n",
      "Epoch 21 batch 100 train Loss 467.7763 test Loss 181.4376 with MSE metric 44536.6927\n",
      "Epoch 21 batch 110 train Loss 466.9180 test Loss 181.1198 with MSE metric 44533.3931\n",
      "Epoch 21 batch 120 train Loss 466.0633 test Loss 180.8031 with MSE metric 44531.6526\n",
      "Epoch 21 batch 130 train Loss 465.2125 test Loss 180.4877 with MSE metric 44532.1801\n",
      "Epoch 21 batch 140 train Loss 464.3646 test Loss 180.1734 with MSE metric 44532.1076\n",
      "Epoch 21 batch 150 train Loss 463.5209 test Loss 179.8604 with MSE metric 44531.6240\n",
      "Epoch 21 batch 160 train Loss 462.6793 test Loss 179.5484 with MSE metric 44530.1659\n",
      "Epoch 21 batch 170 train Loss 461.8398 test Loss 179.2376 with MSE metric 44531.7636\n",
      "Epoch 21 batch 180 train Loss 461.0042 test Loss 178.9280 with MSE metric 44530.2022\n",
      "Epoch 21 batch 190 train Loss 460.1713 test Loss 178.6195 with MSE metric 44529.8319\n",
      "Epoch 21 batch 200 train Loss 459.3415 test Loss 178.3120 with MSE metric 44528.6794\n",
      "Epoch 21 batch 210 train Loss 458.5150 test Loss 178.0057 with MSE metric 44528.1707\n",
      "Epoch 21 batch 220 train Loss 457.6914 test Loss 177.7007 with MSE metric 44528.0325\n",
      "Epoch 21 batch 230 train Loss 456.8706 test Loss 177.3966 with MSE metric 44526.8091\n",
      "Epoch 21 batch 240 train Loss 456.0535 test Loss 177.0936 with MSE metric 44526.0047\n",
      "Time taken for 1 epoch: 27.78231692314148 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 455.2401 test Loss 176.7919 with MSE metric 44524.9635\n",
      "Epoch 22 batch 10 train Loss 454.4282 test Loss 176.4912 with MSE metric 44524.3901\n",
      "Epoch 22 batch 20 train Loss 453.6195 test Loss 176.1915 with MSE metric 44526.1289\n",
      "Epoch 22 batch 30 train Loss 452.8161 test Loss 175.8930 with MSE metric 44526.2864\n",
      "Epoch 22 batch 40 train Loss 452.0447 test Loss 175.5956 with MSE metric 44527.5907\n",
      "Epoch 22 batch 50 train Loss 451.2441 test Loss 175.2994 with MSE metric 44526.3286\n",
      "Epoch 22 batch 60 train Loss 450.4469 test Loss 175.0039 with MSE metric 44527.5434\n",
      "Epoch 22 batch 70 train Loss 449.6531 test Loss 174.7099 with MSE metric 44526.3519\n",
      "Epoch 22 batch 80 train Loss 448.8614 test Loss 174.4166 with MSE metric 44526.2595\n",
      "Epoch 22 batch 90 train Loss 448.0724 test Loss 174.1245 with MSE metric 44525.1485\n",
      "Epoch 22 batch 100 train Loss 447.2866 test Loss 173.8336 with MSE metric 44523.5136\n",
      "Epoch 22 batch 110 train Loss 446.5031 test Loss 173.5437 with MSE metric 44522.2833\n",
      "Epoch 22 batch 120 train Loss 445.7228 test Loss 173.2547 with MSE metric 44521.4576\n",
      "Epoch 22 batch 130 train Loss 444.9455 test Loss 172.9668 with MSE metric 44519.5376\n",
      "Epoch 22 batch 140 train Loss 444.1720 test Loss 172.6798 with MSE metric 44519.5327\n",
      "Epoch 22 batch 150 train Loss 443.4007 test Loss 172.3939 with MSE metric 44518.4265\n",
      "Epoch 22 batch 160 train Loss 442.6321 test Loss 172.1090 with MSE metric 44518.5569\n",
      "Epoch 22 batch 170 train Loss 441.8662 test Loss 171.8251 with MSE metric 44517.7236\n",
      "Epoch 22 batch 180 train Loss 441.1028 test Loss 171.5422 with MSE metric 44517.9067\n",
      "Epoch 22 batch 190 train Loss 440.3419 test Loss 171.2603 with MSE metric 44516.4745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 batch 200 train Loss 439.5845 test Loss 170.9795 with MSE metric 44517.1503\n",
      "Epoch 22 batch 210 train Loss 438.8287 test Loss 170.6996 with MSE metric 44518.5449\n",
      "Epoch 22 batch 220 train Loss 438.0748 test Loss 170.4206 with MSE metric 44515.3944\n",
      "Epoch 22 batch 230 train Loss 437.3248 test Loss 170.1427 with MSE metric 44514.9178\n",
      "Epoch 22 batch 240 train Loss 436.5766 test Loss 169.8657 with MSE metric 44516.0122\n",
      "Time taken for 1 epoch: 30.260584354400635 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 435.8310 test Loss 169.5898 with MSE metric 44513.3356\n",
      "Epoch 23 batch 10 train Loss 435.0883 test Loss 169.3148 with MSE metric 44512.0308\n",
      "Epoch 23 batch 20 train Loss 434.3486 test Loss 169.0408 with MSE metric 44513.1384\n",
      "Epoch 23 batch 30 train Loss 433.6202 test Loss 168.7675 with MSE metric 44513.4129\n",
      "Epoch 23 batch 40 train Loss 432.8849 test Loss 168.4954 with MSE metric 44511.8111\n",
      "Epoch 23 batch 50 train Loss 432.1534 test Loss 168.2243 with MSE metric 44511.1223\n",
      "Epoch 23 batch 60 train Loss 431.4247 test Loss 167.9540 with MSE metric 44508.3982\n",
      "Epoch 23 batch 70 train Loss 430.6979 test Loss 167.6847 with MSE metric 44507.5394\n",
      "Epoch 23 batch 80 train Loss 429.9726 test Loss 167.4163 with MSE metric 44506.6466\n",
      "Epoch 23 batch 90 train Loss 429.2497 test Loss 167.1489 with MSE metric 44506.2492\n",
      "Epoch 23 batch 100 train Loss 428.5315 test Loss 166.8823 with MSE metric 44506.8506\n",
      "Epoch 23 batch 110 train Loss 427.8139 test Loss 166.6166 with MSE metric 44506.0082\n",
      "Epoch 23 batch 120 train Loss 427.0996 test Loss 166.3517 with MSE metric 44505.4590\n",
      "Epoch 23 batch 130 train Loss 426.3896 test Loss 166.0879 with MSE metric 44505.9764\n",
      "Epoch 23 batch 140 train Loss 425.6798 test Loss 165.8250 with MSE metric 44505.8162\n",
      "Epoch 23 batch 150 train Loss 424.9717 test Loss 165.5628 with MSE metric 44505.3034\n",
      "Epoch 23 batch 160 train Loss 424.2654 test Loss 165.3016 with MSE metric 44504.3922\n",
      "Epoch 23 batch 170 train Loss 423.5618 test Loss 165.0414 with MSE metric 44504.5068\n",
      "Epoch 23 batch 180 train Loss 422.8617 test Loss 164.7820 with MSE metric 44504.6756\n",
      "Epoch 23 batch 190 train Loss 422.1629 test Loss 164.5235 with MSE metric 44502.3542\n",
      "Epoch 23 batch 200 train Loss 421.4668 test Loss 164.2659 with MSE metric 44503.2639\n",
      "Epoch 23 batch 210 train Loss 420.7730 test Loss 164.0091 with MSE metric 44502.4669\n",
      "Epoch 23 batch 220 train Loss 420.0812 test Loss 163.7533 with MSE metric 44503.5011\n",
      "Epoch 23 batch 230 train Loss 419.3917 test Loss 163.4983 with MSE metric 44503.7991\n",
      "Epoch 23 batch 240 train Loss 418.7049 test Loss 163.2442 with MSE metric 44501.7502\n",
      "Time taken for 1 epoch: 29.787312030792236 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 418.0205 test Loss 162.9907 with MSE metric 44502.2103\n",
      "Epoch 24 batch 10 train Loss 417.3378 test Loss 162.7382 with MSE metric 44502.9575\n",
      "Epoch 24 batch 20 train Loss 416.6598 test Loss 162.4864 with MSE metric 44503.3249\n",
      "Epoch 24 batch 30 train Loss 415.9820 test Loss 162.2356 with MSE metric 44503.9373\n",
      "Epoch 24 batch 40 train Loss 415.3063 test Loss 161.9856 with MSE metric 44502.4237\n",
      "Epoch 24 batch 50 train Loss 414.6334 test Loss 161.7364 with MSE metric 44502.3820\n",
      "Epoch 24 batch 60 train Loss 413.9619 test Loss 161.4880 with MSE metric 44501.7777\n",
      "Epoch 24 batch 70 train Loss 413.2925 test Loss 161.2404 with MSE metric 44500.2765\n",
      "Epoch 24 batch 80 train Loss 412.6270 test Loss 160.9937 with MSE metric 44500.1991\n",
      "Epoch 24 batch 90 train Loss 411.9636 test Loss 160.7476 with MSE metric 44500.1292\n",
      "Epoch 24 batch 100 train Loss 411.3011 test Loss 160.5025 with MSE metric 44499.8309\n",
      "Epoch 24 batch 110 train Loss 410.6409 test Loss 160.2583 with MSE metric 44499.1573\n",
      "Epoch 24 batch 120 train Loss 409.9837 test Loss 160.0147 with MSE metric 44498.7194\n",
      "Epoch 24 batch 130 train Loss 409.3280 test Loss 159.7720 with MSE metric 44497.9833\n",
      "Epoch 24 batch 140 train Loss 408.6762 test Loss 159.5301 with MSE metric 44496.5639\n",
      "Epoch 24 batch 150 train Loss 408.0267 test Loss 159.2889 with MSE metric 44495.7882\n",
      "Epoch 24 batch 160 train Loss 407.3770 test Loss 159.0485 with MSE metric 44494.3233\n",
      "Epoch 24 batch 170 train Loss 406.7300 test Loss 158.8089 with MSE metric 44493.5046\n",
      "Epoch 24 batch 180 train Loss 406.0849 test Loss 158.5701 with MSE metric 44491.4073\n",
      "Epoch 24 batch 190 train Loss 405.4413 test Loss 158.3321 with MSE metric 44492.0621\n",
      "Epoch 24 batch 200 train Loss 404.8007 test Loss 158.0948 with MSE metric 44491.4432\n",
      "Epoch 24 batch 210 train Loss 404.1613 test Loss 157.8582 with MSE metric 44489.7749\n",
      "Epoch 24 batch 220 train Loss 403.5257 test Loss 157.6225 with MSE metric 44489.0644\n",
      "Epoch 24 batch 230 train Loss 402.8906 test Loss 157.3874 with MSE metric 44490.8394\n",
      "Epoch 24 batch 240 train Loss 402.2578 test Loss 157.1532 with MSE metric 44490.8048\n",
      "Time taken for 1 epoch: 31.27457094192505 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 401.6268 test Loss 156.9197 with MSE metric 44488.0306\n",
      "Epoch 25 batch 10 train Loss 400.9975 test Loss 156.6870 with MSE metric 44488.0853\n",
      "Epoch 25 batch 20 train Loss 400.3706 test Loss 156.4550 with MSE metric 44485.9134\n",
      "Epoch 25 batch 30 train Loss 399.7454 test Loss 156.2236 with MSE metric 44486.0850\n",
      "Epoch 25 batch 40 train Loss 399.1220 test Loss 155.9931 with MSE metric 44484.2154\n",
      "Epoch 25 batch 50 train Loss 398.5028 test Loss 155.7634 with MSE metric 44482.0640\n",
      "Epoch 25 batch 60 train Loss 397.8842 test Loss 155.5344 with MSE metric 44481.5086\n",
      "Epoch 25 batch 70 train Loss 397.2674 test Loss 155.3061 with MSE metric 44480.6203\n",
      "Epoch 25 batch 80 train Loss 396.6529 test Loss 155.0785 with MSE metric 44481.1450\n",
      "Epoch 25 batch 90 train Loss 396.0489 test Loss 154.8516 with MSE metric 44478.6523\n",
      "Epoch 25 batch 100 train Loss 395.4383 test Loss 154.6256 with MSE metric 44477.8937\n",
      "Epoch 25 batch 110 train Loss 394.8297 test Loss 154.4001 with MSE metric 44478.1594\n",
      "Epoch 25 batch 120 train Loss 394.2217 test Loss 154.1754 with MSE metric 44479.8721\n",
      "Epoch 25 batch 130 train Loss 393.6163 test Loss 153.9514 with MSE metric 44479.5171\n",
      "Epoch 25 batch 140 train Loss 393.0127 test Loss 153.7280 with MSE metric 44478.3759\n",
      "Epoch 25 batch 150 train Loss 392.4116 test Loss 153.5055 with MSE metric 44476.9842\n",
      "Epoch 25 batch 160 train Loss 391.8122 test Loss 153.2835 with MSE metric 44477.0386\n",
      "Epoch 25 batch 170 train Loss 391.2140 test Loss 153.0623 with MSE metric 44476.7864\n",
      "Epoch 25 batch 180 train Loss 390.6300 test Loss 152.8417 with MSE metric 44474.1866\n",
      "Epoch 25 batch 190 train Loss 390.0355 test Loss 152.6218 with MSE metric 44472.6137\n",
      "Epoch 25 batch 200 train Loss 389.4442 test Loss 152.4028 with MSE metric 44473.8797\n",
      "Epoch 25 batch 210 train Loss 388.8541 test Loss 152.1844 with MSE metric 44472.8964\n",
      "Epoch 25 batch 220 train Loss 388.2657 test Loss 151.9665 with MSE metric 44469.5572\n",
      "Epoch 25 batch 230 train Loss 387.6784 test Loss 151.7494 with MSE metric 44467.5549\n",
      "Epoch 25 batch 240 train Loss 387.0934 test Loss 151.5329 with MSE metric 44465.7508\n",
      "Time taken for 1 epoch: 31.710872888565063 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 386.5099 test Loss 151.3171 with MSE metric 44464.5398\n",
      "Epoch 26 batch 10 train Loss 385.9280 test Loss 151.1020 with MSE metric 44464.5045\n",
      "Epoch 26 batch 20 train Loss 385.3493 test Loss 150.8875 with MSE metric 44464.3144\n",
      "Epoch 26 batch 30 train Loss 384.7712 test Loss 150.6738 with MSE metric 44464.6405\n",
      "Epoch 26 batch 40 train Loss 384.1945 test Loss 150.4606 with MSE metric 44461.8838\n",
      "Epoch 26 batch 50 train Loss 383.6199 test Loss 150.2481 with MSE metric 44462.6456\n",
      "Epoch 26 batch 60 train Loss 383.0496 test Loss 150.0362 with MSE metric 44461.7897\n",
      "Epoch 26 batch 70 train Loss 382.4783 test Loss 149.8249 with MSE metric 44461.0442\n",
      "Epoch 26 batch 80 train Loss 381.9085 test Loss 149.6143 with MSE metric 44457.9318\n",
      "Epoch 26 batch 90 train Loss 381.3410 test Loss 149.4044 with MSE metric 44456.8822\n",
      "Epoch 26 batch 100 train Loss 380.7748 test Loss 149.1950 with MSE metric 44456.3727\n",
      "Epoch 26 batch 110 train Loss 380.2101 test Loss 148.9863 with MSE metric 44457.0918\n",
      "Epoch 26 batch 120 train Loss 379.6481 test Loss 148.7783 with MSE metric 44456.4343\n",
      "Epoch 26 batch 130 train Loss 379.0873 test Loss 148.5710 with MSE metric 44454.6530\n",
      "Epoch 26 batch 140 train Loss 378.5311 test Loss 148.3642 with MSE metric 44454.9664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 batch 150 train Loss 377.9733 test Loss 148.1580 with MSE metric 44454.7234\n",
      "Epoch 26 batch 160 train Loss 377.4174 test Loss 147.9525 with MSE metric 44453.3625\n",
      "Epoch 26 batch 170 train Loss 376.8630 test Loss 147.7476 with MSE metric 44453.6740\n",
      "Epoch 26 batch 180 train Loss 376.3102 test Loss 147.5433 with MSE metric 44453.7684\n",
      "Epoch 26 batch 190 train Loss 375.7591 test Loss 147.3397 with MSE metric 44451.9496\n",
      "Epoch 26 batch 200 train Loss 375.2098 test Loss 147.1366 with MSE metric 44450.7005\n",
      "Epoch 26 batch 210 train Loss 374.6619 test Loss 146.9341 with MSE metric 44452.3522\n",
      "Epoch 26 batch 220 train Loss 374.1156 test Loss 146.7323 with MSE metric 44451.1834\n",
      "Epoch 26 batch 230 train Loss 373.5711 test Loss 146.5309 with MSE metric 44450.5299\n",
      "Epoch 26 batch 240 train Loss 373.0280 test Loss 146.3302 with MSE metric 44451.0870\n",
      "Time taken for 1 epoch: 29.952781915664673 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 372.4866 test Loss 146.1301 with MSE metric 44450.3651\n",
      "Epoch 27 batch 10 train Loss 371.9473 test Loss 145.9306 with MSE metric 44449.2412\n",
      "Epoch 27 batch 20 train Loss 371.4093 test Loss 145.7317 with MSE metric 44448.9084\n",
      "Epoch 27 batch 30 train Loss 370.8729 test Loss 145.5333 with MSE metric 44449.6830\n",
      "Epoch 27 batch 40 train Loss 370.3386 test Loss 145.3356 with MSE metric 44448.7742\n",
      "Epoch 27 batch 50 train Loss 369.8053 test Loss 145.1385 with MSE metric 44447.6842\n",
      "Epoch 27 batch 60 train Loss 369.2740 test Loss 144.9418 with MSE metric 44446.6652\n",
      "Epoch 27 batch 70 train Loss 368.7440 test Loss 144.7458 with MSE metric 44447.4786\n",
      "Epoch 27 batch 80 train Loss 368.2153 test Loss 144.5504 with MSE metric 44446.3417\n",
      "Epoch 27 batch 90 train Loss 367.6882 test Loss 144.3555 with MSE metric 44445.6922\n",
      "Epoch 27 batch 100 train Loss 367.1627 test Loss 144.1611 with MSE metric 44444.4540\n",
      "Epoch 27 batch 110 train Loss 366.6407 test Loss 143.9674 with MSE metric 44444.1631\n",
      "Epoch 27 batch 120 train Loss 366.1184 test Loss 143.7743 with MSE metric 44443.1881\n",
      "Epoch 27 batch 130 train Loss 365.5988 test Loss 143.5816 with MSE metric 44443.2500\n",
      "Epoch 27 batch 140 train Loss 365.0795 test Loss 143.3895 with MSE metric 44443.1479\n",
      "Epoch 27 batch 150 train Loss 364.5614 test Loss 143.1980 with MSE metric 44442.3447\n",
      "Epoch 27 batch 160 train Loss 364.0450 test Loss 143.0071 with MSE metric 44442.1274\n",
      "Epoch 27 batch 170 train Loss 363.5299 test Loss 142.8167 with MSE metric 44441.4744\n",
      "Epoch 27 batch 180 train Loss 363.0175 test Loss 142.6269 with MSE metric 44441.3993\n",
      "Epoch 27 batch 190 train Loss 362.5054 test Loss 142.4376 with MSE metric 44440.4337\n",
      "Epoch 27 batch 200 train Loss 361.9948 test Loss 142.2489 with MSE metric 44441.4784\n",
      "Epoch 27 batch 210 train Loss 361.4859 test Loss 142.0607 with MSE metric 44439.3951\n",
      "Epoch 27 batch 220 train Loss 360.9829 test Loss 141.8731 with MSE metric 44438.3368\n",
      "Epoch 27 batch 230 train Loss 360.4770 test Loss 141.6860 with MSE metric 44437.3609\n",
      "Epoch 27 batch 240 train Loss 359.9724 test Loss 141.4994 with MSE metric 44436.7279\n",
      "Time taken for 1 epoch: 30.433131217956543 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 359.4695 test Loss 141.3133 with MSE metric 44438.7372\n",
      "Epoch 28 batch 10 train Loss 358.9675 test Loss 141.1279 with MSE metric 44437.4755\n",
      "Epoch 28 batch 20 train Loss 358.4671 test Loss 140.9429 with MSE metric 44437.5265\n",
      "Epoch 28 batch 30 train Loss 357.9683 test Loss 140.7585 with MSE metric 44436.9023\n",
      "Epoch 28 batch 40 train Loss 357.4705 test Loss 140.5746 with MSE metric 44436.6041\n",
      "Epoch 28 batch 50 train Loss 356.9742 test Loss 140.3913 with MSE metric 44437.2828\n",
      "Epoch 28 batch 60 train Loss 356.4793 test Loss 140.2083 with MSE metric 44437.3487\n",
      "Epoch 28 batch 70 train Loss 355.9859 test Loss 140.0260 with MSE metric 44436.1055\n",
      "Epoch 28 batch 80 train Loss 355.4940 test Loss 139.8441 with MSE metric 44435.6005\n",
      "Epoch 28 batch 90 train Loss 355.0031 test Loss 139.6627 with MSE metric 44433.3865\n",
      "Epoch 28 batch 100 train Loss 354.5138 test Loss 139.4819 with MSE metric 44432.6426\n",
      "Epoch 28 batch 110 train Loss 354.0259 test Loss 139.3016 with MSE metric 44432.1475\n",
      "Epoch 28 batch 120 train Loss 353.5395 test Loss 139.1218 with MSE metric 44432.6995\n",
      "Epoch 28 batch 130 train Loss 353.0546 test Loss 138.9425 with MSE metric 44432.1447\n",
      "Epoch 28 batch 140 train Loss 352.5710 test Loss 138.7637 with MSE metric 44432.0027\n",
      "Epoch 28 batch 150 train Loss 352.0897 test Loss 138.5854 with MSE metric 44432.4883\n",
      "Epoch 28 batch 160 train Loss 351.6086 test Loss 138.4076 with MSE metric 44431.3688\n",
      "Epoch 28 batch 170 train Loss 351.1289 test Loss 138.2303 with MSE metric 44430.2415\n",
      "Epoch 28 batch 180 train Loss 350.6507 test Loss 138.0534 with MSE metric 44429.4506\n",
      "Epoch 28 batch 190 train Loss 350.1737 test Loss 137.8771 with MSE metric 44428.0734\n",
      "Epoch 28 batch 200 train Loss 349.6987 test Loss 137.7013 with MSE metric 44426.6721\n",
      "Epoch 28 batch 210 train Loss 349.2244 test Loss 137.5259 with MSE metric 44425.8820\n",
      "Epoch 28 batch 220 train Loss 348.7517 test Loss 137.3510 with MSE metric 44425.3358\n",
      "Epoch 28 batch 230 train Loss 348.2798 test Loss 137.1767 with MSE metric 44424.6874\n",
      "Epoch 28 batch 240 train Loss 347.8092 test Loss 137.0027 with MSE metric 44425.2455\n",
      "Time taken for 1 epoch: 29.607370853424072 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 347.3403 test Loss 136.8293 with MSE metric 44425.7740\n",
      "Epoch 29 batch 10 train Loss 346.8725 test Loss 136.6563 with MSE metric 44425.5950\n",
      "Epoch 29 batch 20 train Loss 346.4059 test Loss 136.4839 with MSE metric 44424.5724\n",
      "Epoch 29 batch 30 train Loss 345.9405 test Loss 136.3119 with MSE metric 44423.6686\n",
      "Epoch 29 batch 40 train Loss 345.4763 test Loss 136.1404 with MSE metric 44422.7239\n",
      "Epoch 29 batch 50 train Loss 345.0135 test Loss 135.9693 with MSE metric 44422.0632\n",
      "Epoch 29 batch 60 train Loss 344.5517 test Loss 135.7987 with MSE metric 44420.0858\n",
      "Epoch 29 batch 70 train Loss 344.0917 test Loss 135.6285 with MSE metric 44422.2485\n",
      "Epoch 29 batch 80 train Loss 343.6329 test Loss 135.4588 with MSE metric 44421.4307\n",
      "Epoch 29 batch 90 train Loss 343.1751 test Loss 135.2896 with MSE metric 44422.5613\n",
      "Epoch 29 batch 100 train Loss 342.7184 test Loss 135.1208 with MSE metric 44421.2323\n",
      "Epoch 29 batch 110 train Loss 342.2631 test Loss 134.9525 with MSE metric 44421.0077\n",
      "Epoch 29 batch 120 train Loss 341.8093 test Loss 134.7848 with MSE metric 44421.0726\n",
      "Epoch 29 batch 130 train Loss 341.3565 test Loss 134.6173 with MSE metric 44420.3308\n",
      "Epoch 29 batch 140 train Loss 340.9048 test Loss 134.4504 with MSE metric 44421.6821\n",
      "Epoch 29 batch 150 train Loss 340.4570 test Loss 134.2839 with MSE metric 44421.8485\n",
      "Epoch 29 batch 160 train Loss 340.0078 test Loss 134.1179 with MSE metric 44420.3539\n",
      "Epoch 29 batch 170 train Loss 339.5603 test Loss 133.9523 with MSE metric 44418.9651\n",
      "Epoch 29 batch 180 train Loss 339.1134 test Loss 133.7871 with MSE metric 44417.8854\n",
      "Epoch 29 batch 190 train Loss 338.6681 test Loss 133.6224 with MSE metric 44417.0549\n",
      "Epoch 29 batch 200 train Loss 338.2237 test Loss 133.4581 with MSE metric 44414.8476\n",
      "Epoch 29 batch 210 train Loss 337.7805 test Loss 133.2942 with MSE metric 44413.2550\n",
      "Epoch 29 batch 220 train Loss 337.3386 test Loss 133.1308 with MSE metric 44412.6882\n",
      "Epoch 29 batch 230 train Loss 336.8982 test Loss 132.9679 with MSE metric 44411.6667\n",
      "Epoch 29 batch 240 train Loss 336.4586 test Loss 132.8054 with MSE metric 44411.4782\n",
      "Time taken for 1 epoch: 30.93397092819214 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 336.0203 test Loss 132.6433 with MSE metric 44410.5437\n",
      "Epoch 30 batch 10 train Loss 335.5829 test Loss 132.4817 with MSE metric 44410.1456\n",
      "Epoch 30 batch 20 train Loss 335.1466 test Loss 132.3204 with MSE metric 44409.1879\n",
      "Epoch 30 batch 30 train Loss 334.7115 test Loss 132.1596 with MSE metric 44408.9858\n",
      "Epoch 30 batch 40 train Loss 334.2780 test Loss 131.9991 with MSE metric 44407.8582\n",
      "Epoch 30 batch 50 train Loss 333.8476 test Loss 131.8392 with MSE metric 44406.0127\n",
      "Epoch 30 batch 60 train Loss 333.4162 test Loss 131.6797 with MSE metric 44405.6028\n",
      "Epoch 30 batch 70 train Loss 332.9860 test Loss 131.5206 with MSE metric 44404.6121\n",
      "Epoch 30 batch 80 train Loss 332.5567 test Loss 131.3619 with MSE metric 44404.7620\n",
      "Epoch 30 batch 90 train Loss 332.1288 test Loss 131.2037 with MSE metric 44403.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 batch 100 train Loss 331.7017 test Loss 131.0458 with MSE metric 44402.5543\n",
      "Epoch 30 batch 110 train Loss 331.2757 test Loss 130.8884 with MSE metric 44401.7364\n",
      "Epoch 30 batch 120 train Loss 330.8509 test Loss 130.7314 with MSE metric 44401.1948\n",
      "Epoch 30 batch 130 train Loss 330.4272 test Loss 130.5748 with MSE metric 44402.4479\n",
      "Epoch 30 batch 140 train Loss 330.0047 test Loss 130.4186 with MSE metric 44402.8617\n",
      "Epoch 30 batch 150 train Loss 329.5861 test Loss 130.2628 with MSE metric 44401.9661\n",
      "Epoch 30 batch 160 train Loss 329.1658 test Loss 130.1074 with MSE metric 44401.9016\n",
      "Epoch 30 batch 170 train Loss 328.7467 test Loss 129.9523 with MSE metric 44402.1558\n",
      "Epoch 30 batch 180 train Loss 328.3287 test Loss 129.7978 with MSE metric 44402.1161\n",
      "Epoch 30 batch 190 train Loss 327.9115 test Loss 129.6436 with MSE metric 44402.0686\n",
      "Epoch 30 batch 200 train Loss 327.4956 test Loss 129.4898 with MSE metric 44400.9390\n",
      "Epoch 30 batch 210 train Loss 327.0813 test Loss 129.3364 with MSE metric 44400.0264\n",
      "Epoch 30 batch 220 train Loss 326.6677 test Loss 129.1834 with MSE metric 44399.6060\n",
      "Epoch 30 batch 230 train Loss 326.2550 test Loss 129.0307 with MSE metric 44398.4512\n",
      "Epoch 30 batch 240 train Loss 325.8431 test Loss 128.8785 with MSE metric 44397.8178\n",
      "Time taken for 1 epoch: 31.132457971572876 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 325.4323 test Loss 128.7267 with MSE metric 44395.2933\n",
      "Epoch 31 batch 10 train Loss 325.0231 test Loss 128.5753 with MSE metric 44394.3125\n",
      "Epoch 31 batch 20 train Loss 324.6146 test Loss 128.4243 with MSE metric 44392.9813\n",
      "Epoch 31 batch 30 train Loss 324.2071 test Loss 128.2736 with MSE metric 44392.2537\n",
      "Epoch 31 batch 40 train Loss 323.8006 test Loss 128.1233 with MSE metric 44392.2389\n",
      "Epoch 31 batch 50 train Loss 323.3963 test Loss 127.9734 with MSE metric 44391.1102\n",
      "Epoch 31 batch 60 train Loss 322.9918 test Loss 127.8238 with MSE metric 44391.0664\n",
      "Epoch 31 batch 70 train Loss 322.5884 test Loss 127.6747 with MSE metric 44390.2452\n",
      "Epoch 31 batch 80 train Loss 322.1888 test Loss 127.5259 with MSE metric 44389.5432\n",
      "Epoch 31 batch 90 train Loss 321.7874 test Loss 127.3775 with MSE metric 44388.6763\n",
      "Epoch 31 batch 100 train Loss 321.3870 test Loss 127.2295 with MSE metric 44389.5070\n",
      "Epoch 31 batch 110 train Loss 320.9879 test Loss 127.0819 with MSE metric 44388.3588\n",
      "Epoch 31 batch 120 train Loss 320.5909 test Loss 126.9346 with MSE metric 44387.7711\n",
      "Epoch 31 batch 130 train Loss 320.1937 test Loss 126.7877 with MSE metric 44388.2631\n",
      "Epoch 31 batch 140 train Loss 319.7977 test Loss 126.6412 with MSE metric 44387.2531\n",
      "Epoch 31 batch 150 train Loss 319.4026 test Loss 126.4950 with MSE metric 44387.2137\n",
      "Epoch 31 batch 160 train Loss 319.0084 test Loss 126.3492 with MSE metric 44388.2219\n",
      "Epoch 31 batch 170 train Loss 318.6172 test Loss 126.2038 with MSE metric 44387.2905\n",
      "Epoch 31 batch 180 train Loss 318.2248 test Loss 126.0588 with MSE metric 44385.9407\n",
      "Epoch 31 batch 190 train Loss 317.8335 test Loss 125.9141 with MSE metric 44384.2443\n",
      "Epoch 31 batch 200 train Loss 317.4435 test Loss 125.7698 with MSE metric 44383.4457\n",
      "Epoch 31 batch 210 train Loss 317.0541 test Loss 125.6258 with MSE metric 44383.4995\n",
      "Epoch 31 batch 220 train Loss 316.6658 test Loss 125.4822 with MSE metric 44383.6009\n",
      "Epoch 31 batch 230 train Loss 316.2797 test Loss 125.3389 with MSE metric 44384.1267\n",
      "Epoch 31 batch 240 train Loss 315.8934 test Loss 125.1960 with MSE metric 44384.4867\n",
      "Time taken for 1 epoch: 33.38770318031311 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 315.5080 test Loss 125.0535 with MSE metric 44383.2921\n",
      "Epoch 32 batch 10 train Loss 315.1234 test Loss 124.9113 with MSE metric 44382.3624\n",
      "Epoch 32 batch 20 train Loss 314.7398 test Loss 124.7695 with MSE metric 44381.7614\n",
      "Epoch 32 batch 30 train Loss 314.3573 test Loss 124.6280 with MSE metric 44381.2901\n",
      "Epoch 32 batch 40 train Loss 313.9757 test Loss 124.4869 with MSE metric 44380.4358\n",
      "Epoch 32 batch 50 train Loss 313.5950 test Loss 124.3461 with MSE metric 44379.5308\n",
      "Epoch 32 batch 60 train Loss 313.2153 test Loss 124.2056 with MSE metric 44379.0752\n",
      "Epoch 32 batch 70 train Loss 312.8365 test Loss 124.0654 with MSE metric 44379.4068\n",
      "Epoch 32 batch 80 train Loss 312.4586 test Loss 123.9257 with MSE metric 44378.3717\n",
      "Epoch 32 batch 90 train Loss 312.0819 test Loss 123.7863 with MSE metric 44378.1085\n",
      "Epoch 32 batch 100 train Loss 311.7059 test Loss 123.6472 with MSE metric 44378.0782\n",
      "Epoch 32 batch 110 train Loss 311.3366 test Loss 123.5085 with MSE metric 44377.8277\n",
      "Epoch 32 batch 120 train Loss 310.9623 test Loss 123.3701 with MSE metric 44376.7873\n",
      "Epoch 32 batch 130 train Loss 310.5890 test Loss 123.2322 with MSE metric 44375.8646\n",
      "Epoch 32 batch 140 train Loss 310.2166 test Loss 123.0945 with MSE metric 44374.4045\n",
      "Epoch 32 batch 150 train Loss 309.8452 test Loss 122.9572 with MSE metric 44372.9337\n",
      "Epoch 32 batch 160 train Loss 309.4761 test Loss 122.8203 with MSE metric 44372.9908\n",
      "Epoch 32 batch 170 train Loss 309.1064 test Loss 122.6836 with MSE metric 44372.3393\n",
      "Epoch 32 batch 180 train Loss 308.7377 test Loss 122.5472 with MSE metric 44370.9973\n",
      "Epoch 32 batch 190 train Loss 308.3701 test Loss 122.4112 with MSE metric 44370.9990\n",
      "Epoch 32 batch 200 train Loss 308.0032 test Loss 122.2756 with MSE metric 44368.7280\n",
      "Epoch 32 batch 210 train Loss 307.6378 test Loss 122.1403 with MSE metric 44366.9120\n",
      "Epoch 32 batch 220 train Loss 307.2744 test Loss 122.0053 with MSE metric 44365.4667\n",
      "Epoch 32 batch 230 train Loss 306.9102 test Loss 121.8707 with MSE metric 44364.8996\n",
      "Epoch 32 batch 240 train Loss 306.5470 test Loss 121.7363 with MSE metric 44365.4152\n",
      "Time taken for 1 epoch: 26.995386123657227 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 306.1863 test Loss 121.6023 with MSE metric 44364.5159\n",
      "Epoch 33 batch 10 train Loss 305.8247 test Loss 121.4686 with MSE metric 44364.8901\n",
      "Epoch 33 batch 20 train Loss 305.4642 test Loss 121.3353 with MSE metric 44363.4346\n",
      "Epoch 33 batch 30 train Loss 305.1044 test Loss 121.2023 with MSE metric 44364.1777\n",
      "Epoch 33 batch 40 train Loss 304.7454 test Loss 121.0696 with MSE metric 44362.8307\n",
      "Epoch 33 batch 50 train Loss 304.3874 test Loss 120.9372 with MSE metric 44360.8439\n",
      "Epoch 33 batch 60 train Loss 304.0304 test Loss 120.8051 with MSE metric 44359.1693\n",
      "Epoch 33 batch 70 train Loss 303.6739 test Loss 120.6733 with MSE metric 44358.7875\n",
      "Epoch 33 batch 80 train Loss 303.3184 test Loss 120.5418 with MSE metric 44357.1135\n",
      "Epoch 33 batch 90 train Loss 302.9639 test Loss 120.4106 with MSE metric 44355.8522\n",
      "Epoch 33 batch 100 train Loss 302.6099 test Loss 120.2798 with MSE metric 44355.5773\n",
      "Epoch 33 batch 110 train Loss 302.2571 test Loss 120.1493 with MSE metric 44355.6622\n",
      "Epoch 33 batch 120 train Loss 301.9050 test Loss 120.0191 with MSE metric 44355.2984\n",
      "Epoch 33 batch 130 train Loss 301.5537 test Loss 119.8892 with MSE metric 44354.3823\n",
      "Epoch 33 batch 140 train Loss 301.2033 test Loss 119.7596 with MSE metric 44352.2472\n",
      "Epoch 33 batch 150 train Loss 300.8537 test Loss 119.6303 with MSE metric 44351.3199\n",
      "Epoch 33 batch 160 train Loss 300.5051 test Loss 119.5013 with MSE metric 44350.6661\n",
      "Epoch 33 batch 170 train Loss 300.1571 test Loss 119.3726 with MSE metric 44350.3086\n",
      "Epoch 33 batch 180 train Loss 299.8100 test Loss 119.2442 with MSE metric 44349.4143\n",
      "Epoch 33 batch 190 train Loss 299.4637 test Loss 119.1161 with MSE metric 44349.9665\n",
      "Epoch 33 batch 200 train Loss 299.1181 test Loss 118.9882 with MSE metric 44349.2473\n",
      "Epoch 33 batch 210 train Loss 298.7734 test Loss 118.8607 with MSE metric 44348.5530\n",
      "Epoch 33 batch 220 train Loss 298.4296 test Loss 118.7335 with MSE metric 44347.2971\n",
      "Epoch 33 batch 230 train Loss 298.0864 test Loss 118.6066 with MSE metric 44345.7185\n",
      "Epoch 33 batch 240 train Loss 297.7441 test Loss 118.4800 with MSE metric 44345.2235\n",
      "Time taken for 1 epoch: 33.21414113044739 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 297.4025 test Loss 118.3536 with MSE metric 44344.1740\n",
      "Epoch 34 batch 10 train Loss 297.0620 test Loss 118.2276 with MSE metric 44343.7262\n",
      "Epoch 34 batch 20 train Loss 296.7221 test Loss 118.1018 with MSE metric 44343.7238\n",
      "Epoch 34 batch 30 train Loss 296.3831 test Loss 117.9763 with MSE metric 44343.3633\n",
      "Epoch 34 batch 40 train Loss 296.0451 test Loss 117.8511 with MSE metric 44343.0560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 batch 50 train Loss 295.7075 test Loss 117.7262 with MSE metric 44341.9728\n",
      "Epoch 34 batch 60 train Loss 295.3707 test Loss 117.6016 with MSE metric 44341.8949\n",
      "Epoch 34 batch 70 train Loss 295.0348 test Loss 117.4773 with MSE metric 44341.6695\n",
      "Epoch 34 batch 80 train Loss 294.7001 test Loss 117.3532 with MSE metric 44340.4276\n",
      "Epoch 34 batch 90 train Loss 294.3657 test Loss 117.2294 with MSE metric 44340.7906\n",
      "Epoch 34 batch 100 train Loss 294.0321 test Loss 117.1060 with MSE metric 44340.4896\n",
      "Epoch 34 batch 110 train Loss 293.6995 test Loss 116.9828 with MSE metric 44340.1295\n",
      "Epoch 34 batch 120 train Loss 293.3678 test Loss 116.8599 with MSE metric 44338.9561\n",
      "Epoch 34 batch 130 train Loss 293.0365 test Loss 116.7373 with MSE metric 44338.4681\n",
      "Epoch 34 batch 140 train Loss 292.7062 test Loss 116.6149 with MSE metric 44336.6368\n",
      "Epoch 34 batch 150 train Loss 292.3764 test Loss 116.4928 with MSE metric 44335.7128\n",
      "Epoch 34 batch 160 train Loss 292.0485 test Loss 116.3710 with MSE metric 44334.6346\n",
      "Epoch 34 batch 170 train Loss 291.7201 test Loss 116.2495 with MSE metric 44335.5717\n",
      "Epoch 34 batch 180 train Loss 291.3925 test Loss 116.1282 with MSE metric 44334.1412\n",
      "Epoch 34 batch 190 train Loss 291.0657 test Loss 116.0073 with MSE metric 44333.5644\n",
      "Epoch 34 batch 200 train Loss 290.7397 test Loss 115.8866 with MSE metric 44332.8458\n",
      "Epoch 34 batch 210 train Loss 290.4143 test Loss 115.7661 with MSE metric 44331.1580\n",
      "Epoch 34 batch 220 train Loss 290.0901 test Loss 115.6460 with MSE metric 44330.2315\n",
      "Epoch 34 batch 230 train Loss 289.7663 test Loss 115.5260 with MSE metric 44329.7985\n",
      "Epoch 34 batch 240 train Loss 289.4433 test Loss 115.4064 with MSE metric 44329.3568\n",
      "Time taken for 1 epoch: 31.681270122528076 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 289.1209 test Loss 115.2871 with MSE metric 44329.9038\n",
      "Epoch 35 batch 10 train Loss 288.7994 test Loss 115.1681 with MSE metric 44328.5986\n",
      "Epoch 35 batch 20 train Loss 288.4785 test Loss 115.0493 with MSE metric 44329.1525\n",
      "Epoch 35 batch 30 train Loss 288.1585 test Loss 114.9308 with MSE metric 44328.8736\n",
      "Epoch 35 batch 40 train Loss 287.8392 test Loss 114.8125 with MSE metric 44328.0815\n",
      "Epoch 35 batch 50 train Loss 287.5206 test Loss 114.6945 with MSE metric 44327.3490\n",
      "Epoch 35 batch 60 train Loss 287.2027 test Loss 114.5767 with MSE metric 44326.8305\n",
      "Epoch 35 batch 70 train Loss 286.8855 test Loss 114.4593 with MSE metric 44326.1724\n",
      "Epoch 35 batch 80 train Loss 286.5692 test Loss 114.3421 with MSE metric 44325.8708\n",
      "Epoch 35 batch 90 train Loss 286.2534 test Loss 114.2251 with MSE metric 44324.7114\n",
      "Epoch 35 batch 100 train Loss 285.9383 test Loss 114.1085 with MSE metric 44322.8426\n",
      "Epoch 35 batch 110 train Loss 285.6240 test Loss 113.9920 with MSE metric 44321.1611\n",
      "Epoch 35 batch 120 train Loss 285.3104 test Loss 113.8758 with MSE metric 44320.7945\n",
      "Epoch 35 batch 130 train Loss 284.9977 test Loss 113.7599 with MSE metric 44320.5615\n",
      "Epoch 35 batch 140 train Loss 284.6855 test Loss 113.6442 with MSE metric 44319.4807\n",
      "Epoch 35 batch 150 train Loss 284.3743 test Loss 113.5288 with MSE metric 44319.4863\n",
      "Epoch 35 batch 160 train Loss 284.0644 test Loss 113.4137 with MSE metric 44319.6822\n",
      "Epoch 35 batch 170 train Loss 283.7543 test Loss 113.2988 with MSE metric 44318.8634\n",
      "Epoch 35 batch 180 train Loss 283.4447 test Loss 113.1841 with MSE metric 44317.8515\n",
      "Epoch 35 batch 190 train Loss 283.1360 test Loss 113.0698 with MSE metric 44317.8409\n",
      "Epoch 35 batch 200 train Loss 282.8279 test Loss 112.9556 with MSE metric 44317.0318\n",
      "Epoch 35 batch 210 train Loss 282.5206 test Loss 112.8417 with MSE metric 44316.4663\n",
      "Epoch 35 batch 220 train Loss 282.2139 test Loss 112.7280 with MSE metric 44315.8152\n",
      "Epoch 35 batch 230 train Loss 281.9078 test Loss 112.6146 with MSE metric 44315.3655\n",
      "Epoch 35 batch 240 train Loss 281.6023 test Loss 112.5015 with MSE metric 44314.8838\n",
      "Time taken for 1 epoch: 30.756860971450806 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 281.2975 test Loss 112.3886 with MSE metric 44314.3072\n",
      "Epoch 36 batch 10 train Loss 280.9935 test Loss 112.2759 with MSE metric 44314.2527\n",
      "Epoch 36 batch 20 train Loss 280.6903 test Loss 112.1635 with MSE metric 44314.4038\n",
      "Epoch 36 batch 30 train Loss 280.3875 test Loss 112.0513 with MSE metric 44312.7680\n",
      "Epoch 36 batch 40 train Loss 280.0855 test Loss 111.9393 with MSE metric 44312.1401\n",
      "Epoch 36 batch 50 train Loss 279.7842 test Loss 111.8276 with MSE metric 44311.2905\n",
      "Epoch 36 batch 60 train Loss 279.4835 test Loss 111.7161 with MSE metric 44311.2226\n",
      "Epoch 36 batch 70 train Loss 279.1835 test Loss 111.6049 with MSE metric 44311.6194\n",
      "Epoch 36 batch 80 train Loss 278.8843 test Loss 111.4940 with MSE metric 44311.6559\n",
      "Epoch 36 batch 90 train Loss 278.5856 test Loss 111.3832 with MSE metric 44310.8827\n",
      "Epoch 36 batch 100 train Loss 278.2875 test Loss 111.2727 with MSE metric 44309.6556\n",
      "Epoch 36 batch 110 train Loss 277.9902 test Loss 111.1624 with MSE metric 44308.6758\n",
      "Epoch 36 batch 120 train Loss 277.6934 test Loss 111.0524 with MSE metric 44308.9942\n",
      "Epoch 36 batch 130 train Loss 277.3972 test Loss 110.9427 with MSE metric 44309.3514\n",
      "Epoch 36 batch 140 train Loss 277.1018 test Loss 110.8331 with MSE metric 44307.9419\n",
      "Epoch 36 batch 150 train Loss 276.8069 test Loss 110.7237 with MSE metric 44306.9068\n",
      "Epoch 36 batch 160 train Loss 276.5128 test Loss 110.6146 with MSE metric 44307.2725\n",
      "Epoch 36 batch 170 train Loss 276.2192 test Loss 110.5057 with MSE metric 44306.1406\n",
      "Epoch 36 batch 180 train Loss 275.9263 test Loss 110.3971 with MSE metric 44304.8760\n",
      "Epoch 36 batch 190 train Loss 275.6342 test Loss 110.2887 with MSE metric 44303.5625\n",
      "Epoch 36 batch 200 train Loss 275.3426 test Loss 110.1806 with MSE metric 44303.4807\n",
      "Epoch 36 batch 210 train Loss 275.0516 test Loss 110.0726 with MSE metric 44301.9333\n",
      "Epoch 36 batch 220 train Loss 274.7613 test Loss 109.9649 with MSE metric 44300.6103\n",
      "Epoch 36 batch 230 train Loss 274.4718 test Loss 109.8574 with MSE metric 44300.1058\n",
      "Epoch 36 batch 240 train Loss 274.1826 test Loss 109.7502 with MSE metric 44299.4267\n",
      "Time taken for 1 epoch: 29.93765091896057 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 273.8941 test Loss 109.6432 with MSE metric 44297.5856\n",
      "Epoch 37 batch 10 train Loss 273.6063 test Loss 109.5363 with MSE metric 44296.6035\n",
      "Epoch 37 batch 20 train Loss 273.3199 test Loss 109.4298 with MSE metric 44294.8407\n",
      "Epoch 37 batch 30 train Loss 273.0332 test Loss 109.3234 with MSE metric 44294.3430\n",
      "Epoch 37 batch 40 train Loss 272.7472 test Loss 109.2173 with MSE metric 44293.7683\n",
      "Epoch 37 batch 50 train Loss 272.4619 test Loss 109.1114 with MSE metric 44292.9573\n",
      "Epoch 37 batch 60 train Loss 272.1771 test Loss 109.0057 with MSE metric 44292.9156\n",
      "Epoch 37 batch 70 train Loss 271.8928 test Loss 108.9002 with MSE metric 44292.3996\n",
      "Epoch 37 batch 80 train Loss 271.6093 test Loss 108.7950 with MSE metric 44290.9496\n",
      "Epoch 37 batch 90 train Loss 271.3262 test Loss 108.6900 with MSE metric 44289.6007\n",
      "Epoch 37 batch 100 train Loss 271.0440 test Loss 108.5852 with MSE metric 44290.1560\n",
      "Epoch 37 batch 110 train Loss 270.7623 test Loss 108.4806 with MSE metric 44290.9327\n",
      "Epoch 37 batch 120 train Loss 270.4812 test Loss 108.3763 with MSE metric 44289.6274\n",
      "Epoch 37 batch 130 train Loss 270.2006 test Loss 108.2721 with MSE metric 44289.2790\n",
      "Epoch 37 batch 140 train Loss 269.9206 test Loss 108.1682 with MSE metric 44290.4066\n",
      "Epoch 37 batch 150 train Loss 269.6412 test Loss 108.0645 with MSE metric 44289.1958\n",
      "Epoch 37 batch 160 train Loss 269.3623 test Loss 107.9610 with MSE metric 44289.8178\n",
      "Epoch 37 batch 170 train Loss 269.0843 test Loss 107.8577 with MSE metric 44288.8699\n",
      "Epoch 37 batch 180 train Loss 268.8067 test Loss 107.7546 with MSE metric 44287.8802\n",
      "Epoch 37 batch 190 train Loss 268.5297 test Loss 107.6517 with MSE metric 44287.3426\n",
      "Epoch 37 batch 200 train Loss 268.2534 test Loss 107.5490 with MSE metric 44286.5428\n",
      "Epoch 37 batch 210 train Loss 267.9776 test Loss 107.4466 with MSE metric 44285.8722\n",
      "Epoch 37 batch 220 train Loss 267.7023 test Loss 107.3444 with MSE metric 44285.2446\n",
      "Epoch 37 batch 230 train Loss 267.4276 test Loss 107.2424 with MSE metric 44283.8042\n",
      "Epoch 37 batch 240 train Loss 267.1535 test Loss 107.1406 with MSE metric 44283.8780\n",
      "Time taken for 1 epoch: 32.63907194137573 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 266.8799 test Loss 107.0390 with MSE metric 44282.0343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 batch 10 train Loss 266.6070 test Loss 106.9376 with MSE metric 44281.3151\n",
      "Epoch 38 batch 20 train Loss 266.3348 test Loss 106.8364 with MSE metric 44281.0579\n",
      "Epoch 38 batch 30 train Loss 266.0629 test Loss 106.7355 with MSE metric 44280.2183\n",
      "Epoch 38 batch 40 train Loss 265.7916 test Loss 106.6347 with MSE metric 44279.0542\n",
      "Epoch 38 batch 50 train Loss 265.5209 test Loss 106.5341 with MSE metric 44279.6080\n",
      "Epoch 38 batch 60 train Loss 265.2507 test Loss 106.4338 with MSE metric 44278.0860\n",
      "Epoch 38 batch 70 train Loss 264.9811 test Loss 106.3336 with MSE metric 44276.4217\n",
      "Epoch 38 batch 80 train Loss 264.7121 test Loss 106.2337 with MSE metric 44275.2926\n",
      "Epoch 38 batch 90 train Loss 264.4436 test Loss 106.1339 with MSE metric 44274.6828\n",
      "Epoch 38 batch 100 train Loss 264.1757 test Loss 106.0344 with MSE metric 44273.8267\n",
      "Epoch 38 batch 110 train Loss 263.9085 test Loss 105.9350 with MSE metric 44273.1122\n",
      "Epoch 38 batch 120 train Loss 263.6417 test Loss 105.8359 with MSE metric 44271.9023\n",
      "Epoch 38 batch 130 train Loss 263.3755 test Loss 105.7369 with MSE metric 44271.1053\n",
      "Epoch 38 batch 140 train Loss 263.1103 test Loss 105.6382 with MSE metric 44270.4151\n",
      "Epoch 38 batch 150 train Loss 262.8453 test Loss 105.5396 with MSE metric 44269.8211\n",
      "Epoch 38 batch 160 train Loss 262.5807 test Loss 105.4413 with MSE metric 44268.6953\n",
      "Epoch 38 batch 170 train Loss 262.3167 test Loss 105.3431 with MSE metric 44269.0247\n",
      "Epoch 38 batch 180 train Loss 262.0532 test Loss 105.2452 with MSE metric 44268.7948\n",
      "Epoch 38 batch 190 train Loss 261.7902 test Loss 105.1474 with MSE metric 44266.9701\n",
      "Epoch 38 batch 200 train Loss 261.5278 test Loss 105.0498 with MSE metric 44266.0274\n",
      "Epoch 38 batch 210 train Loss 261.2660 test Loss 104.9525 with MSE metric 44265.5728\n",
      "Epoch 38 batch 220 train Loss 261.0047 test Loss 104.8553 with MSE metric 44264.4439\n",
      "Epoch 38 batch 230 train Loss 260.7439 test Loss 104.7583 with MSE metric 44263.9222\n",
      "Epoch 38 batch 240 train Loss 260.4837 test Loss 104.6616 with MSE metric 44264.8702\n",
      "Time taken for 1 epoch: 33.288737058639526 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 260.2241 test Loss 104.5650 with MSE metric 44264.4590\n",
      "Epoch 39 batch 10 train Loss 259.9653 test Loss 104.4686 with MSE metric 44265.1907\n",
      "Epoch 39 batch 20 train Loss 259.7066 test Loss 104.3724 with MSE metric 44264.0552\n",
      "Epoch 39 batch 30 train Loss 259.4485 test Loss 104.2764 with MSE metric 44263.9913\n",
      "Epoch 39 batch 40 train Loss 259.1912 test Loss 104.1806 with MSE metric 44262.8369\n",
      "Epoch 39 batch 50 train Loss 258.9342 test Loss 104.0850 with MSE metric 44261.9097\n",
      "Epoch 39 batch 60 train Loss 258.6777 test Loss 103.9896 with MSE metric 44261.1320\n",
      "Epoch 39 batch 70 train Loss 258.4216 test Loss 103.8944 with MSE metric 44260.6871\n",
      "Epoch 39 batch 80 train Loss 258.1661 test Loss 103.7993 with MSE metric 44259.5090\n",
      "Epoch 39 batch 90 train Loss 257.9115 test Loss 103.7045 with MSE metric 44258.0243\n",
      "Epoch 39 batch 100 train Loss 257.6570 test Loss 103.6098 with MSE metric 44256.1147\n",
      "Epoch 39 batch 110 train Loss 257.4031 test Loss 103.5153 with MSE metric 44255.8874\n",
      "Epoch 39 batch 120 train Loss 257.1496 test Loss 103.4211 with MSE metric 44254.0409\n",
      "Epoch 39 batch 130 train Loss 256.8967 test Loss 103.3270 with MSE metric 44252.3346\n",
      "Epoch 39 batch 140 train Loss 256.6443 test Loss 103.2331 with MSE metric 44250.7497\n",
      "Epoch 39 batch 150 train Loss 256.3923 test Loss 103.1394 with MSE metric 44249.3169\n",
      "Epoch 39 batch 160 train Loss 256.1410 test Loss 103.0458 with MSE metric 44247.8565\n",
      "Epoch 39 batch 170 train Loss 255.8900 test Loss 102.9524 with MSE metric 44247.7978\n",
      "Epoch 39 batch 180 train Loss 255.6396 test Loss 102.8593 with MSE metric 44246.6254\n",
      "Epoch 39 batch 190 train Loss 255.3904 test Loss 102.7663 with MSE metric 44246.0773\n",
      "Epoch 39 batch 200 train Loss 255.1415 test Loss 102.6735 with MSE metric 44244.7466\n",
      "Epoch 39 batch 210 train Loss 254.8927 test Loss 102.5809 with MSE metric 44244.7289\n",
      "Epoch 39 batch 220 train Loss 254.6442 test Loss 102.4885 with MSE metric 44244.2515\n",
      "Epoch 39 batch 230 train Loss 254.3963 test Loss 102.3962 with MSE metric 44243.2014\n",
      "Epoch 39 batch 240 train Loss 254.1489 test Loss 102.3042 with MSE metric 44242.6197\n",
      "Time taken for 1 epoch: 32.42719101905823 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 253.9020 test Loss 102.2123 with MSE metric 44241.3190\n",
      "Epoch 40 batch 10 train Loss 253.6557 test Loss 102.1206 with MSE metric 44239.6672\n",
      "Epoch 40 batch 20 train Loss 253.4099 test Loss 102.0291 with MSE metric 44238.6448\n",
      "Epoch 40 batch 30 train Loss 253.1644 test Loss 101.9377 with MSE metric 44237.8432\n",
      "Epoch 40 batch 40 train Loss 252.9196 test Loss 101.8466 with MSE metric 44236.8877\n",
      "Epoch 40 batch 50 train Loss 252.6751 test Loss 101.7556 with MSE metric 44236.7331\n",
      "Epoch 40 batch 60 train Loss 252.4312 test Loss 101.6648 with MSE metric 44236.9618\n",
      "Epoch 40 batch 70 train Loss 252.1877 test Loss 101.5742 with MSE metric 44235.4930\n",
      "Epoch 40 batch 80 train Loss 251.9448 test Loss 101.4837 with MSE metric 44234.6759\n",
      "Epoch 40 batch 90 train Loss 251.7022 test Loss 101.3934 with MSE metric 44232.7219\n",
      "Epoch 40 batch 100 train Loss 251.4601 test Loss 101.3033 with MSE metric 44232.2949\n",
      "Epoch 40 batch 110 train Loss 251.2185 test Loss 101.2134 with MSE metric 44231.5173\n",
      "Epoch 40 batch 120 train Loss 250.9774 test Loss 101.1236 with MSE metric 44230.1837\n",
      "Epoch 40 batch 130 train Loss 250.7368 test Loss 101.0340 with MSE metric 44230.0426\n",
      "Epoch 40 batch 140 train Loss 250.4966 test Loss 100.9446 with MSE metric 44228.8666\n",
      "Epoch 40 batch 150 train Loss 250.2569 test Loss 100.8554 with MSE metric 44228.0661\n",
      "Epoch 40 batch 160 train Loss 250.0178 test Loss 100.7663 with MSE metric 44228.3773\n",
      "Epoch 40 batch 170 train Loss 249.7791 test Loss 100.6774 with MSE metric 44228.0309\n",
      "Epoch 40 batch 180 train Loss 249.5409 test Loss 100.5887 with MSE metric 44227.2670\n",
      "Epoch 40 batch 190 train Loss 249.3030 test Loss 100.5001 with MSE metric 44225.0658\n",
      "Epoch 40 batch 200 train Loss 249.0657 test Loss 100.4117 with MSE metric 44224.5324\n",
      "Epoch 40 batch 210 train Loss 248.8288 test Loss 100.3235 with MSE metric 44223.6498\n",
      "Epoch 40 batch 220 train Loss 248.5924 test Loss 100.2354 with MSE metric 44222.9472\n",
      "Epoch 40 batch 230 train Loss 248.3564 test Loss 100.1475 with MSE metric 44222.1505\n",
      "Epoch 40 batch 240 train Loss 248.1209 test Loss 100.0598 with MSE metric 44221.2615\n",
      "Time taken for 1 epoch: 30.55947208404541 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 247.8859 test Loss 99.9723 with MSE metric 44220.8223\n",
      "Epoch 41 batch 10 train Loss 247.6513 test Loss 99.8849 with MSE metric 44219.3389\n",
      "Epoch 41 batch 20 train Loss 247.4171 test Loss 99.7976 with MSE metric 44218.4671\n",
      "Epoch 41 batch 30 train Loss 247.1835 test Loss 99.7106 with MSE metric 44217.6524\n",
      "Epoch 41 batch 40 train Loss 246.9503 test Loss 99.6237 with MSE metric 44216.3274\n",
      "Epoch 41 batch 50 train Loss 246.7175 test Loss 99.5369 with MSE metric 44216.1125\n",
      "Epoch 41 batch 60 train Loss 246.4854 test Loss 99.4504 with MSE metric 44215.7634\n",
      "Epoch 41 batch 70 train Loss 246.2536 test Loss 99.3640 with MSE metric 44215.1512\n",
      "Epoch 41 batch 80 train Loss 246.0220 test Loss 99.2777 with MSE metric 44213.7824\n",
      "Epoch 41 batch 90 train Loss 245.7911 test Loss 99.1916 with MSE metric 44212.5612\n",
      "Epoch 41 batch 100 train Loss 245.5606 test Loss 99.1057 with MSE metric 44210.3916\n",
      "Epoch 41 batch 110 train Loss 245.3305 test Loss 99.0199 with MSE metric 44209.9257\n",
      "Epoch 41 batch 120 train Loss 245.1010 test Loss 98.9343 with MSE metric 44210.2498\n",
      "Epoch 41 batch 130 train Loss 244.8718 test Loss 98.8489 with MSE metric 44209.8434\n",
      "Epoch 41 batch 140 train Loss 244.6429 test Loss 98.7636 with MSE metric 44208.3411\n",
      "Epoch 41 batch 150 train Loss 244.4146 test Loss 98.6785 with MSE metric 44206.9444\n",
      "Epoch 41 batch 160 train Loss 244.1867 test Loss 98.5935 with MSE metric 44206.3844\n",
      "Epoch 41 batch 170 train Loss 243.9591 test Loss 98.5087 with MSE metric 44205.4863\n",
      "Epoch 41 batch 180 train Loss 243.7321 test Loss 98.4241 with MSE metric 44204.6084\n",
      "Epoch 41 batch 190 train Loss 243.5056 test Loss 98.3396 with MSE metric 44202.9626\n",
      "Epoch 41 batch 200 train Loss 243.2794 test Loss 98.2553 with MSE metric 44201.6485\n",
      "Epoch 41 batch 210 train Loss 243.0537 test Loss 98.1711 with MSE metric 44201.2445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 batch 220 train Loss 242.8285 test Loss 98.0871 with MSE metric 44200.8859\n",
      "Epoch 41 batch 230 train Loss 242.6036 test Loss 98.0032 with MSE metric 44200.7658\n",
      "Epoch 41 batch 240 train Loss 242.3791 test Loss 97.9195 with MSE metric 44200.1878\n",
      "Time taken for 1 epoch: 33.621551752090454 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 242.1551 test Loss 97.8359 with MSE metric 44200.0555\n",
      "Epoch 42 batch 10 train Loss 241.9316 test Loss 97.7525 with MSE metric 44199.4589\n",
      "Epoch 42 batch 20 train Loss 241.7084 test Loss 97.6693 with MSE metric 44199.1145\n",
      "Epoch 42 batch 30 train Loss 241.4857 test Loss 97.5862 with MSE metric 44198.2599\n",
      "Epoch 42 batch 40 train Loss 241.2634 test Loss 97.5032 with MSE metric 44196.2663\n",
      "Epoch 42 batch 50 train Loss 241.0415 test Loss 97.4205 with MSE metric 44195.1520\n",
      "Epoch 42 batch 60 train Loss 240.8200 test Loss 97.3378 with MSE metric 44194.8486\n",
      "Epoch 42 batch 70 train Loss 240.5990 test Loss 97.2553 with MSE metric 44194.6064\n",
      "Epoch 42 batch 80 train Loss 240.3788 test Loss 97.1730 with MSE metric 44193.1305\n",
      "Epoch 42 batch 90 train Loss 240.1586 test Loss 97.0909 with MSE metric 44191.9936\n",
      "Epoch 42 batch 100 train Loss 239.9388 test Loss 97.0089 with MSE metric 44192.2105\n",
      "Epoch 42 batch 110 train Loss 239.7194 test Loss 96.9270 with MSE metric 44191.5003\n",
      "Epoch 42 batch 120 train Loss 239.5004 test Loss 96.8453 with MSE metric 44190.5800\n",
      "Epoch 42 batch 130 train Loss 239.2818 test Loss 96.7638 with MSE metric 44188.9190\n",
      "Epoch 42 batch 140 train Loss 239.0636 test Loss 96.6824 with MSE metric 44188.6206\n",
      "Epoch 42 batch 150 train Loss 238.8460 test Loss 96.6011 with MSE metric 44187.4135\n",
      "Epoch 42 batch 160 train Loss 238.6287 test Loss 96.5200 with MSE metric 44187.4582\n",
      "Epoch 42 batch 170 train Loss 238.4116 test Loss 96.4390 with MSE metric 44186.8488\n",
      "Epoch 42 batch 180 train Loss 238.1951 test Loss 96.3582 with MSE metric 44185.8520\n",
      "Epoch 42 batch 190 train Loss 237.9789 test Loss 96.2776 with MSE metric 44185.9766\n",
      "Epoch 42 batch 200 train Loss 237.7631 test Loss 96.1970 with MSE metric 44184.6833\n",
      "Epoch 42 batch 210 train Loss 237.5477 test Loss 96.1166 with MSE metric 44184.1209\n",
      "Epoch 42 batch 220 train Loss 237.3328 test Loss 96.0364 with MSE metric 44183.3255\n",
      "Epoch 42 batch 230 train Loss 237.1182 test Loss 95.9563 with MSE metric 44182.0264\n",
      "Epoch 42 batch 240 train Loss 236.9041 test Loss 95.8763 with MSE metric 44180.0684\n",
      "Time taken for 1 epoch: 33.12707304954529 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 236.6902 test Loss 95.7965 with MSE metric 44178.6335\n",
      "Epoch 43 batch 10 train Loss 236.4769 test Loss 95.7168 with MSE metric 44177.4926\n",
      "Epoch 43 batch 20 train Loss 236.2639 test Loss 95.6373 with MSE metric 44176.4451\n",
      "Epoch 43 batch 30 train Loss 236.0514 test Loss 95.5579 with MSE metric 44175.7181\n",
      "Epoch 43 batch 40 train Loss 235.8392 test Loss 95.4786 with MSE metric 44174.7513\n",
      "Epoch 43 batch 50 train Loss 235.6274 test Loss 95.3995 with MSE metric 44173.3290\n",
      "Epoch 43 batch 60 train Loss 235.4160 test Loss 95.3206 with MSE metric 44171.4713\n",
      "Epoch 43 batch 70 train Loss 235.2050 test Loss 95.2418 with MSE metric 44170.6628\n",
      "Epoch 43 batch 80 train Loss 234.9943 test Loss 95.1631 with MSE metric 44169.3941\n",
      "Epoch 43 batch 90 train Loss 234.7841 test Loss 95.0845 with MSE metric 44169.0801\n",
      "Epoch 43 batch 100 train Loss 234.5742 test Loss 95.0061 with MSE metric 44167.6645\n",
      "Epoch 43 batch 110 train Loss 234.3647 test Loss 94.9278 with MSE metric 44166.2373\n",
      "Epoch 43 batch 120 train Loss 234.1556 test Loss 94.8497 with MSE metric 44165.0383\n",
      "Epoch 43 batch 130 train Loss 233.9468 test Loss 94.7717 with MSE metric 44163.2692\n",
      "Epoch 43 batch 140 train Loss 233.7385 test Loss 94.6938 with MSE metric 44161.9758\n",
      "Epoch 43 batch 150 train Loss 233.5305 test Loss 94.6161 with MSE metric 44160.9732\n",
      "Epoch 43 batch 160 train Loss 233.3229 test Loss 94.5385 with MSE metric 44159.0930\n",
      "Epoch 43 batch 170 train Loss 233.1158 test Loss 94.4610 with MSE metric 44158.6332\n",
      "Epoch 43 batch 180 train Loss 232.9090 test Loss 94.3837 with MSE metric 44157.0270\n",
      "Epoch 43 batch 190 train Loss 232.7026 test Loss 94.3066 with MSE metric 44155.6314\n",
      "Epoch 43 batch 200 train Loss 232.4965 test Loss 94.2295 with MSE metric 44154.2341\n",
      "Epoch 43 batch 210 train Loss 232.2908 test Loss 94.1526 with MSE metric 44154.2762\n",
      "Epoch 43 batch 220 train Loss 232.0855 test Loss 94.0759 with MSE metric 44152.8231\n",
      "Epoch 43 batch 230 train Loss 231.8806 test Loss 93.9992 with MSE metric 44151.3461\n",
      "Epoch 43 batch 240 train Loss 231.6760 test Loss 93.9227 with MSE metric 44149.8299\n",
      "Time taken for 1 epoch: 31.869013786315918 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 231.4718 test Loss 93.8464 with MSE metric 44149.7274\n",
      "Epoch 44 batch 10 train Loss 231.2680 test Loss 93.7701 with MSE metric 44148.9147\n",
      "Epoch 44 batch 20 train Loss 231.0645 test Loss 93.6941 with MSE metric 44147.2564\n",
      "Epoch 44 batch 30 train Loss 230.8614 test Loss 93.6181 with MSE metric 44146.7435\n",
      "Epoch 44 batch 40 train Loss 230.6587 test Loss 93.5423 with MSE metric 44146.6256\n",
      "Epoch 44 batch 50 train Loss 230.4563 test Loss 93.4666 with MSE metric 44145.7459\n",
      "Epoch 44 batch 60 train Loss 230.2545 test Loss 93.3910 with MSE metric 44145.0781\n",
      "Epoch 44 batch 70 train Loss 230.0528 test Loss 93.3156 with MSE metric 44145.2275\n",
      "Epoch 44 batch 80 train Loss 229.8516 test Loss 93.2403 with MSE metric 44142.7437\n",
      "Epoch 44 batch 90 train Loss 229.6507 test Loss 93.1651 with MSE metric 44141.6895\n",
      "Epoch 44 batch 100 train Loss 229.4502 test Loss 93.0900 with MSE metric 44141.1169\n",
      "Epoch 44 batch 110 train Loss 229.2500 test Loss 93.0151 with MSE metric 44141.3896\n",
      "Epoch 44 batch 120 train Loss 229.0501 test Loss 92.9404 with MSE metric 44140.2872\n",
      "Epoch 44 batch 130 train Loss 228.8506 test Loss 92.8657 with MSE metric 44138.6801\n",
      "Epoch 44 batch 140 train Loss 228.6515 test Loss 92.7912 with MSE metric 44138.0042\n",
      "Epoch 44 batch 150 train Loss 228.4528 test Loss 92.7168 with MSE metric 44135.1291\n",
      "Epoch 44 batch 160 train Loss 228.2544 test Loss 92.6426 with MSE metric 44134.1952\n",
      "Epoch 44 batch 170 train Loss 228.0563 test Loss 92.5685 with MSE metric 44132.9806\n",
      "Epoch 44 batch 180 train Loss 227.8585 test Loss 92.4945 with MSE metric 44129.9665\n",
      "Epoch 44 batch 190 train Loss 227.6612 test Loss 92.4206 with MSE metric 44128.8531\n",
      "Epoch 44 batch 200 train Loss 227.4642 test Loss 92.3468 with MSE metric 44127.5864\n",
      "Epoch 44 batch 210 train Loss 227.2675 test Loss 92.2732 with MSE metric 44126.5566\n",
      "Epoch 44 batch 220 train Loss 227.0712 test Loss 92.1997 with MSE metric 44125.5749\n",
      "Epoch 44 batch 230 train Loss 226.8752 test Loss 92.1263 with MSE metric 44124.5601\n",
      "Epoch 44 batch 240 train Loss 226.6797 test Loss 92.0531 with MSE metric 44125.0060\n",
      "Time taken for 1 epoch: 31.517754793167114 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 226.4845 test Loss 91.9800 with MSE metric 44123.4724\n",
      "Epoch 45 batch 10 train Loss 226.2896 test Loss 91.9070 with MSE metric 44122.6808\n",
      "Epoch 45 batch 20 train Loss 226.0952 test Loss 91.8342 with MSE metric 44121.4780\n",
      "Epoch 45 batch 30 train Loss 225.9010 test Loss 91.7615 with MSE metric 44120.8613\n",
      "Epoch 45 batch 40 train Loss 225.7071 test Loss 91.6889 with MSE metric 44119.1131\n",
      "Epoch 45 batch 50 train Loss 225.5136 test Loss 91.6164 with MSE metric 44118.8455\n",
      "Epoch 45 batch 60 train Loss 225.3203 test Loss 91.5441 with MSE metric 44116.8436\n",
      "Epoch 45 batch 70 train Loss 225.1274 test Loss 91.4718 with MSE metric 44114.5110\n",
      "Epoch 45 batch 80 train Loss 224.9349 test Loss 91.3997 with MSE metric 44113.7890\n",
      "Epoch 45 batch 90 train Loss 224.7427 test Loss 91.3277 with MSE metric 44111.3984\n",
      "Epoch 45 batch 100 train Loss 224.5508 test Loss 91.2559 with MSE metric 44109.8675\n",
      "Epoch 45 batch 110 train Loss 224.3593 test Loss 91.1841 with MSE metric 44109.5829\n",
      "Epoch 45 batch 120 train Loss 224.1682 test Loss 91.1125 with MSE metric 44108.1699\n",
      "Epoch 45 batch 130 train Loss 223.9773 test Loss 91.0410 with MSE metric 44107.7133\n",
      "Epoch 45 batch 140 train Loss 223.7868 test Loss 90.9696 with MSE metric 44107.0000\n",
      "Epoch 45 batch 150 train Loss 223.5966 test Loss 90.8984 with MSE metric 44105.2737\n",
      "Epoch 45 batch 160 train Loss 223.4069 test Loss 90.8272 with MSE metric 44104.0978\n",
      "Epoch 45 batch 170 train Loss 223.2174 test Loss 90.7562 with MSE metric 44102.9904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 batch 180 train Loss 223.0282 test Loss 90.6853 with MSE metric 44101.5011\n",
      "Epoch 45 batch 190 train Loss 222.8394 test Loss 90.6146 with MSE metric 44100.6974\n",
      "Epoch 45 batch 200 train Loss 222.6510 test Loss 90.5439 with MSE metric 44099.5101\n",
      "Epoch 45 batch 210 train Loss 222.4628 test Loss 90.4734 with MSE metric 44098.1245\n",
      "Epoch 45 batch 220 train Loss 222.2749 test Loss 90.4029 with MSE metric 44097.0320\n",
      "Epoch 45 batch 230 train Loss 222.0874 test Loss 90.3326 with MSE metric 44096.3366\n",
      "Epoch 45 batch 240 train Loss 221.9002 test Loss 90.2625 with MSE metric 44094.5095\n",
      "Time taken for 1 epoch: 33.040276288986206 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 221.7133 test Loss 90.1924 with MSE metric 44093.0937\n",
      "Epoch 46 batch 10 train Loss 221.5267 test Loss 90.1225 with MSE metric 44091.4200\n",
      "Epoch 46 batch 20 train Loss 221.3404 test Loss 90.0526 with MSE metric 44089.4272\n",
      "Epoch 46 batch 30 train Loss 221.1545 test Loss 89.9829 with MSE metric 44087.6704\n",
      "Epoch 46 batch 40 train Loss 220.9689 test Loss 89.9133 with MSE metric 44085.8873\n",
      "Epoch 46 batch 50 train Loss 220.7836 test Loss 89.8438 with MSE metric 44084.5413\n",
      "Epoch 46 batch 60 train Loss 220.5986 test Loss 89.7745 with MSE metric 44084.2292\n",
      "Epoch 46 batch 70 train Loss 220.4140 test Loss 89.7052 with MSE metric 44083.1551\n",
      "Epoch 46 batch 80 train Loss 220.2296 test Loss 89.6361 with MSE metric 44081.7022\n",
      "Epoch 46 batch 90 train Loss 220.0457 test Loss 89.5670 with MSE metric 44081.0737\n",
      "Epoch 46 batch 100 train Loss 219.8620 test Loss 89.4981 with MSE metric 44079.8102\n",
      "Epoch 46 batch 110 train Loss 219.6786 test Loss 89.4293 with MSE metric 44077.8945\n",
      "Epoch 46 batch 120 train Loss 219.4955 test Loss 89.3606 with MSE metric 44076.7422\n",
      "Epoch 46 batch 130 train Loss 219.3129 test Loss 89.2921 with MSE metric 44075.3843\n",
      "Epoch 46 batch 140 train Loss 219.1304 test Loss 89.2236 with MSE metric 44073.9651\n",
      "Epoch 46 batch 150 train Loss 218.9483 test Loss 89.1553 with MSE metric 44072.9436\n",
      "Epoch 46 batch 160 train Loss 218.7664 test Loss 89.0871 with MSE metric 44071.4618\n",
      "Epoch 46 batch 170 train Loss 218.5850 test Loss 89.0189 with MSE metric 44070.8005\n",
      "Epoch 46 batch 180 train Loss 218.4038 test Loss 88.9509 with MSE metric 44069.9641\n",
      "Epoch 46 batch 190 train Loss 218.2229 test Loss 88.8830 with MSE metric 44068.8582\n",
      "Epoch 46 batch 200 train Loss 218.0424 test Loss 88.8152 with MSE metric 44067.6283\n",
      "Epoch 46 batch 210 train Loss 217.8623 test Loss 88.7476 with MSE metric 44065.9127\n",
      "Epoch 46 batch 220 train Loss 217.6824 test Loss 88.6801 with MSE metric 44064.1582\n",
      "Epoch 46 batch 230 train Loss 217.5027 test Loss 88.6126 with MSE metric 44062.9305\n",
      "Epoch 46 batch 240 train Loss 217.3233 test Loss 88.5453 with MSE metric 44062.9746\n",
      "Time taken for 1 epoch: 31.011949062347412 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 217.1442 test Loss 88.4781 with MSE metric 44060.7639\n",
      "Epoch 47 batch 10 train Loss 216.9654 test Loss 88.4110 with MSE metric 44058.9695\n",
      "Epoch 47 batch 20 train Loss 216.7870 test Loss 88.3440 with MSE metric 44057.8283\n",
      "Epoch 47 batch 30 train Loss 216.6088 test Loss 88.2771 with MSE metric 44056.5806\n",
      "Epoch 47 batch 40 train Loss 216.4310 test Loss 88.2103 with MSE metric 44054.7489\n",
      "Epoch 47 batch 50 train Loss 216.2535 test Loss 88.1437 with MSE metric 44053.6128\n",
      "Epoch 47 batch 60 train Loss 216.0762 test Loss 88.0771 with MSE metric 44051.8764\n",
      "Epoch 47 batch 70 train Loss 215.8992 test Loss 88.0107 with MSE metric 44049.7983\n",
      "Epoch 47 batch 80 train Loss 215.7226 test Loss 87.9443 with MSE metric 44048.1402\n",
      "Epoch 47 batch 90 train Loss 215.5463 test Loss 87.8781 with MSE metric 44046.5615\n",
      "Epoch 47 batch 100 train Loss 215.3702 test Loss 87.8120 with MSE metric 44044.8843\n",
      "Epoch 47 batch 110 train Loss 215.1944 test Loss 87.7460 with MSE metric 44044.2802\n",
      "Epoch 47 batch 120 train Loss 215.0191 test Loss 87.6801 with MSE metric 44042.8478\n",
      "Epoch 47 batch 130 train Loss 214.8440 test Loss 87.6143 with MSE metric 44042.1005\n",
      "Epoch 47 batch 140 train Loss 214.6691 test Loss 87.5486 with MSE metric 44040.8183\n",
      "Epoch 47 batch 150 train Loss 214.4945 test Loss 87.4830 with MSE metric 44039.8121\n",
      "Epoch 47 batch 160 train Loss 214.3202 test Loss 87.4176 with MSE metric 44038.9089\n",
      "Epoch 47 batch 170 train Loss 214.1462 test Loss 87.3522 with MSE metric 44036.8354\n",
      "Epoch 47 batch 180 train Loss 213.9724 test Loss 87.2869 with MSE metric 44035.7625\n",
      "Epoch 47 batch 190 train Loss 213.7990 test Loss 87.2218 with MSE metric 44034.6874\n",
      "Epoch 47 batch 200 train Loss 213.6258 test Loss 87.1567 with MSE metric 44033.2088\n",
      "Epoch 47 batch 210 train Loss 213.4529 test Loss 87.0917 with MSE metric 44031.2126\n",
      "Epoch 47 batch 220 train Loss 213.2804 test Loss 87.0268 with MSE metric 44030.2011\n",
      "Epoch 47 batch 230 train Loss 213.1081 test Loss 86.9621 with MSE metric 44028.6736\n",
      "Epoch 47 batch 240 train Loss 212.9361 test Loss 86.8974 with MSE metric 44026.6710\n",
      "Time taken for 1 epoch: 31.84079623222351 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 212.7644 test Loss 86.8329 with MSE metric 44024.8405\n",
      "Epoch 48 batch 10 train Loss 212.5929 test Loss 86.7684 with MSE metric 44023.5235\n",
      "Epoch 48 batch 20 train Loss 212.4218 test Loss 86.7040 with MSE metric 44022.8148\n",
      "Epoch 48 batch 30 train Loss 212.2510 test Loss 86.6398 with MSE metric 44021.7463\n",
      "Epoch 48 batch 40 train Loss 212.0804 test Loss 86.5757 with MSE metric 44020.3698\n",
      "Epoch 48 batch 50 train Loss 211.9101 test Loss 86.5116 with MSE metric 44018.4045\n",
      "Epoch 48 batch 60 train Loss 211.7401 test Loss 86.4477 with MSE metric 44017.6237\n",
      "Epoch 48 batch 70 train Loss 211.5703 test Loss 86.3838 with MSE metric 44016.4899\n",
      "Epoch 48 batch 80 train Loss 211.4009 test Loss 86.3201 with MSE metric 44015.0179\n",
      "Epoch 48 batch 90 train Loss 211.2317 test Loss 86.2565 with MSE metric 44013.5394\n",
      "Epoch 48 batch 100 train Loss 211.0628 test Loss 86.1929 with MSE metric 44011.9281\n",
      "Epoch 48 batch 110 train Loss 210.8943 test Loss 86.1295 with MSE metric 44009.9233\n",
      "Epoch 48 batch 120 train Loss 210.7260 test Loss 86.0662 with MSE metric 44008.4332\n",
      "Epoch 48 batch 130 train Loss 210.5579 test Loss 86.0029 with MSE metric 44007.3379\n",
      "Epoch 48 batch 140 train Loss 210.3901 test Loss 85.9398 with MSE metric 44005.7455\n",
      "Epoch 48 batch 150 train Loss 210.2226 test Loss 85.8768 with MSE metric 44004.6245\n",
      "Epoch 48 batch 160 train Loss 210.0554 test Loss 85.8139 with MSE metric 44002.9767\n",
      "Epoch 48 batch 170 train Loss 209.8884 test Loss 85.7510 with MSE metric 44001.0329\n",
      "Epoch 48 batch 180 train Loss 209.7217 test Loss 85.6883 with MSE metric 44000.1619\n",
      "Epoch 48 batch 190 train Loss 209.5552 test Loss 85.6257 with MSE metric 43999.0776\n",
      "Epoch 48 batch 200 train Loss 209.3891 test Loss 85.5631 with MSE metric 43996.8495\n",
      "Epoch 48 batch 210 train Loss 209.2232 test Loss 85.5007 with MSE metric 43995.3758\n",
      "Epoch 48 batch 220 train Loss 209.0577 test Loss 85.4384 with MSE metric 43993.8374\n",
      "Epoch 48 batch 230 train Loss 208.8924 test Loss 85.3761 with MSE metric 43992.5598\n",
      "Epoch 48 batch 240 train Loss 208.7274 test Loss 85.3140 with MSE metric 43991.0288\n",
      "Time taken for 1 epoch: 31.3562970161438 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 208.5626 test Loss 85.2519 with MSE metric 43989.2086\n",
      "Epoch 49 batch 10 train Loss 208.3980 test Loss 85.1900 with MSE metric 43987.9699\n",
      "Epoch 49 batch 20 train Loss 208.2337 test Loss 85.1282 with MSE metric 43986.1418\n",
      "Epoch 49 batch 30 train Loss 208.0697 test Loss 85.0664 with MSE metric 43984.4799\n",
      "Epoch 49 batch 40 train Loss 207.9060 test Loss 85.0047 with MSE metric 43982.0884\n",
      "Epoch 49 batch 50 train Loss 207.7425 test Loss 84.9432 with MSE metric 43980.9463\n",
      "Epoch 49 batch 60 train Loss 207.5793 test Loss 84.8817 with MSE metric 43979.2208\n",
      "Epoch 49 batch 70 train Loss 207.4163 test Loss 84.8203 with MSE metric 43977.8558\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s=32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 10 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s= 32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
