{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dt'] = (pd.to_datetime(df['time']).dt.year % pd.to_datetime(df['time']).dt.year[0] )+ (pd.to_datetime(df['time']).dt.month / 12 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 10000\n",
    "combined_array = np.zeros((n_rows * 3, 40))\n",
    "for row in range(0, n_rows * 3, 3):\n",
    "    uni = list(df['number'].unique())\n",
    "    num = np.random.choice(uni, 2, replace = False)\n",
    "    df_1 = df[df['number'] == num[0]];  df_2 = df[df['number'] == num[1]]\n",
    "    \n",
    "    # pick differing sequence length\n",
    "    seq_len = np.random.choice(np.arange(5, 20), 2);\n",
    "    seq1_idx = list(np.random.choice(list(df_1.index), seq_len[0]))\n",
    "    seq2_idx = list(np.random.choice(list(df_2.index), seq_len[1]))\n",
    "    m = (seq_len[0] + seq_len[1])\n",
    "    combined_array[row, :m] = np.concatenate((df_1.loc[seq1_idx, 't2m'], df_2.loc[seq2_idx, 't2m']))\n",
    "    combined_array[row + 1, :m] = np.concatenate((df_1.loc[seq1_idx, 'dt'], df_2.loc[seq2_idx, 'dt']))\n",
    "    combined_array[row + 2, :m] = np.concatenate((df_1.loc[seq1_idx, 'number'], df_2.loc[seq2_idx, 'number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = combined_array[0::3]\n",
    "t = combined_array[1::3]\n",
    "token = combined_array[2::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Initializing from scratch.\n",
      "Epoch 0 batch 0 train Loss 276635.7271 test Loss 92122.0270 with MSE metric 45551.5664\n",
      "Epoch 0 batch 10 train Loss 147707.3831 test Loss 46122.4647 with MSE metric 46528.5883\n",
      "Epoch 0 batch 20 train Loss 88084.5578 test Loss 30754.6986 with MSE metric 46206.7365\n",
      "Epoch 0 batch 30 train Loss 62429.0003 test Loss 23068.2816 with MSE metric 45921.8479\n",
      "Epoch 0 batch 40 train Loss 48274.3682 test Loss 18456.1269 with MSE metric 46211.5025\n",
      "Epoch 0 batch 50 train Loss 39442.4337 test Loss 15381.2496 with MSE metric 46455.9027\n",
      "Epoch 0 batch 60 train Loss 33508.3575 test Loss 13184.8707 with MSE metric 46346.1084\n",
      "Epoch 0 batch 70 train Loss 29167.5388 test Loss 11537.5780 with MSE metric 46335.6242\n",
      "Epoch 0 batch 80 train Loss 25797.9345 test Loss 10256.3357 with MSE metric 46273.0657\n",
      "Epoch 0 batch 90 train Loss 23146.2318 test Loss 9231.3473 with MSE metric 46227.9679\n",
      "Epoch 0 batch 100 train Loss 21060.0951 test Loss 8392.7229 with MSE metric 46165.1620\n",
      "Epoch 0 batch 110 train Loss 19309.1080 test Loss 7693.8508 with MSE metric 46050.8190\n",
      "Epoch 0 batch 120 train Loss 17813.1833 test Loss 7102.5077 with MSE metric 46037.1066\n",
      "Epoch 0 batch 130 train Loss 16545.6666 test Loss 6595.6374 with MSE metric 45987.3798\n",
      "Epoch 0 batch 140 train Loss 15436.1051 test Loss 6156.3540 with MSE metric 45965.9638\n",
      "Epoch 0 batch 150 train Loss 14490.9189 test Loss 5771.9818 with MSE metric 45837.2313\n",
      "Epoch 0 batch 160 train Loss 13648.2320 test Loss 5432.8268 with MSE metric 45738.6189\n",
      "Epoch 0 batch 170 train Loss 12894.9693 test Loss 5131.3574 with MSE metric 45722.8218\n",
      "Epoch 0 batch 180 train Loss 12231.2676 test Loss 4861.6208 with MSE metric 45704.8512\n",
      "Epoch 0 batch 190 train Loss 11666.5050 test Loss 4618.8589 with MSE metric 45681.4122\n",
      "Epoch 0 batch 200 train Loss 11118.5316 test Loss 4399.2165 with MSE metric 45638.4006\n",
      "Epoch 0 batch 210 train Loss 10623.2994 test Loss 4199.5465 with MSE metric 45616.0264\n",
      "Epoch 0 batch 220 train Loss 10173.6591 test Loss 4017.2419 with MSE metric 45623.0627\n",
      "Epoch 0 batch 230 train Loss 9873.5946 test Loss 3850.1275 with MSE metric 45636.4663\n",
      "Epoch 0 batch 240 train Loss 9480.7455 test Loss 3696.3865 with MSE metric 45682.9595\n",
      "Time taken for 1 epoch: 29.80279779434204 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 9124.2280 test Loss 3554.4753 with MSE metric 45660.4712\n",
      "Epoch 1 batch 10 train Loss 8791.8516 test Loss 3423.0775 with MSE metric 45631.3107\n",
      "Epoch 1 batch 20 train Loss 8483.4931 test Loss 3301.0655 with MSE metric 45624.8822\n",
      "Epoch 1 batch 30 train Loss 8197.5235 test Loss 3187.4693 with MSE metric 45595.5737\n",
      "Epoch 1 batch 40 train Loss 7926.3725 test Loss 3081.4480 with MSE metric 45558.1109\n",
      "Epoch 1 batch 50 train Loss 7672.9370 test Loss 2982.2665 with MSE metric 45564.8212\n",
      "Epoch 1 batch 60 train Loss 7435.2876 test Loss 2889.2848 with MSE metric 45548.5692\n",
      "Epoch 1 batch 70 train Loss 7216.6531 test Loss 2801.9395 with MSE metric 45565.6623\n",
      "Epoch 1 batch 80 train Loss 7013.3297 test Loss 2719.7330 with MSE metric 45543.4595\n",
      "Epoch 1 batch 90 train Loss 6815.8027 test Loss 2642.2251 with MSE metric 45544.6597\n",
      "Epoch 1 batch 100 train Loss 6630.0385 test Loss 2569.0271 with MSE metric 45511.0475\n",
      "Epoch 1 batch 110 train Loss 6454.1176 test Loss 2499.7850 with MSE metric 45552.1871\n",
      "Epoch 1 batch 120 train Loss 6286.4269 test Loss 2434.1891 with MSE metric 45541.7238\n",
      "Epoch 1 batch 130 train Loss 6127.5972 test Loss 2371.9559 with MSE metric 45516.7344\n",
      "Epoch 1 batch 140 train Loss 5977.9277 test Loss 2312.8366 with MSE metric 45525.2810\n",
      "Epoch 1 batch 150 train Loss 5834.2226 test Loss 2256.6018 with MSE metric 45501.9900\n",
      "Epoch 1 batch 160 train Loss 5696.9706 test Loss 2203.0444 with MSE metric 45504.5150\n",
      "Epoch 1 batch 170 train Loss 5567.8414 test Loss 2151.9788 with MSE metric 45511.1050\n",
      "Epoch 1 batch 180 train Loss 5444.3873 test Loss 2103.2348 with MSE metric 45505.7568\n",
      "Epoch 1 batch 190 train Loss 5327.1111 test Loss 2056.6562 with MSE metric 45486.8465\n",
      "Epoch 1 batch 200 train Loss 5215.3255 test Loss 2012.1051 with MSE metric 45456.5968\n",
      "Epoch 1 batch 210 train Loss 5107.2656 test Loss 1969.4498 with MSE metric 45438.7616\n",
      "Epoch 1 batch 220 train Loss 5004.6703 test Loss 1928.5730 with MSE metric 45416.6755\n",
      "Epoch 1 batch 230 train Loss 4904.7037 test Loss 1889.3640 with MSE metric 45411.7511\n",
      "Epoch 1 batch 240 train Loss 4808.9931 test Loss 1851.7242 with MSE metric 45396.3384\n",
      "Time taken for 1 epoch: 30.204960107803345 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 4715.8155 test Loss 1815.5617 with MSE metric 45372.3463\n",
      "Epoch 2 batch 10 train Loss 4626.4286 test Loss 1780.7894 with MSE metric 45407.1597\n",
      "Epoch 2 batch 20 train Loss 4540.6385 test Loss 1747.3304 with MSE metric 45395.9634\n",
      "Epoch 2 batch 30 train Loss 4458.9864 test Loss 1715.1112 with MSE metric 45368.7647\n",
      "Epoch 2 batch 40 train Loss 4378.9445 test Loss 1684.0633 with MSE metric 45358.9707\n",
      "Epoch 2 batch 50 train Loss 4301.6845 test Loss 1654.1259 with MSE metric 45358.0889\n",
      "Epoch 2 batch 60 train Loss 4226.7513 test Loss 1625.2389 with MSE metric 45356.1640\n",
      "Epoch 2 batch 70 train Loss 4155.5887 test Loss 1597.3483 with MSE metric 45337.4026\n",
      "Epoch 2 batch 80 train Loss 4085.8513 test Loss 1570.4030 with MSE metric 45320.4106\n",
      "Epoch 2 batch 90 train Loss 4018.8824 test Loss 1544.3560 with MSE metric 45301.5502\n",
      "Epoch 2 batch 100 train Loss 3953.5983 test Loss 1519.1634 with MSE metric 45312.4614\n",
      "Epoch 2 batch 110 train Loss 3907.1685 test Loss 1494.7844 with MSE metric 45320.0233\n",
      "Epoch 2 batch 120 train Loss 3846.0601 test Loss 1471.1793 with MSE metric 45321.6546\n",
      "Epoch 2 batch 130 train Loss 3786.8848 test Loss 1448.3114 with MSE metric 45310.0291\n",
      "Epoch 2 batch 140 train Loss 3730.0600 test Loss 1426.1475 with MSE metric 45307.0465\n",
      "Epoch 2 batch 150 train Loss 3674.7688 test Loss 1404.6558 with MSE metric 45301.0975\n",
      "Epoch 2 batch 160 train Loss 3622.3668 test Loss 1383.8053 with MSE metric 45284.5422\n",
      "Epoch 2 batch 170 train Loss 3570.4887 test Loss 1363.5687 with MSE metric 45264.1534\n",
      "Epoch 2 batch 180 train Loss 3519.4906 test Loss 1343.9183 with MSE metric 45255.2268\n",
      "Epoch 2 batch 190 train Loss 3469.9788 test Loss 1324.8305 with MSE metric 45242.3446\n",
      "Epoch 2 batch 200 train Loss 3421.7977 test Loss 1306.2798 with MSE metric 45234.8210\n",
      "Epoch 2 batch 210 train Loss 3374.9198 test Loss 1288.2458 with MSE metric 45216.6375\n",
      "Epoch 2 batch 220 train Loss 3329.2466 test Loss 1270.7057 with MSE metric 45224.7367\n",
      "Epoch 2 batch 230 train Loss 3290.2829 test Loss 1253.6399 with MSE metric 45212.4944\n",
      "Epoch 2 batch 240 train Loss 3247.0155 test Loss 1237.0290 with MSE metric 45213.0627\n",
      "Time taken for 1 epoch: 31.725919008255005 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 3204.6864 test Loss 1220.8559 with MSE metric 45215.3255\n",
      "Epoch 3 batch 10 train Loss 3164.4331 test Loss 1205.1031 with MSE metric 45203.2527\n",
      "Epoch 3 batch 20 train Loss 3124.5183 test Loss 1189.7540 with MSE metric 45187.2403\n",
      "Epoch 3 batch 30 train Loss 3085.7194 test Loss 1174.7932 with MSE metric 45186.5168\n",
      "Epoch 3 batch 40 train Loss 3047.8729 test Loss 1160.2082 with MSE metric 45193.8979\n",
      "Epoch 3 batch 50 train Loss 3010.8848 test Loss 1145.9832 with MSE metric 45201.4190\n",
      "Epoch 3 batch 60 train Loss 2974.5083 test Loss 1132.1055 with MSE metric 45197.5738\n",
      "Epoch 3 batch 70 train Loss 2939.4955 test Loss 1118.5618 with MSE metric 45179.5291\n",
      "Epoch 3 batch 80 train Loss 2904.8209 test Loss 1105.3416 with MSE metric 45177.5222\n",
      "Epoch 3 batch 90 train Loss 2871.3609 test Loss 1092.4312 with MSE metric 45154.4694\n",
      "Epoch 3 batch 100 train Loss 2838.4304 test Loss 1079.8221 with MSE metric 45135.9870\n",
      "Epoch 3 batch 110 train Loss 2806.0842 test Loss 1067.5027 with MSE metric 45127.8282\n",
      "Epoch 3 batch 120 train Loss 2774.9793 test Loss 1055.4634 with MSE metric 45137.2418\n",
      "Epoch 3 batch 130 train Loss 2744.1656 test Loss 1043.6953 with MSE metric 45140.2590\n",
      "Epoch 3 batch 140 train Loss 2714.0698 test Loss 1032.1884 with MSE metric 45125.9614\n",
      "Epoch 3 batch 150 train Loss 2684.6101 test Loss 1020.9345 with MSE metric 45127.0517\n",
      "Epoch 3 batch 160 train Loss 2655.7088 test Loss 1009.9257 with MSE metric 45106.0034\n",
      "Epoch 3 batch 170 train Loss 2627.6455 test Loss 999.1540 with MSE metric 45100.1974\n",
      "Epoch 3 batch 180 train Loss 2600.1196 test Loss 988.6107 with MSE metric 45096.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 batch 190 train Loss 2573.7582 test Loss 978.2907 with MSE metric 45097.1019\n",
      "Epoch 3 batch 200 train Loss 2547.3261 test Loss 968.1856 with MSE metric 45086.2424\n",
      "Epoch 3 batch 210 train Loss 2521.5919 test Loss 958.2892 with MSE metric 45083.5910\n",
      "Epoch 3 batch 220 train Loss 2496.2625 test Loss 948.5948 with MSE metric 45077.8469\n",
      "Epoch 3 batch 230 train Loss 2471.7906 test Loss 939.0961 with MSE metric 45075.7935\n",
      "Epoch 3 batch 240 train Loss 2447.3417 test Loss 929.7878 with MSE metric 45074.1080\n",
      "Time taken for 1 epoch: 30.357666015625 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 2423.4229 test Loss 920.6635 with MSE metric 45070.6918\n",
      "Epoch 4 batch 10 train Loss 2399.9280 test Loss 911.7182 with MSE metric 45057.1591\n",
      "Epoch 4 batch 20 train Loss 2376.9605 test Loss 902.9480 with MSE metric 45060.1760\n",
      "Epoch 4 batch 30 train Loss 2354.4834 test Loss 894.3457 with MSE metric 45053.5382\n",
      "Epoch 4 batch 40 train Loss 2332.3808 test Loss 885.9081 with MSE metric 45056.1516\n",
      "Epoch 4 batch 50 train Loss 2310.8005 test Loss 877.6294 with MSE metric 45048.3208\n",
      "Epoch 4 batch 60 train Loss 2289.5329 test Loss 869.5059 with MSE metric 45050.3329\n",
      "Epoch 4 batch 70 train Loss 2268.5803 test Loss 861.5328 with MSE metric 45049.0500\n",
      "Epoch 4 batch 80 train Loss 2248.0512 test Loss 853.7055 with MSE metric 45050.0251\n",
      "Epoch 4 batch 90 train Loss 2228.1946 test Loss 846.0206 with MSE metric 45040.2955\n",
      "Epoch 4 batch 100 train Loss 2208.3677 test Loss 838.4739 with MSE metric 45038.7982\n",
      "Epoch 4 batch 110 train Loss 2189.0255 test Loss 831.0628 with MSE metric 45039.2967\n",
      "Epoch 4 batch 120 train Loss 2169.8542 test Loss 823.7827 with MSE metric 45040.0957\n",
      "Epoch 4 batch 130 train Loss 2151.1224 test Loss 816.6308 with MSE metric 45031.2315\n",
      "Epoch 4 batch 140 train Loss 2132.6633 test Loss 809.6035 with MSE metric 45028.1465\n",
      "Epoch 4 batch 150 train Loss 2114.6679 test Loss 802.6972 with MSE metric 45029.7922\n",
      "Epoch 4 batch 160 train Loss 2096.8272 test Loss 795.9090 with MSE metric 45009.7464\n",
      "Epoch 4 batch 170 train Loss 2079.2895 test Loss 789.2361 with MSE metric 45003.7349\n",
      "Epoch 4 batch 180 train Loss 2062.1005 test Loss 782.6755 with MSE metric 45006.3125\n",
      "Epoch 4 batch 190 train Loss 2045.0445 test Loss 776.2245 with MSE metric 45009.7786\n",
      "Epoch 4 batch 200 train Loss 2028.3304 test Loss 769.8799 with MSE metric 45013.1025\n",
      "Epoch 4 batch 210 train Loss 2012.2038 test Loss 763.6393 with MSE metric 45015.9939\n",
      "Epoch 4 batch 220 train Loss 1996.0434 test Loss 757.4997 with MSE metric 45015.8314\n",
      "Epoch 4 batch 230 train Loss 1980.1839 test Loss 751.4598 with MSE metric 45021.4358\n",
      "Epoch 4 batch 240 train Loss 1964.5745 test Loss 745.5168 with MSE metric 45019.8027\n",
      "Time taken for 1 epoch: 26.902130365371704 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 1949.3237 test Loss 739.6682 with MSE metric 45015.0101\n",
      "Epoch 5 batch 10 train Loss 1934.1405 test Loss 733.9112 with MSE metric 45005.8931\n",
      "Epoch 5 batch 20 train Loss 1919.2996 test Loss 728.2447 with MSE metric 44992.6769\n",
      "Epoch 5 batch 30 train Loss 1904.5790 test Loss 722.6661 with MSE metric 44985.9718\n",
      "Epoch 5 batch 40 train Loss 1890.1015 test Loss 717.1732 with MSE metric 44980.7272\n",
      "Epoch 5 batch 50 train Loss 1875.8885 test Loss 711.7641 with MSE metric 44976.6084\n",
      "Epoch 5 batch 60 train Loss 1861.7850 test Loss 706.4370 with MSE metric 44972.1892\n",
      "Epoch 5 batch 70 train Loss 1848.0188 test Loss 701.1902 with MSE metric 44966.2768\n",
      "Epoch 5 batch 80 train Loss 1834.3533 test Loss 696.0219 with MSE metric 44961.8813\n",
      "Epoch 5 batch 90 train Loss 1820.9810 test Loss 690.9305 with MSE metric 44958.3249\n",
      "Epoch 5 batch 100 train Loss 1807.7856 test Loss 685.9142 with MSE metric 44957.5026\n",
      "Epoch 5 batch 110 train Loss 1794.7100 test Loss 680.9711 with MSE metric 44954.3753\n",
      "Epoch 5 batch 120 train Loss 1781.8545 test Loss 676.0997 with MSE metric 44950.0262\n",
      "Epoch 5 batch 130 train Loss 1769.5992 test Loss 671.2974 with MSE metric 44953.6941\n",
      "Epoch 5 batch 140 train Loss 1757.0611 test Loss 666.5647 with MSE metric 44950.4413\n",
      "Epoch 5 batch 150 train Loss 1744.7342 test Loss 661.8990 with MSE metric 44945.1746\n",
      "Epoch 5 batch 160 train Loss 1732.5453 test Loss 657.2992 with MSE metric 44942.7267\n",
      "Epoch 5 batch 170 train Loss 1720.5524 test Loss 652.7641 with MSE metric 44939.7389\n",
      "Epoch 5 batch 180 train Loss 1708.8043 test Loss 648.2915 with MSE metric 44932.7510\n",
      "Epoch 5 batch 190 train Loss 1697.1367 test Loss 643.8807 with MSE metric 44930.0002\n",
      "Epoch 5 batch 200 train Loss 1685.6918 test Loss 639.5310 with MSE metric 44923.7291\n",
      "Epoch 5 batch 210 train Loss 1674.3916 test Loss 635.2398 with MSE metric 44923.1230\n",
      "Epoch 5 batch 220 train Loss 1663.1726 test Loss 631.0065 with MSE metric 44920.7822\n",
      "Epoch 5 batch 230 train Loss 1652.1218 test Loss 626.8302 with MSE metric 44918.4070\n",
      "Epoch 5 batch 240 train Loss 1641.2626 test Loss 622.7098 with MSE metric 44907.1517\n",
      "Time taken for 1 epoch: 26.943596124649048 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 1630.5116 test Loss 618.6443 with MSE metric 44908.3888\n",
      "Epoch 6 batch 10 train Loss 1619.9084 test Loss 614.6322 with MSE metric 44907.9875\n",
      "Epoch 6 batch 20 train Loss 1609.4370 test Loss 610.6725 with MSE metric 44904.9789\n",
      "Epoch 6 batch 30 train Loss 1599.1753 test Loss 606.7645 with MSE metric 44898.2220\n",
      "Epoch 6 batch 40 train Loss 1589.4027 test Loss 602.9062 with MSE metric 44898.4453\n",
      "Epoch 6 batch 50 train Loss 1579.8025 test Loss 599.0980 with MSE metric 44895.3265\n",
      "Epoch 6 batch 60 train Loss 1570.1713 test Loss 595.3385 with MSE metric 44895.1295\n",
      "Epoch 6 batch 70 train Loss 1560.3152 test Loss 591.6264 with MSE metric 44889.9835\n",
      "Epoch 6 batch 80 train Loss 1550.6231 test Loss 587.9608 with MSE metric 44877.7429\n",
      "Epoch 6 batch 90 train Loss 1540.9968 test Loss 584.3415 with MSE metric 44868.1489\n",
      "Epoch 6 batch 100 train Loss 1531.5152 test Loss 580.7673 with MSE metric 44863.2595\n",
      "Epoch 6 batch 110 train Loss 1522.2312 test Loss 577.2369 with MSE metric 44860.4943\n",
      "Epoch 6 batch 120 train Loss 1512.9760 test Loss 573.7503 with MSE metric 44855.4648\n",
      "Epoch 6 batch 130 train Loss 1503.8501 test Loss 570.3058 with MSE metric 44853.5367\n",
      "Epoch 6 batch 140 train Loss 1494.8289 test Loss 566.9034 with MSE metric 44852.7956\n",
      "Epoch 6 batch 150 train Loss 1485.9243 test Loss 563.5417 with MSE metric 44845.7155\n",
      "Epoch 6 batch 160 train Loss 1477.0828 test Loss 560.2209 with MSE metric 44849.0325\n",
      "Epoch 6 batch 170 train Loss 1468.3799 test Loss 556.9394 with MSE metric 44841.9069\n",
      "Epoch 6 batch 180 train Loss 1459.7514 test Loss 553.6971 with MSE metric 44841.4974\n",
      "Epoch 6 batch 190 train Loss 1451.2333 test Loss 550.4925 with MSE metric 44835.1646\n",
      "Epoch 6 batch 200 train Loss 1442.8199 test Loss 547.3256 with MSE metric 44837.8007\n",
      "Epoch 6 batch 210 train Loss 1434.5088 test Loss 544.1954 with MSE metric 44837.0209\n",
      "Epoch 6 batch 220 train Loss 1426.3004 test Loss 541.1014 with MSE metric 44837.4252\n",
      "Epoch 6 batch 230 train Loss 1418.2052 test Loss 538.0432 with MSE metric 44834.3945\n",
      "Epoch 6 batch 240 train Loss 1410.2322 test Loss 535.0199 with MSE metric 44837.7895\n",
      "Time taken for 1 epoch: 28.42843222618103 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 1402.3128 test Loss 532.0310 with MSE metric 44838.7347\n",
      "Epoch 7 batch 10 train Loss 1394.8938 test Loss 529.0759 with MSE metric 44837.8097\n",
      "Epoch 7 batch 20 train Loss 1387.1896 test Loss 526.1541 with MSE metric 44836.9873\n",
      "Epoch 7 batch 30 train Loss 1379.5406 test Loss 523.2653 with MSE metric 44838.1247\n",
      "Epoch 7 batch 40 train Loss 1371.9384 test Loss 520.4084 with MSE metric 44842.7618\n",
      "Epoch 7 batch 50 train Loss 1364.4487 test Loss 517.5830 with MSE metric 44842.7576\n",
      "Epoch 7 batch 60 train Loss 1357.0505 test Loss 514.7889 with MSE metric 44840.7958\n",
      "Epoch 7 batch 70 train Loss 1349.6931 test Loss 512.0254 with MSE metric 44841.6104\n",
      "Epoch 7 batch 80 train Loss 1342.4208 test Loss 509.2917 with MSE metric 44838.3105\n",
      "Epoch 7 batch 90 train Loss 1335.2635 test Loss 506.5879 with MSE metric 44837.1282\n",
      "Epoch 7 batch 100 train Loss 1328.2923 test Loss 503.9134 with MSE metric 44828.0234\n",
      "Epoch 7 batch 110 train Loss 1321.3661 test Loss 501.2670 with MSE metric 44822.9268\n",
      "Epoch 7 batch 120 train Loss 1314.4068 test Loss 498.6491 with MSE metric 44821.2390\n",
      "Epoch 7 batch 130 train Loss 1307.5278 test Loss 496.0590 with MSE metric 44819.3020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 batch 140 train Loss 1300.7095 test Loss 493.4961 with MSE metric 44818.2943\n",
      "Epoch 7 batch 150 train Loss 1294.0040 test Loss 490.9602 with MSE metric 44817.6849\n",
      "Epoch 7 batch 160 train Loss 1287.3511 test Loss 488.4506 with MSE metric 44813.3637\n",
      "Epoch 7 batch 170 train Loss 1280.7618 test Loss 485.9669 with MSE metric 44810.6646\n",
      "Epoch 7 batch 180 train Loss 1274.3010 test Loss 483.5091 with MSE metric 44811.2059\n",
      "Epoch 7 batch 190 train Loss 1267.8261 test Loss 481.0765 with MSE metric 44810.5494\n",
      "Epoch 7 batch 200 train Loss 1261.4127 test Loss 478.6689 with MSE metric 44809.8589\n",
      "Epoch 7 batch 210 train Loss 1255.0965 test Loss 476.2854 with MSE metric 44813.5021\n",
      "Epoch 7 batch 220 train Loss 1248.8292 test Loss 473.9266 with MSE metric 44812.7825\n",
      "Epoch 7 batch 230 train Loss 1242.6034 test Loss 471.5913 with MSE metric 44808.9969\n",
      "Epoch 7 batch 240 train Loss 1236.4422 test Loss 469.2792 with MSE metric 44808.6132\n",
      "Time taken for 1 epoch: 29.421195030212402 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 1230.3906 test Loss 466.9904 with MSE metric 44808.7149\n",
      "Epoch 8 batch 10 train Loss 1224.4285 test Loss 464.7240 with MSE metric 44806.7802\n",
      "Epoch 8 batch 20 train Loss 1218.6171 test Loss 462.4802 with MSE metric 44807.5621\n",
      "Epoch 8 batch 30 train Loss 1212.7419 test Loss 460.2585 with MSE metric 44804.1905\n",
      "Epoch 8 batch 40 train Loss 1206.9594 test Loss 458.0585 with MSE metric 44804.8762\n",
      "Epoch 8 batch 50 train Loss 1201.1744 test Loss 455.8799 with MSE metric 44804.1438\n",
      "Epoch 8 batch 60 train Loss 1195.4953 test Loss 453.7222 with MSE metric 44800.6782\n",
      "Epoch 8 batch 70 train Loss 1189.8997 test Loss 451.5857 with MSE metric 44800.6376\n",
      "Epoch 8 batch 80 train Loss 1184.2786 test Loss 449.4694 with MSE metric 44794.7038\n",
      "Epoch 8 batch 90 train Loss 1178.7679 test Loss 447.3732 with MSE metric 44790.2379\n",
      "Epoch 8 batch 100 train Loss 1173.2369 test Loss 445.2970 with MSE metric 44784.2181\n",
      "Epoch 8 batch 110 train Loss 1167.7877 test Loss 443.2406 with MSE metric 44779.5893\n",
      "Epoch 8 batch 120 train Loss 1162.3806 test Loss 441.2032 with MSE metric 44783.8443\n",
      "Epoch 8 batch 130 train Loss 1156.9951 test Loss 439.1849 with MSE metric 44780.4207\n",
      "Epoch 8 batch 140 train Loss 1151.6675 test Loss 437.1854 with MSE metric 44780.1581\n",
      "Epoch 8 batch 150 train Loss 1146.3919 test Loss 435.2046 with MSE metric 44781.6780\n",
      "Epoch 8 batch 160 train Loss 1141.1671 test Loss 433.2420 with MSE metric 44780.8104\n",
      "Epoch 8 batch 170 train Loss 1135.9960 test Loss 431.2975 with MSE metric 44782.0080\n",
      "Epoch 8 batch 180 train Loss 1130.8632 test Loss 429.3708 with MSE metric 44782.8237\n",
      "Epoch 8 batch 190 train Loss 1125.7662 test Loss 427.4617 with MSE metric 44784.3824\n",
      "Epoch 8 batch 200 train Loss 1120.7611 test Loss 425.5697 with MSE metric 44780.5057\n",
      "Epoch 8 batch 210 train Loss 1115.8185 test Loss 423.6950 with MSE metric 44778.1294\n",
      "Epoch 8 batch 220 train Loss 1110.8698 test Loss 421.8371 with MSE metric 44776.9149\n",
      "Epoch 8 batch 230 train Loss 1105.9861 test Loss 419.9959 with MSE metric 44780.6554\n",
      "Epoch 8 batch 240 train Loss 1101.1204 test Loss 418.1710 with MSE metric 44781.0386\n",
      "Time taken for 1 epoch: 28.709784269332886 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 1096.3074 test Loss 416.3621 with MSE metric 44782.9400\n",
      "Epoch 9 batch 10 train Loss 1091.5239 test Loss 414.5694 with MSE metric 44785.1685\n",
      "Epoch 9 batch 20 train Loss 1086.7956 test Loss 412.7924 with MSE metric 44780.8820\n",
      "Epoch 9 batch 30 train Loss 1082.1020 test Loss 411.0307 with MSE metric 44778.6129\n",
      "Epoch 9 batch 40 train Loss 1077.4411 test Loss 409.2844 with MSE metric 44776.2438\n",
      "Epoch 9 batch 50 train Loss 1072.8193 test Loss 407.5533 with MSE metric 44781.0837\n",
      "Epoch 9 batch 60 train Loss 1068.2353 test Loss 405.8372 with MSE metric 44782.0508\n",
      "Epoch 9 batch 70 train Loss 1063.6886 test Loss 404.1360 with MSE metric 44780.8341\n",
      "Epoch 9 batch 80 train Loss 1059.1905 test Loss 402.4494 with MSE metric 44781.4039\n",
      "Epoch 9 batch 90 train Loss 1054.7699 test Loss 400.7771 with MSE metric 44787.7362\n",
      "Epoch 9 batch 100 train Loss 1050.3931 test Loss 399.1191 with MSE metric 44785.3247\n",
      "Epoch 9 batch 110 train Loss 1046.0192 test Loss 397.4750 with MSE metric 44784.5071\n",
      "Epoch 9 batch 120 train Loss 1041.6685 test Loss 395.8446 with MSE metric 44786.4044\n",
      "Epoch 9 batch 130 train Loss 1037.3602 test Loss 394.2278 with MSE metric 44781.6735\n",
      "Epoch 9 batch 140 train Loss 1033.0749 test Loss 392.6245 with MSE metric 44776.6956\n",
      "Epoch 9 batch 150 train Loss 1028.8282 test Loss 391.0346 with MSE metric 44776.2279\n",
      "Epoch 9 batch 160 train Loss 1024.6156 test Loss 389.4578 with MSE metric 44767.4653\n",
      "Epoch 9 batch 170 train Loss 1020.4697 test Loss 387.8941 with MSE metric 44768.6716\n",
      "Epoch 9 batch 180 train Loss 1016.3315 test Loss 386.3434 with MSE metric 44766.8421\n",
      "Epoch 9 batch 190 train Loss 1012.2648 test Loss 384.8051 with MSE metric 44765.5127\n",
      "Epoch 9 batch 200 train Loss 1008.1914 test Loss 383.2797 with MSE metric 44766.9645\n",
      "Epoch 9 batch 210 train Loss 1004.1462 test Loss 381.7665 with MSE metric 44762.0403\n",
      "Epoch 9 batch 220 train Loss 1000.1362 test Loss 380.2652 with MSE metric 44756.9332\n",
      "Epoch 9 batch 230 train Loss 996.1593 test Loss 378.7764 with MSE metric 44755.8938\n",
      "Epoch 9 batch 240 train Loss 992.2195 test Loss 377.2995 with MSE metric 44757.9395\n",
      "Time taken for 1 epoch: 26.26639413833618 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 988.3106 test Loss 375.8343 with MSE metric 44752.6474\n",
      "Epoch 10 batch 10 train Loss 984.4457 test Loss 374.3808 with MSE metric 44752.1910\n",
      "Epoch 10 batch 20 train Loss 980.5911 test Loss 372.9387 with MSE metric 44749.4991\n",
      "Epoch 10 batch 30 train Loss 976.7615 test Loss 371.5080 with MSE metric 44745.4841\n",
      "Epoch 10 batch 40 train Loss 972.9814 test Loss 370.0887 with MSE metric 44743.7243\n",
      "Epoch 10 batch 50 train Loss 969.2190 test Loss 368.6804 with MSE metric 44738.4643\n",
      "Epoch 10 batch 60 train Loss 965.4848 test Loss 367.2831 with MSE metric 44735.0319\n",
      "Epoch 10 batch 70 train Loss 961.7912 test Loss 365.8969 with MSE metric 44737.4210\n",
      "Epoch 10 batch 80 train Loss 958.1134 test Loss 364.5213 with MSE metric 44735.4580\n",
      "Epoch 10 batch 90 train Loss 954.4746 test Loss 363.1560 with MSE metric 44734.0153\n",
      "Epoch 10 batch 100 train Loss 951.0032 test Loss 361.8011 with MSE metric 44734.3537\n",
      "Epoch 10 batch 110 train Loss 947.4074 test Loss 360.4568 with MSE metric 44735.1766\n",
      "Epoch 10 batch 120 train Loss 943.8441 test Loss 359.1229 with MSE metric 44730.9161\n",
      "Epoch 10 batch 130 train Loss 940.3111 test Loss 357.7991 with MSE metric 44727.8943\n",
      "Epoch 10 batch 140 train Loss 936.8113 test Loss 356.4852 with MSE metric 44727.6135\n",
      "Epoch 10 batch 150 train Loss 933.3255 test Loss 355.1812 with MSE metric 44725.6370\n",
      "Epoch 10 batch 160 train Loss 929.8681 test Loss 353.8870 with MSE metric 44724.3893\n",
      "Epoch 10 batch 170 train Loss 926.4301 test Loss 352.6024 with MSE metric 44724.1716\n",
      "Epoch 10 batch 180 train Loss 923.0297 test Loss 351.3273 with MSE metric 44721.5506\n",
      "Epoch 10 batch 190 train Loss 919.7079 test Loss 350.0621 with MSE metric 44718.6745\n",
      "Epoch 10 batch 200 train Loss 916.3466 test Loss 348.8059 with MSE metric 44717.5225\n",
      "Epoch 10 batch 210 train Loss 913.0072 test Loss 347.5592 with MSE metric 44715.3538\n",
      "Epoch 10 batch 220 train Loss 909.6953 test Loss 346.3217 with MSE metric 44713.4314\n",
      "Epoch 10 batch 230 train Loss 906.4107 test Loss 345.0931 with MSE metric 44709.1637\n",
      "Epoch 10 batch 240 train Loss 903.1823 test Loss 343.8733 with MSE metric 44708.9765\n",
      "Time taken for 1 epoch: 28.26937174797058 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 899.9468 test Loss 342.6624 with MSE metric 44709.2745\n",
      "Epoch 11 batch 10 train Loss 896.7564 test Loss 341.4603 with MSE metric 44710.2116\n",
      "Epoch 11 batch 20 train Loss 893.5612 test Loss 340.2668 with MSE metric 44711.2036\n",
      "Epoch 11 batch 30 train Loss 890.3962 test Loss 339.0820 with MSE metric 44711.3188\n",
      "Epoch 11 batch 40 train Loss 887.2443 test Loss 337.9054 with MSE metric 44713.8833\n",
      "Epoch 11 batch 50 train Loss 884.1199 test Loss 336.7374 with MSE metric 44711.1275\n",
      "Epoch 11 batch 60 train Loss 881.0252 test Loss 335.5779 with MSE metric 44708.5708\n",
      "Epoch 11 batch 70 train Loss 877.9422 test Loss 334.4263 with MSE metric 44707.4125\n",
      "Epoch 11 batch 80 train Loss 874.8821 test Loss 333.2829 with MSE metric 44706.8448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 batch 90 train Loss 871.8436 test Loss 332.1477 with MSE metric 44707.9802\n",
      "Epoch 11 batch 100 train Loss 868.8273 test Loss 331.0205 with MSE metric 44706.4711\n",
      "Epoch 11 batch 110 train Loss 865.8288 test Loss 329.9011 with MSE metric 44702.7393\n",
      "Epoch 11 batch 120 train Loss 862.8777 test Loss 328.7893 with MSE metric 44698.5043\n",
      "Epoch 11 batch 130 train Loss 859.9492 test Loss 327.6854 with MSE metric 44698.4619\n",
      "Epoch 11 batch 140 train Loss 857.0172 test Loss 326.5892 with MSE metric 44696.4532\n",
      "Epoch 11 batch 150 train Loss 854.1239 test Loss 325.5003 with MSE metric 44692.4952\n",
      "Epoch 11 batch 160 train Loss 851.2339 test Loss 324.4191 with MSE metric 44690.6223\n",
      "Epoch 11 batch 170 train Loss 848.3575 test Loss 323.3452 with MSE metric 44689.6701\n",
      "Epoch 11 batch 180 train Loss 845.5189 test Loss 322.2788 with MSE metric 44688.6152\n",
      "Epoch 11 batch 190 train Loss 842.7060 test Loss 321.2197 with MSE metric 44688.2442\n",
      "Epoch 11 batch 200 train Loss 839.8990 test Loss 320.1676 with MSE metric 44689.1026\n",
      "Epoch 11 batch 210 train Loss 837.1020 test Loss 319.1224 with MSE metric 44685.8370\n",
      "Epoch 11 batch 220 train Loss 834.3277 test Loss 318.0846 with MSE metric 44685.0183\n",
      "Epoch 11 batch 230 train Loss 831.5774 test Loss 317.0537 with MSE metric 44688.6831\n",
      "Epoch 11 batch 240 train Loss 828.8410 test Loss 316.0297 with MSE metric 44687.6819\n",
      "Time taken for 1 epoch: 27.862637758255005 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 826.1139 test Loss 315.0125 with MSE metric 44686.7473\n",
      "Epoch 12 batch 10 train Loss 823.4161 test Loss 314.0018 with MSE metric 44686.8018\n",
      "Epoch 12 batch 20 train Loss 820.7248 test Loss 312.9979 with MSE metric 44684.6282\n",
      "Epoch 12 batch 30 train Loss 818.0516 test Loss 312.0006 with MSE metric 44682.8346\n",
      "Epoch 12 batch 40 train Loss 815.3965 test Loss 311.0100 with MSE metric 44678.7482\n",
      "Epoch 12 batch 50 train Loss 812.7663 test Loss 310.0258 with MSE metric 44680.0629\n",
      "Epoch 12 batch 60 train Loss 810.1732 test Loss 309.0481 with MSE metric 44682.5152\n",
      "Epoch 12 batch 70 train Loss 807.5722 test Loss 308.0768 with MSE metric 44683.3552\n",
      "Epoch 12 batch 80 train Loss 804.9882 test Loss 307.1117 with MSE metric 44681.0805\n",
      "Epoch 12 batch 90 train Loss 802.4322 test Loss 306.1529 with MSE metric 44679.0264\n",
      "Epoch 12 batch 100 train Loss 799.8885 test Loss 305.2003 with MSE metric 44684.1487\n",
      "Epoch 12 batch 110 train Loss 797.3700 test Loss 304.2537 with MSE metric 44682.2568\n",
      "Epoch 12 batch 120 train Loss 794.8503 test Loss 303.3133 with MSE metric 44679.5334\n",
      "Epoch 12 batch 130 train Loss 792.3456 test Loss 302.3790 with MSE metric 44675.8627\n",
      "Epoch 12 batch 140 train Loss 789.8730 test Loss 301.4506 with MSE metric 44672.1712\n",
      "Epoch 12 batch 150 train Loss 787.4012 test Loss 300.5279 with MSE metric 44672.5554\n",
      "Epoch 12 batch 160 train Loss 784.9648 test Loss 299.6110 with MSE metric 44675.9904\n",
      "Epoch 12 batch 170 train Loss 782.5254 test Loss 298.7000 with MSE metric 44681.6546\n",
      "Epoch 12 batch 180 train Loss 780.1048 test Loss 297.7949 with MSE metric 44680.5267\n",
      "Epoch 12 batch 190 train Loss 777.6986 test Loss 296.8954 with MSE metric 44677.7837\n",
      "Epoch 12 batch 200 train Loss 775.3045 test Loss 296.0015 with MSE metric 44678.0041\n",
      "Epoch 12 batch 210 train Loss 772.9262 test Loss 295.1132 with MSE metric 44675.5806\n",
      "Epoch 12 batch 220 train Loss 770.5724 test Loss 294.2304 with MSE metric 44677.7565\n",
      "Epoch 12 batch 230 train Loss 768.2180 test Loss 293.3529 with MSE metric 44679.8394\n",
      "Epoch 12 batch 240 train Loss 765.8847 test Loss 292.4810 with MSE metric 44678.7118\n",
      "Time taken for 1 epoch: 28.55362367630005 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 763.5676 test Loss 291.6143 with MSE metric 44672.9973\n",
      "Epoch 13 batch 10 train Loss 761.3746 test Loss 290.7530 with MSE metric 44671.4468\n",
      "Epoch 13 batch 20 train Loss 759.0766 test Loss 289.8969 with MSE metric 44671.0782\n",
      "Epoch 13 batch 30 train Loss 756.7954 test Loss 289.0460 with MSE metric 44669.3588\n",
      "Epoch 13 batch 40 train Loss 754.5238 test Loss 288.2005 with MSE metric 44671.2155\n",
      "Epoch 13 batch 50 train Loss 752.2770 test Loss 287.3601 with MSE metric 44674.3512\n",
      "Epoch 13 batch 60 train Loss 750.0392 test Loss 286.5248 with MSE metric 44676.4519\n",
      "Epoch 13 batch 70 train Loss 747.8237 test Loss 285.6944 with MSE metric 44676.9571\n",
      "Epoch 13 batch 80 train Loss 745.6089 test Loss 284.8692 with MSE metric 44676.2174\n",
      "Epoch 13 batch 90 train Loss 743.4107 test Loss 284.0488 with MSE metric 44676.2114\n",
      "Epoch 13 batch 100 train Loss 741.2214 test Loss 283.2334 with MSE metric 44674.6699\n",
      "Epoch 13 batch 110 train Loss 739.0465 test Loss 282.4228 with MSE metric 44671.0623\n",
      "Epoch 13 batch 120 train Loss 736.8860 test Loss 281.6169 with MSE metric 44669.2436\n",
      "Epoch 13 batch 130 train Loss 734.7384 test Loss 280.8158 with MSE metric 44667.1430\n",
      "Epoch 13 batch 140 train Loss 732.6059 test Loss 280.0193 with MSE metric 44662.0977\n",
      "Epoch 13 batch 150 train Loss 730.4834 test Loss 279.2276 with MSE metric 44665.6378\n",
      "Epoch 13 batch 160 train Loss 728.3706 test Loss 278.4405 with MSE metric 44665.4503\n",
      "Epoch 13 batch 170 train Loss 726.3468 test Loss 277.6581 with MSE metric 44665.1478\n",
      "Epoch 13 batch 180 train Loss 724.2588 test Loss 276.8801 with MSE metric 44661.0026\n",
      "Epoch 13 batch 190 train Loss 722.1830 test Loss 276.1065 with MSE metric 44664.2451\n",
      "Epoch 13 batch 200 train Loss 720.1283 test Loss 275.3376 with MSE metric 44665.0625\n",
      "Epoch 13 batch 210 train Loss 718.0773 test Loss 274.5731 with MSE metric 44665.2590\n",
      "Epoch 13 batch 220 train Loss 716.0477 test Loss 273.8132 with MSE metric 44666.0939\n",
      "Epoch 13 batch 230 train Loss 714.0193 test Loss 273.0575 with MSE metric 44669.2406\n",
      "Epoch 13 batch 240 train Loss 712.0108 test Loss 272.3061 with MSE metric 44670.3363\n",
      "Time taken for 1 epoch: 25.893537998199463 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 710.0040 test Loss 271.5590 with MSE metric 44667.5588\n",
      "Epoch 14 batch 10 train Loss 708.0099 test Loss 270.8161 with MSE metric 44664.6253\n",
      "Epoch 14 batch 20 train Loss 706.0275 test Loss 270.0775 with MSE metric 44659.6701\n",
      "Epoch 14 batch 30 train Loss 704.0543 test Loss 269.3430 with MSE metric 44658.5434\n",
      "Epoch 14 batch 40 train Loss 702.0951 test Loss 268.6127 with MSE metric 44659.1883\n",
      "Epoch 14 batch 50 train Loss 700.1510 test Loss 267.8866 with MSE metric 44656.3263\n",
      "Epoch 14 batch 60 train Loss 698.2208 test Loss 267.1644 with MSE metric 44656.3787\n",
      "Epoch 14 batch 70 train Loss 696.2933 test Loss 266.4465 with MSE metric 44655.9660\n",
      "Epoch 14 batch 80 train Loss 694.3830 test Loss 265.7326 with MSE metric 44656.6759\n",
      "Epoch 14 batch 90 train Loss 692.4755 test Loss 265.0223 with MSE metric 44658.0571\n",
      "Epoch 14 batch 100 train Loss 690.5795 test Loss 264.3162 with MSE metric 44656.7533\n",
      "Epoch 14 batch 110 train Loss 688.6938 test Loss 263.6140 with MSE metric 44659.1272\n",
      "Epoch 14 batch 120 train Loss 686.8213 test Loss 262.9157 with MSE metric 44659.9702\n",
      "Epoch 14 batch 130 train Loss 684.9554 test Loss 262.2213 with MSE metric 44656.1318\n",
      "Epoch 14 batch 140 train Loss 683.1003 test Loss 261.5305 with MSE metric 44652.9989\n",
      "Epoch 14 batch 150 train Loss 681.2574 test Loss 260.8438 with MSE metric 44651.5726\n",
      "Epoch 14 batch 160 train Loss 679.4220 test Loss 260.1608 with MSE metric 44650.6180\n",
      "Epoch 14 batch 170 train Loss 677.6114 test Loss 259.4814 with MSE metric 44650.9449\n",
      "Epoch 14 batch 180 train Loss 675.8012 test Loss 258.8057 with MSE metric 44646.6062\n",
      "Epoch 14 batch 190 train Loss 673.9959 test Loss 258.1337 with MSE metric 44644.9843\n",
      "Epoch 14 batch 200 train Loss 672.2022 test Loss 257.4655 with MSE metric 44643.9647\n",
      "Epoch 14 batch 210 train Loss 670.4183 test Loss 256.8008 with MSE metric 44645.5900\n",
      "Epoch 14 batch 220 train Loss 668.6418 test Loss 256.1397 with MSE metric 44644.3464\n",
      "Epoch 14 batch 230 train Loss 666.8784 test Loss 255.4822 with MSE metric 44644.5990\n",
      "Epoch 14 batch 240 train Loss 665.1224 test Loss 254.8280 with MSE metric 44645.2015\n",
      "Time taken for 1 epoch: 28.45941400527954 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 663.3777 test Loss 254.1773 with MSE metric 44646.0120\n",
      "Epoch 15 batch 10 train Loss 661.6410 test Loss 253.5301 with MSE metric 44645.7942\n",
      "Epoch 15 batch 20 train Loss 659.9102 test Loss 252.8864 with MSE metric 44646.8515\n",
      "Epoch 15 batch 30 train Loss 658.2015 test Loss 252.2461 with MSE metric 44648.8572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 batch 40 train Loss 656.4897 test Loss 251.6091 with MSE metric 44649.6013\n",
      "Epoch 15 batch 50 train Loss 654.7873 test Loss 250.9754 with MSE metric 44650.0012\n",
      "Epoch 15 batch 60 train Loss 653.1027 test Loss 250.3452 with MSE metric 44650.0421\n",
      "Epoch 15 batch 70 train Loss 651.4181 test Loss 249.7183 with MSE metric 44649.1121\n",
      "Epoch 15 batch 80 train Loss 649.7408 test Loss 249.0947 with MSE metric 44647.7067\n",
      "Epoch 15 batch 90 train Loss 648.0725 test Loss 248.4744 with MSE metric 44646.1725\n",
      "Epoch 15 batch 100 train Loss 646.4165 test Loss 247.8572 with MSE metric 44645.2851\n",
      "Epoch 15 batch 110 train Loss 644.7653 test Loss 247.2429 with MSE metric 44644.8591\n",
      "Epoch 15 batch 120 train Loss 643.1229 test Loss 246.6321 with MSE metric 44643.1733\n",
      "Epoch 15 batch 130 train Loss 641.4899 test Loss 246.0245 with MSE metric 44641.2036\n",
      "Epoch 15 batch 140 train Loss 639.8646 test Loss 245.4198 with MSE metric 44639.2454\n",
      "Epoch 15 batch 150 train Loss 638.2462 test Loss 244.8183 with MSE metric 44637.1294\n",
      "Epoch 15 batch 160 train Loss 636.6366 test Loss 244.2200 with MSE metric 44636.2056\n",
      "Epoch 15 batch 170 train Loss 635.0384 test Loss 243.6246 with MSE metric 44633.5415\n",
      "Epoch 15 batch 180 train Loss 633.4459 test Loss 243.0323 with MSE metric 44634.9891\n",
      "Epoch 15 batch 190 train Loss 631.8672 test Loss 242.4429 with MSE metric 44632.7174\n",
      "Epoch 15 batch 200 train Loss 630.2952 test Loss 241.8566 with MSE metric 44630.0086\n",
      "Epoch 15 batch 210 train Loss 628.7249 test Loss 241.2731 with MSE metric 44627.8200\n",
      "Epoch 15 batch 220 train Loss 627.1637 test Loss 240.6927 with MSE metric 44626.1303\n",
      "Epoch 15 batch 230 train Loss 625.6177 test Loss 240.1152 with MSE metric 44621.7435\n",
      "Epoch 15 batch 240 train Loss 624.0721 test Loss 239.5405 with MSE metric 44619.6470\n",
      "Time taken for 1 epoch: 28.90988326072693 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 622.5375 test Loss 238.9688 with MSE metric 44619.5518\n",
      "Epoch 16 batch 10 train Loss 621.0085 test Loss 238.3998 with MSE metric 44616.2723\n",
      "Epoch 16 batch 20 train Loss 619.4871 test Loss 237.8339 with MSE metric 44612.1733\n",
      "Epoch 16 batch 30 train Loss 617.9728 test Loss 237.2707 with MSE metric 44613.5428\n",
      "Epoch 16 batch 40 train Loss 616.4645 test Loss 236.7103 with MSE metric 44615.1343\n",
      "Epoch 16 batch 50 train Loss 614.9680 test Loss 236.1526 with MSE metric 44615.7748\n",
      "Epoch 16 batch 60 train Loss 613.4840 test Loss 235.5977 with MSE metric 44617.3193\n",
      "Epoch 16 batch 70 train Loss 611.9978 test Loss 235.0456 with MSE metric 44615.2659\n",
      "Epoch 16 batch 80 train Loss 610.5251 test Loss 234.4961 with MSE metric 44617.2815\n",
      "Epoch 16 batch 90 train Loss 609.0752 test Loss 233.9494 with MSE metric 44619.3489\n",
      "Epoch 16 batch 100 train Loss 607.6105 test Loss 233.4053 with MSE metric 44616.7754\n",
      "Epoch 16 batch 110 train Loss 606.1545 test Loss 232.8640 with MSE metric 44617.7027\n",
      "Epoch 16 batch 120 train Loss 604.7073 test Loss 232.3252 with MSE metric 44618.3692\n",
      "Epoch 16 batch 130 train Loss 603.2639 test Loss 231.7892 with MSE metric 44620.2422\n",
      "Epoch 16 batch 140 train Loss 601.8282 test Loss 231.2557 with MSE metric 44621.7492\n",
      "Epoch 16 batch 150 train Loss 600.3996 test Loss 230.7247 with MSE metric 44622.8676\n",
      "Epoch 16 batch 160 train Loss 598.9808 test Loss 230.1964 with MSE metric 44621.7593\n",
      "Epoch 16 batch 170 train Loss 597.5665 test Loss 229.6704 with MSE metric 44623.5806\n",
      "Epoch 16 batch 180 train Loss 596.1589 test Loss 229.1471 with MSE metric 44622.9014\n",
      "Epoch 16 batch 190 train Loss 594.7965 test Loss 228.6263 with MSE metric 44623.5280\n",
      "Epoch 16 batch 200 train Loss 593.4124 test Loss 228.1079 with MSE metric 44621.6565\n",
      "Epoch 16 batch 210 train Loss 592.0234 test Loss 227.5920 with MSE metric 44622.1929\n",
      "Epoch 16 batch 220 train Loss 590.6417 test Loss 227.0783 with MSE metric 44622.4778\n",
      "Epoch 16 batch 230 train Loss 589.2674 test Loss 226.5673 with MSE metric 44622.3663\n",
      "Epoch 16 batch 240 train Loss 587.9002 test Loss 226.0586 with MSE metric 44621.4502\n",
      "Time taken for 1 epoch: 28.559781074523926 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 586.5374 test Loss 225.5524 with MSE metric 44619.4639\n",
      "Epoch 17 batch 10 train Loss 585.1842 test Loss 225.0485 with MSE metric 44618.4106\n",
      "Epoch 17 batch 20 train Loss 583.8353 test Loss 224.5471 with MSE metric 44619.4588\n",
      "Epoch 17 batch 30 train Loss 582.4959 test Loss 224.0481 with MSE metric 44618.4556\n",
      "Epoch 17 batch 40 train Loss 581.1578 test Loss 223.5512 with MSE metric 44617.4340\n",
      "Epoch 17 batch 50 train Loss 579.8283 test Loss 223.0567 with MSE metric 44616.7769\n",
      "Epoch 17 batch 60 train Loss 578.5057 test Loss 222.5645 with MSE metric 44616.3809\n",
      "Epoch 17 batch 70 train Loss 577.1856 test Loss 222.0746 with MSE metric 44616.3480\n",
      "Epoch 17 batch 80 train Loss 575.8727 test Loss 221.5867 with MSE metric 44614.7490\n",
      "Epoch 17 batch 90 train Loss 574.5689 test Loss 221.1013 with MSE metric 44618.5417\n",
      "Epoch 17 batch 100 train Loss 573.2682 test Loss 220.6182 with MSE metric 44617.8802\n",
      "Epoch 17 batch 110 train Loss 571.9757 test Loss 220.1373 with MSE metric 44617.5010\n",
      "Epoch 17 batch 120 train Loss 570.6888 test Loss 219.6584 with MSE metric 44616.9643\n",
      "Epoch 17 batch 130 train Loss 569.4081 test Loss 219.1819 with MSE metric 44616.1957\n",
      "Epoch 17 batch 140 train Loss 568.1315 test Loss 218.7074 with MSE metric 44614.6499\n",
      "Epoch 17 batch 150 train Loss 566.8612 test Loss 218.2352 with MSE metric 44612.1384\n",
      "Epoch 17 batch 160 train Loss 565.5968 test Loss 217.7651 with MSE metric 44609.1376\n",
      "Epoch 17 batch 170 train Loss 564.3373 test Loss 217.2971 with MSE metric 44609.7966\n",
      "Epoch 17 batch 180 train Loss 563.0825 test Loss 216.8313 with MSE metric 44610.3200\n",
      "Epoch 17 batch 190 train Loss 561.8392 test Loss 216.3677 with MSE metric 44609.1782\n",
      "Epoch 17 batch 200 train Loss 560.5953 test Loss 215.9061 with MSE metric 44609.8612\n",
      "Epoch 17 batch 210 train Loss 559.3602 test Loss 215.4465 with MSE metric 44609.8614\n",
      "Epoch 17 batch 220 train Loss 558.1292 test Loss 214.9890 with MSE metric 44608.8580\n",
      "Epoch 17 batch 230 train Loss 556.9032 test Loss 214.5336 with MSE metric 44610.3394\n",
      "Epoch 17 batch 240 train Loss 555.6828 test Loss 214.0800 with MSE metric 44611.4293\n",
      "Time taken for 1 epoch: 29.99758791923523 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 554.4664 test Loss 213.6286 with MSE metric 44611.7451\n",
      "Epoch 18 batch 10 train Loss 553.2573 test Loss 213.1791 with MSE metric 44610.5758\n",
      "Epoch 18 batch 20 train Loss 552.0523 test Loss 212.7318 with MSE metric 44608.1104\n",
      "Epoch 18 batch 30 train Loss 550.8527 test Loss 212.2865 with MSE metric 44605.2050\n",
      "Epoch 18 batch 40 train Loss 549.6578 test Loss 211.8430 with MSE metric 44601.0230\n",
      "Epoch 18 batch 50 train Loss 548.4685 test Loss 211.4016 with MSE metric 44599.6690\n",
      "Epoch 18 batch 60 train Loss 547.2831 test Loss 210.9620 with MSE metric 44600.2956\n",
      "Epoch 18 batch 70 train Loss 546.1045 test Loss 210.5242 with MSE metric 44598.7428\n",
      "Epoch 18 batch 80 train Loss 544.9328 test Loss 210.0884 with MSE metric 44598.1984\n",
      "Epoch 18 batch 90 train Loss 543.7638 test Loss 209.6547 with MSE metric 44597.9782\n",
      "Epoch 18 batch 100 train Loss 542.5996 test Loss 209.2227 with MSE metric 44596.8771\n",
      "Epoch 18 batch 110 train Loss 541.4404 test Loss 208.7927 with MSE metric 44594.8061\n",
      "Epoch 18 batch 120 train Loss 540.2863 test Loss 208.3646 with MSE metric 44591.9276\n",
      "Epoch 18 batch 130 train Loss 539.1372 test Loss 207.9383 with MSE metric 44591.9650\n",
      "Epoch 18 batch 140 train Loss 537.9936 test Loss 207.5138 with MSE metric 44591.0258\n",
      "Epoch 18 batch 150 train Loss 536.8629 test Loss 207.0911 with MSE metric 44591.6491\n",
      "Epoch 18 batch 160 train Loss 535.7331 test Loss 206.6703 with MSE metric 44590.9638\n",
      "Epoch 18 batch 170 train Loss 534.6034 test Loss 206.2512 with MSE metric 44589.4598\n",
      "Epoch 18 batch 180 train Loss 533.4796 test Loss 205.8339 with MSE metric 44588.8982\n",
      "Epoch 18 batch 190 train Loss 532.3612 test Loss 205.4184 with MSE metric 44590.7852\n",
      "Epoch 18 batch 200 train Loss 531.2461 test Loss 205.0045 with MSE metric 44591.5490\n",
      "Epoch 18 batch 210 train Loss 530.1370 test Loss 204.5927 with MSE metric 44589.9326\n",
      "Epoch 18 batch 220 train Loss 529.0327 test Loss 204.1825 with MSE metric 44587.4440\n",
      "Epoch 18 batch 230 train Loss 527.9343 test Loss 203.7739 with MSE metric 44586.7431\n",
      "Epoch 18 batch 240 train Loss 526.8394 test Loss 203.3671 with MSE metric 44585.4931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 29.84224796295166 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 525.7473 test Loss 202.9620 with MSE metric 44585.1595\n",
      "Epoch 19 batch 10 train Loss 524.6610 test Loss 202.5586 with MSE metric 44583.3717\n",
      "Epoch 19 batch 20 train Loss 523.5810 test Loss 202.1569 with MSE metric 44583.4698\n",
      "Epoch 19 batch 30 train Loss 522.5030 test Loss 201.7569 with MSE metric 44582.7458\n",
      "Epoch 19 batch 40 train Loss 521.4332 test Loss 201.3587 with MSE metric 44581.1580\n",
      "Epoch 19 batch 50 train Loss 520.3638 test Loss 200.9622 with MSE metric 44579.6533\n",
      "Epoch 19 batch 60 train Loss 519.2989 test Loss 200.5672 with MSE metric 44577.5454\n",
      "Epoch 19 batch 70 train Loss 518.2389 test Loss 200.1739 with MSE metric 44574.7417\n",
      "Epoch 19 batch 80 train Loss 517.1837 test Loss 199.7822 with MSE metric 44573.0875\n",
      "Epoch 19 batch 90 train Loss 516.1344 test Loss 199.3921 with MSE metric 44571.1268\n",
      "Epoch 19 batch 100 train Loss 515.0874 test Loss 199.0036 with MSE metric 44570.0729\n",
      "Epoch 19 batch 110 train Loss 514.0465 test Loss 198.6167 with MSE metric 44570.3090\n",
      "Epoch 19 batch 120 train Loss 513.0080 test Loss 198.2315 with MSE metric 44567.8395\n",
      "Epoch 19 batch 130 train Loss 511.9804 test Loss 197.8478 with MSE metric 44565.7494\n",
      "Epoch 19 batch 140 train Loss 510.9505 test Loss 197.4656 with MSE metric 44564.7961\n",
      "Epoch 19 batch 150 train Loss 509.9253 test Loss 197.0852 with MSE metric 44566.4279\n",
      "Epoch 19 batch 160 train Loss 508.9041 test Loss 196.7063 with MSE metric 44564.1533\n",
      "Epoch 19 batch 170 train Loss 507.8868 test Loss 196.3288 with MSE metric 44562.3490\n",
      "Epoch 19 batch 180 train Loss 506.8751 test Loss 195.9528 with MSE metric 44562.4334\n",
      "Epoch 19 batch 190 train Loss 505.8698 test Loss 195.5784 with MSE metric 44564.2516\n",
      "Epoch 19 batch 200 train Loss 504.8644 test Loss 195.2056 with MSE metric 44563.3392\n",
      "Epoch 19 batch 210 train Loss 503.8771 test Loss 194.8343 with MSE metric 44563.3639\n",
      "Epoch 19 batch 220 train Loss 502.8801 test Loss 194.4645 with MSE metric 44562.4938\n",
      "Epoch 19 batch 230 train Loss 501.8873 test Loss 194.0961 with MSE metric 44560.2809\n",
      "Epoch 19 batch 240 train Loss 500.8983 test Loss 193.7293 with MSE metric 44559.0701\n",
      "Time taken for 1 epoch: 30.568185091018677 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 499.9135 test Loss 193.3640 with MSE metric 44559.5486\n",
      "Epoch 20 batch 10 train Loss 498.9328 test Loss 193.0001 with MSE metric 44559.9785\n",
      "Epoch 20 batch 20 train Loss 497.9550 test Loss 192.6377 with MSE metric 44560.1646\n",
      "Epoch 20 batch 30 train Loss 496.9813 test Loss 192.2765 with MSE metric 44557.6269\n",
      "Epoch 20 batch 40 train Loss 496.0113 test Loss 191.9170 with MSE metric 44555.8084\n",
      "Epoch 20 batch 50 train Loss 495.0456 test Loss 191.5588 with MSE metric 44556.9033\n",
      "Epoch 20 batch 60 train Loss 494.0829 test Loss 191.2020 with MSE metric 44556.1448\n",
      "Epoch 20 batch 70 train Loss 493.1243 test Loss 190.8467 with MSE metric 44554.2765\n",
      "Epoch 20 batch 80 train Loss 492.1821 test Loss 190.4927 with MSE metric 44553.2407\n",
      "Epoch 20 batch 90 train Loss 491.2308 test Loss 190.1401 with MSE metric 44552.6936\n",
      "Epoch 20 batch 100 train Loss 490.2838 test Loss 189.7890 with MSE metric 44552.7122\n",
      "Epoch 20 batch 110 train Loss 489.3421 test Loss 189.4391 with MSE metric 44552.6955\n",
      "Epoch 20 batch 120 train Loss 488.4034 test Loss 189.0906 with MSE metric 44551.7263\n",
      "Epoch 20 batch 130 train Loss 487.4682 test Loss 188.7435 with MSE metric 44550.7526\n",
      "Epoch 20 batch 140 train Loss 486.5357 test Loss 188.3978 with MSE metric 44549.9307\n",
      "Epoch 20 batch 150 train Loss 485.6121 test Loss 188.0535 with MSE metric 44547.1567\n",
      "Epoch 20 batch 160 train Loss 484.6869 test Loss 187.7105 with MSE metric 44545.5417\n",
      "Epoch 20 batch 170 train Loss 483.7676 test Loss 187.3688 with MSE metric 44545.3842\n",
      "Epoch 20 batch 180 train Loss 482.8502 test Loss 187.0285 with MSE metric 44544.4560\n",
      "Epoch 20 batch 190 train Loss 481.9357 test Loss 186.6895 with MSE metric 44544.5368\n",
      "Epoch 20 batch 200 train Loss 481.0239 test Loss 186.3518 with MSE metric 44544.0892\n",
      "Epoch 20 batch 210 train Loss 480.1160 test Loss 186.0154 with MSE metric 44543.4862\n",
      "Epoch 20 batch 220 train Loss 479.2114 test Loss 185.6802 with MSE metric 44541.3666\n",
      "Epoch 20 batch 230 train Loss 478.3118 test Loss 185.3463 with MSE metric 44539.3565\n",
      "Epoch 20 batch 240 train Loss 477.4146 test Loss 185.0136 with MSE metric 44538.8495\n",
      "Time taken for 1 epoch: 30.16786217689514 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 476.5206 test Loss 184.6823 with MSE metric 44541.2643\n",
      "Epoch 21 batch 10 train Loss 475.6377 test Loss 184.3524 with MSE metric 44540.3382\n",
      "Epoch 21 batch 20 train Loss 474.7501 test Loss 184.0235 with MSE metric 44540.6327\n",
      "Epoch 21 batch 30 train Loss 473.8670 test Loss 183.6959 with MSE metric 44539.9980\n",
      "Epoch 21 batch 40 train Loss 472.9864 test Loss 183.3695 with MSE metric 44539.7680\n",
      "Epoch 21 batch 50 train Loss 472.1109 test Loss 183.0446 with MSE metric 44538.6627\n",
      "Epoch 21 batch 60 train Loss 471.2366 test Loss 182.7208 with MSE metric 44539.1137\n",
      "Epoch 21 batch 70 train Loss 470.3668 test Loss 182.3981 with MSE metric 44538.3571\n",
      "Epoch 21 batch 80 train Loss 469.5000 test Loss 182.0767 with MSE metric 44539.5117\n",
      "Epoch 21 batch 90 train Loss 468.6358 test Loss 181.7566 with MSE metric 44537.4115\n",
      "Epoch 21 batch 100 train Loss 467.7763 test Loss 181.4376 with MSE metric 44536.6927\n",
      "Epoch 21 batch 110 train Loss 466.9180 test Loss 181.1198 with MSE metric 44533.3931\n",
      "Epoch 21 batch 120 train Loss 466.0633 test Loss 180.8031 with MSE metric 44531.6526\n",
      "Epoch 21 batch 130 train Loss 465.2125 test Loss 180.4877 with MSE metric 44532.1801\n",
      "Epoch 21 batch 140 train Loss 464.3646 test Loss 180.1734 with MSE metric 44532.1076\n",
      "Epoch 21 batch 150 train Loss 463.5209 test Loss 179.8604 with MSE metric 44531.6240\n",
      "Epoch 21 batch 160 train Loss 462.6793 test Loss 179.5484 with MSE metric 44530.1659\n",
      "Epoch 21 batch 170 train Loss 461.8398 test Loss 179.2376 with MSE metric 44531.7636\n",
      "Epoch 21 batch 180 train Loss 461.0042 test Loss 178.9280 with MSE metric 44530.2022\n",
      "Epoch 21 batch 190 train Loss 460.1713 test Loss 178.6195 with MSE metric 44529.8319\n",
      "Epoch 21 batch 200 train Loss 459.3415 test Loss 178.3120 with MSE metric 44528.6794\n",
      "Epoch 21 batch 210 train Loss 458.5150 test Loss 178.0057 with MSE metric 44528.1707\n",
      "Epoch 21 batch 220 train Loss 457.6914 test Loss 177.7007 with MSE metric 44528.0325\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s=32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 10 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s= 32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
