{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, data_combine, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, t, token = data_combine.climate_data_to_model_input('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Restored from /Users/omernivron/Downloads/GPT_climate/ckpt/check_0/ckpt-16573\n",
      "Epoch 0 batch 0 train Loss 5.2503 test Loss 5.3098 with MSE metric 6651.7617\n",
      "Epoch 0 batch 100 train Loss 5.3334 test Loss 5.3419 with MSE metric 7829.4185\n",
      "Epoch 0 batch 200 train Loss 5.3171 test Loss 5.3750 with MSE metric 7637.5947\n",
      "Time taken for 1 epoch: 25.849642992019653 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 5.3063 test Loss 5.2360 with MSE metric 7469.3574\n",
      "Epoch 1 batch 100 train Loss 5.3863 test Loss 5.4182 with MSE metric 8681.6406\n",
      "Epoch 1 batch 200 train Loss 5.3110 test Loss 5.4515 with MSE metric 7478.4180\n",
      "Time taken for 1 epoch: 23.60657000541687 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 5.3496 test Loss 5.3069 with MSE metric 8074.8354\n",
      "Epoch 2 batch 100 train Loss 5.1630 test Loss 5.2977 with MSE metric 5534.8193\n",
      "Epoch 2 batch 200 train Loss 5.1804 test Loss 5.4084 with MSE metric 5231.6543\n",
      "Time taken for 1 epoch: 24.078569889068604 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 5.3359 test Loss 5.3252 with MSE metric 7930.7959\n",
      "Epoch 3 batch 100 train Loss 5.2890 test Loss 5.3661 with MSE metric 7191.6479\n",
      "Epoch 3 batch 200 train Loss 5.2231 test Loss 5.3472 with MSE metric 6250.2861\n",
      "Time taken for 1 epoch: 22.51027202606201 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 5.3189 test Loss 5.4272 with MSE metric 7665.8779\n",
      "Epoch 4 batch 100 train Loss 5.3246 test Loss 5.2761 with MSE metric 7710.2852\n",
      "Epoch 4 batch 200 train Loss 5.2981 test Loss 5.3242 with MSE metric 7332.2603\n",
      "Time taken for 1 epoch: 27.01328992843628 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 5.2066 test Loss 5.3865 with MSE metric 6048.9805\n",
      "Epoch 5 batch 100 train Loss 5.3243 test Loss 5.3473 with MSE metric 7732.3413\n",
      "Epoch 5 batch 200 train Loss 5.2435 test Loss 5.2692 with MSE metric 6485.6802\n",
      "Time taken for 1 epoch: 23.09327530860901 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 5.3439 test Loss 5.3026 with MSE metric 8053.5498\n",
      "Epoch 6 batch 100 train Loss 5.3190 test Loss 5.2951 with MSE metric 7663.9429\n",
      "Epoch 6 batch 200 train Loss 5.2796 test Loss 5.3439 with MSE metric 7066.3447\n",
      "Time taken for 1 epoch: 23.811733961105347 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 5.2507 test Loss 5.3613 with MSE metric 6622.2920\n",
      "Epoch 7 batch 100 train Loss 5.2455 test Loss 5.3349 with MSE metric 6569.4102\n",
      "Epoch 7 batch 200 train Loss 5.2466 test Loss 5.3979 with MSE metric 6588.7075\n",
      "Time taken for 1 epoch: 24.17702889442444 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 5.3047 test Loss 5.3696 with MSE metric 7451.3989\n",
      "Epoch 8 batch 100 train Loss 5.2854 test Loss 5.4050 with MSE metric 7152.3345\n",
      "Epoch 8 batch 200 train Loss 5.2627 test Loss 5.4096 with MSE metric 6844.1440\n",
      "Time taken for 1 epoch: 25.685102939605713 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 5.3676 test Loss 5.3335 with MSE metric 8442.2744\n",
      "Epoch 9 batch 100 train Loss 5.3360 test Loss 5.3510 with MSE metric 7883.0049\n",
      "Epoch 9 batch 200 train Loss 5.3498 test Loss 5.2776 with MSE metric 8138.5264\n",
      "Time taken for 1 epoch: 24.14201807975769 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 5.2756 test Loss 5.4111 with MSE metric 6997.7515\n",
      "Epoch 10 batch 100 train Loss 5.2955 test Loss 5.3572 with MSE metric 7312.8403\n",
      "Epoch 10 batch 200 train Loss 5.3015 test Loss 5.2913 with MSE metric 7398.5962\n",
      "Time taken for 1 epoch: 23.861839771270752 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 5.2664 test Loss 5.4700 with MSE metric 6842.2158\n",
      "Epoch 11 batch 100 train Loss 5.3579 test Loss 5.2524 with MSE metric 8242.3486\n",
      "Epoch 11 batch 200 train Loss 5.3931 test Loss 5.3943 with MSE metric 8557.2637\n",
      "Time taken for 1 epoch: 25.152579069137573 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 5.2870 test Loss 5.5456 with MSE metric 7126.7295\n",
      "Epoch 12 batch 100 train Loss 5.3382 test Loss 5.3096 with MSE metric 7936.2036\n",
      "Epoch 12 batch 200 train Loss 5.2591 test Loss 5.3316 with MSE metric 6778.4561\n",
      "Time taken for 1 epoch: 24.41728901863098 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 5.3253 test Loss 5.3264 with MSE metric 7729.9292\n",
      "Epoch 13 batch 100 train Loss 5.2908 test Loss 5.3573 with MSE metric 7240.4907\n",
      "Epoch 13 batch 200 train Loss 5.3543 test Loss 5.3041 with MSE metric 8205.8672\n",
      "Time taken for 1 epoch: 23.977036237716675 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 5.3398 test Loss 5.4291 with MSE metric 7938.2510\n",
      "Epoch 14 batch 100 train Loss 5.4427 test Loss 5.3452 with MSE metric 9439.8066\n",
      "Epoch 14 batch 200 train Loss 5.2934 test Loss 5.4270 with MSE metric 7284.8086\n",
      "Time taken for 1 epoch: 24.502116918563843 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 5.2831 test Loss 5.3181 with MSE metric 7121.9580\n",
      "Epoch 15 batch 100 train Loss 5.3748 test Loss 5.4101 with MSE metric 8418.3711\n",
      "Epoch 15 batch 200 train Loss 5.2578 test Loss 5.2644 with MSE metric 6713.0723\n",
      "Time taken for 1 epoch: 23.667796850204468 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 5.3564 test Loss 5.3265 with MSE metric 8244.3281\n",
      "Epoch 16 batch 100 train Loss 5.1677 test Loss 5.4154 with MSE metric 5345.9072\n",
      "Epoch 16 batch 200 train Loss 5.3562 test Loss 5.3863 with MSE metric 8249.4492\n",
      "Time taken for 1 epoch: 23.631426095962524 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 5.3985 test Loss 5.4229 with MSE metric 8737.0859\n",
      "Epoch 17 batch 100 train Loss 5.3100 test Loss 5.3202 with MSE metric 7524.2627\n",
      "Epoch 17 batch 200 train Loss 5.3890 test Loss 5.2011 with MSE metric 8716.5312\n",
      "Time taken for 1 epoch: 24.100277185440063 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 5.2376 test Loss 5.3001 with MSE metric 6362.9531\n",
      "Epoch 18 batch 100 train Loss 5.3090 test Loss 5.4147 with MSE metric 7511.4653\n",
      "Epoch 18 batch 200 train Loss 5.2950 test Loss 5.3353 with MSE metric 7304.4419\n",
      "Time taken for 1 epoch: 23.22602605819702 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 5.2173 test Loss 5.3617 with MSE metric 6187.8564\n",
      "Epoch 19 batch 100 train Loss 5.2543 test Loss 5.3182 with MSE metric 6667.3965\n",
      "Epoch 19 batch 200 train Loss 5.4490 test Loss 5.2995 with MSE metric 9639.1367\n",
      "Time taken for 1 epoch: 24.828556060791016 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 5.2697 test Loss 5.4437 with MSE metric 6947.4697\n",
      "Epoch 20 batch 100 train Loss 5.2409 test Loss 5.4111 with MSE metric 6535.6069\n",
      "Epoch 20 batch 200 train Loss 5.3124 test Loss 5.4129 with MSE metric 7565.8901\n",
      "Time taken for 1 epoch: 23.41496205329895 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 5.3171 test Loss 5.3314 with MSE metric 7638.7871\n",
      "Epoch 21 batch 100 train Loss 5.3208 test Loss 5.3599 with MSE metric 7671.7969\n",
      "Epoch 21 batch 200 train Loss 5.3527 test Loss 5.2362 with MSE metric 8150.8467\n",
      "Time taken for 1 epoch: 23.288187980651855 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 5.2501 test Loss 5.2924 with MSE metric 6623.7300\n",
      "Epoch 22 batch 100 train Loss 5.2972 test Loss 5.3956 with MSE metric 7339.9692\n",
      "Epoch 22 batch 200 train Loss 5.2376 test Loss 5.3731 with MSE metric 6392.9727\n",
      "Time taken for 1 epoch: 23.66571283340454 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 5.2587 test Loss 5.3118 with MSE metric 6762.5400\n",
      "Epoch 23 batch 100 train Loss 5.2733 test Loss 5.3286 with MSE metric 6968.0513\n",
      "Epoch 23 batch 200 train Loss 5.2765 test Loss 5.4135 with MSE metric 7042.1196\n",
      "Time taken for 1 epoch: 26.224602222442627 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 5.2387 test Loss 5.3823 with MSE metric 6498.6162\n",
      "Epoch 24 batch 100 train Loss 5.2991 test Loss 5.3157 with MSE metric 7347.1030\n",
      "Epoch 24 batch 200 train Loss 5.3254 test Loss 5.4120 with MSE metric 7763.8174\n",
      "Time taken for 1 epoch: 25.173414945602417 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 5.3191 test Loss 5.4317 with MSE metric 7665.6953\n",
      "Epoch 25 batch 100 train Loss 5.3040 test Loss 5.3808 with MSE metric 7442.2764\n",
      "Epoch 25 batch 200 train Loss 5.2328 test Loss 5.2538 with MSE metric 6449.6914\n",
      "Time taken for 1 epoch: 24.448577165603638 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 5.3342 test Loss 5.3558 with MSE metric 7896.8232\n",
      "Epoch 26 batch 100 train Loss 5.3824 test Loss 5.3707 with MSE metric 8534.4199\n",
      "Epoch 26 batch 200 train Loss 5.3453 test Loss 5.3112 with MSE metric 8023.2124\n",
      "Time taken for 1 epoch: 23.702926874160767 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 5.2924 test Loss 5.3001 with MSE metric 7265.5898\n",
      "Epoch 27 batch 100 train Loss 5.2636 test Loss 5.2944 with MSE metric 6834.5996\n",
      "Epoch 27 batch 200 train Loss 5.3472 test Loss 5.3076 with MSE metric 8024.6426\n",
      "Time taken for 1 epoch: 23.762485027313232 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 5.3944 test Loss 5.4476 with MSE metric 8750.8389\n",
      "Epoch 28 batch 100 train Loss 5.3582 test Loss 5.3564 with MSE metric 8222.1553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 batch 200 train Loss 5.3002 test Loss 5.3958 with MSE metric 7368.8594\n",
      "Time taken for 1 epoch: 24.058087825775146 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 5.2617 test Loss 5.4073 with MSE metric 6769.7119\n",
      "Epoch 29 batch 100 train Loss 5.4562 test Loss 5.3170 with MSE metric 9675.4932\n",
      "Epoch 29 batch 200 train Loss 5.3578 test Loss 5.3442 with MSE metric 8129.1162\n",
      "Time taken for 1 epoch: 23.884035110473633 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 5.3557 test Loss 5.3563 with MSE metric 8173.7373\n",
      "Epoch 30 batch 100 train Loss 5.3098 test Loss 5.3838 with MSE metric 7522.5220\n",
      "Epoch 30 batch 200 train Loss 5.3745 test Loss 5.3140 with MSE metric 8515.8477\n",
      "Time taken for 1 epoch: 23.977431058883667 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 5.2341 test Loss 5.4183 with MSE metric 6371.4541\n",
      "Epoch 31 batch 100 train Loss 5.3232 test Loss 5.3746 with MSE metric 7728.9375\n",
      "Epoch 31 batch 200 train Loss 5.2468 test Loss 5.3602 with MSE metric 6606.9883\n",
      "Time taken for 1 epoch: 24.268733024597168 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 5.3030 test Loss 5.4102 with MSE metric 7403.2598\n",
      "Epoch 32 batch 100 train Loss 5.2453 test Loss 5.2910 with MSE metric 6615.1172\n",
      "Epoch 32 batch 200 train Loss 5.3080 test Loss 5.3924 with MSE metric 7493.0547\n",
      "Time taken for 1 epoch: 23.670051097869873 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 5.2058 test Loss 5.2964 with MSE metric 6061.6113\n",
      "Epoch 33 batch 100 train Loss 5.2123 test Loss 5.3728 with MSE metric 6131.5454\n",
      "Epoch 33 batch 200 train Loss 5.3067 test Loss 5.2888 with MSE metric 7478.5869\n",
      "Time taken for 1 epoch: 23.82981586456299 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 5.2688 test Loss 5.2850 with MSE metric 6882.6016\n",
      "Epoch 34 batch 100 train Loss 5.3328 test Loss 5.2854 with MSE metric 7874.1948\n",
      "Epoch 34 batch 200 train Loss 5.2887 test Loss 5.4012 with MSE metric 7214.1719\n",
      "Time taken for 1 epoch: 24.630040645599365 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 5.4163 test Loss 5.3776 with MSE metric 9189.7539\n",
      "Epoch 35 batch 100 train Loss 5.2077 test Loss 5.4534 with MSE metric 6012.4033\n",
      "Epoch 35 batch 200 train Loss 5.2236 test Loss 5.3565 with MSE metric 6080.5068\n",
      "Time taken for 1 epoch: 24.687288999557495 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 5.2988 test Loss 5.3146 with MSE metric 7361.2100\n",
      "Epoch 36 batch 100 train Loss 5.3478 test Loss 5.3803 with MSE metric 8049.1499\n",
      "Epoch 36 batch 200 train Loss 5.3542 test Loss 5.4094 with MSE metric 8221.2920\n",
      "Time taken for 1 epoch: 24.511558055877686 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 5.3600 test Loss 5.3135 with MSE metric 8290.7480\n",
      "Epoch 37 batch 100 train Loss 5.1952 test Loss 5.3063 with MSE metric 5956.7754\n",
      "Epoch 37 batch 200 train Loss 5.2966 test Loss 5.3612 with MSE metric 7332.1934\n",
      "Time taken for 1 epoch: 24.766733646392822 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 5.3301 test Loss 5.3940 with MSE metric 7820.7368\n",
      "Epoch 38 batch 100 train Loss 5.2839 test Loss 5.3432 with MSE metric 7140.3354\n",
      "Epoch 38 batch 200 train Loss 5.3336 test Loss 5.3309 with MSE metric 7890.9668\n",
      "Time taken for 1 epoch: 24.157762050628662 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 5.2672 test Loss 5.3144 with MSE metric 6883.7529\n",
      "Epoch 39 batch 100 train Loss 5.2347 test Loss 5.3467 with MSE metric 6418.9648\n",
      "Epoch 39 batch 200 train Loss 5.3071 test Loss 5.2737 with MSE metric 7470.4272\n",
      "Time taken for 1 epoch: 24.55631995201111 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 5.3046 test Loss 5.4083 with MSE metric 7418.8638\n",
      "Epoch 40 batch 100 train Loss 5.4329 test Loss 5.3739 with MSE metric 9226.8125\n",
      "Epoch 40 batch 200 train Loss 5.2403 test Loss 5.4548 with MSE metric 6491.6201\n",
      "Time taken for 1 epoch: 24.071237802505493 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 5.2971 test Loss 5.3207 with MSE metric 7338.2754\n",
      "Epoch 41 batch 100 train Loss 5.3746 test Loss 5.3209 with MSE metric 8488.6367\n",
      "Epoch 41 batch 200 train Loss 5.2504 test Loss 5.3778 with MSE metric 6670.2114\n",
      "Time taken for 1 epoch: 23.796143770217896 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 5.3030 test Loss 5.3163 with MSE metric 7427.0122\n",
      "Epoch 42 batch 100 train Loss 5.1663 test Loss 5.3811 with MSE metric 5500.6270\n",
      "Epoch 42 batch 200 train Loss 5.2675 test Loss 5.3924 with MSE metric 6822.8926\n",
      "Time taken for 1 epoch: 24.47148585319519 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 5.2397 test Loss 5.3213 with MSE metric 6465.0527\n",
      "Epoch 43 batch 100 train Loss 5.3662 test Loss 5.3881 with MSE metric 8343.5527\n",
      "Epoch 43 batch 200 train Loss 5.3049 test Loss 5.3691 with MSE metric 7454.1353\n",
      "Time taken for 1 epoch: 25.504178047180176 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 5.3732 test Loss 5.3678 with MSE metric 8533.8965\n",
      "Epoch 44 batch 100 train Loss 5.2833 test Loss 5.4123 with MSE metric 7138.5986\n",
      "Epoch 44 batch 200 train Loss 5.3218 test Loss 5.2534 with MSE metric 7633.2979\n",
      "Time taken for 1 epoch: 24.88033699989319 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 5.1848 test Loss 5.3336 with MSE metric 5706.8281\n",
      "Epoch 45 batch 100 train Loss 5.3174 test Loss 5.2989 with MSE metric 7637.2705\n",
      "Epoch 45 batch 200 train Loss 5.1848 test Loss 5.3196 with MSE metric 5628.3809\n",
      "Time taken for 1 epoch: 24.391961812973022 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 5.2509 test Loss 5.3536 with MSE metric 6679.3643\n",
      "Epoch 46 batch 100 train Loss 5.2496 test Loss 5.4485 with MSE metric 6664.3335\n",
      "Epoch 46 batch 200 train Loss 5.3588 test Loss 5.4231 with MSE metric 8254.3496\n",
      "Time taken for 1 epoch: 24.858994960784912 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 5.3307 test Loss 5.2940 with MSE metric 7804.2646\n",
      "Epoch 47 batch 100 train Loss 5.2925 test Loss 5.2984 with MSE metric 7269.4697\n",
      "Epoch 47 batch 200 train Loss 5.2690 test Loss 5.3741 with MSE metric 6935.0146\n",
      "Time taken for 1 epoch: 24.661542892456055 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 5.3039 test Loss 5.2821 with MSE metric 7429.5225\n",
      "Epoch 48 batch 100 train Loss 5.3453 test Loss 5.4669 with MSE metric 8059.2681\n",
      "Epoch 48 batch 200 train Loss 5.2540 test Loss 5.2172 with MSE metric 6695.4336\n",
      "Time taken for 1 epoch: 23.8230619430542 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 5.3870 test Loss 5.4070 with MSE metric 8637.9766\n",
      "Epoch 49 batch 100 train Loss 5.3109 test Loss 5.2864 with MSE metric 7533.3535\n",
      "Epoch 49 batch 200 train Loss 5.3745 test Loss 5.3340 with MSE metric 8492.5781\n",
      "Time taken for 1 epoch: 24.424815893173218 secs\n",
      "\n",
      "Epoch 50 batch 0 train Loss 5.2782 test Loss 5.3371 with MSE metric 7034.4414\n",
      "Epoch 50 batch 100 train Loss 5.2212 test Loss 5.3580 with MSE metric 6268.9131\n",
      "Epoch 50 batch 200 train Loss 5.3582 test Loss 5.2935 with MSE metric 8157.7969\n",
      "Time taken for 1 epoch: 24.12725806236267 secs\n",
      "\n",
      "Epoch 51 batch 0 train Loss 5.3433 test Loss 5.3342 with MSE metric 8049.4873\n",
      "Epoch 51 batch 100 train Loss 5.3112 test Loss 5.4015 with MSE metric 7542.8564\n",
      "Epoch 51 batch 200 train Loss 5.3308 test Loss 5.3000 with MSE metric 7813.3428\n",
      "Time taken for 1 epoch: 24.031040906906128 secs\n",
      "\n",
      "Epoch 52 batch 0 train Loss 5.3437 test Loss 5.4305 with MSE metric 8056.0806\n",
      "Epoch 52 batch 100 train Loss 5.3933 test Loss 5.4046 with MSE metric 8721.0898\n",
      "Epoch 52 batch 200 train Loss 5.2453 test Loss 5.3523 with MSE metric 6473.5425\n",
      "Time taken for 1 epoch: 23.873843908309937 secs\n",
      "\n",
      "Epoch 53 batch 0 train Loss 5.3368 test Loss 5.4318 with MSE metric 7916.7646\n",
      "Epoch 53 batch 100 train Loss 5.2665 test Loss 5.4069 with MSE metric 6884.9565\n",
      "Epoch 53 batch 200 train Loss 5.3331 test Loss 5.3305 with MSE metric 7878.5962\n",
      "Time taken for 1 epoch: 23.835639715194702 secs\n",
      "\n",
      "Epoch 54 batch 0 train Loss 5.3310 test Loss 5.3816 with MSE metric 7824.8164\n",
      "Epoch 54 batch 100 train Loss 5.1921 test Loss 5.3885 with MSE metric 5791.2524\n",
      "Epoch 54 batch 200 train Loss 5.2005 test Loss 5.3799 with MSE metric 5872.4365\n",
      "Time taken for 1 epoch: 23.650545835494995 secs\n",
      "\n",
      "Epoch 55 batch 0 train Loss 5.2651 test Loss 5.3893 with MSE metric 6869.5156\n",
      "Epoch 55 batch 100 train Loss 5.3447 test Loss 5.4707 with MSE metric 8028.2793\n",
      "Epoch 55 batch 200 train Loss 5.1708 test Loss 5.3365 with MSE metric 5586.9346\n",
      "Time taken for 1 epoch: 24.513808250427246 secs\n",
      "\n",
      "Epoch 56 batch 0 train Loss 5.3731 test Loss 5.3353 with MSE metric 8518.8926\n",
      "Epoch 56 batch 100 train Loss 5.3287 test Loss 5.2865 with MSE metric 7815.8486\n",
      "Epoch 56 batch 200 train Loss 5.2645 test Loss 5.3619 with MSE metric 6869.1401\n",
      "Time taken for 1 epoch: 24.936461210250854 secs\n",
      "\n",
      "Epoch 57 batch 0 train Loss 5.3123 test Loss 5.3979 with MSE metric 7558.2959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 batch 100 train Loss 5.2768 test Loss 5.2575 with MSE metric 7034.8906\n",
      "Epoch 57 batch 200 train Loss 5.1675 test Loss 5.2635 with MSE metric 5167.5264\n",
      "Time taken for 1 epoch: 27.70429277420044 secs\n",
      "\n",
      "Epoch 58 batch 0 train Loss 5.2523 test Loss 5.2944 with MSE metric 6652.1499\n",
      "Epoch 58 batch 100 train Loss 5.2737 test Loss 5.4599 with MSE metric 6984.2568\n",
      "Epoch 58 batch 200 train Loss 5.2744 test Loss 5.2755 with MSE metric 6965.1821\n",
      "Time taken for 1 epoch: 23.318583011627197 secs\n",
      "\n",
      "Epoch 59 batch 0 train Loss 5.3634 test Loss 5.4142 with MSE metric 8377.0752\n",
      "Epoch 59 batch 100 train Loss 5.2843 test Loss 5.3189 with MSE metric 7135.9751\n",
      "Epoch 59 batch 200 train Loss 5.3167 test Loss 5.4112 with MSE metric 7633.4160\n",
      "Time taken for 1 epoch: 23.31714367866516 secs\n",
      "\n",
      "Epoch 60 batch 0 train Loss 5.2706 test Loss 5.3299 with MSE metric 6923.7129\n",
      "Epoch 60 batch 100 train Loss 5.3463 test Loss 5.4544 with MSE metric 8073.1250\n",
      "Epoch 60 batch 200 train Loss 5.2807 test Loss 5.3599 with MSE metric 7093.1343\n",
      "Time taken for 1 epoch: 23.658143043518066 secs\n",
      "\n",
      "Epoch 61 batch 0 train Loss 5.2519 test Loss 5.3726 with MSE metric 6671.6475\n",
      "Epoch 61 batch 100 train Loss 5.3382 test Loss 5.3641 with MSE metric 7966.4229\n",
      "Epoch 61 batch 200 train Loss 5.3195 test Loss 5.3826 with MSE metric 7668.9380\n",
      "Time taken for 1 epoch: 23.963479042053223 secs\n",
      "\n",
      "Epoch 62 batch 0 train Loss 5.3066 test Loss 5.4081 with MSE metric 7429.9580\n",
      "Epoch 62 batch 100 train Loss 5.2818 test Loss 5.4044 with MSE metric 7096.4082\n",
      "Epoch 62 batch 200 train Loss 5.3474 test Loss 5.3275 with MSE metric 7991.4746\n",
      "Time taken for 1 epoch: 24.130332231521606 secs\n",
      "\n",
      "Epoch 63 batch 0 train Loss 5.3458 test Loss 5.3513 with MSE metric 8086.5986\n",
      "Epoch 63 batch 100 train Loss 5.3013 test Loss 5.3664 with MSE metric 7383.5430\n",
      "Epoch 63 batch 200 train Loss 5.3983 test Loss 5.3657 with MSE metric 8971.0039\n",
      "Time taken for 1 epoch: 24.333691120147705 secs\n",
      "\n",
      "Epoch 64 batch 0 train Loss 5.3573 test Loss 5.3404 with MSE metric 8092.9932\n",
      "Epoch 64 batch 100 train Loss 5.3347 test Loss 5.4092 with MSE metric 7875.9351\n",
      "Epoch 64 batch 200 train Loss 5.2233 test Loss 5.2975 with MSE metric 6273.0518\n",
      "Time taken for 1 epoch: 24.48498797416687 secs\n",
      "\n",
      "Epoch 65 batch 0 train Loss 5.3104 test Loss 5.2579 with MSE metric 7517.7559\n",
      "Epoch 65 batch 100 train Loss 5.3937 test Loss 5.2499 with MSE metric 8829.5176\n",
      "Epoch 65 batch 200 train Loss 5.2407 test Loss 5.3695 with MSE metric 6546.1328\n",
      "Time taken for 1 epoch: 24.388354063034058 secs\n",
      "\n",
      "Epoch 66 batch 0 train Loss 5.2650 test Loss 5.3618 with MSE metric 6861.0508\n",
      "Epoch 66 batch 100 train Loss 5.4659 test Loss 5.3425 with MSE metric 9662.3281\n",
      "Epoch 66 batch 200 train Loss 5.2491 test Loss 5.3941 with MSE metric 6581.7769\n",
      "Time taken for 1 epoch: 24.430356979370117 secs\n",
      "\n",
      "Epoch 67 batch 0 train Loss 5.3161 test Loss 5.3643 with MSE metric 7623.0488\n",
      "Epoch 67 batch 100 train Loss 5.3139 test Loss 5.3372 with MSE metric 7588.2773\n",
      "Epoch 67 batch 200 train Loss 5.2735 test Loss 5.4878 with MSE metric 7001.2734\n",
      "Time taken for 1 epoch: 24.481970071792603 secs\n",
      "\n",
      "Epoch 68 batch 0 train Loss 5.2051 test Loss 5.3517 with MSE metric 5985.1807\n",
      "Epoch 68 batch 100 train Loss 5.2653 test Loss 5.3651 with MSE metric 6865.7266\n",
      "Epoch 68 batch 200 train Loss 5.3655 test Loss 5.3756 with MSE metric 8393.1738\n",
      "Time taken for 1 epoch: 26.093024969100952 secs\n",
      "\n",
      "Epoch 69 batch 0 train Loss 5.3518 test Loss 5.3774 with MSE metric 8187.1802\n",
      "Epoch 69 batch 100 train Loss 5.3835 test Loss 5.3745 with MSE metric 8673.3438\n",
      "Epoch 69 batch 200 train Loss 5.2082 test Loss 5.3756 with MSE metric 5805.9136\n",
      "Time taken for 1 epoch: 23.90269684791565 secs\n",
      "\n",
      "Epoch 70 batch 0 train Loss 5.2192 test Loss 5.3675 with MSE metric 6116.7114\n",
      "Epoch 70 batch 100 train Loss 5.3697 test Loss 5.3542 with MSE metric 8447.4707\n",
      "Epoch 70 batch 200 train Loss 5.2847 test Loss 5.2930 with MSE metric 7153.0386\n",
      "Time taken for 1 epoch: 24.09819269180298 secs\n",
      "\n",
      "Epoch 71 batch 0 train Loss 5.2400 test Loss 5.4438 with MSE metric 6482.2729\n",
      "Epoch 71 batch 100 train Loss 5.3333 test Loss 5.2995 with MSE metric 7885.8120\n",
      "Epoch 71 batch 200 train Loss 5.4031 test Loss 5.4378 with MSE metric 8859.3643\n",
      "Time taken for 1 epoch: 25.215381145477295 secs\n",
      "\n",
      "Epoch 72 batch 0 train Loss 5.3108 test Loss 5.3701 with MSE metric 7531.4873\n",
      "Epoch 72 batch 100 train Loss 5.3924 test Loss 5.4004 with MSE metric 8750.1729\n",
      "Epoch 72 batch 200 train Loss 5.2806 test Loss 5.3916 with MSE metric 7067.1362\n",
      "Time taken for 1 epoch: 24.64347815513611 secs\n",
      "\n",
      "Epoch 73 batch 0 train Loss 5.3145 test Loss 5.4238 with MSE metric 7596.6621\n",
      "Epoch 73 batch 100 train Loss 5.2486 test Loss 5.4580 with MSE metric 6653.6099\n",
      "Epoch 73 batch 200 train Loss 5.3268 test Loss 5.3126 with MSE metric 7787.8901\n",
      "Time taken for 1 epoch: 26.089602947235107 secs\n",
      "\n",
      "Epoch 74 batch 0 train Loss 5.2915 test Loss 5.3844 with MSE metric 7250.8496\n",
      "Epoch 74 batch 100 train Loss 5.3187 test Loss 5.4128 with MSE metric 7645.4072\n",
      "Epoch 74 batch 200 train Loss 5.2574 test Loss 5.3877 with MSE metric 6751.7861\n",
      "Time taken for 1 epoch: 24.8170747756958 secs\n",
      "\n",
      "Epoch 75 batch 0 train Loss 5.2607 test Loss 5.2722 with MSE metric 6773.1768\n",
      "Epoch 75 batch 100 train Loss 5.3953 test Loss 5.4579 with MSE metric 8743.5645\n",
      "Epoch 75 batch 200 train Loss 5.2680 test Loss 5.3940 with MSE metric 6762.1953\n",
      "Time taken for 1 epoch: 22.887208938598633 secs\n",
      "\n",
      "Epoch 76 batch 0 train Loss 5.1998 test Loss 5.3843 with MSE metric 5861.6040\n",
      "Epoch 76 batch 100 train Loss 5.2517 test Loss 5.4589 with MSE metric 6686.2759\n",
      "Epoch 76 batch 200 train Loss 5.2972 test Loss 5.2828 with MSE metric 7340.5654\n",
      "Time taken for 1 epoch: 23.054362058639526 secs\n",
      "\n",
      "Epoch 77 batch 0 train Loss 5.2643 test Loss 5.3788 with MSE metric 6856.9248\n",
      "Epoch 77 batch 100 train Loss 5.2287 test Loss 5.3274 with MSE metric 6364.8330\n",
      "Epoch 77 batch 200 train Loss 5.3163 test Loss 5.4885 with MSE metric 7588.9048\n",
      "Time taken for 1 epoch: 23.33616304397583 secs\n",
      "\n",
      "Epoch 78 batch 0 train Loss 5.4067 test Loss 5.3182 with MSE metric 9022.9561\n",
      "Epoch 78 batch 100 train Loss 5.2981 test Loss 5.3329 with MSE metric 7353.7778\n",
      "Epoch 78 batch 200 train Loss 5.2101 test Loss 5.3946 with MSE metric 6143.0908\n",
      "Time taken for 1 epoch: 22.944217681884766 secs\n",
      "\n",
      "Epoch 79 batch 0 train Loss 5.3429 test Loss 5.4819 with MSE metric 8014.4131\n",
      "Epoch 79 batch 100 train Loss 5.2230 test Loss 5.3407 with MSE metric 6298.6973\n",
      "Epoch 79 batch 200 train Loss 5.3374 test Loss 5.4232 with MSE metric 7952.9277\n",
      "Time taken for 1 epoch: 23.19276213645935 secs\n",
      "\n",
      "Epoch 80 batch 0 train Loss 5.2996 test Loss 5.4540 with MSE metric 7374.6445\n",
      "Epoch 80 batch 100 train Loss 5.3638 test Loss 5.3767 with MSE metric 8370.9824\n",
      "Epoch 80 batch 200 train Loss 5.3502 test Loss 5.3348 with MSE metric 8127.0732\n",
      "Time taken for 1 epoch: 23.05231785774231 secs\n",
      "\n",
      "Epoch 81 batch 0 train Loss 5.2074 test Loss 5.2710 with MSE metric 6048.9424\n",
      "Epoch 81 batch 100 train Loss 5.1770 test Loss 5.3625 with MSE metric 5613.4556\n",
      "Epoch 81 batch 200 train Loss 5.2515 test Loss 5.3453 with MSE metric 6691.4819\n",
      "Time taken for 1 epoch: 23.024932861328125 secs\n",
      "\n",
      "Epoch 82 batch 0 train Loss 5.3398 test Loss 5.3519 with MSE metric 7976.5420\n",
      "Epoch 82 batch 100 train Loss 5.3909 test Loss 5.3662 with MSE metric 8732.8086\n",
      "Epoch 82 batch 200 train Loss 5.3151 test Loss 5.4381 with MSE metric 7575.1406\n",
      "Time taken for 1 epoch: 22.962843894958496 secs\n",
      "\n",
      "Epoch 83 batch 0 train Loss 5.2680 test Loss 5.4184 with MSE metric 6880.8848\n",
      "Epoch 83 batch 100 train Loss 5.2781 test Loss 5.3394 with MSE metric 7058.9707\n",
      "Epoch 83 batch 200 train Loss 5.2635 test Loss 5.4297 with MSE metric 6851.6919\n",
      "Time taken for 1 epoch: 23.018440008163452 secs\n",
      "\n",
      "Epoch 84 batch 0 train Loss 5.2997 test Loss 5.3938 with MSE metric 7369.4800\n",
      "Epoch 84 batch 100 train Loss 5.3417 test Loss 5.3801 with MSE metric 8018.4805\n",
      "Epoch 84 batch 200 train Loss 5.3275 test Loss 5.3373 with MSE metric 7786.2295\n",
      "Time taken for 1 epoch: 23.06336522102356 secs\n",
      "\n",
      "Epoch 85 batch 0 train Loss 5.2815 test Loss 5.3221 with MSE metric 7104.0708\n",
      "Epoch 85 batch 100 train Loss 5.3602 test Loss 5.3402 with MSE metric 8082.4868\n",
      "Epoch 85 batch 200 train Loss 5.2911 test Loss 5.2960 with MSE metric 7248.3716\n",
      "Time taken for 1 epoch: 23.07262897491455 secs\n",
      "\n",
      "Epoch 86 batch 0 train Loss 5.2943 test Loss 5.4215 with MSE metric 7288.5771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 batch 100 train Loss 5.3635 test Loss 5.2790 with MSE metric 8243.8467\n",
      "Epoch 86 batch 200 train Loss 5.3255 test Loss 5.4615 with MSE metric 7698.4170\n",
      "Time taken for 1 epoch: 23.096320152282715 secs\n",
      "\n",
      "Epoch 87 batch 0 train Loss 5.2727 test Loss 5.3547 with MSE metric 6957.2783\n",
      "Epoch 87 batch 100 train Loss 5.3596 test Loss 5.3885 with MSE metric 8241.0059\n",
      "Epoch 87 batch 200 train Loss 5.3039 test Loss 5.2472 with MSE metric 7423.5825\n",
      "Time taken for 1 epoch: 23.191839933395386 secs\n",
      "\n",
      "Epoch 88 batch 0 train Loss 5.3660 test Loss 5.3232 with MSE metric 8376.6064\n",
      "Epoch 88 batch 100 train Loss 5.3406 test Loss 5.4629 with MSE metric 7999.7759\n",
      "Epoch 88 batch 200 train Loss 5.2865 test Loss 5.4052 with MSE metric 7183.7788\n",
      "Time taken for 1 epoch: 23.118685960769653 secs\n",
      "\n",
      "Epoch 89 batch 0 train Loss 5.3343 test Loss 5.2537 with MSE metric 7863.9746\n",
      "Epoch 89 batch 100 train Loss 5.3742 test Loss 5.3112 with MSE metric 8497.5234\n",
      "Epoch 89 batch 200 train Loss 5.3451 test Loss 5.4739 with MSE metric 8078.7173\n",
      "Time taken for 1 epoch: 23.05047297477722 secs\n",
      "\n",
      "Epoch 90 batch 0 train Loss 5.3333 test Loss 5.3548 with MSE metric 7845.2822\n",
      "Epoch 90 batch 100 train Loss 5.2904 test Loss 5.4240 with MSE metric 7231.9150\n",
      "Epoch 90 batch 200 train Loss 5.3563 test Loss 5.4009 with MSE metric 8218.3428\n",
      "Time taken for 1 epoch: 23.021301984786987 secs\n",
      "\n",
      "Epoch 91 batch 0 train Loss 5.4451 test Loss 5.3722 with MSE metric 9470.4150\n",
      "Epoch 91 batch 100 train Loss 5.3343 test Loss 5.4224 with MSE metric 7893.3535\n",
      "Epoch 91 batch 200 train Loss 5.3514 test Loss 5.3900 with MSE metric 8159.6055\n",
      "Time taken for 1 epoch: 22.80749225616455 secs\n",
      "\n",
      "Epoch 92 batch 0 train Loss 5.2759 test Loss 5.2719 with MSE metric 7034.3394\n",
      "Epoch 92 batch 100 train Loss 5.3199 test Loss 5.3148 with MSE metric 7681.3521\n",
      "Epoch 92 batch 200 train Loss 5.1964 test Loss 5.3532 with MSE metric 5910.6040\n",
      "Time taken for 1 epoch: 22.63063883781433 secs\n",
      "\n",
      "Epoch 93 batch 0 train Loss 5.3339 test Loss 5.4106 with MSE metric 7894.1182\n",
      "Epoch 93 batch 100 train Loss 5.3126 test Loss 5.4560 with MSE metric 7568.2578\n",
      "Epoch 93 batch 200 train Loss 5.3016 test Loss 5.3070 with MSE metric 7404.6328\n",
      "Time taken for 1 epoch: 22.604371070861816 secs\n",
      "\n",
      "Epoch 94 batch 0 train Loss 5.3083 test Loss 5.3375 with MSE metric 7503.8740\n",
      "Epoch 94 batch 100 train Loss 5.2708 test Loss 5.3812 with MSE metric 6941.6338\n",
      "Epoch 94 batch 200 train Loss 5.3308 test Loss 5.3991 with MSE metric 7840.8662\n",
      "Time taken for 1 epoch: 22.5871639251709 secs\n",
      "\n",
      "Epoch 95 batch 0 train Loss 5.3393 test Loss 5.4124 with MSE metric 7859.4614\n",
      "Epoch 95 batch 100 train Loss 5.3511 test Loss 5.3446 with MSE metric 8167.6016\n",
      "Epoch 95 batch 200 train Loss 5.2694 test Loss 5.3179 with MSE metric 6943.5391\n",
      "Time taken for 1 epoch: 22.494943141937256 secs\n",
      "\n",
      "Epoch 96 batch 0 train Loss 5.3054 test Loss 5.3545 with MSE metric 7458.2227\n",
      "Epoch 96 batch 100 train Loss 5.3188 test Loss 5.4450 with MSE metric 7651.0439\n",
      "Epoch 96 batch 200 train Loss 5.3668 test Loss 5.4135 with MSE metric 8428.0488\n",
      "Time taken for 1 epoch: 22.648401021957397 secs\n",
      "\n",
      "Epoch 97 batch 0 train Loss 5.3038 test Loss 5.4163 with MSE metric 7424.7007\n",
      "Epoch 97 batch 100 train Loss 5.2758 test Loss 5.4189 with MSE metric 7027.7310\n",
      "Epoch 97 batch 200 train Loss 5.1840 test Loss 5.4309 with MSE metric 5555.8389\n",
      "Time taken for 1 epoch: 22.61997127532959 secs\n",
      "\n",
      "Epoch 98 batch 0 train Loss 5.3579 test Loss 5.3981 with MSE metric 8219.6582\n",
      "Epoch 98 batch 100 train Loss 5.3482 test Loss 5.5032 with MSE metric 8089.2852\n",
      "Epoch 98 batch 200 train Loss 5.3155 test Loss 5.4454 with MSE metric 7611.4785\n",
      "Time taken for 1 epoch: 22.64123797416687 secs\n",
      "\n",
      "Epoch 99 batch 0 train Loss 5.2830 test Loss 5.4236 with MSE metric 7126.4160\n",
      "Epoch 99 batch 100 train Loss 5.2892 test Loss 5.3784 with MSE metric 7224.6992\n",
      "Epoch 99 batch 200 train Loss 5.3524 test Loss 5.2670 with MSE metric 8180.0415\n",
      "Time taken for 1 epoch: 22.528568029403687 secs\n",
      "\n",
      "Epoch 100 batch 0 train Loss 5.3376 test Loss 5.3705 with MSE metric 7892.6279\n",
      "Epoch 100 batch 100 train Loss 5.2655 test Loss 5.3517 with MSE metric 6844.0117\n",
      "Epoch 100 batch 200 train Loss 5.2571 test Loss 5.3618 with MSE metric 6743.1885\n",
      "Time taken for 1 epoch: 22.671453952789307 secs\n",
      "\n",
      "Epoch 101 batch 0 train Loss 5.2348 test Loss 5.4080 with MSE metric 6445.2700\n",
      "Epoch 101 batch 100 train Loss 5.3174 test Loss 5.4071 with MSE metric 7640.9727\n",
      "Epoch 101 batch 200 train Loss 5.3351 test Loss 5.4297 with MSE metric 7905.1733\n",
      "Time taken for 1 epoch: 22.65250825881958 secs\n",
      "\n",
      "Epoch 102 batch 0 train Loss 5.2419 test Loss 5.2870 with MSE metric 6561.4692\n",
      "Epoch 102 batch 100 train Loss 5.2474 test Loss 5.3605 with MSE metric 6610.9629\n",
      "Epoch 102 batch 200 train Loss 5.2489 test Loss 5.3244 with MSE metric 6653.3232\n",
      "Time taken for 1 epoch: 22.568932056427002 secs\n",
      "\n",
      "Epoch 103 batch 0 train Loss 5.2937 test Loss 5.3896 with MSE metric 7289.7080\n",
      "Epoch 103 batch 100 train Loss 5.2875 test Loss 5.3406 with MSE metric 7198.9805\n",
      "Epoch 103 batch 200 train Loss 5.2899 test Loss 5.4378 with MSE metric 7231.8477\n",
      "Time taken for 1 epoch: 22.61504292488098 secs\n",
      "\n",
      "Epoch 104 batch 0 train Loss 5.2574 test Loss 5.3361 with MSE metric 6735.0576\n",
      "Epoch 104 batch 100 train Loss 5.2812 test Loss 5.4073 with MSE metric 7064.2051\n",
      "Epoch 104 batch 200 train Loss 5.2402 test Loss 5.4276 with MSE metric 6528.3955\n",
      "Time taken for 1 epoch: 22.727766036987305 secs\n",
      "\n",
      "Epoch 105 batch 0 train Loss 5.2889 test Loss 5.3566 with MSE metric 7196.7241\n",
      "Epoch 105 batch 100 train Loss 5.2565 test Loss 5.3946 with MSE metric 6764.4111\n",
      "Epoch 105 batch 200 train Loss 5.3141 test Loss 5.4552 with MSE metric 7593.5312\n",
      "Time taken for 1 epoch: 22.648059844970703 secs\n",
      "\n",
      "Epoch 106 batch 0 train Loss 5.3002 test Loss 5.3531 with MSE metric 7354.5049\n",
      "Epoch 106 batch 100 train Loss 5.3311 test Loss 5.4531 with MSE metric 7796.9004\n",
      "Epoch 106 batch 200 train Loss 5.3257 test Loss 5.3673 with MSE metric 7752.9204\n",
      "Time taken for 1 epoch: 22.685242891311646 secs\n",
      "\n",
      "Epoch 107 batch 0 train Loss 5.2891 test Loss 5.3580 with MSE metric 7196.5249\n",
      "Epoch 107 batch 100 train Loss 5.3698 test Loss 5.3091 with MSE metric 8446.0664\n",
      "Epoch 107 batch 200 train Loss 5.3426 test Loss 5.3813 with MSE metric 8029.3828\n",
      "Time taken for 1 epoch: 22.743457078933716 secs\n",
      "\n",
      "Epoch 108 batch 0 train Loss 5.5002 test Loss 5.4190 with MSE metric 10623.2129\n",
      "Epoch 108 batch 100 train Loss 5.3061 test Loss 5.3192 with MSE metric 7428.1948\n",
      "Epoch 108 batch 200 train Loss 5.3053 test Loss 5.3784 with MSE metric 7434.3398\n",
      "Time taken for 1 epoch: 22.733731031417847 secs\n",
      "\n",
      "Epoch 109 batch 0 train Loss 5.2225 test Loss 5.3572 with MSE metric 6282.0762\n",
      "Epoch 109 batch 100 train Loss 5.2246 test Loss 5.2237 with MSE metric 6322.2451\n",
      "Epoch 109 batch 200 train Loss 5.3082 test Loss 5.3370 with MSE metric 7504.0405\n",
      "Time taken for 1 epoch: 22.68421196937561 secs\n",
      "\n",
      "Epoch 110 batch 0 train Loss 5.2561 test Loss 5.3636 with MSE metric 6579.0283\n",
      "Epoch 110 batch 100 train Loss 5.2529 test Loss 5.5168 with MSE metric 6696.1011\n",
      "Epoch 110 batch 200 train Loss 5.2875 test Loss 5.4060 with MSE metric 7189.4131\n",
      "Time taken for 1 epoch: 22.61674213409424 secs\n",
      "\n",
      "Epoch 111 batch 0 train Loss 5.2760 test Loss 5.2675 with MSE metric 7019.9648\n",
      "Epoch 111 batch 100 train Loss 5.3171 test Loss 5.3351 with MSE metric 7639.0869\n",
      "Epoch 111 batch 200 train Loss 5.3575 test Loss 5.3238 with MSE metric 8257.2695\n",
      "Time taken for 1 epoch: 22.696702003479004 secs\n",
      "\n",
      "Epoch 112 batch 0 train Loss 5.4029 test Loss 5.3719 with MSE metric 8847.2178\n",
      "Epoch 112 batch 100 train Loss 5.2521 test Loss 5.3220 with MSE metric 6659.1182\n",
      "Epoch 112 batch 200 train Loss 5.2948 test Loss 5.2497 with MSE metric 7293.9956\n",
      "Time taken for 1 epoch: 22.72708296775818 secs\n",
      "\n",
      "Epoch 113 batch 0 train Loss 5.3277 test Loss 5.2828 with MSE metric 7792.3740\n",
      "Epoch 113 batch 100 train Loss 5.2463 test Loss 5.3985 with MSE metric 6626.2266\n",
      "Epoch 113 batch 200 train Loss 5.2651 test Loss 5.3063 with MSE metric 6851.0840\n",
      "Time taken for 1 epoch: 22.643678903579712 secs\n",
      "\n",
      "Epoch 114 batch 0 train Loss 5.3207 test Loss 5.3585 with MSE metric 7692.5820\n",
      "Epoch 114 batch 100 train Loss 5.3355 test Loss 5.4767 with MSE metric 7893.1709\n",
      "Epoch 114 batch 200 train Loss 5.2808 test Loss 5.3347 with MSE metric 6953.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 22.76567816734314 secs\n",
      "\n",
      "Epoch 115 batch 0 train Loss 5.3127 test Loss 5.3904 with MSE metric 7569.4131\n",
      "Epoch 115 batch 100 train Loss 5.2722 test Loss 5.3495 with MSE metric 6974.2471\n",
      "Epoch 115 batch 200 train Loss 5.3063 test Loss 5.3424 with MSE metric 7473.8213\n",
      "Time taken for 1 epoch: 22.785687923431396 secs\n",
      "\n",
      "Epoch 116 batch 0 train Loss 5.3021 test Loss 5.3540 with MSE metric 7390.6094\n",
      "Epoch 116 batch 100 train Loss 5.2696 test Loss 5.4576 with MSE metric 6946.2207\n",
      "Epoch 116 batch 200 train Loss 5.2412 test Loss 5.3245 with MSE metric 6495.5737\n",
      "Time taken for 1 epoch: 22.696279048919678 secs\n",
      "\n",
      "Epoch 117 batch 0 train Loss 5.3386 test Loss 5.2392 with MSE metric 7953.4053\n",
      "Epoch 117 batch 100 train Loss 5.3437 test Loss 5.2938 with MSE metric 8052.5874\n",
      "Epoch 117 batch 200 train Loss 5.3384 test Loss 5.2812 with MSE metric 7878.6128\n",
      "Time taken for 1 epoch: 22.70211887359619 secs\n",
      "\n",
      "Epoch 118 batch 0 train Loss 5.2441 test Loss 5.4138 with MSE metric 6518.5903\n",
      "Epoch 118 batch 100 train Loss 5.3064 test Loss 5.5085 with MSE metric 7458.8350\n",
      "Epoch 118 batch 200 train Loss 5.3069 test Loss 5.4473 with MSE metric 7482.5215\n",
      "Time taken for 1 epoch: 22.76654314994812 secs\n",
      "\n",
      "Epoch 119 batch 0 train Loss 5.3352 test Loss 5.3438 with MSE metric 7898.4531\n",
      "Epoch 119 batch 100 train Loss 5.4074 test Loss 5.3809 with MSE metric 9072.2363\n",
      "Epoch 119 batch 200 train Loss 5.2871 test Loss 5.2655 with MSE metric 7188.1528\n",
      "Time taken for 1 epoch: 22.71866512298584 secs\n",
      "\n",
      "Epoch 120 batch 0 train Loss 5.3294 test Loss 5.2906 with MSE metric 7828.7788\n",
      "Epoch 120 batch 100 train Loss 5.2496 test Loss 5.2887 with MSE metric 6673.0068\n",
      "Epoch 120 batch 200 train Loss 5.2830 test Loss 5.3543 with MSE metric 7112.7627\n",
      "Time taken for 1 epoch: 22.80578088760376 secs\n",
      "\n",
      "Epoch 121 batch 0 train Loss 5.3340 test Loss 5.4201 with MSE metric 7901.6982\n",
      "Epoch 121 batch 100 train Loss 5.2654 test Loss 5.4523 with MSE metric 6888.5127\n",
      "Epoch 121 batch 200 train Loss 5.3172 test Loss 5.3042 with MSE metric 7632.1333\n",
      "Time taken for 1 epoch: 22.719638109207153 secs\n",
      "\n",
      "Epoch 122 batch 0 train Loss 5.3451 test Loss 5.3131 with MSE metric 8017.8188\n",
      "Epoch 122 batch 100 train Loss 5.2470 test Loss 5.4329 with MSE metric 6596.7598\n",
      "Epoch 122 batch 200 train Loss 5.2451 test Loss 5.2449 with MSE metric 6459.9482\n",
      "Time taken for 1 epoch: 22.794713973999023 secs\n",
      "\n",
      "Epoch 123 batch 0 train Loss 5.2772 test Loss 5.3781 with MSE metric 7052.4512\n",
      "Epoch 123 batch 100 train Loss 5.2853 test Loss 5.5953 with MSE metric 7157.2529\n",
      "Epoch 123 batch 200 train Loss 5.2871 test Loss 5.2990 with MSE metric 7098.4785\n",
      "Time taken for 1 epoch: 22.79376983642578 secs\n",
      "\n",
      "Epoch 124 batch 0 train Loss 5.3004 test Loss 5.4217 with MSE metric 7328.2881\n",
      "Epoch 124 batch 100 train Loss 5.2380 test Loss 5.3339 with MSE metric 6505.4629\n",
      "Epoch 124 batch 200 train Loss 5.2393 test Loss 5.4766 with MSE metric 6464.5283\n",
      "Time taken for 1 epoch: 22.739012956619263 secs\n",
      "\n",
      "Epoch 125 batch 0 train Loss 5.2502 test Loss 5.4406 with MSE metric 6580.7520\n",
      "Epoch 125 batch 100 train Loss 5.2569 test Loss 5.3408 with MSE metric 6761.9482\n",
      "Epoch 125 batch 200 train Loss 5.1815 test Loss 5.3958 with MSE metric 5693.5098\n",
      "Time taken for 1 epoch: 22.68754291534424 secs\n",
      "\n",
      "Epoch 126 batch 0 train Loss 5.3035 test Loss 5.3780 with MSE metric 7428.2349\n",
      "Epoch 126 batch 100 train Loss 5.3827 test Loss 5.3826 with MSE metric 8554.2256\n",
      "Epoch 126 batch 200 train Loss 5.2846 test Loss 5.4250 with MSE metric 7153.3857\n",
      "Time taken for 1 epoch: 22.773648262023926 secs\n",
      "\n",
      "Epoch 127 batch 0 train Loss 5.2405 test Loss 5.3930 with MSE metric 6446.4482\n",
      "Epoch 127 batch 100 train Loss 5.2508 test Loss 5.3976 with MSE metric 6690.1445\n",
      "Epoch 127 batch 200 train Loss 5.3103 test Loss 5.4228 with MSE metric 7533.0063\n",
      "Time taken for 1 epoch: 22.695683002471924 secs\n",
      "\n",
      "Epoch 128 batch 0 train Loss 5.3013 test Loss 5.3781 with MSE metric 7379.5557\n",
      "Epoch 128 batch 100 train Loss 5.2971 test Loss 5.3476 with MSE metric 7329.3877\n",
      "Epoch 128 batch 200 train Loss 5.2862 test Loss 5.3923 with MSE metric 7146.4824\n",
      "Time taken for 1 epoch: 22.66392421722412 secs\n",
      "\n",
      "Epoch 129 batch 0 train Loss 5.2901 test Loss 5.3858 with MSE metric 7232.8120\n",
      "Epoch 129 batch 100 train Loss 5.3114 test Loss 5.3134 with MSE metric 7552.2080\n",
      "Epoch 129 batch 200 train Loss 5.4147 test Loss 5.3440 with MSE metric 9173.3066\n",
      "Time taken for 1 epoch: 22.637899160385132 secs\n",
      "\n",
      "Epoch 130 batch 0 train Loss 5.2789 test Loss 5.3703 with MSE metric 7037.2925\n",
      "Epoch 130 batch 100 train Loss 5.2559 test Loss 5.3682 with MSE metric 6751.2119\n",
      "Epoch 130 batch 200 train Loss 5.2769 test Loss 5.3972 with MSE metric 7046.9697\n",
      "Time taken for 1 epoch: 22.66222095489502 secs\n",
      "\n",
      "Epoch 131 batch 0 train Loss 5.3502 test Loss 5.3046 with MSE metric 8089.7412\n",
      "Epoch 131 batch 100 train Loss 5.2250 test Loss 5.3799 with MSE metric 6248.7925\n",
      "Epoch 131 batch 200 train Loss 5.2312 test Loss 5.4517 with MSE metric 6338.2334\n",
      "Time taken for 1 epoch: 22.56839895248413 secs\n",
      "\n",
      "Epoch 132 batch 0 train Loss 5.3109 test Loss 5.4771 with MSE metric 7504.1133\n",
      "Epoch 132 batch 100 train Loss 5.3245 test Loss 5.3571 with MSE metric 7747.2217\n",
      "Epoch 132 batch 200 train Loss 5.3259 test Loss 5.3630 with MSE metric 7768.3735\n",
      "Time taken for 1 epoch: 22.612001180648804 secs\n",
      "\n",
      "Epoch 133 batch 0 train Loss 5.1838 test Loss 5.3749 with MSE metric 5712.0332\n",
      "Epoch 133 batch 100 train Loss 5.2457 test Loss 5.3278 with MSE metric 6581.8438\n",
      "Epoch 133 batch 200 train Loss 5.2225 test Loss 5.3542 with MSE metric 6281.4229\n",
      "Time taken for 1 epoch: 22.713945865631104 secs\n",
      "\n",
      "Epoch 134 batch 0 train Loss 5.2916 test Loss 5.3319 with MSE metric 7241.7500\n",
      "Epoch 134 batch 100 train Loss 5.2167 test Loss 5.3764 with MSE metric 6180.6523\n",
      "Epoch 134 batch 200 train Loss 5.2310 test Loss 5.3765 with MSE metric 6307.8218\n",
      "Time taken for 1 epoch: 22.583418130874634 secs\n",
      "\n",
      "Epoch 135 batch 0 train Loss 5.2983 test Loss 5.3668 with MSE metric 7356.6714\n",
      "Epoch 135 batch 100 train Loss 5.4122 test Loss 5.3437 with MSE metric 9062.9404\n",
      "Epoch 135 batch 200 train Loss 5.2953 test Loss 5.4730 with MSE metric 7291.2656\n",
      "Time taken for 1 epoch: 22.561792850494385 secs\n",
      "\n",
      "Epoch 136 batch 0 train Loss 5.3058 test Loss 5.4487 with MSE metric 7455.3867\n",
      "Epoch 136 batch 100 train Loss 5.2096 test Loss 5.3447 with MSE metric 6064.7715\n",
      "Epoch 136 batch 200 train Loss 5.2848 test Loss 5.2955 with MSE metric 7136.2139\n",
      "Time taken for 1 epoch: 22.725272178649902 secs\n",
      "\n",
      "Epoch 137 batch 0 train Loss 5.3051 test Loss 5.3727 with MSE metric 7447.5757\n",
      "Epoch 137 batch 100 train Loss 5.3897 test Loss 5.2451 with MSE metric 8740.0469\n",
      "Epoch 137 batch 200 train Loss 5.2068 test Loss 5.2930 with MSE metric 5912.7798\n",
      "Time taken for 1 epoch: 24.464726209640503 secs\n",
      "\n",
      "Epoch 138 batch 0 train Loss 5.3702 test Loss 5.4466 with MSE metric 8383.2773\n",
      "Epoch 138 batch 100 train Loss 5.2180 test Loss 5.3522 with MSE metric 6236.6553\n",
      "Epoch 138 batch 200 train Loss 5.3451 test Loss 5.3587 with MSE metric 8038.0176\n",
      "Time taken for 1 epoch: 24.551172971725464 secs\n",
      "\n",
      "Epoch 139 batch 0 train Loss 5.3689 test Loss 5.2654 with MSE metric 8363.5654\n",
      "Epoch 139 batch 100 train Loss 5.2963 test Loss 5.4057 with MSE metric 7322.5542\n",
      "Epoch 139 batch 200 train Loss 5.3164 test Loss 5.2678 with MSE metric 7619.5723\n",
      "Time taken for 1 epoch: 27.05208921432495 secs\n",
      "\n",
      "Epoch 140 batch 0 train Loss 5.2924 test Loss 5.2230 with MSE metric 7270.5771\n",
      "Epoch 140 batch 100 train Loss 5.2492 test Loss 5.3618 with MSE metric 6668.7720\n",
      "Epoch 140 batch 200 train Loss 5.2757 test Loss 5.3590 with MSE metric 7016.2095\n",
      "Time taken for 1 epoch: 24.030921936035156 secs\n",
      "\n",
      "Epoch 141 batch 0 train Loss 5.3215 test Loss 5.2889 with MSE metric 7687.0815\n",
      "Epoch 141 batch 100 train Loss 5.2560 test Loss 5.4371 with MSE metric 6684.8223\n",
      "Epoch 141 batch 200 train Loss 5.2167 test Loss 5.3024 with MSE metric 6021.9238\n",
      "Time taken for 1 epoch: 24.33730411529541 secs\n",
      "\n",
      "Epoch 142 batch 0 train Loss 5.2861 test Loss 5.3290 with MSE metric 7174.5000\n",
      "Epoch 142 batch 100 train Loss 5.2606 test Loss 5.2664 with MSE metric 6800.5044\n",
      "Epoch 142 batch 200 train Loss 5.2814 test Loss 5.3056 with MSE metric 7105.5117\n",
      "Time taken for 1 epoch: 28.290973901748657 secs\n",
      "\n",
      "Epoch 143 batch 0 train Loss 5.1864 test Loss 5.4008 with MSE metric 5658.9385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143 batch 100 train Loss 5.3631 test Loss 5.3626 with MSE metric 8276.8018\n",
      "Epoch 143 batch 200 train Loss 5.2443 test Loss 5.3615 with MSE metric 6457.0249\n",
      "Time taken for 1 epoch: 25.156306982040405 secs\n",
      "\n",
      "Epoch 144 batch 0 train Loss 5.3876 test Loss 5.4072 with MSE metric 8667.7598\n",
      "Epoch 144 batch 100 train Loss 5.3050 test Loss 5.3772 with MSE metric 7450.9160\n",
      "Epoch 144 batch 200 train Loss 5.2017 test Loss 5.2178 with MSE metric 5984.2568\n",
      "Time taken for 1 epoch: 25.413201808929443 secs\n",
      "\n",
      "Epoch 145 batch 0 train Loss 5.2165 test Loss 5.2883 with MSE metric 6166.8740\n",
      "Epoch 145 batch 100 train Loss 5.2306 test Loss 5.4070 with MSE metric 6365.7524\n",
      "Epoch 145 batch 200 train Loss 5.2778 test Loss 5.4011 with MSE metric 7061.0869\n",
      "Time taken for 1 epoch: 25.551324367523193 secs\n",
      "\n",
      "Epoch 146 batch 0 train Loss 5.2987 test Loss 5.3305 with MSE metric 7350.6748\n",
      "Epoch 146 batch 100 train Loss 5.4136 test Loss 5.4091 with MSE metric 9053.6875\n",
      "Epoch 146 batch 200 train Loss 5.2763 test Loss 5.4244 with MSE metric 7040.0664\n",
      "Time taken for 1 epoch: 24.539623975753784 secs\n",
      "\n",
      "Epoch 147 batch 0 train Loss 5.2808 test Loss 5.3704 with MSE metric 7103.3784\n",
      "Epoch 147 batch 100 train Loss 5.3289 test Loss 5.2978 with MSE metric 7793.6948\n",
      "Epoch 147 batch 200 train Loss 5.3387 test Loss 5.3836 with MSE metric 7958.4297\n",
      "Time taken for 1 epoch: 23.716450929641724 secs\n",
      "\n",
      "Epoch 148 batch 0 train Loss 5.3464 test Loss 5.4011 with MSE metric 8063.7915\n",
      "Epoch 148 batch 100 train Loss 5.4003 test Loss 5.3261 with MSE metric 8835.1592\n",
      "Epoch 148 batch 200 train Loss 5.1890 test Loss 5.3704 with MSE metric 5637.6963\n",
      "Time taken for 1 epoch: 24.235045194625854 secs\n",
      "\n",
      "Epoch 149 batch 0 train Loss 5.4221 test Loss 5.5329 with MSE metric 9297.3760\n",
      "Epoch 149 batch 100 train Loss 5.3327 test Loss 5.3557 with MSE metric 7868.2236\n",
      "Epoch 149 batch 200 train Loss 5.3842 test Loss 5.3777 with MSE metric 8591.4092\n",
      "Time taken for 1 epoch: 24.483691930770874 secs\n",
      "\n",
      "Epoch 150 batch 0 train Loss 5.2692 test Loss 5.3311 with MSE metric 6921.1348\n",
      "Epoch 150 batch 100 train Loss 5.3645 test Loss 5.3321 with MSE metric 8261.0840\n",
      "Epoch 150 batch 200 train Loss 5.3231 test Loss 5.3397 with MSE metric 7728.3662\n",
      "Time taken for 1 epoch: 24.464869737625122 secs\n",
      "\n",
      "Epoch 151 batch 0 train Loss 5.3185 test Loss 5.3885 with MSE metric 7642.8730\n",
      "Epoch 151 batch 100 train Loss 5.2591 test Loss 5.4134 with MSE metric 6791.7676\n",
      "Epoch 151 batch 200 train Loss 5.3117 test Loss 5.3659 with MSE metric 7507.9043\n",
      "Time taken for 1 epoch: 24.256829977035522 secs\n",
      "\n",
      "Epoch 152 batch 0 train Loss 5.2650 test Loss 5.2797 with MSE metric 6856.8213\n",
      "Epoch 152 batch 100 train Loss 5.4306 test Loss 5.3895 with MSE metric 9204.9922\n",
      "Epoch 152 batch 200 train Loss 5.2400 test Loss 5.4176 with MSE metric 6506.0098\n",
      "Time taken for 1 epoch: 24.240005016326904 secs\n",
      "\n",
      "Epoch 153 batch 0 train Loss 5.3759 test Loss 5.2848 with MSE metric 8541.4082\n",
      "Epoch 153 batch 100 train Loss 5.2960 test Loss 5.3789 with MSE metric 7314.7002\n",
      "Epoch 153 batch 200 train Loss 5.3710 test Loss 5.3998 with MSE metric 8477.8359\n",
      "Time taken for 1 epoch: 23.923548221588135 secs\n",
      "\n",
      "Epoch 154 batch 0 train Loss 5.4026 test Loss 5.3855 with MSE metric 8895.9619\n",
      "Epoch 154 batch 100 train Loss 5.2377 test Loss 5.4118 with MSE metric 6483.1382\n",
      "Epoch 154 batch 200 train Loss 5.3354 test Loss 5.3047 with MSE metric 7902.8809\n",
      "Time taken for 1 epoch: 23.869773864746094 secs\n",
      "\n",
      "Epoch 155 batch 0 train Loss 5.2649 test Loss 5.3766 with MSE metric 6827.7793\n",
      "Epoch 155 batch 100 train Loss 5.3388 test Loss 5.3327 with MSE metric 7969.2827\n",
      "Epoch 155 batch 200 train Loss 5.3167 test Loss 5.3093 with MSE metric 7631.7422\n",
      "Time taken for 1 epoch: 24.71853804588318 secs\n",
      "\n",
      "Epoch 156 batch 0 train Loss 5.1722 test Loss 5.4254 with MSE metric 5555.8682\n",
      "Epoch 156 batch 100 train Loss 5.2756 test Loss 5.4048 with MSE metric 7014.9951\n",
      "Epoch 156 batch 200 train Loss 5.1970 test Loss 5.2714 with MSE metric 5846.8618\n",
      "Time taken for 1 epoch: 25.56333899497986 secs\n",
      "\n",
      "Epoch 157 batch 0 train Loss 5.3129 test Loss 5.3719 with MSE metric 7572.6582\n",
      "Epoch 157 batch 100 train Loss 5.2470 test Loss 5.3532 with MSE metric 6483.7383\n",
      "Epoch 157 batch 200 train Loss 5.2664 test Loss 5.3779 with MSE metric 6874.4233\n",
      "Time taken for 1 epoch: 27.482322931289673 secs\n",
      "\n",
      "Epoch 158 batch 0 train Loss 5.2333 test Loss 5.3684 with MSE metric 6373.2598\n",
      "Epoch 158 batch 100 train Loss 5.2587 test Loss 5.3539 with MSE metric 6774.3447\n",
      "Epoch 158 batch 200 train Loss 5.4407 test Loss 5.3824 with MSE metric 9550.6582\n",
      "Time taken for 1 epoch: 27.60853910446167 secs\n",
      "\n",
      "Epoch 159 batch 0 train Loss 5.2024 test Loss 5.4490 with MSE metric 5981.4487\n",
      "Epoch 159 batch 100 train Loss 5.2443 test Loss 5.3565 with MSE metric 6542.8164\n",
      "Epoch 159 batch 200 train Loss 5.2748 test Loss 5.2556 with MSE metric 7009.1592\n",
      "Time taken for 1 epoch: 27.859941244125366 secs\n",
      "\n",
      "Epoch 160 batch 0 train Loss 5.2543 test Loss 5.3815 with MSE metric 6738.0088\n",
      "Epoch 160 batch 100 train Loss 5.2962 test Loss 5.2657 with MSE metric 7325.4277\n",
      "Epoch 160 batch 200 train Loss 5.2607 test Loss 5.3605 with MSE metric 6814.3066\n",
      "Time taken for 1 epoch: 27.466480016708374 secs\n",
      "\n",
      "Epoch 161 batch 0 train Loss 5.3320 test Loss 5.3283 with MSE metric 7853.1348\n",
      "Epoch 161 batch 100 train Loss 5.2897 test Loss 5.3702 with MSE metric 7231.1128\n",
      "Epoch 161 batch 200 train Loss 5.3798 test Loss 5.3333 with MSE metric 8522.7744\n",
      "Time taken for 1 epoch: 26.844829082489014 secs\n",
      "\n",
      "Epoch 162 batch 0 train Loss 5.3142 test Loss 5.4049 with MSE metric 7588.4014\n",
      "Epoch 162 batch 100 train Loss 5.3917 test Loss 5.3725 with MSE metric 8579.9043\n",
      "Epoch 162 batch 200 train Loss 5.3217 test Loss 5.4598 with MSE metric 7693.6670\n",
      "Time taken for 1 epoch: 27.0679669380188 secs\n",
      "\n",
      "Epoch 163 batch 0 train Loss 5.3066 test Loss 5.3585 with MSE metric 7472.9375\n",
      "Epoch 163 batch 100 train Loss 5.3125 test Loss 5.3041 with MSE metric 7558.7437\n",
      "Epoch 163 batch 200 train Loss 5.3643 test Loss 5.4367 with MSE metric 8262.4512\n",
      "Time taken for 1 epoch: 26.781869173049927 secs\n",
      "\n",
      "Epoch 164 batch 0 train Loss 5.3475 test Loss 5.4552 with MSE metric 8114.3774\n",
      "Epoch 164 batch 100 train Loss 5.2621 test Loss 5.4837 with MSE metric 6842.7285\n",
      "Epoch 164 batch 200 train Loss 5.1994 test Loss 5.3533 with MSE metric 5884.4932\n",
      "Time taken for 1 epoch: 26.90159583091736 secs\n",
      "\n",
      "Epoch 165 batch 0 train Loss 5.3458 test Loss 5.3158 with MSE metric 7984.9805\n",
      "Epoch 165 batch 100 train Loss 5.2922 test Loss 5.3810 with MSE metric 7267.4976\n",
      "Epoch 165 batch 200 train Loss 5.3462 test Loss 5.4151 with MSE metric 8043.1914\n",
      "Time taken for 1 epoch: 26.419057846069336 secs\n",
      "\n",
      "Epoch 166 batch 0 train Loss 5.2405 test Loss 5.2855 with MSE metric 6458.7402\n",
      "Epoch 166 batch 100 train Loss 5.3488 test Loss 5.3113 with MSE metric 8085.7822\n",
      "Epoch 166 batch 200 train Loss 5.2768 test Loss 5.3739 with MSE metric 7039.0073\n",
      "Time taken for 1 epoch: 26.481143951416016 secs\n",
      "\n",
      "Epoch 167 batch 0 train Loss 5.3339 test Loss 5.2568 with MSE metric 7884.9746\n",
      "Epoch 167 batch 100 train Loss 5.3372 test Loss 5.3185 with MSE metric 7951.4941\n",
      "Epoch 167 batch 200 train Loss 5.3378 test Loss 5.3894 with MSE metric 7928.9248\n",
      "Time taken for 1 epoch: 25.941138982772827 secs\n",
      "\n",
      "Epoch 168 batch 0 train Loss 5.3255 test Loss 5.2846 with MSE metric 7765.6826\n",
      "Epoch 168 batch 100 train Loss 5.4215 test Loss 5.4204 with MSE metric 9196.3721\n",
      "Epoch 168 batch 200 train Loss 5.3736 test Loss 5.2679 with MSE metric 8496.1689\n",
      "Time taken for 1 epoch: 25.926267862319946 secs\n",
      "\n",
      "Epoch 169 batch 0 train Loss 5.2764 test Loss 5.3409 with MSE metric 7028.1729\n",
      "Epoch 169 batch 100 train Loss 5.2633 test Loss 5.3857 with MSE metric 6845.3076\n",
      "Epoch 169 batch 200 train Loss 5.1923 test Loss 5.4498 with MSE metric 5827.5322\n",
      "Time taken for 1 epoch: 26.07885503768921 secs\n",
      "\n",
      "Epoch 170 batch 0 train Loss 5.2429 test Loss 5.3337 with MSE metric 6496.1240\n",
      "Epoch 170 batch 100 train Loss 5.3214 test Loss 5.3343 with MSE metric 7694.5938\n",
      "Epoch 170 batch 200 train Loss 5.2702 test Loss 5.3422 with MSE metric 6943.3926\n",
      "Time taken for 1 epoch: 25.620736122131348 secs\n",
      "\n",
      "Epoch 171 batch 0 train Loss 5.3016 test Loss 5.1993 with MSE metric 7403.2188\n",
      "Epoch 171 batch 100 train Loss 5.3367 test Loss 5.3670 with MSE metric 7943.6299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 batch 200 train Loss 5.2752 test Loss 5.3184 with MSE metric 7024.9180\n",
      "Time taken for 1 epoch: 25.895297050476074 secs\n",
      "\n",
      "Epoch 172 batch 0 train Loss 5.3632 test Loss 5.2931 with MSE metric 8359.8232\n",
      "Epoch 172 batch 100 train Loss 5.3189 test Loss 5.3237 with MSE metric 7640.3486\n",
      "Epoch 172 batch 200 train Loss 5.3363 test Loss 5.3022 with MSE metric 7923.2402\n",
      "Time taken for 1 epoch: 25.682255029678345 secs\n",
      "\n",
      "Epoch 173 batch 0 train Loss 5.2615 test Loss 5.3010 with MSE metric 6773.4639\n",
      "Epoch 173 batch 100 train Loss 5.2832 test Loss 5.3077 with MSE metric 7104.6567\n",
      "Epoch 173 batch 200 train Loss 5.2452 test Loss 5.3224 with MSE metric 6584.0088\n",
      "Time taken for 1 epoch: 26.326202869415283 secs\n",
      "\n",
      "Epoch 174 batch 0 train Loss 5.2333 test Loss 5.3246 with MSE metric 6391.9243\n",
      "Epoch 174 batch 100 train Loss 5.2138 test Loss 5.4214 with MSE metric 6118.4834\n",
      "Epoch 174 batch 200 train Loss 5.3338 test Loss 5.3556 with MSE metric 7894.9746\n",
      "Time taken for 1 epoch: 25.731635093688965 secs\n",
      "\n",
      "Epoch 175 batch 0 train Loss 5.4051 test Loss 5.3438 with MSE metric 9014.5566\n",
      "Epoch 175 batch 100 train Loss 5.4037 test Loss 5.3218 with MSE metric 8713.9180\n",
      "Epoch 175 batch 200 train Loss 5.2906 test Loss 5.3624 with MSE metric 7199.8896\n",
      "Time taken for 1 epoch: 25.798399925231934 secs\n",
      "\n",
      "Epoch 176 batch 0 train Loss 5.3120 test Loss 5.3971 with MSE metric 7554.7051\n",
      "Epoch 176 batch 100 train Loss 5.1904 test Loss 5.3317 with MSE metric 5872.8643\n",
      "Epoch 176 batch 200 train Loss 5.2508 test Loss 5.3734 with MSE metric 6639.9834\n",
      "Time taken for 1 epoch: 25.799176931381226 secs\n",
      "\n",
      "Epoch 177 batch 0 train Loss 5.2946 test Loss 5.2474 with MSE metric 7302.6562\n",
      "Epoch 177 batch 100 train Loss 5.3021 test Loss 5.4272 with MSE metric 7386.0566\n",
      "Epoch 177 batch 200 train Loss 5.3237 test Loss 5.3798 with MSE metric 7715.2734\n",
      "Time taken for 1 epoch: 25.685594081878662 secs\n",
      "\n",
      "Epoch 178 batch 0 train Loss 5.3577 test Loss 5.3668 with MSE metric 8253.5645\n",
      "Epoch 178 batch 100 train Loss 5.2688 test Loss 5.3802 with MSE metric 6929.2080\n",
      "Epoch 178 batch 200 train Loss 5.2810 test Loss 5.2927 with MSE metric 7104.5171\n",
      "Time taken for 1 epoch: 25.662188053131104 secs\n",
      "\n",
      "Epoch 179 batch 0 train Loss 5.2083 test Loss 5.3893 with MSE metric 6087.8740\n",
      "Epoch 179 batch 100 train Loss 5.3402 test Loss 5.2864 with MSE metric 7927.2461\n",
      "Epoch 179 batch 200 train Loss 5.3284 test Loss 5.3551 with MSE metric 7796.6836\n",
      "Time taken for 1 epoch: 26.021563053131104 secs\n",
      "\n",
      "Epoch 180 batch 0 train Loss 5.1828 test Loss 5.3627 with MSE metric 5592.5039\n",
      "Epoch 180 batch 100 train Loss 5.2992 test Loss 5.3412 with MSE metric 7369.3301\n",
      "Epoch 180 batch 200 train Loss 5.3116 test Loss 5.3288 with MSE metric 7549.7568\n",
      "Time taken for 1 epoch: 25.83658194541931 secs\n",
      "\n",
      "Epoch 181 batch 0 train Loss 5.3005 test Loss 5.4854 with MSE metric 7389.5654\n",
      "Epoch 181 batch 100 train Loss 5.2324 test Loss 5.3778 with MSE metric 6443.6235\n",
      "Epoch 181 batch 200 train Loss 5.2312 test Loss 5.4579 with MSE metric 6368.0752\n",
      "Time taken for 1 epoch: 25.981960773468018 secs\n",
      "\n",
      "Epoch 182 batch 0 train Loss 5.2567 test Loss 5.3787 with MSE metric 6666.1880\n",
      "Epoch 182 batch 100 train Loss 5.2527 test Loss 5.4001 with MSE metric 6707.4248\n",
      "Epoch 182 batch 200 train Loss 5.3780 test Loss 5.4005 with MSE metric 8546.4570\n",
      "Time taken for 1 epoch: 25.960535049438477 secs\n",
      "\n",
      "Epoch 183 batch 0 train Loss 5.4501 test Loss 5.3534 with MSE metric 9700.7422\n",
      "Epoch 183 batch 100 train Loss 5.4248 test Loss 5.4549 with MSE metric 9145.8438\n",
      "Epoch 183 batch 200 train Loss 5.2404 test Loss 5.3011 with MSE metric 6494.2617\n",
      "Time taken for 1 epoch: 26.61315417289734 secs\n",
      "\n",
      "Epoch 184 batch 0 train Loss 5.2782 test Loss 5.3629 with MSE metric 7061.3730\n",
      "Epoch 184 batch 100 train Loss 5.3351 test Loss 5.2666 with MSE metric 7914.1372\n",
      "Epoch 184 batch 200 train Loss 5.2604 test Loss 5.4349 with MSE metric 6815.2939\n",
      "Time taken for 1 epoch: 26.692827224731445 secs\n",
      "\n",
      "Epoch 185 batch 0 train Loss 5.3517 test Loss 5.3291 with MSE metric 8180.8853\n",
      "Epoch 185 batch 100 train Loss 5.3023 test Loss 5.3549 with MSE metric 7414.1299\n",
      "Epoch 185 batch 200 train Loss 5.3238 test Loss 5.3528 with MSE metric 7708.6709\n",
      "Time taken for 1 epoch: 26.48755979537964 secs\n",
      "\n",
      "Epoch 186 batch 0 train Loss 5.2903 test Loss 5.4015 with MSE metric 7233.3389\n",
      "Epoch 186 batch 100 train Loss 5.2531 test Loss 5.3473 with MSE metric 6720.8076\n",
      "Epoch 186 batch 200 train Loss 5.3641 test Loss 5.3542 with MSE metric 8368.7939\n",
      "Time taken for 1 epoch: 26.72779083251953 secs\n",
      "\n",
      "Epoch 187 batch 0 train Loss 5.2229 test Loss 5.3356 with MSE metric 6293.8042\n",
      "Epoch 187 batch 100 train Loss 5.2430 test Loss 5.3982 with MSE metric 6582.5376\n",
      "Epoch 187 batch 200 train Loss 5.2922 test Loss 5.3588 with MSE metric 7266.0918\n",
      "Time taken for 1 epoch: 26.739707708358765 secs\n",
      "\n",
      "Epoch 188 batch 0 train Loss 5.1538 test Loss 5.4031 with MSE metric 5295.1284\n",
      "Epoch 188 batch 100 train Loss 5.2716 test Loss 5.3788 with MSE metric 6942.1016\n",
      "Epoch 188 batch 200 train Loss 5.2732 test Loss 5.3586 with MSE metric 6978.1421\n",
      "Time taken for 1 epoch: 26.727185249328613 secs\n",
      "\n",
      "Epoch 189 batch 0 train Loss 5.3539 test Loss 5.4237 with MSE metric 8222.0293\n",
      "Epoch 189 batch 100 train Loss 5.3087 test Loss 5.3575 with MSE metric 7510.8921\n",
      "Epoch 189 batch 200 train Loss 5.3532 test Loss 5.4386 with MSE metric 8157.4575\n",
      "Time taken for 1 epoch: 26.591449737548828 secs\n",
      "\n",
      "Epoch 190 batch 0 train Loss 5.2603 test Loss 5.3496 with MSE metric 6789.8232\n",
      "Epoch 190 batch 100 train Loss 5.3721 test Loss 5.4736 with MSE metric 8492.8574\n",
      "Epoch 190 batch 200 train Loss 5.3471 test Loss 5.3710 with MSE metric 8110.6406\n",
      "Time taken for 1 epoch: 26.588993072509766 secs\n",
      "\n",
      "Epoch 191 batch 0 train Loss 5.2544 test Loss 5.3577 with MSE metric 6726.2422\n",
      "Epoch 191 batch 100 train Loss 5.4094 test Loss 5.3441 with MSE metric 9064.7676\n",
      "Epoch 191 batch 200 train Loss 5.3669 test Loss 5.3067 with MSE metric 8336.1328\n",
      "Time taken for 1 epoch: 26.669570207595825 secs\n",
      "\n",
      "Epoch 192 batch 0 train Loss 5.2965 test Loss 5.3562 with MSE metric 7330.2231\n",
      "Epoch 192 batch 100 train Loss 5.3261 test Loss 5.3387 with MSE metric 7774.6958\n",
      "Epoch 192 batch 200 train Loss 5.3425 test Loss 5.3788 with MSE metric 8028.1211\n",
      "Time taken for 1 epoch: 26.619920253753662 secs\n",
      "\n",
      "Epoch 193 batch 0 train Loss 5.2632 test Loss 5.3972 with MSE metric 6846.0225\n",
      "Epoch 193 batch 100 train Loss 5.2968 test Loss 5.5144 with MSE metric 7315.5391\n",
      "Epoch 193 batch 200 train Loss 5.3071 test Loss 5.3160 with MSE metric 7486.9731\n",
      "Time taken for 1 epoch: 26.531661987304688 secs\n",
      "\n",
      "Epoch 194 batch 0 train Loss 5.1979 test Loss 5.3919 with MSE metric 5800.6489\n",
      "Epoch 194 batch 100 train Loss 5.2649 test Loss 5.3239 with MSE metric 6867.6255\n",
      "Epoch 194 batch 200 train Loss 5.3365 test Loss 5.3598 with MSE metric 7931.2056\n",
      "Time taken for 1 epoch: 26.359448194503784 secs\n",
      "\n",
      "Epoch 195 batch 0 train Loss 5.3747 test Loss 5.3313 with MSE metric 8469.4814\n",
      "Epoch 195 batch 100 train Loss 5.3418 test Loss 5.2765 with MSE metric 8011.9116\n",
      "Epoch 195 batch 200 train Loss 5.2665 test Loss 5.3918 with MSE metric 6843.3066\n",
      "Time taken for 1 epoch: 26.005446195602417 secs\n",
      "\n",
      "Epoch 196 batch 0 train Loss 5.3938 test Loss 5.3754 with MSE metric 8755.3418\n",
      "Epoch 196 batch 100 train Loss 5.3014 test Loss 5.3676 with MSE metric 7351.8721\n",
      "Epoch 196 batch 200 train Loss 5.3487 test Loss 5.4316 with MSE metric 8068.4385\n",
      "Time taken for 1 epoch: 25.46464514732361 secs\n",
      "\n",
      "Epoch 197 batch 0 train Loss 5.3328 test Loss 5.2912 with MSE metric 7859.8975\n",
      "Epoch 197 batch 100 train Loss 5.2557 test Loss 5.3975 with MSE metric 6718.5122\n",
      "Epoch 197 batch 200 train Loss 5.3181 test Loss 5.3979 with MSE metric 7652.7607\n",
      "Time taken for 1 epoch: 26.580104112625122 secs\n",
      "\n",
      "Epoch 198 batch 0 train Loss 5.2815 test Loss 5.4106 with MSE metric 7113.9902\n",
      "Epoch 198 batch 100 train Loss 5.2989 test Loss 5.3272 with MSE metric 7365.2432\n",
      "Epoch 198 batch 200 train Loss 5.2610 test Loss 5.2665 with MSE metric 6804.9614\n",
      "Time taken for 1 epoch: 26.535658836364746 secs\n",
      "\n",
      "Epoch 199 batch 0 train Loss 5.2242 test Loss 5.4292 with MSE metric 6305.0840\n",
      "Epoch 199 batch 100 train Loss 5.3250 test Loss 5.3439 with MSE metric 7760.4062\n",
      "Epoch 199 batch 200 train Loss 5.2936 test Loss 5.3978 with MSE metric 7280.3184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 26.607697010040283 secs\n",
      "\n",
      "Epoch 200 batch 0 train Loss 5.3449 test Loss 5.4078 with MSE metric 7988.3018\n",
      "Epoch 200 batch 100 train Loss 5.2565 test Loss 5.4328 with MSE metric 6720.6626\n",
      "Epoch 200 batch 200 train Loss 5.2096 test Loss 5.3525 with MSE metric 6099.7471\n",
      "Time taken for 1 epoch: 26.57576608657837 secs\n",
      "\n",
      "Epoch 201 batch 0 train Loss 5.3040 test Loss 5.2785 with MSE metric 7436.6777\n",
      "Epoch 201 batch 100 train Loss 5.3422 test Loss 5.3292 with MSE metric 8000.6528\n",
      "Epoch 201 batch 200 train Loss 5.2788 test Loss 5.3307 with MSE metric 7056.0127\n",
      "Time taken for 1 epoch: 26.602109909057617 secs\n",
      "\n",
      "Epoch 202 batch 0 train Loss 5.2766 test Loss 5.3853 with MSE metric 6988.5352\n",
      "Epoch 202 batch 100 train Loss 5.3545 test Loss 5.3651 with MSE metric 8217.6328\n",
      "Epoch 202 batch 200 train Loss 5.3293 test Loss 5.3244 with MSE metric 7790.2935\n",
      "Time taken for 1 epoch: 26.600423097610474 secs\n",
      "\n",
      "Epoch 203 batch 0 train Loss 5.3108 test Loss 5.4500 with MSE metric 7542.4604\n",
      "Epoch 203 batch 100 train Loss 5.4170 test Loss 5.3458 with MSE metric 9095.8887\n",
      "Epoch 203 batch 200 train Loss 5.3746 test Loss 5.3016 with MSE metric 8467.0430\n",
      "Time taken for 1 epoch: 26.638332843780518 secs\n",
      "\n",
      "Epoch 204 batch 0 train Loss 5.3225 test Loss 5.3654 with MSE metric 7714.3193\n",
      "Epoch 204 batch 100 train Loss 5.2852 test Loss 5.2100 with MSE metric 7164.5220\n",
      "Epoch 204 batch 200 train Loss 5.2752 test Loss 5.4878 with MSE metric 7016.0513\n",
      "Time taken for 1 epoch: 26.578327894210815 secs\n",
      "\n",
      "Epoch 205 batch 0 train Loss 5.2029 test Loss 5.4207 with MSE metric 5989.1460\n",
      "Epoch 205 batch 100 train Loss 5.3490 test Loss 5.2828 with MSE metric 8107.1396\n",
      "Epoch 205 batch 200 train Loss 5.3348 test Loss 5.3569 with MSE metric 7819.0312\n",
      "Time taken for 1 epoch: 26.2621910572052 secs\n",
      "\n",
      "Epoch 206 batch 0 train Loss 5.3073 test Loss 5.3949 with MSE metric 7487.7666\n",
      "Epoch 206 batch 100 train Loss 5.2642 test Loss 5.2887 with MSE metric 6870.0498\n",
      "Epoch 206 batch 200 train Loss 5.2874 test Loss 5.4275 with MSE metric 7187.5859\n",
      "Time taken for 1 epoch: 26.94868803024292 secs\n",
      "\n",
      "Epoch 207 batch 0 train Loss 5.2444 test Loss 5.3355 with MSE metric 6525.3560\n",
      "Epoch 207 batch 100 train Loss 5.4153 test Loss 5.3544 with MSE metric 9092.7500\n",
      "Epoch 207 batch 200 train Loss 5.3264 test Loss 5.3953 with MSE metric 7766.8452\n",
      "Time taken for 1 epoch: 26.137588024139404 secs\n",
      "\n",
      "Epoch 208 batch 0 train Loss 5.2424 test Loss 5.4329 with MSE metric 6524.7769\n",
      "Epoch 208 batch 100 train Loss 5.2633 test Loss 5.3202 with MSE metric 6855.7227\n",
      "Epoch 208 batch 200 train Loss 5.4366 test Loss 5.4009 with MSE metric 9437.7549\n",
      "Time taken for 1 epoch: 26.191532850265503 secs\n",
      "\n",
      "Epoch 209 batch 0 train Loss 5.2743 test Loss 5.4455 with MSE metric 6998.8618\n",
      "Epoch 209 batch 100 train Loss 5.3151 test Loss 5.4073 with MSE metric 7596.3442\n",
      "Epoch 209 batch 200 train Loss 5.3260 test Loss 5.4009 with MSE metric 7725.9253\n",
      "Time taken for 1 epoch: 26.523679971694946 secs\n",
      "\n",
      "Epoch 210 batch 0 train Loss 5.3348 test Loss 5.4428 with MSE metric 7792.4727\n",
      "Epoch 210 batch 100 train Loss 5.3126 test Loss 5.3703 with MSE metric 7484.8174\n",
      "Epoch 210 batch 200 train Loss 5.1764 test Loss 5.4187 with MSE metric 5631.3223\n",
      "Time taken for 1 epoch: 26.63140606880188 secs\n",
      "\n",
      "Epoch 211 batch 0 train Loss 5.2266 test Loss 5.4442 with MSE metric 6239.3105\n",
      "Epoch 211 batch 100 train Loss 5.2296 test Loss 5.2352 with MSE metric 6380.3594\n",
      "Epoch 211 batch 200 train Loss 5.3412 test Loss 5.4193 with MSE metric 7911.5161\n",
      "Time taken for 1 epoch: 26.4839928150177 secs\n",
      "\n",
      "Epoch 212 batch 0 train Loss 5.3227 test Loss 5.2958 with MSE metric 7718.2549\n",
      "Epoch 212 batch 100 train Loss 5.2727 test Loss 5.4494 with MSE metric 6960.7437\n",
      "Epoch 212 batch 200 train Loss 5.4159 test Loss 5.3981 with MSE metric 9138.8252\n",
      "Time taken for 1 epoch: 26.75617003440857 secs\n",
      "\n",
      "Epoch 213 batch 0 train Loss 5.3129 test Loss 5.4046 with MSE metric 7565.2295\n",
      "Epoch 213 batch 100 train Loss 5.2768 test Loss 5.4034 with MSE metric 7034.9375\n",
      "Epoch 213 batch 200 train Loss 5.2136 test Loss 5.3470 with MSE metric 5938.7827\n",
      "Time taken for 1 epoch: 26.575620889663696 secs\n",
      "\n",
      "Epoch 214 batch 0 train Loss 5.2523 test Loss 5.3584 with MSE metric 6643.8691\n",
      "Epoch 214 batch 100 train Loss 5.2221 test Loss 5.3735 with MSE metric 6165.6338\n",
      "Epoch 214 batch 200 train Loss 5.2754 test Loss 5.2566 with MSE metric 7028.0815\n",
      "Time taken for 1 epoch: 26.56206512451172 secs\n",
      "\n",
      "Epoch 215 batch 0 train Loss 5.3574 test Loss 5.3151 with MSE metric 8210.1455\n",
      "Epoch 215 batch 100 train Loss 5.3178 test Loss 5.3347 with MSE metric 7639.7773\n",
      "Epoch 215 batch 200 train Loss 5.2613 test Loss 5.2973 with MSE metric 6770.6465\n",
      "Time taken for 1 epoch: 26.54067611694336 secs\n",
      "\n",
      "Epoch 216 batch 0 train Loss 5.3294 test Loss 5.3917 with MSE metric 7812.9971\n",
      "Epoch 216 batch 100 train Loss 5.3390 test Loss 5.2957 with MSE metric 7979.6641\n",
      "Epoch 216 batch 200 train Loss 5.3563 test Loss 5.2665 with MSE metric 8258.8691\n",
      "Time taken for 1 epoch: 26.40767502784729 secs\n",
      "\n",
      "Epoch 217 batch 0 train Loss 5.2789 test Loss 5.3041 with MSE metric 7068.6597\n",
      "Epoch 217 batch 100 train Loss 5.3209 test Loss 5.3784 with MSE metric 7680.9590\n",
      "Epoch 217 batch 200 train Loss 5.2111 test Loss 5.3376 with MSE metric 6152.4907\n",
      "Time taken for 1 epoch: 26.359705924987793 secs\n",
      "\n",
      "Epoch 218 batch 0 train Loss 5.3223 test Loss 5.3337 with MSE metric 7674.6445\n",
      "Epoch 218 batch 100 train Loss 5.2580 test Loss 5.4338 with MSE metric 6767.6182\n",
      "Epoch 218 batch 200 train Loss 5.3033 test Loss 5.3585 with MSE metric 7407.4834\n",
      "Time taken for 1 epoch: 26.746320247650146 secs\n",
      "\n",
      "Epoch 219 batch 0 train Loss 5.3358 test Loss 5.2984 with MSE metric 7911.9268\n",
      "Epoch 219 batch 100 train Loss 5.3947 test Loss 5.3647 with MSE metric 8613.0537\n",
      "Epoch 219 batch 200 train Loss 5.2959 test Loss 5.3705 with MSE metric 7320.2085\n",
      "Time taken for 1 epoch: 26.591851949691772 secs\n",
      "\n",
      "Epoch 220 batch 0 train Loss 5.3541 test Loss 5.3369 with MSE metric 8199.6318\n",
      "Epoch 220 batch 100 train Loss 5.2838 test Loss 5.2895 with MSE metric 7142.5156\n",
      "Epoch 220 batch 200 train Loss 5.3065 test Loss 5.3533 with MSE metric 7479.4131\n",
      "Time taken for 1 epoch: 26.639144897460938 secs\n",
      "\n",
      "Epoch 221 batch 0 train Loss 5.3036 test Loss 5.3779 with MSE metric 7424.2109\n",
      "Epoch 221 batch 100 train Loss 5.2696 test Loss 5.3792 with MSE metric 6930.4219\n",
      "Epoch 221 batch 200 train Loss 5.3259 test Loss 5.4204 with MSE metric 7737.8950\n",
      "Time taken for 1 epoch: 26.821243047714233 secs\n",
      "\n",
      "Epoch 222 batch 0 train Loss 5.2167 test Loss 5.3305 with MSE metric 6164.3462\n",
      "Epoch 222 batch 100 train Loss 5.3127 test Loss 5.3283 with MSE metric 7571.1895\n",
      "Epoch 222 batch 200 train Loss 5.3281 test Loss 5.2512 with MSE metric 7806.8115\n",
      "Time taken for 1 epoch: 26.60106897354126 secs\n",
      "\n",
      "Epoch 223 batch 0 train Loss 5.3439 test Loss 5.3112 with MSE metric 8028.2275\n",
      "Epoch 223 batch 100 train Loss 5.3359 test Loss 5.3346 with MSE metric 7834.0151\n",
      "Epoch 223 batch 200 train Loss 5.2484 test Loss 5.3575 with MSE metric 6582.5151\n",
      "Time taken for 1 epoch: 26.55996084213257 secs\n",
      "\n",
      "Epoch 224 batch 0 train Loss 5.3158 test Loss 5.3127 with MSE metric 7607.3877\n",
      "Epoch 224 batch 100 train Loss 5.3341 test Loss 5.3635 with MSE metric 7871.4570\n",
      "Epoch 224 batch 200 train Loss 5.4247 test Loss 5.4048 with MSE metric 9152.8828\n",
      "Time taken for 1 epoch: 26.935604095458984 secs\n",
      "\n",
      "Epoch 225 batch 0 train Loss 5.3547 test Loss 5.3125 with MSE metric 8168.8984\n",
      "Epoch 225 batch 100 train Loss 5.2997 test Loss 5.4613 with MSE metric 7366.9307\n",
      "Epoch 225 batch 200 train Loss 5.2641 test Loss 5.3175 with MSE metric 6864.7598\n",
      "Time taken for 1 epoch: 26.470691680908203 secs\n",
      "\n",
      "Epoch 226 batch 0 train Loss 5.3303 test Loss 5.3369 with MSE metric 7812.5269\n",
      "Epoch 226 batch 100 train Loss 5.2845 test Loss 5.4324 with MSE metric 7134.8389\n",
      "Epoch 226 batch 200 train Loss 5.3307 test Loss 5.2885 with MSE metric 7773.4722\n",
      "Time taken for 1 epoch: 26.54866671562195 secs\n",
      "\n",
      "Epoch 227 batch 0 train Loss 5.3262 test Loss 5.3492 with MSE metric 7734.2983\n",
      "Epoch 227 batch 100 train Loss 5.3838 test Loss 5.2936 with MSE metric 8650.1484\n",
      "Epoch 227 batch 200 train Loss 5.3703 test Loss 5.4625 with MSE metric 8366.4922\n",
      "Time taken for 1 epoch: 26.60905909538269 secs\n",
      "\n",
      "Epoch 228 batch 0 train Loss 5.1570 test Loss 5.4374 with MSE metric 5262.7935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228 batch 100 train Loss 5.2827 test Loss 5.3778 with MSE metric 7129.8027\n",
      "Epoch 228 batch 200 train Loss 5.3432 test Loss 5.2428 with MSE metric 8041.5356\n",
      "Time taken for 1 epoch: 26.323616981506348 secs\n",
      "\n",
      "Epoch 229 batch 0 train Loss 5.2660 test Loss 5.3340 with MSE metric 6868.2856\n",
      "Epoch 229 batch 100 train Loss 5.3116 test Loss 5.3820 with MSE metric 7555.7383\n",
      "Epoch 229 batch 200 train Loss 5.3031 test Loss 5.3482 with MSE metric 7423.9746\n",
      "Time taken for 1 epoch: 26.67631196975708 secs\n",
      "\n",
      "Epoch 230 batch 0 train Loss 5.2470 test Loss 5.2811 with MSE metric 6577.9238\n",
      "Epoch 230 batch 100 train Loss 5.3574 test Loss 5.2389 with MSE metric 8255.4170\n",
      "Epoch 230 batch 200 train Loss 5.3420 test Loss 5.4372 with MSE metric 7999.0879\n",
      "Time taken for 1 epoch: 26.78191590309143 secs\n",
      "\n",
      "Epoch 231 batch 0 train Loss 5.3114 test Loss 5.2593 with MSE metric 7502.8765\n",
      "Epoch 231 batch 100 train Loss 5.2784 test Loss 5.2600 with MSE metric 7011.4531\n",
      "Epoch 231 batch 200 train Loss 5.2177 test Loss 5.3375 with MSE metric 6143.8799\n",
      "Time taken for 1 epoch: 27.003756046295166 secs\n",
      "\n",
      "Epoch 232 batch 0 train Loss 5.3472 test Loss 5.5207 with MSE metric 8079.5547\n",
      "Epoch 232 batch 100 train Loss 5.2901 test Loss 5.3073 with MSE metric 7237.5039\n",
      "Epoch 232 batch 200 train Loss 5.2771 test Loss 5.3963 with MSE metric 7005.9102\n",
      "Time taken for 1 epoch: 26.88436484336853 secs\n",
      "\n",
      "Epoch 233 batch 0 train Loss 5.3485 test Loss 5.3422 with MSE metric 7979.7520\n",
      "Epoch 233 batch 100 train Loss 5.3000 test Loss 5.2629 with MSE metric 7380.5757\n",
      "Epoch 233 batch 200 train Loss 5.2574 test Loss 5.4138 with MSE metric 6775.5654\n",
      "Time taken for 1 epoch: 26.826356887817383 secs\n",
      "\n",
      "Epoch 234 batch 0 train Loss 5.2063 test Loss 5.3541 with MSE metric 6048.5293\n",
      "Epoch 234 batch 100 train Loss 5.2740 test Loss 5.3941 with MSE metric 6982.4204\n",
      "Epoch 234 batch 200 train Loss 5.2318 test Loss 5.4031 with MSE metric 6417.2744\n",
      "Time taken for 1 epoch: 26.945999145507812 secs\n",
      "\n",
      "Epoch 235 batch 0 train Loss 5.2993 test Loss 5.2508 with MSE metric 7352.2617\n",
      "Epoch 235 batch 100 train Loss 5.3604 test Loss 5.3067 with MSE metric 8289.0273\n",
      "Epoch 235 batch 200 train Loss 5.2758 test Loss 5.4415 with MSE metric 7030.7056\n",
      "Time taken for 1 epoch: 26.727477073669434 secs\n",
      "\n",
      "Epoch 236 batch 0 train Loss 5.3024 test Loss 5.3633 with MSE metric 7411.8389\n",
      "Epoch 236 batch 100 train Loss 5.2356 test Loss 5.3806 with MSE metric 6396.7227\n",
      "Epoch 236 batch 200 train Loss 5.3393 test Loss 5.3196 with MSE metric 7984.0693\n",
      "Time taken for 1 epoch: 26.826842069625854 secs\n",
      "\n",
      "Epoch 237 batch 0 train Loss 5.3112 test Loss 5.3327 with MSE metric 7547.9233\n",
      "Epoch 237 batch 100 train Loss 5.2472 test Loss 5.3595 with MSE metric 6589.2109\n",
      "Epoch 237 batch 200 train Loss 5.3107 test Loss 5.4133 with MSE metric 7540.3232\n",
      "Time taken for 1 epoch: 26.948747158050537 secs\n",
      "\n",
      "Epoch 238 batch 0 train Loss 5.3920 test Loss 5.4191 with MSE metric 8829.0889\n",
      "Epoch 238 batch 100 train Loss 5.3247 test Loss 5.3834 with MSE metric 7731.1992\n",
      "Epoch 238 batch 200 train Loss 5.3051 test Loss 5.3501 with MSE metric 7457.4438\n",
      "Time taken for 1 epoch: 26.986474990844727 secs\n",
      "\n",
      "Epoch 239 batch 0 train Loss 5.2671 test Loss 5.3572 with MSE metric 6860.8008\n",
      "Epoch 239 batch 100 train Loss 5.2519 test Loss 5.4062 with MSE metric 6703.5830\n",
      "Epoch 239 batch 200 train Loss 5.2956 test Loss 5.3111 with MSE metric 7289.9849\n",
      "Time taken for 1 epoch: 26.863260746002197 secs\n",
      "\n",
      "Epoch 240 batch 0 train Loss 5.3515 test Loss 5.3080 with MSE metric 8137.8979\n",
      "Epoch 240 batch 100 train Loss 5.2358 test Loss 5.4034 with MSE metric 6424.6279\n",
      "Epoch 240 batch 200 train Loss 5.3531 test Loss 5.3568 with MSE metric 8150.7559\n",
      "Time taken for 1 epoch: 26.770139932632446 secs\n",
      "\n",
      "Epoch 241 batch 0 train Loss 5.2700 test Loss 5.2755 with MSE metric 6920.5547\n",
      "Epoch 241 batch 100 train Loss 5.2767 test Loss 5.3833 with MSE metric 6984.7080\n",
      "Epoch 241 batch 200 train Loss 5.2526 test Loss 5.3290 with MSE metric 6639.8252\n",
      "Time taken for 1 epoch: 26.779411792755127 secs\n",
      "\n",
      "Epoch 242 batch 0 train Loss 5.2262 test Loss 5.4580 with MSE metric 6320.0098\n",
      "Epoch 242 batch 100 train Loss 5.3334 test Loss 5.3816 with MSE metric 7881.8291\n",
      "Epoch 242 batch 200 train Loss 5.3369 test Loss 5.3899 with MSE metric 7946.8667\n",
      "Time taken for 1 epoch: 26.787538051605225 secs\n",
      "\n",
      "Epoch 243 batch 0 train Loss 5.3043 test Loss 5.3229 with MSE metric 7417.2861\n",
      "Epoch 243 batch 100 train Loss 5.2499 test Loss 5.3463 with MSE metric 6539.2417\n",
      "Epoch 243 batch 200 train Loss 5.3283 test Loss 5.3922 with MSE metric 7811.0469\n",
      "Time taken for 1 epoch: 26.836402893066406 secs\n",
      "\n",
      "Epoch 244 batch 0 train Loss 5.2892 test Loss 5.4247 with MSE metric 7213.4482\n",
      "Epoch 244 batch 100 train Loss 5.3020 test Loss 5.3908 with MSE metric 7409.9448\n",
      "Epoch 244 batch 200 train Loss 5.3296 test Loss 5.3330 with MSE metric 7810.6748\n",
      "Time taken for 1 epoch: 26.917242765426636 secs\n",
      "\n",
      "Epoch 245 batch 0 train Loss 5.3289 test Loss 5.3272 with MSE metric 7802.3896\n",
      "Epoch 245 batch 100 train Loss 5.2426 test Loss 5.4515 with MSE metric 6581.0884\n",
      "Epoch 245 batch 200 train Loss 5.3144 test Loss 5.3532 with MSE metric 7546.1318\n",
      "Time taken for 1 epoch: 26.601643085479736 secs\n",
      "\n",
      "Epoch 246 batch 0 train Loss 5.2750 test Loss 5.3176 with MSE metric 7001.3701\n",
      "Epoch 246 batch 100 train Loss 5.3554 test Loss 5.3937 with MSE metric 8237.2676\n",
      "Epoch 246 batch 200 train Loss 5.2787 test Loss 5.3601 with MSE metric 6993.9722\n",
      "Time taken for 1 epoch: 26.90458393096924 secs\n",
      "\n",
      "Epoch 247 batch 0 train Loss 5.3258 test Loss 5.4108 with MSE metric 7772.8877\n",
      "Epoch 247 batch 100 train Loss 5.3178 test Loss 5.3591 with MSE metric 7627.1729\n",
      "Epoch 247 batch 200 train Loss 5.3383 test Loss 5.3972 with MSE metric 7879.4575\n",
      "Time taken for 1 epoch: 26.915607929229736 secs\n",
      "\n",
      "Epoch 248 batch 0 train Loss 5.2688 test Loss 5.4532 with MSE metric 6917.8750\n",
      "Epoch 248 batch 100 train Loss 5.3335 test Loss 5.3430 with MSE metric 7854.8008\n",
      "Epoch 248 batch 200 train Loss 5.2733 test Loss 5.3332 with MSE metric 6992.6621\n",
      "Time taken for 1 epoch: 26.872318983078003 secs\n",
      "\n",
      "Epoch 249 batch 0 train Loss 5.3022 test Loss 5.3473 with MSE metric 7407.7070\n",
      "Epoch 249 batch 100 train Loss 5.3319 test Loss 5.3570 with MSE metric 7867.2148\n",
      "Epoch 249 batch 200 train Loss 5.4006 test Loss 5.3688 with MSE metric 8658.3047\n",
      "Time taken for 1 epoch: 26.882135152816772 secs\n",
      "\n",
      "Epoch 250 batch 0 train Loss 5.3328 test Loss 5.3535 with MSE metric 7876.6113\n",
      "Epoch 250 batch 100 train Loss 5.2432 test Loss 5.3167 with MSE metric 6565.8096\n",
      "Epoch 250 batch 200 train Loss 5.2753 test Loss 5.2879 with MSE metric 7026.2695\n",
      "Time taken for 1 epoch: 26.68032717704773 secs\n",
      "\n",
      "Epoch 251 batch 0 train Loss 5.2820 test Loss 5.4263 with MSE metric 7118.0967\n",
      "Epoch 251 batch 100 train Loss 5.3211 test Loss 5.3886 with MSE metric 7687.0835\n",
      "Epoch 251 batch 200 train Loss 5.2447 test Loss 5.2475 with MSE metric 6568.0425\n",
      "Time taken for 1 epoch: 26.470834016799927 secs\n",
      "\n",
      "Epoch 252 batch 0 train Loss 5.2539 test Loss 5.3788 with MSE metric 6722.3867\n",
      "Epoch 252 batch 100 train Loss 5.2196 test Loss 5.3514 with MSE metric 6254.4023\n",
      "Epoch 252 batch 200 train Loss 5.2848 test Loss 5.2594 with MSE metric 7160.7549\n",
      "Time taken for 1 epoch: 26.556910753250122 secs\n",
      "\n",
      "Epoch 253 batch 0 train Loss 5.3684 test Loss 5.3120 with MSE metric 8385.0781\n",
      "Epoch 253 batch 100 train Loss 5.3547 test Loss 5.2759 with MSE metric 8234.6172\n",
      "Epoch 253 batch 200 train Loss 5.3342 test Loss 5.2396 with MSE metric 7903.9014\n",
      "Time taken for 1 epoch: 26.539013862609863 secs\n",
      "\n",
      "Epoch 254 batch 0 train Loss 5.2997 test Loss 5.4210 with MSE metric 7378.1992\n",
      "Epoch 254 batch 100 train Loss 5.3284 test Loss 5.3639 with MSE metric 7772.9180\n",
      "Epoch 254 batch 200 train Loss 5.2972 test Loss 5.3808 with MSE metric 7327.8203\n",
      "Time taken for 1 epoch: 26.496400117874146 secs\n",
      "\n",
      "Epoch 255 batch 0 train Loss 5.2724 test Loss 5.4360 with MSE metric 6969.5557\n",
      "Epoch 255 batch 100 train Loss 5.3405 test Loss 5.3810 with MSE metric 7942.2441\n",
      "Epoch 255 batch 200 train Loss 5.3011 test Loss 5.4297 with MSE metric 7398.8789\n",
      "Time taken for 1 epoch: 26.543684720993042 secs\n",
      "\n",
      "Epoch 256 batch 0 train Loss 5.3552 test Loss 5.4106 with MSE metric 8195.2900\n",
      "Epoch 256 batch 100 train Loss 5.3187 test Loss 5.3947 with MSE metric 7663.5542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 batch 200 train Loss 5.1455 test Loss 5.4848 with MSE metric 5265.2969\n",
      "Time taken for 1 epoch: 26.530865907669067 secs\n",
      "\n",
      "Epoch 257 batch 0 train Loss 5.4327 test Loss 5.3424 with MSE metric 9252.1309\n",
      "Epoch 257 batch 100 train Loss 5.3090 test Loss 5.2705 with MSE metric 7504.7627\n",
      "Epoch 257 batch 200 train Loss 5.3108 test Loss 5.2630 with MSE metric 7542.9102\n",
      "Time taken for 1 epoch: 26.848063945770264 secs\n",
      "\n",
      "Epoch 258 batch 0 train Loss 5.3651 test Loss 5.3498 with MSE metric 8338.7686\n",
      "Epoch 258 batch 100 train Loss 5.3199 test Loss 5.3622 with MSE metric 7657.1255\n",
      "Epoch 258 batch 200 train Loss 5.2952 test Loss 5.3230 with MSE metric 7310.7720\n",
      "Time taken for 1 epoch: 26.51411199569702 secs\n",
      "\n",
      "Epoch 259 batch 0 train Loss 5.2157 test Loss 5.3151 with MSE metric 6088.8916\n",
      "Epoch 259 batch 100 train Loss 5.1974 test Loss 5.3478 with MSE metric 5967.9463\n",
      "Epoch 259 batch 200 train Loss 5.3181 test Loss 5.2993 with MSE metric 7653.1401\n",
      "Time taken for 1 epoch: 26.479319095611572 secs\n",
      "\n",
      "Epoch 260 batch 0 train Loss 5.2974 test Loss 5.2863 with MSE metric 7343.5752\n",
      "Epoch 260 batch 100 train Loss 5.3212 test Loss 5.3634 with MSE metric 7699.4653\n",
      "Epoch 260 batch 200 train Loss 5.2975 test Loss 5.2813 with MSE metric 7336.2183\n",
      "Time taken for 1 epoch: 26.69777011871338 secs\n",
      "\n",
      "Epoch 261 batch 0 train Loss 5.3770 test Loss 5.4144 with MSE metric 8601.3379\n",
      "Epoch 261 batch 100 train Loss 5.2766 test Loss 5.3403 with MSE metric 6971.1377\n",
      "Epoch 261 batch 200 train Loss 5.4103 test Loss 5.4175 with MSE metric 9068.4365\n",
      "Time taken for 1 epoch: 26.48977303504944 secs\n",
      "\n",
      "Epoch 262 batch 0 train Loss 5.2445 test Loss 5.3869 with MSE metric 6470.8809\n",
      "Epoch 262 batch 100 train Loss 5.3171 test Loss 5.2747 with MSE metric 7624.3311\n",
      "Epoch 262 batch 200 train Loss 5.3374 test Loss 5.3032 with MSE metric 7928.1836\n",
      "Time taken for 1 epoch: 26.5506591796875 secs\n",
      "\n",
      "Epoch 263 batch 0 train Loss 5.3548 test Loss 5.4534 with MSE metric 8217.1328\n",
      "Epoch 263 batch 100 train Loss 5.3333 test Loss 5.3186 with MSE metric 7863.2842\n",
      "Epoch 263 batch 200 train Loss 5.2538 test Loss 5.3308 with MSE metric 6700.6338\n",
      "Time taken for 1 epoch: 26.92454695701599 secs\n",
      "\n",
      "Epoch 264 batch 0 train Loss 5.3642 test Loss 5.3534 with MSE metric 8352.6738\n",
      "Epoch 264 batch 100 train Loss 5.2984 test Loss 5.4080 with MSE metric 7328.6992\n",
      "Epoch 264 batch 200 train Loss 5.2269 test Loss 5.3731 with MSE metric 6178.7744\n",
      "Time taken for 1 epoch: 26.906296253204346 secs\n",
      "\n",
      "Epoch 265 batch 0 train Loss 5.3065 test Loss 5.3095 with MSE metric 7412.8477\n",
      "Epoch 265 batch 100 train Loss 5.3330 test Loss 5.5211 with MSE metric 7886.3091\n",
      "Epoch 265 batch 200 train Loss 5.3774 test Loss 5.4209 with MSE metric 8493.3359\n",
      "Time taken for 1 epoch: 26.874265909194946 secs\n",
      "\n",
      "Epoch 266 batch 0 train Loss 5.1917 test Loss 5.4759 with MSE metric 5854.9585\n",
      "Epoch 266 batch 100 train Loss 5.3902 test Loss 5.2686 with MSE metric 8680.3555\n",
      "Epoch 266 batch 200 train Loss 5.1909 test Loss 5.4587 with MSE metric 5849.9556\n",
      "Time taken for 1 epoch: 26.807159900665283 secs\n",
      "\n",
      "Epoch 267 batch 0 train Loss 5.2907 test Loss 5.3732 with MSE metric 7244.2061\n",
      "Epoch 267 batch 100 train Loss 5.3557 test Loss 5.3310 with MSE metric 8185.5557\n",
      "Epoch 267 batch 200 train Loss 5.1700 test Loss 5.3226 with MSE metric 5329.5879\n",
      "Time taken for 1 epoch: 26.45277500152588 secs\n",
      "\n",
      "Epoch 268 batch 0 train Loss 5.3142 test Loss 5.3259 with MSE metric 7573.3550\n",
      "Epoch 268 batch 100 train Loss 5.4090 test Loss 5.4530 with MSE metric 9122.0840\n",
      "Epoch 268 batch 200 train Loss 5.2600 test Loss 5.2474 with MSE metric 6723.4751\n",
      "Time taken for 1 epoch: 26.418564081192017 secs\n",
      "\n",
      "Epoch 269 batch 0 train Loss 5.3425 test Loss 5.2996 with MSE metric 8013.8467\n",
      "Epoch 269 batch 100 train Loss 5.2721 test Loss 5.2499 with MSE metric 6975.4434\n",
      "Epoch 269 batch 200 train Loss 5.2968 test Loss 5.3766 with MSE metric 7335.5542\n",
      "Time taken for 1 epoch: 26.11495614051819 secs\n",
      "\n",
      "Epoch 270 batch 0 train Loss 5.2668 test Loss 5.3930 with MSE metric 6873.2676\n",
      "Epoch 270 batch 100 train Loss 5.3592 test Loss 5.2599 with MSE metric 8282.1855\n",
      "Epoch 270 batch 200 train Loss 5.3250 test Loss 5.3897 with MSE metric 7755.7314\n",
      "Time taken for 1 epoch: 26.3375883102417 secs\n",
      "\n",
      "Epoch 271 batch 0 train Loss 5.3175 test Loss 5.4098 with MSE metric 7645.0420\n",
      "Epoch 271 batch 100 train Loss 5.2893 test Loss 5.4097 with MSE metric 7206.8115\n",
      "Epoch 271 batch 200 train Loss 5.2346 test Loss 5.2623 with MSE metric 6451.1758\n",
      "Time taken for 1 epoch: 26.38482403755188 secs\n",
      "\n",
      "Epoch 272 batch 0 train Loss 5.3510 test Loss 5.3359 with MSE metric 8139.2441\n",
      "Epoch 272 batch 100 train Loss 5.3717 test Loss 5.3160 with MSE metric 8490.7969\n",
      "Epoch 272 batch 200 train Loss 5.1705 test Loss 5.3625 with MSE metric 5585.5645\n",
      "Time taken for 1 epoch: 26.476154088974 secs\n",
      "\n",
      "Epoch 273 batch 0 train Loss 5.3277 test Loss 5.3157 with MSE metric 7796.1396\n",
      "Epoch 273 batch 100 train Loss 5.3399 test Loss 5.4111 with MSE metric 7983.2915\n",
      "Epoch 273 batch 200 train Loss 5.3254 test Loss 5.3381 with MSE metric 7749.1680\n",
      "Time taken for 1 epoch: 26.380542993545532 secs\n",
      "\n",
      "Epoch 274 batch 0 train Loss 5.3266 test Loss 5.4006 with MSE metric 7719.8008\n",
      "Epoch 274 batch 100 train Loss 5.3375 test Loss 5.3122 with MSE metric 7938.2847\n",
      "Epoch 274 batch 200 train Loss 5.2983 test Loss 5.5616 with MSE metric 7348.1943\n",
      "Time taken for 1 epoch: 23.839121341705322 secs\n",
      "\n",
      "Epoch 275 batch 0 train Loss 5.3332 test Loss 5.4218 with MSE metric 7840.8706\n",
      "Epoch 275 batch 100 train Loss 5.3181 test Loss 5.3877 with MSE metric 7653.7358\n",
      "Epoch 275 batch 200 train Loss 5.2622 test Loss 5.3993 with MSE metric 6839.5117\n",
      "Time taken for 1 epoch: 23.058720111846924 secs\n",
      "\n",
      "Epoch 276 batch 0 train Loss 5.3211 test Loss 5.3564 with MSE metric 7662.8301\n",
      "Epoch 276 batch 100 train Loss 5.2501 test Loss 5.4129 with MSE metric 6659.8301\n",
      "Epoch 276 batch 200 train Loss 5.3921 test Loss 5.3485 with MSE metric 8812.9521\n",
      "Time taken for 1 epoch: 23.005402088165283 secs\n",
      "\n",
      "Epoch 277 batch 0 train Loss 5.2927 test Loss 5.3610 with MSE metric 7274.0723\n",
      "Epoch 277 batch 100 train Loss 5.3820 test Loss 5.3073 with MSE metric 8479.2979\n",
      "Epoch 277 batch 200 train Loss 5.3613 test Loss 5.3384 with MSE metric 8289.9297\n",
      "Time taken for 1 epoch: 22.991432905197144 secs\n",
      "\n",
      "Epoch 278 batch 0 train Loss 5.3615 test Loss 5.4327 with MSE metric 8312.6465\n",
      "Epoch 278 batch 100 train Loss 5.3514 test Loss 5.3525 with MSE metric 8121.6348\n",
      "Epoch 278 batch 200 train Loss 5.2990 test Loss 5.3869 with MSE metric 7361.8643\n",
      "Time taken for 1 epoch: 23.019304037094116 secs\n",
      "\n",
      "Epoch 279 batch 0 train Loss 5.2943 test Loss 5.3722 with MSE metric 7297.3926\n",
      "Epoch 279 batch 100 train Loss 5.3370 test Loss 5.3731 with MSE metric 7922.9741\n",
      "Epoch 279 batch 200 train Loss 5.1723 test Loss 5.4500 with MSE metric 5382.6016\n",
      "Time taken for 1 epoch: 23.29512882232666 secs\n",
      "\n",
      "Epoch 280 batch 0 train Loss 5.3604 test Loss 5.3337 with MSE metric 8323.2422\n",
      "Epoch 280 batch 100 train Loss 5.3595 test Loss 5.3356 with MSE metric 8209.8008\n",
      "Epoch 280 batch 200 train Loss 5.2538 test Loss 5.2748 with MSE metric 6685.7578\n",
      "Time taken for 1 epoch: 23.05222725868225 secs\n",
      "\n",
      "Epoch 281 batch 0 train Loss 5.2802 test Loss 5.4165 with MSE metric 7091.2632\n",
      "Epoch 281 batch 100 train Loss 5.3006 test Loss 5.2746 with MSE metric 7354.4512\n",
      "Epoch 281 batch 200 train Loss 5.3276 test Loss 5.2747 with MSE metric 7798.2012\n",
      "Time taken for 1 epoch: 23.091673851013184 secs\n",
      "\n",
      "Epoch 282 batch 0 train Loss 5.3114 test Loss 5.3784 with MSE metric 7527.5718\n",
      "Epoch 282 batch 100 train Loss 5.3423 test Loss 5.4209 with MSE metric 7893.4487\n",
      "Epoch 282 batch 200 train Loss 5.3674 test Loss 5.4212 with MSE metric 8394.1328\n",
      "Time taken for 1 epoch: 23.122922897338867 secs\n",
      "\n",
      "Epoch 283 batch 0 train Loss 5.3048 test Loss 5.3487 with MSE metric 7448.0620\n",
      "Epoch 283 batch 100 train Loss 5.2204 test Loss 5.2340 with MSE metric 6192.4160\n",
      "Epoch 283 batch 200 train Loss 5.1939 test Loss 5.2633 with MSE metric 5877.9712\n",
      "Time taken for 1 epoch: 23.050374269485474 secs\n",
      "\n",
      "Epoch 284 batch 0 train Loss 5.4491 test Loss 5.4070 with MSE metric 9559.2695\n",
      "Epoch 284 batch 100 train Loss 5.3137 test Loss 5.4592 with MSE metric 7559.0254\n",
      "Epoch 284 batch 200 train Loss 5.3288 test Loss 5.3951 with MSE metric 7816.6621\n",
      "Time taken for 1 epoch: 23.03704595565796 secs\n",
      "\n",
      "Epoch 285 batch 0 train Loss 5.3113 test Loss 5.4082 with MSE metric 7524.3467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285 batch 100 train Loss 5.3868 test Loss 5.3579 with MSE metric 8731.7686\n",
      "Epoch 285 batch 200 train Loss 5.4217 test Loss 5.2743 with MSE metric 9029.5938\n",
      "Time taken for 1 epoch: 23.031870126724243 secs\n",
      "\n",
      "Epoch 286 batch 0 train Loss 5.2674 test Loss 5.4392 with MSE metric 6855.3926\n",
      "Epoch 286 batch 100 train Loss 5.3344 test Loss 5.2938 with MSE metric 7867.7266\n",
      "Epoch 286 batch 200 train Loss 5.2657 test Loss 5.3238 with MSE metric 6818.1133\n",
      "Time taken for 1 epoch: 23.02661967277527 secs\n",
      "\n",
      "Epoch 287 batch 0 train Loss 5.3316 test Loss 5.3596 with MSE metric 7850.6460\n",
      "Epoch 287 batch 100 train Loss 5.3401 test Loss 5.3464 with MSE metric 7998.5684\n",
      "Epoch 287 batch 200 train Loss 5.2437 test Loss 5.2596 with MSE metric 6566.4326\n",
      "Time taken for 1 epoch: 23.06029987335205 secs\n",
      "\n",
      "Epoch 288 batch 0 train Loss 5.3233 test Loss 5.4849 with MSE metric 7653.8105\n",
      "Epoch 288 batch 100 train Loss 5.3475 test Loss 5.2819 with MSE metric 8117.6426\n",
      "Epoch 288 batch 200 train Loss 5.2052 test Loss 5.3391 with MSE metric 6018.1655\n",
      "Time taken for 1 epoch: 22.96719002723694 secs\n",
      "\n",
      "Epoch 289 batch 0 train Loss 5.3309 test Loss 5.2896 with MSE metric 7845.8511\n",
      "Epoch 289 batch 100 train Loss 5.2932 test Loss 5.4207 with MSE metric 7230.8862\n",
      "Epoch 289 batch 200 train Loss 5.2724 test Loss 5.3881 with MSE metric 6981.5986\n",
      "Time taken for 1 epoch: 23.041684865951538 secs\n",
      "\n",
      "Epoch 290 batch 0 train Loss 5.3315 test Loss 5.4888 with MSE metric 7821.2593\n",
      "Epoch 290 batch 100 train Loss 5.2625 test Loss 5.3158 with MSE metric 6823.1333\n",
      "Epoch 290 batch 200 train Loss 5.3884 test Loss 5.4247 with MSE metric 8607.2070\n",
      "Time taken for 1 epoch: 22.996905088424683 secs\n",
      "\n",
      "Epoch 291 batch 0 train Loss 5.2865 test Loss 5.4483 with MSE metric 7170.0127\n",
      "Epoch 291 batch 100 train Loss 5.2602 test Loss 5.3904 with MSE metric 6816.5640\n",
      "Epoch 291 batch 200 train Loss 5.2635 test Loss 5.3206 with MSE metric 6783.9395\n",
      "Time taken for 1 epoch: 22.999321937561035 secs\n",
      "\n",
      "Epoch 292 batch 0 train Loss 5.3309 test Loss 5.4200 with MSE metric 7831.2900\n",
      "Epoch 292 batch 100 train Loss 5.2669 test Loss 5.3227 with MSE metric 6860.1309\n",
      "Epoch 292 batch 200 train Loss 5.3731 test Loss 5.3903 with MSE metric 8479.4111\n",
      "Time taken for 1 epoch: 24.07514214515686 secs\n",
      "\n",
      "Epoch 293 batch 0 train Loss 5.2204 test Loss 5.3960 with MSE metric 6193.7090\n",
      "Epoch 293 batch 100 train Loss 5.2499 test Loss 5.3457 with MSE metric 6601.8311\n",
      "Epoch 293 batch 200 train Loss 5.3059 test Loss 5.4184 with MSE metric 7442.0400\n",
      "Time taken for 1 epoch: 23.80649209022522 secs\n",
      "\n",
      "Epoch 294 batch 0 train Loss 5.2315 test Loss 5.4214 with MSE metric 6353.5283\n",
      "Epoch 294 batch 100 train Loss 5.3126 test Loss 5.4162 with MSE metric 7550.9111\n",
      "Epoch 294 batch 200 train Loss 5.2947 test Loss 5.2858 with MSE metric 7303.3936\n",
      "Time taken for 1 epoch: 23.98184823989868 secs\n",
      "\n",
      "Epoch 295 batch 0 train Loss 5.3030 test Loss 5.4518 with MSE metric 7415.3604\n",
      "Epoch 295 batch 100 train Loss 5.2362 test Loss 5.4377 with MSE metric 6468.2349\n",
      "Epoch 295 batch 200 train Loss 5.3097 test Loss 5.4058 with MSE metric 7513.1333\n",
      "Time taken for 1 epoch: 24.319156169891357 secs\n",
      "\n",
      "Epoch 296 batch 0 train Loss 5.3598 test Loss 5.4329 with MSE metric 8172.8135\n",
      "Epoch 296 batch 100 train Loss 5.3485 test Loss 5.3097 with MSE metric 8086.8037\n",
      "Epoch 296 batch 200 train Loss 5.2209 test Loss 5.3913 with MSE metric 6293.1484\n",
      "Time taken for 1 epoch: 24.151269674301147 secs\n",
      "\n",
      "Epoch 297 batch 0 train Loss 5.4173 test Loss 5.3817 with MSE metric 9129.5059\n",
      "Epoch 297 batch 100 train Loss 5.2088 test Loss 5.4006 with MSE metric 6109.8428\n",
      "Epoch 297 batch 200 train Loss 5.2986 test Loss 5.3681 with MSE metric 7347.2803\n",
      "Time taken for 1 epoch: 23.478415966033936 secs\n",
      "\n",
      "Epoch 298 batch 0 train Loss 5.2732 test Loss 5.3838 with MSE metric 6985.5874\n",
      "Epoch 298 batch 100 train Loss 5.3329 test Loss 5.3540 with MSE metric 7875.6890\n",
      "Epoch 298 batch 200 train Loss 5.3628 test Loss 5.3864 with MSE metric 8326.0664\n",
      "Time taken for 1 epoch: 24.69457507133484 secs\n",
      "\n",
      "Epoch 299 batch 0 train Loss 5.2537 test Loss 5.3837 with MSE metric 6683.5830\n",
      "Epoch 299 batch 100 train Loss 5.3149 test Loss 5.3468 with MSE metric 7584.7729\n",
      "Epoch 299 batch 200 train Loss 5.2879 test Loss 5.4280 with MSE metric 7193.8613\n",
      "Time taken for 1 epoch: 23.63608980178833 secs\n",
      "\n",
      "Epoch 300 batch 0 train Loss 5.2606 test Loss 5.2851 with MSE metric 6715.5205\n",
      "Epoch 300 batch 100 train Loss 5.2212 test Loss 5.4870 with MSE metric 6288.6646\n",
      "Epoch 300 batch 200 train Loss 5.2551 test Loss 5.4261 with MSE metric 6701.0869\n",
      "Time taken for 1 epoch: 24.572853088378906 secs\n",
      "\n",
      "Epoch 301 batch 0 train Loss 5.2521 test Loss 5.4969 with MSE metric 6679.4731\n",
      "Epoch 301 batch 100 train Loss 5.3914 test Loss 5.3631 with MSE metric 8732.4443\n",
      "Epoch 301 batch 200 train Loss 5.3669 test Loss 5.3506 with MSE metric 8420.4043\n",
      "Time taken for 1 epoch: 24.061182975769043 secs\n",
      "\n",
      "Epoch 302 batch 0 train Loss 5.2561 test Loss 5.3958 with MSE metric 6755.9961\n",
      "Epoch 302 batch 100 train Loss 5.3006 test Loss 5.3091 with MSE metric 7390.2305\n",
      "Epoch 302 batch 200 train Loss 5.3604 test Loss 5.3530 with MSE metric 8271.2676\n",
      "Time taken for 1 epoch: 23.321726083755493 secs\n",
      "\n",
      "Epoch 303 batch 0 train Loss 5.3122 test Loss 5.3534 with MSE metric 7563.6338\n",
      "Epoch 303 batch 100 train Loss 5.3390 test Loss 5.3789 with MSE metric 7955.4058\n",
      "Epoch 303 batch 200 train Loss 5.3128 test Loss 5.3908 with MSE metric 7573.5820\n",
      "Time taken for 1 epoch: 23.152182817459106 secs\n",
      "\n",
      "Epoch 304 batch 0 train Loss 5.3022 test Loss 5.3326 with MSE metric 7397.3228\n",
      "Epoch 304 batch 100 train Loss 5.2407 test Loss 5.4071 with MSE metric 6500.1641\n",
      "Epoch 304 batch 200 train Loss 5.1838 test Loss 5.2633 with MSE metric 5732.4805\n",
      "Time taken for 1 epoch: 23.177608013153076 secs\n",
      "\n",
      "Epoch 305 batch 0 train Loss 5.2593 test Loss 5.3459 with MSE metric 6795.4512\n",
      "Epoch 305 batch 100 train Loss 5.4772 test Loss 5.3862 with MSE metric 9708.5703\n",
      "Epoch 305 batch 200 train Loss 5.3183 test Loss 5.3084 with MSE metric 7654.8350\n",
      "Time taken for 1 epoch: 23.836061000823975 secs\n",
      "\n",
      "Epoch 306 batch 0 train Loss 5.3291 test Loss 5.3172 with MSE metric 7809.5459\n",
      "Epoch 306 batch 100 train Loss 5.1448 test Loss 5.2854 with MSE metric 4974.8174\n",
      "Epoch 306 batch 200 train Loss 5.2688 test Loss 5.5258 with MSE metric 6935.1396\n",
      "Time taken for 1 epoch: 23.850350856781006 secs\n",
      "\n",
      "Epoch 307 batch 0 train Loss 5.3048 test Loss 5.4198 with MSE metric 7453.7158\n",
      "Epoch 307 batch 100 train Loss 5.3312 test Loss 5.3805 with MSE metric 7814.2993\n",
      "Epoch 307 batch 200 train Loss 5.3556 test Loss 5.4145 with MSE metric 8189.5527\n",
      "Time taken for 1 epoch: 24.44730496406555 secs\n",
      "\n",
      "Epoch 308 batch 0 train Loss 5.3271 test Loss 5.3011 with MSE metric 7761.2607\n",
      "Epoch 308 batch 100 train Loss 5.3018 test Loss 5.3233 with MSE metric 7402.6807\n",
      "Epoch 308 batch 200 train Loss 5.1924 test Loss 5.3301 with MSE metric 5844.9336\n",
      "Time taken for 1 epoch: 24.380548238754272 secs\n",
      "\n",
      "Epoch 309 batch 0 train Loss 5.2449 test Loss 5.3648 with MSE metric 6609.8799\n",
      "Epoch 309 batch 100 train Loss 5.3615 test Loss 5.3430 with MSE metric 8317.1562\n",
      "Epoch 309 batch 200 train Loss 5.3208 test Loss 5.3880 with MSE metric 7694.5000\n",
      "Time taken for 1 epoch: 24.465008974075317 secs\n",
      "\n",
      "Epoch 310 batch 0 train Loss 5.2545 test Loss 5.3628 with MSE metric 6690.3452\n",
      "Epoch 310 batch 100 train Loss 5.3247 test Loss 5.3720 with MSE metric 7746.1475\n",
      "Epoch 310 batch 200 train Loss 5.1724 test Loss 5.3729 with MSE metric 5359.6958\n",
      "Time taken for 1 epoch: 24.506245136260986 secs\n",
      "\n",
      "Epoch 311 batch 0 train Loss 5.3094 test Loss 5.3269 with MSE metric 7522.4062\n",
      "Epoch 311 batch 100 train Loss 5.2923 test Loss 5.3956 with MSE metric 7247.1338\n",
      "Epoch 311 batch 200 train Loss 5.2192 test Loss 5.3479 with MSE metric 6180.9609\n",
      "Time taken for 1 epoch: 23.912245988845825 secs\n",
      "\n",
      "Epoch 312 batch 0 train Loss 5.2564 test Loss 5.3343 with MSE metric 6751.9209\n",
      "Epoch 312 batch 100 train Loss 5.2599 test Loss 5.3472 with MSE metric 6774.0596\n",
      "Epoch 312 batch 200 train Loss 5.3227 test Loss 5.3603 with MSE metric 7701.4370\n",
      "Time taken for 1 epoch: 23.387824058532715 secs\n",
      "\n",
      "Epoch 313 batch 0 train Loss 5.2089 test Loss 5.3867 with MSE metric 6031.4810\n",
      "Epoch 313 batch 100 train Loss 5.3210 test Loss 5.3658 with MSE metric 7698.8447\n",
      "Epoch 313 batch 200 train Loss 5.2174 test Loss 5.2749 with MSE metric 6159.1138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 24.494733095169067 secs\n",
      "\n",
      "Epoch 314 batch 0 train Loss 5.2390 test Loss 5.4291 with MSE metric 6530.1152\n",
      "Epoch 314 batch 100 train Loss 5.3744 test Loss 5.4189 with MSE metric 8412.9902\n",
      "Epoch 314 batch 200 train Loss 5.3735 test Loss 5.4526 with MSE metric 8464.1230\n",
      "Time taken for 1 epoch: 23.71581196784973 secs\n",
      "\n",
      "Epoch 315 batch 0 train Loss 5.2913 test Loss 5.4041 with MSE metric 7253.0059\n",
      "Epoch 315 batch 100 train Loss 5.3261 test Loss 5.3346 with MSE metric 7742.5107\n",
      "Epoch 315 batch 200 train Loss 5.3984 test Loss 5.4105 with MSE metric 8832.5391\n",
      "Time taken for 1 epoch: 24.06606101989746 secs\n",
      "\n",
      "Epoch 316 batch 0 train Loss 5.3407 test Loss 5.4171 with MSE metric 7867.4609\n",
      "Epoch 316 batch 100 train Loss 5.2997 test Loss 5.3442 with MSE metric 7377.8086\n",
      "Epoch 316 batch 200 train Loss 5.3054 test Loss 5.3280 with MSE metric 7436.1182\n",
      "Time taken for 1 epoch: 23.755082845687866 secs\n",
      "\n",
      "Epoch 317 batch 0 train Loss 5.2895 test Loss 5.3110 with MSE metric 7223.5830\n",
      "Epoch 317 batch 100 train Loss 5.2787 test Loss 5.2776 with MSE metric 7069.0928\n",
      "Epoch 317 batch 200 train Loss 5.2006 test Loss 5.3824 with MSE metric 5992.9019\n",
      "Time taken for 1 epoch: 23.669191122055054 secs\n",
      "\n",
      "Epoch 318 batch 0 train Loss 5.3126 test Loss 5.2865 with MSE metric 7551.6387\n",
      "Epoch 318 batch 100 train Loss 5.3630 test Loss 5.3148 with MSE metric 8290.9980\n",
      "Epoch 318 batch 200 train Loss 5.2606 test Loss 5.3919 with MSE metric 6796.1704\n",
      "Time taken for 1 epoch: 23.51061701774597 secs\n",
      "\n",
      "Epoch 319 batch 0 train Loss 5.2537 test Loss 5.4280 with MSE metric 6610.7100\n",
      "Epoch 319 batch 100 train Loss 5.2930 test Loss 5.3407 with MSE metric 7278.1533\n",
      "Epoch 319 batch 200 train Loss 5.3012 test Loss 5.3560 with MSE metric 7366.8721\n",
      "Time taken for 1 epoch: 23.317593812942505 secs\n",
      "\n",
      "Epoch 320 batch 0 train Loss 5.2845 test Loss 5.4176 with MSE metric 7156.2158\n",
      "Epoch 320 batch 100 train Loss 5.2976 test Loss 5.3374 with MSE metric 7346.5576\n",
      "Epoch 320 batch 200 train Loss 5.3365 test Loss 5.3730 with MSE metric 7866.6553\n",
      "Time taken for 1 epoch: 23.339980125427246 secs\n",
      "\n",
      "Epoch 321 batch 0 train Loss 5.2392 test Loss 5.3565 with MSE metric 6502.7461\n",
      "Epoch 321 batch 100 train Loss 5.2557 test Loss 5.3775 with MSE metric 6750.6636\n",
      "Epoch 321 batch 200 train Loss 5.3220 test Loss 5.4478 with MSE metric 7709.1240\n",
      "Time taken for 1 epoch: 23.703187942504883 secs\n",
      "\n",
      "Epoch 322 batch 0 train Loss 5.2080 test Loss 5.2782 with MSE metric 5713.6909\n",
      "Epoch 322 batch 100 train Loss 5.3822 test Loss 5.3299 with MSE metric 8573.0176\n",
      "Epoch 322 batch 200 train Loss 5.4032 test Loss 5.3001 with MSE metric 8702.3242\n",
      "Time taken for 1 epoch: 23.72756791114807 secs\n",
      "\n",
      "Epoch 323 batch 0 train Loss 5.2760 test Loss 5.3166 with MSE metric 7029.6943\n",
      "Epoch 323 batch 100 train Loss 5.3162 test Loss 5.3888 with MSE metric 7600.4863\n",
      "Epoch 323 batch 200 train Loss 5.2976 test Loss 5.4031 with MSE metric 7345.9976\n",
      "Time taken for 1 epoch: 24.918452978134155 secs\n",
      "\n",
      "Epoch 324 batch 0 train Loss 5.2333 test Loss 5.4538 with MSE metric 6391.2568\n",
      "Epoch 324 batch 100 train Loss 5.3706 test Loss 5.3480 with MSE metric 8487.0000\n",
      "Epoch 324 batch 200 train Loss 5.2417 test Loss 5.3350 with MSE metric 6553.8379\n",
      "Time taken for 1 epoch: 23.7557270526886 secs\n",
      "\n",
      "Epoch 325 batch 0 train Loss 5.3631 test Loss 5.2939 with MSE metric 8301.7646\n",
      "Epoch 325 batch 100 train Loss 5.3711 test Loss 5.4112 with MSE metric 8407.6309\n",
      "Epoch 325 batch 200 train Loss 5.3131 test Loss 5.3913 with MSE metric 7571.6924\n",
      "Time taken for 1 epoch: 23.678755044937134 secs\n",
      "\n",
      "Epoch 326 batch 0 train Loss 5.3816 test Loss 5.3412 with MSE metric 8625.4805\n",
      "Epoch 326 batch 100 train Loss 5.1836 test Loss 5.3456 with MSE metric 5562.6851\n",
      "Epoch 326 batch 200 train Loss 5.1961 test Loss 5.4482 with MSE metric 5876.6504\n",
      "Time taken for 1 epoch: 23.56108808517456 secs\n",
      "\n",
      "Epoch 327 batch 0 train Loss 5.1967 test Loss 5.2943 with MSE metric 5904.7305\n",
      "Epoch 327 batch 100 train Loss 5.3483 test Loss 5.3304 with MSE metric 8127.4609\n",
      "Epoch 327 batch 200 train Loss 5.3833 test Loss 5.3870 with MSE metric 8597.9062\n",
      "Time taken for 1 epoch: 23.567583084106445 secs\n",
      "\n",
      "Epoch 328 batch 0 train Loss 5.2445 test Loss 5.3200 with MSE metric 6574.7397\n",
      "Epoch 328 batch 100 train Loss 5.2857 test Loss 5.3955 with MSE metric 7151.1758\n",
      "Epoch 328 batch 200 train Loss 5.2765 test Loss 5.2979 with MSE metric 7017.8950\n",
      "Time taken for 1 epoch: 23.604734897613525 secs\n",
      "\n",
      "Epoch 329 batch 0 train Loss 5.3294 test Loss 5.3247 with MSE metric 7794.2002\n",
      "Epoch 329 batch 100 train Loss 5.2450 test Loss 5.2758 with MSE metric 6511.5977\n",
      "Epoch 329 batch 200 train Loss 5.3041 test Loss 5.2911 with MSE metric 7441.4067\n",
      "Time taken for 1 epoch: 23.520642042160034 secs\n",
      "\n",
      "Epoch 330 batch 0 train Loss 5.2796 test Loss 5.4014 with MSE metric 7081.6338\n",
      "Epoch 330 batch 100 train Loss 5.2599 test Loss 5.3655 with MSE metric 6765.9771\n",
      "Epoch 330 batch 200 train Loss 5.2868 test Loss 5.4543 with MSE metric 7189.8481\n",
      "Time taken for 1 epoch: 23.339300632476807 secs\n",
      "\n",
      "Epoch 331 batch 0 train Loss 5.3020 test Loss 5.3504 with MSE metric 7381.4717\n",
      "Epoch 331 batch 100 train Loss 5.3028 test Loss 5.3905 with MSE metric 7422.8931\n",
      "Epoch 331 batch 200 train Loss 5.3240 test Loss 5.3052 with MSE metric 7737.3643\n",
      "Time taken for 1 epoch: 23.506714820861816 secs\n",
      "\n",
      "Epoch 332 batch 0 train Loss 5.3073 test Loss 5.3986 with MSE metric 7490.2334\n",
      "Epoch 332 batch 100 train Loss 5.3217 test Loss 5.3406 with MSE metric 7709.1865\n",
      "Epoch 332 batch 200 train Loss 5.3172 test Loss 5.3207 with MSE metric 7635.0625\n",
      "Time taken for 1 epoch: 23.986020803451538 secs\n",
      "\n",
      "Epoch 333 batch 0 train Loss 5.2779 test Loss 5.4189 with MSE metric 7056.7373\n",
      "Epoch 333 batch 100 train Loss 5.3559 test Loss 5.2884 with MSE metric 8241.5889\n",
      "Epoch 333 batch 200 train Loss 5.3483 test Loss 5.3280 with MSE metric 8060.6880\n",
      "Time taken for 1 epoch: 23.576425790786743 secs\n",
      "\n",
      "Epoch 334 batch 0 train Loss 5.2448 test Loss 5.3061 with MSE metric 6543.1875\n",
      "Epoch 334 batch 100 train Loss 5.3072 test Loss 5.3576 with MSE metric 7486.2803\n",
      "Epoch 334 batch 200 train Loss 5.5114 test Loss 5.4017 with MSE metric 10553.0537\n",
      "Time taken for 1 epoch: 23.52619171142578 secs\n",
      "\n",
      "Epoch 335 batch 0 train Loss 5.3047 test Loss 5.3118 with MSE metric 7443.0806\n",
      "Epoch 335 batch 100 train Loss 5.3123 test Loss 5.3605 with MSE metric 7555.8823\n",
      "Epoch 335 batch 200 train Loss 5.2494 test Loss 5.3499 with MSE metric 6625.9189\n",
      "Time taken for 1 epoch: 23.377479791641235 secs\n",
      "\n",
      "Epoch 336 batch 0 train Loss 5.2899 test Loss 5.3938 with MSE metric 7233.6738\n",
      "Epoch 336 batch 100 train Loss 5.2773 test Loss 5.5177 with MSE metric 7053.0972\n",
      "Epoch 336 batch 200 train Loss 5.2602 test Loss 5.2885 with MSE metric 6804.7388\n",
      "Time taken for 1 epoch: 23.521743059158325 secs\n",
      "\n",
      "Epoch 337 batch 0 train Loss 5.3803 test Loss 5.3454 with MSE metric 8513.3516\n",
      "Epoch 337 batch 100 train Loss 5.3907 test Loss 5.3093 with MSE metric 8634.8066\n",
      "Epoch 337 batch 200 train Loss 5.3097 test Loss 5.3378 with MSE metric 7519.5259\n",
      "Time taken for 1 epoch: 23.31462335586548 secs\n",
      "\n",
      "Epoch 338 batch 0 train Loss 5.3270 test Loss 5.4173 with MSE metric 7787.7891\n",
      "Epoch 338 batch 100 train Loss 5.3105 test Loss 5.3696 with MSE metric 7521.9463\n",
      "Epoch 338 batch 200 train Loss 5.2598 test Loss 5.3024 with MSE metric 6774.8711\n",
      "Time taken for 1 epoch: 23.54125213623047 secs\n",
      "\n",
      "Epoch 339 batch 0 train Loss 5.4360 test Loss 5.4173 with MSE metric 9564.9111\n",
      "Epoch 339 batch 100 train Loss 5.3306 test Loss 5.3336 with MSE metric 7802.6436\n",
      "Epoch 339 batch 200 train Loss 5.4047 test Loss 5.3970 with MSE metric 8951.4131\n",
      "Time taken for 1 epoch: 23.593253135681152 secs\n",
      "\n",
      "Epoch 340 batch 0 train Loss 5.3108 test Loss 5.3597 with MSE metric 7531.2085\n",
      "Epoch 340 batch 100 train Loss 5.2581 test Loss 5.3620 with MSE metric 6709.8223\n",
      "Epoch 340 batch 200 train Loss 5.3385 test Loss 5.3247 with MSE metric 7943.0693\n",
      "Time taken for 1 epoch: 23.596031188964844 secs\n",
      "\n",
      "Epoch 341 batch 0 train Loss 5.2750 test Loss 5.3494 with MSE metric 6986.3813\n",
      "Epoch 341 batch 100 train Loss 5.3665 test Loss 5.4152 with MSE metric 8376.2461\n",
      "Epoch 341 batch 200 train Loss 5.2810 test Loss 5.3597 with MSE metric 7081.0825\n",
      "Time taken for 1 epoch: 23.603280782699585 secs\n",
      "\n",
      "Epoch 342 batch 0 train Loss 5.3429 test Loss 5.3560 with MSE metric 7903.0303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 batch 100 train Loss 5.3616 test Loss 5.3792 with MSE metric 8321.7988\n",
      "Epoch 342 batch 200 train Loss 5.2658 test Loss 5.2171 with MSE metric 6831.0264\n",
      "Time taken for 1 epoch: 23.395739793777466 secs\n",
      "\n",
      "Epoch 343 batch 0 train Loss 5.3072 test Loss 5.3200 with MSE metric 7484.1992\n",
      "Epoch 343 batch 100 train Loss 5.3500 test Loss 5.2942 with MSE metric 8158.5601\n",
      "Epoch 343 batch 200 train Loss 5.3112 test Loss 5.3119 with MSE metric 7549.9312\n",
      "Time taken for 1 epoch: 23.60182762145996 secs\n",
      "\n",
      "Epoch 344 batch 0 train Loss 5.2042 test Loss 5.3921 with MSE metric 5969.0522\n",
      "Epoch 344 batch 100 train Loss 5.3262 test Loss 5.4200 with MSE metric 7753.6377\n",
      "Epoch 344 batch 200 train Loss 5.2643 test Loss 5.3070 with MSE metric 6872.8809\n",
      "Time taken for 1 epoch: 23.63816499710083 secs\n",
      "\n",
      "Epoch 345 batch 0 train Loss 5.3065 test Loss 5.3583 with MSE metric 7474.7173\n",
      "Epoch 345 batch 100 train Loss 5.2342 test Loss 5.3397 with MSE metric 6446.3955\n",
      "Epoch 345 batch 200 train Loss 5.2785 test Loss 5.3779 with MSE metric 7069.7153\n",
      "Time taken for 1 epoch: 23.656784057617188 secs\n",
      "\n",
      "Epoch 346 batch 0 train Loss 5.1831 test Loss 5.2233 with MSE metric 5659.1680\n",
      "Epoch 346 batch 100 train Loss 5.3384 test Loss 5.3269 with MSE metric 7970.0371\n",
      "Epoch 346 batch 200 train Loss 5.3187 test Loss 5.3274 with MSE metric 7642.6792\n",
      "Time taken for 1 epoch: 23.681576013565063 secs\n",
      "\n",
      "Epoch 347 batch 0 train Loss 5.3475 test Loss 5.4188 with MSE metric 8097.7983\n",
      "Epoch 347 batch 100 train Loss 5.2704 test Loss 5.3788 with MSE metric 6950.1270\n",
      "Epoch 347 batch 200 train Loss 5.2643 test Loss 5.3874 with MSE metric 6857.7539\n",
      "Time taken for 1 epoch: 23.769882917404175 secs\n",
      "\n",
      "Epoch 348 batch 0 train Loss 5.2529 test Loss 5.3860 with MSE metric 6700.5762\n",
      "Epoch 348 batch 100 train Loss 5.3539 test Loss 5.3915 with MSE metric 8208.5293\n",
      "Epoch 348 batch 200 train Loss 5.3278 test Loss 5.3407 with MSE metric 7739.4097\n",
      "Time taken for 1 epoch: 23.60246706008911 secs\n",
      "\n",
      "Epoch 349 batch 0 train Loss 5.2235 test Loss 5.2695 with MSE metric 6232.1182\n",
      "Epoch 349 batch 100 train Loss 5.2944 test Loss 5.3289 with MSE metric 7299.3398\n",
      "Epoch 349 batch 200 train Loss 5.3028 test Loss 5.3727 with MSE metric 7419.1064\n",
      "Time taken for 1 epoch: 23.42716908454895 secs\n",
      "\n",
      "Epoch 350 batch 0 train Loss 5.3237 test Loss 5.4369 with MSE metric 7739.6768\n",
      "Epoch 350 batch 100 train Loss 5.2784 test Loss 5.3869 with MSE metric 7054.6284\n",
      "Epoch 350 batch 200 train Loss 5.2196 test Loss 5.3392 with MSE metric 6195.3105\n",
      "Time taken for 1 epoch: 24.46684694290161 secs\n",
      "\n",
      "Epoch 351 batch 0 train Loss 5.2846 test Loss 5.3347 with MSE metric 7153.7119\n",
      "Epoch 351 batch 100 train Loss 5.2775 test Loss 5.3993 with MSE metric 7023.4263\n",
      "Epoch 351 batch 200 train Loss 5.3006 test Loss 5.4066 with MSE metric 7351.1738\n",
      "Time taken for 1 epoch: 23.29678201675415 secs\n",
      "\n",
      "Epoch 352 batch 0 train Loss 5.2303 test Loss 5.4334 with MSE metric 6377.7129\n",
      "Epoch 352 batch 100 train Loss 5.3788 test Loss 5.3566 with MSE metric 8522.1318\n",
      "Epoch 352 batch 200 train Loss 5.2892 test Loss 5.2875 with MSE metric 7209.2031\n",
      "Time taken for 1 epoch: 23.150664806365967 secs\n",
      "\n",
      "Epoch 353 batch 0 train Loss 5.3797 test Loss 5.2765 with MSE metric 8597.5273\n",
      "Epoch 353 batch 100 train Loss 5.3241 test Loss 5.3298 with MSE metric 7747.0298\n",
      "Epoch 353 batch 200 train Loss 5.3039 test Loss 5.2928 with MSE metric 7433.3027\n",
      "Time taken for 1 epoch: 23.53938102722168 secs\n",
      "\n",
      "Epoch 354 batch 0 train Loss 5.3313 test Loss 5.3064 with MSE metric 7848.5059\n",
      "Epoch 354 batch 100 train Loss 5.3754 test Loss 5.3729 with MSE metric 8435.2305\n",
      "Epoch 354 batch 200 train Loss 5.3938 test Loss 5.3598 with MSE metric 8706.8965\n",
      "Time taken for 1 epoch: 24.001410961151123 secs\n",
      "\n",
      "Epoch 355 batch 0 train Loss 5.3589 test Loss 5.3414 with MSE metric 8256.7734\n",
      "Epoch 355 batch 100 train Loss 5.3153 test Loss 5.3624 with MSE metric 7607.4834\n",
      "Epoch 355 batch 200 train Loss 5.3703 test Loss 5.3260 with MSE metric 8449.9473\n",
      "Time taken for 1 epoch: 23.632161140441895 secs\n",
      "\n",
      "Epoch 356 batch 0 train Loss 5.2566 test Loss 5.4346 with MSE metric 6760.7803\n",
      "Epoch 356 batch 100 train Loss 5.1377 test Loss 5.3402 with MSE metric 5111.8662\n",
      "Epoch 356 batch 200 train Loss 5.2083 test Loss 5.3933 with MSE metric 5940.2812\n",
      "Time taken for 1 epoch: 23.781252145767212 secs\n",
      "\n",
      "Epoch 357 batch 0 train Loss 5.2931 test Loss 5.3303 with MSE metric 7206.9321\n",
      "Epoch 357 batch 100 train Loss 5.2389 test Loss 5.4446 with MSE metric 6528.1582\n",
      "Epoch 357 batch 200 train Loss 5.3177 test Loss 5.3600 with MSE metric 7623.6611\n",
      "Time taken for 1 epoch: 23.683197021484375 secs\n",
      "\n",
      "Epoch 358 batch 0 train Loss 5.2048 test Loss 5.2513 with MSE metric 5872.8340\n",
      "Epoch 358 batch 100 train Loss 5.2908 test Loss 5.3739 with MSE metric 7246.9243\n",
      "Epoch 358 batch 200 train Loss 5.2469 test Loss 5.3467 with MSE metric 6626.1685\n",
      "Time taken for 1 epoch: 24.367838859558105 secs\n",
      "\n",
      "Epoch 359 batch 0 train Loss 5.2532 test Loss 5.3557 with MSE metric 6546.3447\n",
      "Epoch 359 batch 100 train Loss 5.2862 test Loss 5.3997 with MSE metric 7177.4053\n",
      "Epoch 359 batch 200 train Loss 5.2261 test Loss 5.3664 with MSE metric 6232.1890\n",
      "Time taken for 1 epoch: 23.796934843063354 secs\n",
      "\n",
      "Epoch 360 batch 0 train Loss 5.3553 test Loss 5.3976 with MSE metric 8243.1348\n",
      "Epoch 360 batch 100 train Loss 5.3119 test Loss 5.3622 with MSE metric 7560.3633\n",
      "Epoch 360 batch 200 train Loss 5.2685 test Loss 5.3208 with MSE metric 6925.5293\n",
      "Time taken for 1 epoch: 23.531210899353027 secs\n",
      "\n",
      "Epoch 361 batch 0 train Loss 5.2763 test Loss 5.4092 with MSE metric 7037.1846\n",
      "Epoch 361 batch 100 train Loss 5.2533 test Loss 5.3876 with MSE metric 6666.1104\n",
      "Epoch 361 batch 200 train Loss 5.1600 test Loss 5.2476 with MSE metric 5311.5078\n",
      "Time taken for 1 epoch: 23.66891384124756 secs\n",
      "\n",
      "Epoch 362 batch 0 train Loss 5.2970 test Loss 5.2387 with MSE metric 7321.2275\n",
      "Epoch 362 batch 100 train Loss 5.2169 test Loss 5.2741 with MSE metric 6195.1045\n",
      "Epoch 362 batch 200 train Loss 5.2741 test Loss 5.4527 with MSE metric 7005.7656\n",
      "Time taken for 1 epoch: 23.44505214691162 secs\n",
      "\n",
      "Epoch 363 batch 0 train Loss 5.3376 test Loss 5.3257 with MSE metric 7922.0645\n",
      "Epoch 363 batch 100 train Loss 5.3766 test Loss 5.3665 with MSE metric 8519.4424\n",
      "Epoch 363 batch 200 train Loss 5.3517 test Loss 5.2249 with MSE metric 8176.3433\n",
      "Time taken for 1 epoch: 23.595360040664673 secs\n",
      "\n",
      "Epoch 364 batch 0 train Loss 5.2781 test Loss 5.3406 with MSE metric 7054.8882\n",
      "Epoch 364 batch 100 train Loss 5.2901 test Loss 5.3732 with MSE metric 7222.2334\n",
      "Epoch 364 batch 200 train Loss 5.2396 test Loss 5.3144 with MSE metric 6536.9727\n",
      "Time taken for 1 epoch: 24.76558780670166 secs\n",
      "\n",
      "Epoch 365 batch 0 train Loss 5.3687 test Loss 5.3413 with MSE metric 8354.0049\n",
      "Epoch 365 batch 100 train Loss 5.2952 test Loss 5.3274 with MSE metric 7304.6304\n",
      "Epoch 365 batch 200 train Loss 5.2565 test Loss 5.3117 with MSE metric 6672.7852\n",
      "Time taken for 1 epoch: 25.515928030014038 secs\n",
      "\n",
      "Epoch 366 batch 0 train Loss 5.3806 test Loss 5.3845 with MSE metric 8579.2734\n",
      "Epoch 366 batch 100 train Loss 5.1930 test Loss 5.3012 with MSE metric 5734.0854\n",
      "Epoch 366 batch 200 train Loss 5.3443 test Loss 5.3679 with MSE metric 8058.9902\n",
      "Time taken for 1 epoch: 23.16615390777588 secs\n",
      "\n",
      "Epoch 367 batch 0 train Loss 5.2408 test Loss 5.4011 with MSE metric 6517.9604\n",
      "Epoch 367 batch 100 train Loss 5.3805 test Loss 5.4280 with MSE metric 8662.9229\n",
      "Epoch 367 batch 200 train Loss 5.3425 test Loss 5.3861 with MSE metric 8011.8584\n",
      "Time taken for 1 epoch: 22.737802982330322 secs\n",
      "\n",
      "Epoch 368 batch 0 train Loss 5.3596 test Loss 5.3784 with MSE metric 8279.9766\n",
      "Epoch 368 batch 100 train Loss 5.3552 test Loss 5.3218 with MSE metric 8239.9365\n",
      "Epoch 368 batch 200 train Loss 5.3174 test Loss 5.3580 with MSE metric 7639.6714\n",
      "Time taken for 1 epoch: 25.304179906845093 secs\n",
      "\n",
      "Epoch 369 batch 0 train Loss 5.2790 test Loss 5.4611 with MSE metric 7077.6577\n",
      "Epoch 369 batch 100 train Loss 5.3392 test Loss 5.4010 with MSE metric 7972.9824\n",
      "Epoch 369 batch 200 train Loss 5.2384 test Loss 5.3521 with MSE metric 6501.5640\n",
      "Time taken for 1 epoch: 22.002655029296875 secs\n",
      "\n",
      "Epoch 370 batch 0 train Loss 5.2509 test Loss 5.2854 with MSE metric 6493.1689\n",
      "Epoch 370 batch 100 train Loss 5.3030 test Loss 5.3923 with MSE metric 7413.3647\n",
      "Epoch 370 batch 200 train Loss 5.2944 test Loss 5.3042 with MSE metric 7273.5776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 24.312705993652344 secs\n",
      "\n",
      "Epoch 371 batch 0 train Loss 5.2393 test Loss 5.3640 with MSE metric 6491.9272\n",
      "Epoch 371 batch 100 train Loss 5.3382 test Loss 5.3831 with MSE metric 7960.5757\n",
      "Epoch 371 batch 200 train Loss 5.2739 test Loss 5.4045 with MSE metric 6985.0107\n",
      "Time taken for 1 epoch: 23.379619121551514 secs\n",
      "\n",
      "Epoch 372 batch 0 train Loss 5.2601 test Loss 5.2996 with MSE metric 6664.7207\n",
      "Epoch 372 batch 100 train Loss 5.2934 test Loss 5.3316 with MSE metric 7284.2847\n",
      "Epoch 372 batch 200 train Loss 5.3184 test Loss 5.3281 with MSE metric 7652.9473\n",
      "Time taken for 1 epoch: 25.820667266845703 secs\n",
      "\n",
      "Epoch 373 batch 0 train Loss 5.3073 test Loss 5.2161 with MSE metric 7489.0615\n",
      "Epoch 373 batch 100 train Loss 5.2794 test Loss 5.3763 with MSE metric 7083.6108\n",
      "Epoch 373 batch 200 train Loss 5.2902 test Loss 5.4384 with MSE metric 7233.6099\n",
      "Time taken for 1 epoch: 23.016315937042236 secs\n",
      "\n",
      "Epoch 374 batch 0 train Loss 5.3406 test Loss 5.2704 with MSE metric 8002.8730\n",
      "Epoch 374 batch 100 train Loss 5.2815 test Loss 5.3843 with MSE metric 7112.5566\n",
      "Epoch 374 batch 200 train Loss 5.3401 test Loss 5.4030 with MSE metric 7919.6548\n",
      "Time taken for 1 epoch: 23.659218311309814 secs\n",
      "\n",
      "Epoch 375 batch 0 train Loss 5.2563 test Loss 5.2476 with MSE metric 6762.6846\n",
      "Epoch 375 batch 100 train Loss 5.3810 test Loss 5.3319 with MSE metric 8617.2832\n",
      "Epoch 375 batch 200 train Loss 5.4353 test Loss 5.3273 with MSE metric 9325.2422\n",
      "Time taken for 1 epoch: 23.993839740753174 secs\n",
      "\n",
      "Epoch 376 batch 0 train Loss 5.2706 test Loss 5.3164 with MSE metric 6952.6162\n",
      "Epoch 376 batch 100 train Loss 5.2548 test Loss 5.4219 with MSE metric 6743.7651\n",
      "Epoch 376 batch 200 train Loss 5.3129 test Loss 5.3287 with MSE metric 7544.1846\n",
      "Time taken for 1 epoch: 23.658128023147583 secs\n",
      "\n",
      "Epoch 377 batch 0 train Loss 5.3519 test Loss 5.3304 with MSE metric 8049.9741\n",
      "Epoch 377 batch 100 train Loss 5.2850 test Loss 5.3848 with MSE metric 7050.2456\n",
      "Epoch 377 batch 200 train Loss 5.2156 test Loss 5.3849 with MSE metric 6196.8018\n",
      "Time taken for 1 epoch: 23.647711992263794 secs\n",
      "\n",
      "Epoch 378 batch 0 train Loss 5.2452 test Loss 5.3136 with MSE metric 6561.0386\n",
      "Epoch 378 batch 100 train Loss 5.2407 test Loss 5.2725 with MSE metric 6508.7349\n",
      "Epoch 378 batch 200 train Loss 5.3994 test Loss 5.4115 with MSE metric 8924.1035\n",
      "Time taken for 1 epoch: 24.52494168281555 secs\n",
      "\n",
      "Epoch 379 batch 0 train Loss 5.3287 test Loss 5.3173 with MSE metric 7799.2334\n",
      "Epoch 379 batch 100 train Loss 5.2833 test Loss 5.3690 with MSE metric 7139.7861\n",
      "Epoch 379 batch 200 train Loss 5.3017 test Loss 5.4293 with MSE metric 7406.5469\n",
      "Time taken for 1 epoch: 25.17817997932434 secs\n",
      "\n",
      "Epoch 380 batch 0 train Loss 5.2229 test Loss 5.3132 with MSE metric 6319.8770\n",
      "Epoch 380 batch 100 train Loss 5.3473 test Loss 5.3004 with MSE metric 8021.0908\n",
      "Epoch 380 batch 200 train Loss 5.3377 test Loss 5.4055 with MSE metric 7899.5176\n",
      "Time taken for 1 epoch: 22.067906141281128 secs\n",
      "\n",
      "Epoch 381 batch 0 train Loss 5.3110 test Loss 5.2907 with MSE metric 7534.1387\n",
      "Epoch 381 batch 100 train Loss 5.3159 test Loss 5.3432 with MSE metric 7617.8379\n",
      "Epoch 381 batch 200 train Loss 5.2652 test Loss 5.3523 with MSE metric 6834.3979\n",
      "Time taken for 1 epoch: 22.157634258270264 secs\n",
      "\n",
      "Epoch 382 batch 0 train Loss 5.1746 test Loss 5.3287 with MSE metric 5573.9170\n",
      "Epoch 382 batch 100 train Loss 5.2683 test Loss 5.3224 with MSE metric 6924.1611\n",
      "Epoch 382 batch 200 train Loss 5.2593 test Loss 5.4115 with MSE metric 6743.0864\n",
      "Time taken for 1 epoch: 22.09139394760132 secs\n",
      "\n",
      "Epoch 383 batch 0 train Loss 5.1779 test Loss 5.2859 with MSE metric 5680.0352\n",
      "Epoch 383 batch 100 train Loss 5.2682 test Loss 5.3708 with MSE metric 6916.8008\n",
      "Epoch 383 batch 200 train Loss 5.2718 test Loss 5.4343 with MSE metric 6973.5454\n",
      "Time taken for 1 epoch: 22.95992088317871 secs\n",
      "\n",
      "Epoch 384 batch 0 train Loss 5.2729 test Loss 5.4596 with MSE metric 6984.6450\n",
      "Epoch 384 batch 100 train Loss 5.3457 test Loss 5.2569 with MSE metric 8066.3291\n",
      "Epoch 384 batch 200 train Loss 5.3482 test Loss 5.4155 with MSE metric 8038.6885\n",
      "Time taken for 1 epoch: 22.65426993370056 secs\n",
      "\n",
      "Epoch 385 batch 0 train Loss 5.3163 test Loss 5.3912 with MSE metric 7625.2373\n",
      "Epoch 385 batch 100 train Loss 5.3319 test Loss 5.4187 with MSE metric 7857.1797\n",
      "Epoch 385 batch 200 train Loss 5.3416 test Loss 5.3994 with MSE metric 7999.3999\n",
      "Time taken for 1 epoch: 25.474976778030396 secs\n",
      "\n",
      "Epoch 386 batch 0 train Loss 5.2938 test Loss 5.3202 with MSE metric 7289.5469\n",
      "Epoch 386 batch 100 train Loss 5.2789 test Loss 5.2490 with MSE metric 7071.5083\n",
      "Epoch 386 batch 200 train Loss 5.2752 test Loss 5.3085 with MSE metric 7025.1758\n",
      "Time taken for 1 epoch: 24.810539960861206 secs\n",
      "\n",
      "Epoch 387 batch 0 train Loss 5.2488 test Loss 5.3165 with MSE metric 6649.1445\n",
      "Epoch 387 batch 100 train Loss 5.3639 test Loss 5.3178 with MSE metric 8352.5234\n",
      "Epoch 387 batch 200 train Loss 5.3362 test Loss 5.4089 with MSE metric 7912.0049\n",
      "Time taken for 1 epoch: 23.15630578994751 secs\n",
      "\n",
      "Epoch 388 batch 0 train Loss 5.3401 test Loss 5.4048 with MSE metric 7989.2168\n",
      "Epoch 388 batch 100 train Loss 5.2482 test Loss 5.3783 with MSE metric 6526.7832\n",
      "Epoch 388 batch 200 train Loss 5.3172 test Loss 5.2647 with MSE metric 7631.5889\n",
      "Time taken for 1 epoch: 23.142902135849 secs\n",
      "\n",
      "Epoch 389 batch 0 train Loss 5.3093 test Loss 5.3518 with MSE metric 7496.5234\n",
      "Epoch 389 batch 100 train Loss 5.2640 test Loss 5.3847 with MSE metric 6762.7041\n",
      "Epoch 389 batch 200 train Loss 5.2488 test Loss 5.4354 with MSE metric 6663.2163\n",
      "Time taken for 1 epoch: 22.09235143661499 secs\n",
      "\n",
      "Epoch 390 batch 0 train Loss 5.2180 test Loss 5.3642 with MSE metric 6065.1533\n",
      "Epoch 390 batch 100 train Loss 5.3328 test Loss 5.3252 with MSE metric 7881.5967\n",
      "Epoch 390 batch 200 train Loss 5.2396 test Loss 5.3199 with MSE metric 6487.8945\n",
      "Time taken for 1 epoch: 21.98790693283081 secs\n",
      "\n",
      "Epoch 391 batch 0 train Loss 5.3296 test Loss 5.3449 with MSE metric 7828.3657\n",
      "Epoch 391 batch 100 train Loss 5.3481 test Loss 5.4255 with MSE metric 8116.9561\n",
      "Epoch 391 batch 200 train Loss 5.3124 test Loss 5.4331 with MSE metric 7559.1709\n",
      "Time taken for 1 epoch: 22.001728057861328 secs\n",
      "\n",
      "Epoch 392 batch 0 train Loss 5.2454 test Loss 5.3762 with MSE metric 6488.2676\n",
      "Epoch 392 batch 100 train Loss 5.3320 test Loss 5.2663 with MSE metric 7867.6450\n",
      "Epoch 392 batch 200 train Loss 5.3332 test Loss 5.4281 with MSE metric 7788.5107\n",
      "Time taken for 1 epoch: 22.044514179229736 secs\n",
      "\n",
      "Epoch 393 batch 0 train Loss 5.2693 test Loss 5.3374 with MSE metric 6923.5938\n",
      "Epoch 393 batch 100 train Loss 5.2567 test Loss 5.3205 with MSE metric 6648.0049\n",
      "Epoch 393 batch 200 train Loss 5.2501 test Loss 5.3615 with MSE metric 6608.7080\n",
      "Time taken for 1 epoch: 21.784082889556885 secs\n",
      "\n",
      "Epoch 394 batch 0 train Loss 5.3233 test Loss 5.3681 with MSE metric 7701.2388\n",
      "Epoch 394 batch 100 train Loss 5.3132 test Loss 5.3136 with MSE metric 7567.8164\n",
      "Epoch 394 batch 200 train Loss 5.2800 test Loss 5.3978 with MSE metric 7086.0117\n",
      "Time taken for 1 epoch: 22.563199043273926 secs\n",
      "\n",
      "Epoch 395 batch 0 train Loss 5.3716 test Loss 5.4451 with MSE metric 8245.9023\n",
      "Epoch 395 batch 100 train Loss 5.2591 test Loss 5.3009 with MSE metric 6797.5000\n",
      "Epoch 395 batch 200 train Loss 5.3836 test Loss 5.3592 with MSE metric 8698.7578\n",
      "Time taken for 1 epoch: 22.648029804229736 secs\n",
      "\n",
      "Epoch 396 batch 0 train Loss 5.3194 test Loss 5.2608 with MSE metric 7673.2793\n",
      "Epoch 396 batch 100 train Loss 5.3408 test Loss 5.2825 with MSE metric 8004.1797\n",
      "Epoch 396 batch 200 train Loss 5.2849 test Loss 5.4220 with MSE metric 7152.9707\n",
      "Time taken for 1 epoch: 22.742852926254272 secs\n",
      "\n",
      "Epoch 397 batch 0 train Loss 5.3031 test Loss 5.3053 with MSE metric 7402.6416\n",
      "Epoch 397 batch 100 train Loss 5.3878 test Loss 5.2589 with MSE metric 8632.9150\n",
      "Epoch 397 batch 200 train Loss 5.2957 test Loss 5.4422 with MSE metric 7316.9336\n",
      "Time taken for 1 epoch: 22.811827898025513 secs\n",
      "\n",
      "Epoch 398 batch 0 train Loss 5.3213 test Loss 5.4410 with MSE metric 7665.0942\n",
      "Epoch 398 batch 100 train Loss 5.3033 test Loss 5.2498 with MSE metric 7424.7617\n",
      "Epoch 398 batch 200 train Loss 5.3113 test Loss 5.4098 with MSE metric 7534.2490\n",
      "Time taken for 1 epoch: 22.87098503112793 secs\n",
      "\n",
      "Epoch 399 batch 0 train Loss 5.2297 test Loss 5.3082 with MSE metric 6347.9707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399 batch 100 train Loss 5.3569 test Loss 5.3377 with MSE metric 8271.0723\n",
      "Epoch 399 batch 200 train Loss 5.2587 test Loss 5.3148 with MSE metric 6751.7808\n",
      "Time taken for 1 epoch: 22.78200912475586 secs\n",
      "\n",
      "Epoch 400 batch 0 train Loss 5.2319 test Loss 5.2675 with MSE metric 6429.4756\n",
      "Epoch 400 batch 100 train Loss 5.4003 test Loss 5.3196 with MSE metric 8916.1562\n",
      "Epoch 400 batch 200 train Loss 5.2827 test Loss 5.3708 with MSE metric 7130.4609\n",
      "Time taken for 1 epoch: 22.81319284439087 secs\n",
      "\n",
      "Epoch 401 batch 0 train Loss 5.3000 test Loss 5.3546 with MSE metric 7380.0967\n",
      "Epoch 401 batch 100 train Loss 5.2042 test Loss 5.4125 with MSE metric 5983.3613\n",
      "Epoch 401 batch 200 train Loss 5.2826 test Loss 5.4023 with MSE metric 7104.2920\n",
      "Time taken for 1 epoch: 22.747803926467896 secs\n",
      "\n",
      "Epoch 402 batch 0 train Loss 5.3460 test Loss 5.4022 with MSE metric 8093.0020\n",
      "Epoch 402 batch 100 train Loss 5.2858 test Loss 5.3099 with MSE metric 7171.9082\n",
      "Epoch 402 batch 200 train Loss 5.2822 test Loss 5.3367 with MSE metric 7119.8037\n",
      "Time taken for 1 epoch: 22.72879195213318 secs\n",
      "\n",
      "Epoch 403 batch 0 train Loss 5.3126 test Loss 5.3290 with MSE metric 7570.1323\n",
      "Epoch 403 batch 100 train Loss 5.2865 test Loss 5.3282 with MSE metric 7180.2969\n",
      "Epoch 403 batch 200 train Loss 5.3462 test Loss 5.3581 with MSE metric 8086.9785\n",
      "Time taken for 1 epoch: 23.09972882270813 secs\n",
      "\n",
      "Epoch 404 batch 0 train Loss 5.2923 test Loss 5.2785 with MSE metric 7245.5439\n",
      "Epoch 404 batch 100 train Loss 5.2714 test Loss 5.4053 with MSE metric 6891.1475\n",
      "Epoch 404 batch 200 train Loss 5.2416 test Loss 5.2732 with MSE metric 6503.8330\n",
      "Time taken for 1 epoch: 22.828371047973633 secs\n",
      "\n",
      "Epoch 405 batch 0 train Loss 5.2768 test Loss 5.2975 with MSE metric 7023.9614\n",
      "Epoch 405 batch 100 train Loss 5.3083 test Loss 5.3671 with MSE metric 7493.1709\n",
      "Epoch 405 batch 200 train Loss 5.3601 test Loss 5.3509 with MSE metric 8286.3467\n",
      "Time taken for 1 epoch: 22.755120992660522 secs\n",
      "\n",
      "Epoch 406 batch 0 train Loss 5.2767 test Loss 5.4184 with MSE metric 7033.7471\n",
      "Epoch 406 batch 100 train Loss 5.3072 test Loss 5.3876 with MSE metric 7453.4219\n",
      "Epoch 406 batch 200 train Loss 5.2189 test Loss 5.3960 with MSE metric 6246.7515\n",
      "Time taken for 1 epoch: 22.912126064300537 secs\n",
      "\n",
      "Epoch 407 batch 0 train Loss 5.4246 test Loss 5.3382 with MSE metric 9241.1328\n",
      "Epoch 407 batch 100 train Loss 5.2202 test Loss 5.4620 with MSE metric 6238.3984\n",
      "Epoch 407 batch 200 train Loss 5.1576 test Loss 5.4503 with MSE metric 5321.3335\n",
      "Time taken for 1 epoch: 24.66110897064209 secs\n",
      "\n",
      "Epoch 408 batch 0 train Loss 5.2679 test Loss 5.3144 with MSE metric 6835.8115\n",
      "Epoch 408 batch 100 train Loss 5.2522 test Loss 5.4513 with MSE metric 6703.8223\n",
      "Epoch 408 batch 200 train Loss 5.3632 test Loss 5.3418 with MSE metric 8329.0029\n",
      "Time taken for 1 epoch: 23.8611741065979 secs\n",
      "\n",
      "Epoch 409 batch 0 train Loss 5.3181 test Loss 5.3707 with MSE metric 7648.0840\n",
      "Epoch 409 batch 100 train Loss 5.3331 test Loss 5.3582 with MSE metric 7887.2612\n",
      "Epoch 409 batch 200 train Loss 5.3141 test Loss 5.3097 with MSE metric 7592.3511\n",
      "Time taken for 1 epoch: 23.338818788528442 secs\n",
      "\n",
      "Epoch 410 batch 0 train Loss 5.3306 test Loss 5.4088 with MSE metric 7847.4229\n",
      "Epoch 410 batch 100 train Loss 5.2027 test Loss 5.3504 with MSE metric 5973.5762\n",
      "Epoch 410 batch 200 train Loss 5.3791 test Loss 5.3432 with MSE metric 8543.8936\n",
      "Time taken for 1 epoch: 22.19894790649414 secs\n",
      "\n",
      "Epoch 411 batch 0 train Loss 5.3217 test Loss 5.3994 with MSE metric 7671.2178\n",
      "Epoch 411 batch 100 train Loss 5.4160 test Loss 5.3023 with MSE metric 9039.5020\n",
      "Epoch 411 batch 200 train Loss 5.3084 test Loss 5.4412 with MSE metric 7506.9229\n",
      "Time taken for 1 epoch: 22.277136087417603 secs\n",
      "\n",
      "Epoch 412 batch 0 train Loss 5.2335 test Loss 5.4307 with MSE metric 6398.9697\n",
      "Epoch 412 batch 100 train Loss 5.2575 test Loss 5.4358 with MSE metric 6755.2334\n",
      "Epoch 412 batch 200 train Loss 5.3853 test Loss 5.3700 with MSE metric 8713.8975\n",
      "Time taken for 1 epoch: 22.199842929840088 secs\n",
      "\n",
      "Epoch 413 batch 0 train Loss 5.2638 test Loss 5.3542 with MSE metric 6865.2676\n",
      "Epoch 413 batch 100 train Loss 5.3037 test Loss 5.2826 with MSE metric 7436.4395\n",
      "Epoch 413 batch 200 train Loss 5.3182 test Loss 5.3527 with MSE metric 7655.6538\n",
      "Time taken for 1 epoch: 22.418978929519653 secs\n",
      "\n",
      "Epoch 414 batch 0 train Loss 5.2612 test Loss 5.3516 with MSE metric 6788.8350\n",
      "Epoch 414 batch 100 train Loss 5.2810 test Loss 5.3902 with MSE metric 7103.5234\n",
      "Epoch 414 batch 200 train Loss 5.4027 test Loss 5.3578 with MSE metric 8751.2002\n",
      "Time taken for 1 epoch: 22.37520408630371 secs\n",
      "\n",
      "Epoch 415 batch 0 train Loss 5.2556 test Loss 5.4016 with MSE metric 6740.6797\n",
      "Epoch 415 batch 100 train Loss 5.2309 test Loss 5.3807 with MSE metric 6393.9795\n",
      "Epoch 415 batch 200 train Loss 5.4017 test Loss 5.3214 with MSE metric 8963.7793\n",
      "Time taken for 1 epoch: 22.70422911643982 secs\n",
      "\n",
      "Epoch 416 batch 0 train Loss 5.2621 test Loss 5.3209 with MSE metric 6829.8667\n",
      "Epoch 416 batch 100 train Loss 5.3084 test Loss 5.3538 with MSE metric 7503.9355\n",
      "Epoch 416 batch 200 train Loss 5.2987 test Loss 5.2781 with MSE metric 7362.1704\n",
      "Time taken for 1 epoch: 22.67833709716797 secs\n",
      "\n",
      "Epoch 417 batch 0 train Loss 5.3734 test Loss 5.3533 with MSE metric 8536.1504\n",
      "Epoch 417 batch 100 train Loss 5.3620 test Loss 5.4148 with MSE metric 8347.5693\n",
      "Epoch 417 batch 200 train Loss 5.3199 test Loss 5.2437 with MSE metric 7680.5571\n",
      "Time taken for 1 epoch: 22.60872507095337 secs\n",
      "\n",
      "Epoch 418 batch 0 train Loss 5.2725 test Loss 5.2293 with MSE metric 6982.5439\n",
      "Epoch 418 batch 100 train Loss 5.3467 test Loss 5.2878 with MSE metric 8104.1260\n",
      "Epoch 418 batch 200 train Loss 5.2408 test Loss 5.3183 with MSE metric 6541.1768\n",
      "Time taken for 1 epoch: 22.578625917434692 secs\n",
      "\n",
      "Epoch 419 batch 0 train Loss 5.2832 test Loss 5.5147 with MSE metric 7128.5068\n",
      "Epoch 419 batch 100 train Loss 5.2853 test Loss 5.3021 with MSE metric 7162.4219\n",
      "Epoch 419 batch 200 train Loss 5.2165 test Loss 5.3029 with MSE metric 6230.3560\n",
      "Time taken for 1 epoch: 22.685873985290527 secs\n",
      "\n",
      "Epoch 420 batch 0 train Loss 5.2930 test Loss 5.4164 with MSE metric 7279.6074\n",
      "Epoch 420 batch 100 train Loss 5.2800 test Loss 5.3757 with MSE metric 7089.1265\n",
      "Epoch 420 batch 200 train Loss 5.3144 test Loss 5.4137 with MSE metric 7597.2622\n",
      "Time taken for 1 epoch: 22.674203157424927 secs\n",
      "\n",
      "Epoch 421 batch 0 train Loss 5.2985 test Loss 5.3166 with MSE metric 7359.8125\n",
      "Epoch 421 batch 100 train Loss 5.3142 test Loss 5.4129 with MSE metric 7593.3311\n",
      "Epoch 421 batch 200 train Loss 5.2637 test Loss 5.4689 with MSE metric 6797.8140\n",
      "Time taken for 1 epoch: 22.594141960144043 secs\n",
      "\n",
      "Epoch 422 batch 0 train Loss 5.3137 test Loss 5.2086 with MSE metric 7563.8755\n",
      "Epoch 422 batch 100 train Loss 5.2007 test Loss 5.3992 with MSE metric 5918.4375\n",
      "Epoch 422 batch 200 train Loss 5.3280 test Loss 5.3008 with MSE metric 7805.4805\n",
      "Time taken for 1 epoch: 22.57036519050598 secs\n",
      "\n",
      "Epoch 423 batch 0 train Loss 5.4541 test Loss 5.3901 with MSE metric 9607.8350\n",
      "Epoch 423 batch 100 train Loss 5.2515 test Loss 5.3359 with MSE metric 6614.6406\n",
      "Epoch 423 batch 200 train Loss 5.2136 test Loss 5.3128 with MSE metric 6163.9746\n",
      "Time taken for 1 epoch: 22.577425956726074 secs\n",
      "\n",
      "Epoch 424 batch 0 train Loss 5.2243 test Loss 5.4480 with MSE metric 6272.9902\n",
      "Epoch 424 batch 100 train Loss 5.4229 test Loss 5.3716 with MSE metric 9275.2754\n",
      "Epoch 424 batch 200 train Loss 5.3256 test Loss 5.4592 with MSE metric 7638.1606\n",
      "Time taken for 1 epoch: 22.616133213043213 secs\n",
      "\n",
      "Epoch 425 batch 0 train Loss 5.3178 test Loss 5.2335 with MSE metric 7641.1523\n",
      "Epoch 425 batch 100 train Loss 5.3053 test Loss 5.4223 with MSE metric 7406.2329\n",
      "Epoch 425 batch 200 train Loss 5.2502 test Loss 5.2864 with MSE metric 6660.4170\n",
      "Time taken for 1 epoch: 22.624028205871582 secs\n",
      "\n",
      "Epoch 426 batch 0 train Loss 5.3159 test Loss 5.2724 with MSE metric 7578.6191\n",
      "Epoch 426 batch 100 train Loss 5.3666 test Loss 5.2558 with MSE metric 8310.4521\n",
      "Epoch 426 batch 200 train Loss 5.2613 test Loss 5.4354 with MSE metric 6745.9961\n",
      "Time taken for 1 epoch: 22.712583303451538 secs\n",
      "\n",
      "Epoch 427 batch 0 train Loss 5.2379 test Loss 5.3675 with MSE metric 6462.3623\n",
      "Epoch 427 batch 100 train Loss 5.2808 test Loss 5.3620 with MSE metric 7086.7979\n",
      "Epoch 427 batch 200 train Loss 5.3476 test Loss 5.4412 with MSE metric 8060.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 22.586896896362305 secs\n",
      "\n",
      "Epoch 428 batch 0 train Loss 5.3534 test Loss 5.2841 with MSE metric 8167.2373\n",
      "Epoch 428 batch 100 train Loss 5.3101 test Loss 5.4304 with MSE metric 7533.4023\n",
      "Epoch 428 batch 200 train Loss 5.2828 test Loss 5.3584 with MSE metric 7103.8730\n",
      "Time taken for 1 epoch: 22.27204418182373 secs\n",
      "\n",
      "Epoch 429 batch 0 train Loss 5.2722 test Loss 5.3948 with MSE metric 6980.4922\n",
      "Epoch 429 batch 100 train Loss 5.2714 test Loss 5.3702 with MSE metric 6963.9951\n",
      "Epoch 429 batch 200 train Loss 5.3332 test Loss 5.3722 with MSE metric 7869.1992\n",
      "Time taken for 1 epoch: 22.03881287574768 secs\n",
      "\n",
      "Epoch 430 batch 0 train Loss 5.2826 test Loss 5.2925 with MSE metric 7129.0176\n",
      "Epoch 430 batch 100 train Loss 5.3509 test Loss 5.4483 with MSE metric 8161.0586\n",
      "Epoch 430 batch 200 train Loss 5.3127 test Loss 5.3917 with MSE metric 7552.7280\n",
      "Time taken for 1 epoch: 22.216126918792725 secs\n",
      "\n",
      "Epoch 431 batch 0 train Loss 5.3011 test Loss 5.2509 with MSE metric 7389.9873\n",
      "Epoch 431 batch 100 train Loss 5.3601 test Loss 5.2929 with MSE metric 8237.5469\n",
      "Epoch 431 batch 200 train Loss 5.2890 test Loss 5.3461 with MSE metric 7204.1299\n",
      "Time taken for 1 epoch: 21.913635730743408 secs\n",
      "\n",
      "Epoch 432 batch 0 train Loss 5.4196 test Loss 5.4104 with MSE metric 9225.5723\n",
      "Epoch 432 batch 100 train Loss 5.3163 test Loss 5.3760 with MSE metric 7624.5762\n",
      "Epoch 432 batch 200 train Loss 5.2579 test Loss 5.3699 with MSE metric 6786.0991\n",
      "Time taken for 1 epoch: 22.036797046661377 secs\n",
      "\n",
      "Epoch 433 batch 0 train Loss 5.2708 test Loss 5.2006 with MSE metric 6947.6191\n",
      "Epoch 433 batch 100 train Loss 5.3394 test Loss 5.2891 with MSE metric 7974.3203\n",
      "Epoch 433 batch 200 train Loss 5.3629 test Loss 5.3444 with MSE metric 8305.1387\n",
      "Time taken for 1 epoch: 21.973477125167847 secs\n",
      "\n",
      "Epoch 434 batch 0 train Loss 5.3366 test Loss 5.3052 with MSE metric 7938.2866\n",
      "Epoch 434 batch 100 train Loss 5.2512 test Loss 5.4407 with MSE metric 6658.2466\n",
      "Epoch 434 batch 200 train Loss 5.3557 test Loss 5.3460 with MSE metric 8107.1909\n",
      "Time taken for 1 epoch: 22.099869966506958 secs\n",
      "\n",
      "Epoch 435 batch 0 train Loss 5.2534 test Loss 5.4092 with MSE metric 6724.1221\n",
      "Epoch 435 batch 100 train Loss 5.2701 test Loss 5.3334 with MSE metric 6849.2900\n",
      "Epoch 435 batch 200 train Loss 5.2990 test Loss 5.3849 with MSE metric 7364.5693\n",
      "Time taken for 1 epoch: 22.255100965499878 secs\n",
      "\n",
      "Epoch 436 batch 0 train Loss 5.3957 test Loss 5.3919 with MSE metric 8889.6250\n",
      "Epoch 436 batch 100 train Loss 5.3075 test Loss 5.3906 with MSE metric 7494.2173\n",
      "Epoch 436 batch 200 train Loss 5.3397 test Loss 5.3665 with MSE metric 7981.7808\n",
      "Time taken for 1 epoch: 22.37467098236084 secs\n",
      "\n",
      "Epoch 437 batch 0 train Loss 5.2451 test Loss 5.4020 with MSE metric 6612.8691\n",
      "Epoch 437 batch 100 train Loss 5.2240 test Loss 5.2596 with MSE metric 6260.7773\n",
      "Epoch 437 batch 200 train Loss 5.2531 test Loss 5.4139 with MSE metric 6709.7402\n",
      "Time taken for 1 epoch: 22.64604687690735 secs\n",
      "\n",
      "Epoch 438 batch 0 train Loss 5.2010 test Loss 5.4623 with MSE metric 5964.9639\n",
      "Epoch 438 batch 100 train Loss 5.3352 test Loss 5.3269 with MSE metric 7915.4355\n",
      "Epoch 438 batch 200 train Loss 5.2985 test Loss 5.3269 with MSE metric 7351.0259\n",
      "Time taken for 1 epoch: 22.491999864578247 secs\n",
      "\n",
      "Epoch 439 batch 0 train Loss 5.2373 test Loss 5.3808 with MSE metric 6490.2163\n",
      "Epoch 439 batch 100 train Loss 5.3397 test Loss 5.3339 with MSE metric 7984.6655\n",
      "Epoch 439 batch 200 train Loss 5.3134 test Loss 5.2567 with MSE metric 7572.9219\n",
      "Time taken for 1 epoch: 22.41670298576355 secs\n",
      "\n",
      "Epoch 440 batch 0 train Loss 5.2274 test Loss 5.3904 with MSE metric 6354.5312\n",
      "Epoch 440 batch 100 train Loss 5.3116 test Loss 5.3597 with MSE metric 7553.9463\n",
      "Epoch 440 batch 200 train Loss 5.3284 test Loss 5.3568 with MSE metric 7783.9365\n",
      "Time taken for 1 epoch: 22.50376009941101 secs\n",
      "\n",
      "Epoch 441 batch 0 train Loss 5.2920 test Loss 5.2768 with MSE metric 7265.0645\n",
      "Epoch 441 batch 100 train Loss 5.3187 test Loss 5.3487 with MSE metric 7633.2910\n",
      "Epoch 441 batch 200 train Loss 5.2697 test Loss 5.3630 with MSE metric 6879.0996\n",
      "Time taken for 1 epoch: 22.40444278717041 secs\n",
      "\n",
      "Epoch 442 batch 0 train Loss 5.3822 test Loss 5.3788 with MSE metric 8639.0449\n",
      "Epoch 442 batch 100 train Loss 5.2560 test Loss 5.4312 with MSE metric 6749.1611\n",
      "Epoch 442 batch 200 train Loss 5.3553 test Loss 5.4010 with MSE metric 8220.6602\n",
      "Time taken for 1 epoch: 22.525758981704712 secs\n",
      "\n",
      "Epoch 443 batch 0 train Loss 5.2614 test Loss 5.3492 with MSE metric 6797.6816\n",
      "Epoch 443 batch 100 train Loss 5.1789 test Loss 5.3843 with MSE metric 5346.8975\n",
      "Epoch 443 batch 200 train Loss 5.2864 test Loss 5.4086 with MSE metric 7184.4717\n",
      "Time taken for 1 epoch: 22.53092098236084 secs\n",
      "\n",
      "Epoch 444 batch 0 train Loss 5.3140 test Loss 5.3953 with MSE metric 7590.4619\n",
      "Epoch 444 batch 100 train Loss 5.3183 test Loss 5.3814 with MSE metric 7656.1221\n",
      "Epoch 444 batch 200 train Loss 5.4239 test Loss 5.3600 with MSE metric 9296.5439\n",
      "Time taken for 1 epoch: 22.31934690475464 secs\n",
      "\n",
      "Epoch 445 batch 0 train Loss 5.2539 test Loss 5.4033 with MSE metric 6719.4844\n",
      "Epoch 445 batch 100 train Loss 5.3610 test Loss 5.2474 with MSE metric 8234.1357\n",
      "Epoch 445 batch 200 train Loss 5.2554 test Loss 5.3074 with MSE metric 6645.9102\n",
      "Time taken for 1 epoch: 22.670641899108887 secs\n",
      "\n",
      "Epoch 446 batch 0 train Loss 5.2381 test Loss 5.3457 with MSE metric 6505.6816\n",
      "Epoch 446 batch 100 train Loss 5.3125 test Loss 5.4326 with MSE metric 7568.8672\n",
      "Epoch 446 batch 200 train Loss 5.2895 test Loss 5.3363 with MSE metric 7228.0674\n",
      "Time taken for 1 epoch: 23.38664698600769 secs\n",
      "\n",
      "Epoch 447 batch 0 train Loss 5.3326 test Loss 5.3546 with MSE metric 7871.1724\n",
      "Epoch 447 batch 100 train Loss 5.4190 test Loss 5.2834 with MSE metric 8982.4189\n",
      "Epoch 447 batch 200 train Loss 5.3048 test Loss 5.4445 with MSE metric 7450.7935\n",
      "Time taken for 1 epoch: 23.256200790405273 secs\n",
      "\n",
      "Epoch 448 batch 0 train Loss 5.2875 test Loss 5.3944 with MSE metric 7196.5435\n",
      "Epoch 448 batch 100 train Loss 5.3735 test Loss 5.3758 with MSE metric 8439.5195\n",
      "Epoch 448 batch 200 train Loss 5.4199 test Loss 5.2721 with MSE metric 9257.6182\n",
      "Time taken for 1 epoch: 23.448097229003906 secs\n",
      "\n",
      "Epoch 449 batch 0 train Loss 5.3165 test Loss 5.2986 with MSE metric 7629.0752\n",
      "Epoch 449 batch 100 train Loss 5.3098 test Loss 5.3737 with MSE metric 7523.6060\n",
      "Epoch 449 batch 200 train Loss 5.2285 test Loss 5.2908 with MSE metric 6354.0771\n",
      "Time taken for 1 epoch: 23.013141870498657 secs\n",
      "\n",
      "Epoch 450 batch 0 train Loss 5.3706 test Loss 5.3692 with MSE metric 8407.2646\n",
      "Epoch 450 batch 100 train Loss 5.2478 test Loss 5.3687 with MSE metric 6606.6968\n",
      "Epoch 450 batch 200 train Loss 5.2995 test Loss 5.3775 with MSE metric 7344.4360\n",
      "Time taken for 1 epoch: 23.45964789390564 secs\n",
      "\n",
      "Epoch 451 batch 0 train Loss 5.3111 test Loss 5.3951 with MSE metric 7535.9297\n",
      "Epoch 451 batch 100 train Loss 5.4310 test Loss 5.4140 with MSE metric 9260.7246\n",
      "Epoch 451 batch 200 train Loss 5.2973 test Loss 5.3201 with MSE metric 7324.2295\n",
      "Time taken for 1 epoch: 22.80960488319397 secs\n",
      "\n",
      "Epoch 452 batch 0 train Loss 5.3189 test Loss 5.4246 with MSE metric 7663.0479\n",
      "Epoch 452 batch 100 train Loss 5.3271 test Loss 5.4016 with MSE metric 7790.8740\n",
      "Epoch 452 batch 200 train Loss 5.2707 test Loss 5.4116 with MSE metric 6926.4771\n",
      "Time taken for 1 epoch: 22.689811944961548 secs\n",
      "\n",
      "Epoch 453 batch 0 train Loss 5.2353 test Loss 5.3520 with MSE metric 6277.0542\n",
      "Epoch 453 batch 100 train Loss 5.2607 test Loss 5.3138 with MSE metric 6817.5264\n",
      "Epoch 453 batch 200 train Loss 5.3278 test Loss 5.3390 with MSE metric 7786.2256\n",
      "Time taken for 1 epoch: 22.736119985580444 secs\n",
      "\n",
      "Epoch 454 batch 0 train Loss 5.3377 test Loss 5.3491 with MSE metric 7950.0269\n",
      "Epoch 454 batch 100 train Loss 5.3502 test Loss 5.2932 with MSE metric 8159.9761\n",
      "Epoch 454 batch 200 train Loss 5.2558 test Loss 5.3502 with MSE metric 6735.2383\n",
      "Time taken for 1 epoch: 22.96392512321472 secs\n",
      "\n",
      "Epoch 455 batch 0 train Loss 5.3039 test Loss 5.4286 with MSE metric 7438.9971\n",
      "Epoch 455 batch 100 train Loss 5.2553 test Loss 5.3453 with MSE metric 6654.7637\n",
      "Epoch 455 batch 200 train Loss 5.3241 test Loss 5.3447 with MSE metric 7742.4912\n",
      "Time taken for 1 epoch: 23.91397786140442 secs\n",
      "\n",
      "Epoch 456 batch 0 train Loss 5.2235 test Loss 5.5131 with MSE metric 6242.6558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456 batch 100 train Loss 5.3067 test Loss 5.4482 with MSE metric 7481.0298\n",
      "Epoch 456 batch 200 train Loss 5.2987 test Loss 5.3478 with MSE metric 7362.8682\n",
      "Time taken for 1 epoch: 22.934611082077026 secs\n",
      "\n",
      "Epoch 457 batch 0 train Loss 5.4416 test Loss 5.5021 with MSE metric 9599.8525\n",
      "Epoch 457 batch 100 train Loss 5.2699 test Loss 5.4269 with MSE metric 6949.8408\n",
      "Epoch 457 batch 200 train Loss 5.4079 test Loss 5.2897 with MSE metric 8940.8799\n",
      "Time taken for 1 epoch: 23.09203791618347 secs\n",
      "\n",
      "Epoch 458 batch 0 train Loss 5.2831 test Loss 5.2672 with MSE metric 7084.2646\n",
      "Epoch 458 batch 100 train Loss 5.3507 test Loss 5.3522 with MSE metric 8158.6836\n",
      "Epoch 458 batch 200 train Loss 5.3422 test Loss 5.3558 with MSE metric 8027.7637\n",
      "Time taken for 1 epoch: 23.04433798789978 secs\n",
      "\n",
      "Epoch 459 batch 0 train Loss 5.3444 test Loss 5.4092 with MSE metric 8039.5596\n",
      "Epoch 459 batch 100 train Loss 5.3554 test Loss 5.3728 with MSE metric 8232.0977\n",
      "Epoch 459 batch 200 train Loss 5.2736 test Loss 5.3430 with MSE metric 6929.1758\n",
      "Time taken for 1 epoch: 22.950676202774048 secs\n",
      "\n",
      "Epoch 460 batch 0 train Loss 5.3499 test Loss 5.4021 with MSE metric 8151.7339\n",
      "Epoch 460 batch 100 train Loss 5.3247 test Loss 5.4051 with MSE metric 7743.7109\n",
      "Epoch 460 batch 200 train Loss 5.3199 test Loss 5.4223 with MSE metric 7678.0762\n",
      "Time taken for 1 epoch: 22.87377405166626 secs\n",
      "\n",
      "Epoch 461 batch 0 train Loss 5.2442 test Loss 5.4057 with MSE metric 6568.7832\n",
      "Epoch 461 batch 100 train Loss 5.4079 test Loss 5.4393 with MSE metric 9024.8027\n",
      "Epoch 461 batch 200 train Loss 5.3778 test Loss 5.3016 with MSE metric 8597.3223\n",
      "Time taken for 1 epoch: 23.4408700466156 secs\n",
      "\n",
      "Epoch 462 batch 0 train Loss 5.2694 test Loss 5.3473 with MSE metric 6905.5986\n",
      "Epoch 462 batch 100 train Loss 5.2312 test Loss 5.3312 with MSE metric 6425.0557\n",
      "Epoch 462 batch 200 train Loss 5.4182 test Loss 5.4462 with MSE metric 9155.6387\n",
      "Time taken for 1 epoch: 23.024879693984985 secs\n",
      "\n",
      "Epoch 463 batch 0 train Loss 5.2775 test Loss 5.3250 with MSE metric 7052.0430\n",
      "Epoch 463 batch 100 train Loss 5.3198 test Loss 5.3570 with MSE metric 7680.2959\n",
      "Epoch 463 batch 200 train Loss 5.3874 test Loss 5.3610 with MSE metric 8663.1953\n",
      "Time taken for 1 epoch: 23.335641860961914 secs\n",
      "\n",
      "Epoch 464 batch 0 train Loss 5.2138 test Loss 5.3565 with MSE metric 6124.8521\n",
      "Epoch 464 batch 100 train Loss 5.2663 test Loss 5.4586 with MSE metric 6848.5146\n",
      "Epoch 464 batch 200 train Loss 5.3619 test Loss 5.5045 with MSE metric 8324.2334\n",
      "Time taken for 1 epoch: 23.445033073425293 secs\n",
      "\n",
      "Epoch 465 batch 0 train Loss 5.2767 test Loss 5.4088 with MSE metric 7043.6392\n",
      "Epoch 465 batch 100 train Loss 5.2990 test Loss 5.4610 with MSE metric 7366.5151\n",
      "Epoch 465 batch 200 train Loss 5.2509 test Loss 5.3752 with MSE metric 6646.5967\n",
      "Time taken for 1 epoch: 22.851659059524536 secs\n",
      "\n",
      "Epoch 466 batch 0 train Loss 5.2483 test Loss 5.4011 with MSE metric 6568.7988\n",
      "Epoch 466 batch 100 train Loss 5.2408 test Loss 5.2993 with MSE metric 6544.4810\n",
      "Epoch 466 batch 200 train Loss 5.2946 test Loss 5.3343 with MSE metric 7301.6191\n",
      "Time taken for 1 epoch: 22.838506937026978 secs\n",
      "\n",
      "Epoch 467 batch 0 train Loss 5.3093 test Loss 5.3837 with MSE metric 7509.1143\n",
      "Epoch 467 batch 100 train Loss 5.2550 test Loss 5.3798 with MSE metric 6574.1860\n",
      "Epoch 467 batch 200 train Loss 5.2911 test Loss 5.4245 with MSE metric 7236.8369\n",
      "Time taken for 1 epoch: 22.892905950546265 secs\n",
      "\n",
      "Epoch 468 batch 0 train Loss 5.3547 test Loss 5.3407 with MSE metric 8151.7207\n",
      "Epoch 468 batch 100 train Loss 5.3748 test Loss 5.2221 with MSE metric 8501.0898\n",
      "Epoch 468 batch 200 train Loss 5.2815 test Loss 5.3767 with MSE metric 7108.4697\n",
      "Time taken for 1 epoch: 22.893295288085938 secs\n",
      "\n",
      "Epoch 469 batch 0 train Loss 5.3304 test Loss 5.4036 with MSE metric 7825.8877\n",
      "Epoch 469 batch 100 train Loss 5.3496 test Loss 5.3877 with MSE metric 8102.7319\n",
      "Epoch 469 batch 200 train Loss 5.2697 test Loss 5.2901 with MSE metric 6876.9204\n",
      "Time taken for 1 epoch: 22.513167142868042 secs\n",
      "\n",
      "Epoch 470 batch 0 train Loss 5.2763 test Loss 5.3232 with MSE metric 7019.4590\n",
      "Epoch 470 batch 100 train Loss 5.2991 test Loss 5.3183 with MSE metric 7354.8809\n",
      "Epoch 470 batch 200 train Loss 5.3196 test Loss 5.4088 with MSE metric 7673.0352\n",
      "Time taken for 1 epoch: 22.559351205825806 secs\n",
      "\n",
      "Epoch 471 batch 0 train Loss 5.2596 test Loss 5.3790 with MSE metric 6764.8467\n",
      "Epoch 471 batch 100 train Loss 5.3269 test Loss 5.3065 with MSE metric 7772.5000\n",
      "Epoch 471 batch 200 train Loss 5.3587 test Loss 5.3741 with MSE metric 8207.8086\n",
      "Time taken for 1 epoch: 25.563241004943848 secs\n",
      "\n",
      "Epoch 472 batch 0 train Loss 5.2800 test Loss 5.3486 with MSE metric 7082.8008\n",
      "Epoch 472 batch 100 train Loss 5.3337 test Loss 5.3917 with MSE metric 7897.6211\n",
      "Epoch 472 batch 200 train Loss 5.2473 test Loss 5.3268 with MSE metric 6638.2920\n",
      "Time taken for 1 epoch: 25.422519207000732 secs\n",
      "\n",
      "Epoch 473 batch 0 train Loss 5.3419 test Loss 5.3443 with MSE metric 7990.8408\n",
      "Epoch 473 batch 100 train Loss 5.3232 test Loss 5.3869 with MSE metric 7706.3672\n",
      "Epoch 473 batch 200 train Loss 5.2823 test Loss 5.2939 with MSE metric 7086.2412\n",
      "Time taken for 1 epoch: 25.338112115859985 secs\n",
      "\n",
      "Epoch 474 batch 0 train Loss 5.1897 test Loss 5.5083 with MSE metric 5864.9678\n",
      "Epoch 474 batch 100 train Loss 5.3112 test Loss 5.3885 with MSE metric 7546.2773\n",
      "Epoch 474 batch 200 train Loss 5.3071 test Loss 5.3654 with MSE metric 7482.5903\n",
      "Time taken for 1 epoch: 25.472129106521606 secs\n",
      "\n",
      "Epoch 475 batch 0 train Loss 5.2199 test Loss 5.3863 with MSE metric 6170.0879\n",
      "Epoch 475 batch 100 train Loss 5.3376 test Loss 5.4168 with MSE metric 7925.4268\n",
      "Epoch 475 batch 200 train Loss 5.1718 test Loss 5.3188 with MSE metric 5418.5879\n",
      "Time taken for 1 epoch: 25.81782293319702 secs\n",
      "\n",
      "Epoch 476 batch 0 train Loss 5.4001 test Loss 5.3805 with MSE metric 8833.0098\n",
      "Epoch 476 batch 100 train Loss 5.2931 test Loss 5.3583 with MSE metric 7271.9092\n",
      "Epoch 476 batch 200 train Loss 5.2618 test Loss 5.2470 with MSE metric 6774.3042\n",
      "Time taken for 1 epoch: 26.479984998703003 secs\n",
      "\n",
      "Epoch 477 batch 0 train Loss 5.1233 test Loss 5.4037 with MSE metric 4883.8896\n",
      "Epoch 477 batch 100 train Loss 5.3524 test Loss 5.3312 with MSE metric 8165.1230\n",
      "Epoch 477 batch 200 train Loss 5.3320 test Loss 5.3206 with MSE metric 7861.6162\n",
      "Time taken for 1 epoch: 24.323221921920776 secs\n",
      "\n",
      "Epoch 478 batch 0 train Loss 5.3541 test Loss 5.3698 with MSE metric 8224.2891\n",
      "Epoch 478 batch 100 train Loss 5.2987 test Loss 5.4347 with MSE metric 7362.8809\n",
      "Epoch 478 batch 200 train Loss 5.3354 test Loss 5.3993 with MSE metric 7909.3135\n",
      "Time taken for 1 epoch: 23.287270069122314 secs\n",
      "\n",
      "Epoch 479 batch 0 train Loss 5.2044 test Loss 5.4357 with MSE metric 5972.2051\n",
      "Epoch 479 batch 100 train Loss 5.2542 test Loss 5.2971 with MSE metric 6735.8701\n",
      "Epoch 479 batch 200 train Loss 5.3381 test Loss 5.3312 with MSE metric 7958.9297\n",
      "Time taken for 1 epoch: 23.367393016815186 secs\n",
      "\n",
      "Epoch 480 batch 0 train Loss 5.3686 test Loss 5.3700 with MSE metric 8382.6494\n",
      "Epoch 480 batch 100 train Loss 5.2787 test Loss 5.3687 with MSE metric 7067.9775\n",
      "Epoch 480 batch 200 train Loss 5.3094 test Loss 5.3953 with MSE metric 7519.4551\n",
      "Time taken for 1 epoch: 23.12149405479431 secs\n",
      "\n",
      "Epoch 481 batch 0 train Loss 5.2695 test Loss 5.3710 with MSE metric 6945.9121\n",
      "Epoch 481 batch 100 train Loss 5.2879 test Loss 5.4067 with MSE metric 7204.9365\n",
      "Epoch 481 batch 200 train Loss 5.3258 test Loss 5.3600 with MSE metric 7741.8184\n",
      "Time taken for 1 epoch: 23.57989192008972 secs\n",
      "\n",
      "Epoch 482 batch 0 train Loss 5.3695 test Loss 5.4083 with MSE metric 8261.6621\n",
      "Epoch 482 batch 100 train Loss 5.3182 test Loss 5.4085 with MSE metric 7640.7559\n",
      "Epoch 482 batch 200 train Loss 5.2231 test Loss 5.3934 with MSE metric 6242.9746\n",
      "Time taken for 1 epoch: 23.491918087005615 secs\n",
      "\n",
      "Epoch 483 batch 0 train Loss 5.2504 test Loss 5.3306 with MSE metric 6577.2812\n",
      "Epoch 483 batch 100 train Loss 5.3161 test Loss 5.3680 with MSE metric 7623.2002\n",
      "Epoch 483 batch 200 train Loss 5.2658 test Loss 5.4066 with MSE metric 6854.9272\n",
      "Time taken for 1 epoch: 23.472201108932495 secs\n",
      "\n",
      "Epoch 484 batch 0 train Loss 5.3375 test Loss 5.3108 with MSE metric 7954.6992\n",
      "Epoch 484 batch 100 train Loss 5.3588 test Loss 5.3367 with MSE metric 8234.5918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484 batch 200 train Loss 5.2420 test Loss 5.3028 with MSE metric 6458.3608\n",
      "Time taken for 1 epoch: 23.660197019577026 secs\n",
      "\n",
      "Epoch 485 batch 0 train Loss 5.3051 test Loss 5.3489 with MSE metric 7457.2070\n",
      "Epoch 485 batch 100 train Loss 5.2350 test Loss 5.3045 with MSE metric 6478.1973\n",
      "Epoch 485 batch 200 train Loss 5.3132 test Loss 5.3144 with MSE metric 7576.9502\n",
      "Time taken for 1 epoch: 24.92916989326477 secs\n",
      "\n",
      "Epoch 486 batch 0 train Loss 5.2404 test Loss 5.4326 with MSE metric 6519.9209\n",
      "Epoch 486 batch 100 train Loss 5.2721 test Loss 5.3489 with MSE metric 6980.4683\n",
      "Epoch 486 batch 200 train Loss 5.2665 test Loss 5.3194 with MSE metric 6896.2842\n",
      "Time taken for 1 epoch: 23.154803037643433 secs\n",
      "\n",
      "Epoch 487 batch 0 train Loss 5.3395 test Loss 5.3868 with MSE metric 7960.2202\n",
      "Epoch 487 batch 100 train Loss 5.2903 test Loss 5.3609 with MSE metric 7230.6240\n",
      "Epoch 487 batch 200 train Loss 5.3537 test Loss 5.2279 with MSE metric 8185.1792\n",
      "Time taken for 1 epoch: 23.24729585647583 secs\n",
      "\n",
      "Epoch 488 batch 0 train Loss 5.1709 test Loss 5.3669 with MSE metric 5549.5811\n",
      "Epoch 488 batch 100 train Loss 5.3520 test Loss 5.4183 with MSE metric 8165.4951\n",
      "Epoch 488 batch 200 train Loss 5.3163 test Loss 5.4084 with MSE metric 7626.2598\n",
      "Time taken for 1 epoch: 23.222262859344482 secs\n",
      "\n",
      "Epoch 489 batch 0 train Loss 5.3661 test Loss 5.3103 with MSE metric 8294.9072\n",
      "Epoch 489 batch 100 train Loss 5.1818 test Loss 5.5185 with MSE metric 5722.2012\n",
      "Epoch 489 batch 200 train Loss 5.2407 test Loss 5.2567 with MSE metric 6493.4873\n",
      "Time taken for 1 epoch: 23.494112968444824 secs\n",
      "\n",
      "Epoch 490 batch 0 train Loss 5.2582 test Loss 5.3158 with MSE metric 6762.4229\n",
      "Epoch 490 batch 100 train Loss 5.2893 test Loss 5.3289 with MSE metric 7225.9736\n",
      "Epoch 490 batch 200 train Loss 5.3154 test Loss 5.3693 with MSE metric 7612.4375\n",
      "Time taken for 1 epoch: 23.72492003440857 secs\n",
      "\n",
      "Epoch 491 batch 0 train Loss 5.2469 test Loss 5.3664 with MSE metric 6613.1074\n",
      "Epoch 491 batch 100 train Loss 5.2424 test Loss 5.3650 with MSE metric 6486.6538\n",
      "Epoch 491 batch 200 train Loss 5.3311 test Loss 5.3896 with MSE metric 7765.9653\n",
      "Time taken for 1 epoch: 23.56756091117859 secs\n",
      "\n",
      "Epoch 492 batch 0 train Loss 5.4040 test Loss 5.2948 with MSE metric 8950.5029\n",
      "Epoch 492 batch 100 train Loss 5.2987 test Loss 5.4187 with MSE metric 7352.6411\n",
      "Epoch 492 batch 200 train Loss 5.1932 test Loss 5.2399 with MSE metric 5838.3882\n",
      "Time taken for 1 epoch: 23.55481505393982 secs\n",
      "\n",
      "Epoch 493 batch 0 train Loss 5.2702 test Loss 5.4479 with MSE metric 6909.0420\n",
      "Epoch 493 batch 100 train Loss 5.3066 test Loss 5.4433 with MSE metric 7471.1377\n",
      "Epoch 493 batch 200 train Loss 5.3125 test Loss 5.3621 with MSE metric 7568.7900\n",
      "Time taken for 1 epoch: 23.295607089996338 secs\n",
      "\n",
      "Epoch 494 batch 0 train Loss 5.2619 test Loss 5.3161 with MSE metric 6790.6260\n",
      "Epoch 494 batch 100 train Loss 5.3463 test Loss 5.3371 with MSE metric 8088.9395\n",
      "Epoch 494 batch 200 train Loss 5.2582 test Loss 5.3833 with MSE metric 6661.3330\n",
      "Time taken for 1 epoch: 23.008450984954834 secs\n",
      "\n",
      "Epoch 495 batch 0 train Loss 5.3703 test Loss 5.3531 with MSE metric 8361.6416\n",
      "Epoch 495 batch 100 train Loss 5.3206 test Loss 5.3322 with MSE metric 7690.2588\n",
      "Epoch 495 batch 200 train Loss 5.2223 test Loss 5.2706 with MSE metric 6256.8770\n",
      "Time taken for 1 epoch: 23.585854053497314 secs\n",
      "\n",
      "Epoch 496 batch 0 train Loss 5.2724 test Loss 5.2849 with MSE metric 6986.2017\n",
      "Epoch 496 batch 100 train Loss 5.1252 test Loss 5.3920 with MSE metric 4979.9287\n",
      "Epoch 496 batch 200 train Loss 5.2499 test Loss 5.3402 with MSE metric 6600.5259\n",
      "Time taken for 1 epoch: 23.532598972320557 secs\n",
      "\n",
      "Epoch 497 batch 0 train Loss 5.3084 test Loss 5.3067 with MSE metric 7507.2744\n",
      "Epoch 497 batch 100 train Loss 5.4024 test Loss 5.2193 with MSE metric 8810.0098\n",
      "Epoch 497 batch 200 train Loss 5.3142 test Loss 5.3536 with MSE metric 7574.4331\n",
      "Time taken for 1 epoch: 24.24459218978882 secs\n",
      "\n",
      "Epoch 498 batch 0 train Loss 5.2743 test Loss 5.3112 with MSE metric 7010.0791\n",
      "Epoch 498 batch 100 train Loss 5.2886 test Loss 5.2404 with MSE metric 7198.6318\n",
      "Epoch 498 batch 200 train Loss 5.3391 test Loss 5.3460 with MSE metric 7909.9546\n",
      "Time taken for 1 epoch: 24.399741888046265 secs\n",
      "\n",
      "Epoch 499 batch 0 train Loss 5.2478 test Loss 5.3199 with MSE metric 6597.4385\n",
      "Epoch 499 batch 100 train Loss 5.3192 test Loss 5.3310 with MSE metric 7666.3237\n",
      "Epoch 499 batch 200 train Loss 5.3220 test Loss 5.4836 with MSE metric 7685.2393\n",
      "Time taken for 1 epoch: 24.086517095565796 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam(0.0004)\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s=32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 100 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s= 32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
