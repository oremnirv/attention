{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, data_combine, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, t, token = data_combine.climate_data_to_model_input('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Restored from /Users/omernivron/Downloads/GPT_climate/ckpt/check_0/ckpt-19573\n",
      "Epoch 0 batch 0 train Loss 5.2927 test Loss 5.3505 with MSE metric 7275.1221\n",
      "Epoch 0 batch 100 train Loss 5.2945 test Loss 5.3048 with MSE metric 7300.6060\n",
      "Epoch 0 batch 200 train Loss 5.3605 test Loss 5.5079 with MSE metric 8308.3613\n",
      "Time taken for 1 epoch: 26.369020223617554 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 5.3342 test Loss 5.4670 with MSE metric 7899.3184\n",
      "Epoch 1 batch 100 train Loss 5.3044 test Loss 5.3164 with MSE metric 7447.8867\n",
      "Epoch 1 batch 200 train Loss 5.3215 test Loss 5.5254 with MSE metric 7637.6289\n",
      "Time taken for 1 epoch: 24.330953121185303 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 5.3616 test Loss 5.2796 with MSE metric 8343.4844\n",
      "Epoch 2 batch 100 train Loss 5.3445 test Loss 5.3949 with MSE metric 8069.1621\n",
      "Epoch 2 batch 200 train Loss 5.3408 test Loss 5.3492 with MSE metric 7946.4360\n",
      "Time taken for 1 epoch: 24.74238109588623 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 5.2888 test Loss 5.2884 with MSE metric 7186.2134\n",
      "Epoch 3 batch 100 train Loss 5.3160 test Loss 5.4663 with MSE metric 7622.3228\n",
      "Epoch 3 batch 200 train Loss 5.2611 test Loss 5.4240 with MSE metric 6676.4624\n",
      "Time taken for 1 epoch: 24.34378480911255 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 5.3035 test Loss 5.3945 with MSE metric 7426.8813\n",
      "Epoch 4 batch 100 train Loss 5.3348 test Loss 5.3782 with MSE metric 7881.7021\n",
      "Epoch 4 batch 200 train Loss 5.2904 test Loss 5.4090 with MSE metric 7209.9556\n",
      "Time taken for 1 epoch: 24.328545093536377 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 5.3796 test Loss 5.2302 with MSE metric 8577.3066\n",
      "Epoch 5 batch 100 train Loss 5.3881 test Loss 5.4034 with MSE metric 8733.5664\n",
      "Epoch 5 batch 200 train Loss 5.3764 test Loss 5.3024 with MSE metric 8396.5059\n",
      "Time taken for 1 epoch: 24.21567702293396 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 5.2591 test Loss 5.3446 with MSE metric 6791.6519\n",
      "Epoch 6 batch 100 train Loss 5.3620 test Loss 5.3849 with MSE metric 8327.0293\n",
      "Epoch 6 batch 200 train Loss 5.3356 test Loss 5.4087 with MSE metric 7915.6855\n",
      "Time taken for 1 epoch: 24.408517837524414 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 5.2164 test Loss 5.5053 with MSE metric 6079.3623\n",
      "Epoch 7 batch 100 train Loss 5.2867 test Loss 5.2885 with MSE metric 7134.7056\n",
      "Epoch 7 batch 200 train Loss 5.3011 test Loss 5.3977 with MSE metric 7398.3032\n",
      "Time taken for 1 epoch: 24.22480010986328 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 5.3571 test Loss 5.2915 with MSE metric 8190.5771\n",
      "Epoch 8 batch 100 train Loss 5.2302 test Loss 5.3801 with MSE metric 6368.7656\n",
      "Epoch 8 batch 200 train Loss 5.2189 test Loss 5.3721 with MSE metric 6141.3662\n",
      "Time taken for 1 epoch: 23.86680316925049 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 5.2994 test Loss 5.3722 with MSE metric 7365.6797\n",
      "Epoch 9 batch 100 train Loss 5.2767 test Loss 5.2668 with MSE metric 7010.7520\n",
      "Epoch 9 batch 200 train Loss 5.3023 test Loss 5.3374 with MSE metric 7415.1606\n",
      "Time taken for 1 epoch: 24.15740418434143 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 5.3347 test Loss 5.3246 with MSE metric 7905.6162\n",
      "Epoch 10 batch 100 train Loss 5.2376 test Loss 5.3920 with MSE metric 6447.4053\n",
      "Epoch 10 batch 200 train Loss 5.3465 test Loss 5.2695 with MSE metric 8061.8496\n",
      "Time taken for 1 epoch: 23.52823305130005 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 5.3420 test Loss 5.3121 with MSE metric 7943.4033\n",
      "Epoch 11 batch 100 train Loss 5.3456 test Loss 5.3515 with MSE metric 8043.2109\n",
      "Epoch 11 batch 200 train Loss 5.2752 test Loss 5.3623 with MSE metric 7025.1777\n",
      "Time taken for 1 epoch: 23.572407007217407 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 5.3421 test Loss 5.3593 with MSE metric 8028.2354\n",
      "Epoch 12 batch 100 train Loss 5.3662 test Loss 5.3129 with MSE metric 8398.2051\n",
      "Epoch 12 batch 200 train Loss 5.2498 test Loss 5.2962 with MSE metric 6666.6831\n",
      "Time taken for 1 epoch: 23.513895273208618 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 5.2367 test Loss 5.3010 with MSE metric 6395.8530\n",
      "Epoch 13 batch 100 train Loss 5.3304 test Loss 5.3676 with MSE metric 7811.2866\n",
      "Epoch 13 batch 200 train Loss 5.3006 test Loss 5.4263 with MSE metric 7387.6348\n",
      "Time taken for 1 epoch: 23.647620916366577 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 5.1113 test Loss 5.2842 with MSE metric 4523.4492\n",
      "Epoch 14 batch 100 train Loss 5.3395 test Loss 5.3580 with MSE metric 7965.8853\n",
      "Epoch 14 batch 200 train Loss 5.2894 test Loss 5.2816 with MSE metric 7226.6763\n",
      "Time taken for 1 epoch: 24.424686193466187 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 5.2987 test Loss 5.3395 with MSE metric 7349.3179\n",
      "Epoch 15 batch 100 train Loss 5.3227 test Loss 5.3475 with MSE metric 7723.5166\n",
      "Epoch 15 batch 200 train Loss 5.3851 test Loss 5.4117 with MSE metric 8557.9199\n",
      "Time taken for 1 epoch: 24.240747928619385 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 5.3127 test Loss 5.3423 with MSE metric 7569.6543\n",
      "Epoch 16 batch 100 train Loss 5.3307 test Loss 5.2841 with MSE metric 7827.7114\n",
      "Epoch 16 batch 200 train Loss 5.3602 test Loss 5.3920 with MSE metric 8296.3311\n",
      "Time taken for 1 epoch: 24.434716939926147 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 5.2787 test Loss 5.3076 with MSE metric 7038.5986\n",
      "Epoch 17 batch 100 train Loss 5.3361 test Loss 5.4171 with MSE metric 7920.4287\n",
      "Epoch 17 batch 200 train Loss 5.3463 test Loss 5.4815 with MSE metric 8066.0996\n",
      "Time taken for 1 epoch: 23.869997024536133 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 5.2529 test Loss 5.2987 with MSE metric 6712.7461\n",
      "Epoch 18 batch 100 train Loss 5.3625 test Loss 5.3779 with MSE metric 8278.9258\n",
      "Epoch 18 batch 200 train Loss 5.2574 test Loss 5.2505 with MSE metric 6779.3042\n",
      "Time taken for 1 epoch: 23.787750959396362 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 5.3764 test Loss 5.3271 with MSE metric 8493.6699\n",
      "Epoch 19 batch 100 train Loss 5.4053 test Loss 5.3581 with MSE metric 8906.2422\n",
      "Epoch 19 batch 200 train Loss 5.2672 test Loss 5.3615 with MSE metric 6893.2534\n",
      "Time taken for 1 epoch: 24.609081745147705 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 5.2544 test Loss 5.2729 with MSE metric 6706.1665\n",
      "Epoch 20 batch 100 train Loss 5.3260 test Loss 5.3426 with MSE metric 7745.7822\n",
      "Epoch 20 batch 200 train Loss 5.3432 test Loss 5.3960 with MSE metric 8032.5898\n",
      "Time taken for 1 epoch: 24.60156488418579 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 5.3084 test Loss 5.3438 with MSE metric 7495.4722\n",
      "Epoch 21 batch 100 train Loss 5.3075 test Loss 5.3243 with MSE metric 7492.8330\n",
      "Epoch 21 batch 200 train Loss 5.2532 test Loss 5.4254 with MSE metric 6608.4785\n",
      "Time taken for 1 epoch: 24.61721920967102 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 5.3671 test Loss 5.3866 with MSE metric 8433.2529\n",
      "Epoch 22 batch 100 train Loss 5.2673 test Loss 5.3236 with MSE metric 6911.8379\n",
      "Epoch 22 batch 200 train Loss 5.2765 test Loss 5.3790 with MSE metric 7037.2910\n",
      "Time taken for 1 epoch: 23.12402105331421 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 5.3212 test Loss 5.2885 with MSE metric 7662.3604\n",
      "Epoch 23 batch 100 train Loss 5.4072 test Loss 5.3179 with MSE metric 8996.3428\n",
      "Epoch 23 batch 200 train Loss 5.2293 test Loss 5.3070 with MSE metric 6316.1221\n",
      "Time taken for 1 epoch: 23.293179273605347 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 5.2047 test Loss 5.4426 with MSE metric 5836.9482\n",
      "Epoch 24 batch 100 train Loss 5.3940 test Loss 5.3585 with MSE metric 8771.2773\n",
      "Epoch 24 batch 200 train Loss 5.3240 test Loss 5.4320 with MSE metric 7728.1523\n",
      "Time taken for 1 epoch: 23.318416118621826 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 5.2210 test Loss 5.2699 with MSE metric 6186.1660\n",
      "Epoch 25 batch 100 train Loss 5.2152 test Loss 5.3532 with MSE metric 5989.8052\n",
      "Epoch 25 batch 200 train Loss 5.2683 test Loss 5.3820 with MSE metric 6927.9219\n",
      "Time taken for 1 epoch: 23.25557518005371 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 5.3162 test Loss 5.2872 with MSE metric 7624.4463\n",
      "Epoch 26 batch 100 train Loss 5.3675 test Loss 5.3406 with MSE metric 8408.1738\n",
      "Epoch 26 batch 200 train Loss 5.3803 test Loss 5.2245 with MSE metric 8610.2129\n",
      "Time taken for 1 epoch: 23.63508677482605 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 5.3550 test Loss 5.4467 with MSE metric 8156.2305\n",
      "Epoch 27 batch 100 train Loss 5.2571 test Loss 5.4147 with MSE metric 6769.6562\n",
      "Epoch 27 batch 200 train Loss 5.2024 test Loss 5.3333 with MSE metric 5948.1338\n",
      "Time taken for 1 epoch: 23.179907083511353 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 5.2071 test Loss 5.3266 with MSE metric 6016.2993\n",
      "Epoch 28 batch 100 train Loss 5.3634 test Loss 5.4156 with MSE metric 8290.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 batch 200 train Loss 5.4150 test Loss 5.3162 with MSE metric 9051.6963\n",
      "Time taken for 1 epoch: 23.093926906585693 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 5.3094 test Loss 5.4605 with MSE metric 7516.2891\n",
      "Epoch 29 batch 100 train Loss 5.2739 test Loss 5.3807 with MSE metric 6956.3340\n",
      "Epoch 29 batch 200 train Loss 5.2576 test Loss 5.3476 with MSE metric 6745.0264\n",
      "Time taken for 1 epoch: 23.10165500640869 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 5.2898 test Loss 5.4235 with MSE metric 7221.7285\n",
      "Epoch 30 batch 100 train Loss 5.2868 test Loss 5.3276 with MSE metric 7178.3320\n",
      "Epoch 30 batch 200 train Loss 5.2720 test Loss 5.2856 with MSE metric 6977.1328\n",
      "Time taken for 1 epoch: 23.20391607284546 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 5.2899 test Loss 5.2574 with MSE metric 7219.4038\n",
      "Epoch 31 batch 100 train Loss 5.3167 test Loss 5.3384 with MSE metric 7623.7651\n",
      "Epoch 31 batch 200 train Loss 5.3365 test Loss 5.3841 with MSE metric 7932.7373\n",
      "Time taken for 1 epoch: 23.141672134399414 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 5.2900 test Loss 5.2687 with MSE metric 7234.7656\n",
      "Epoch 32 batch 100 train Loss 5.2833 test Loss 5.3757 with MSE metric 7137.8140\n",
      "Epoch 32 batch 200 train Loss 5.2591 test Loss 5.3253 with MSE metric 6801.1123\n",
      "Time taken for 1 epoch: 23.142322063446045 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 5.2784 test Loss 5.3202 with MSE metric 7057.3926\n",
      "Epoch 33 batch 100 train Loss 5.2477 test Loss 5.3407 with MSE metric 6585.5947\n",
      "Epoch 33 batch 200 train Loss 5.2607 test Loss 5.3621 with MSE metric 6819.1074\n",
      "Time taken for 1 epoch: 23.13906478881836 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 5.1916 test Loss 5.3588 with MSE metric 5670.3027\n",
      "Epoch 34 batch 100 train Loss 5.2379 test Loss 5.3743 with MSE metric 6434.8955\n",
      "Epoch 34 batch 200 train Loss 5.2812 test Loss 5.2785 with MSE metric 7083.5352\n",
      "Time taken for 1 epoch: 22.79763102531433 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 5.2990 test Loss 5.3891 with MSE metric 7366.3696\n",
      "Epoch 35 batch 100 train Loss 5.3294 test Loss 5.3942 with MSE metric 7822.3701\n",
      "Epoch 35 batch 200 train Loss 5.4267 test Loss 5.3925 with MSE metric 9125.9922\n",
      "Time taken for 1 epoch: 21.695746898651123 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 5.3469 test Loss 5.3393 with MSE metric 8073.6997\n",
      "Epoch 36 batch 100 train Loss 5.2548 test Loss 5.4100 with MSE metric 6735.3516\n",
      "Epoch 36 batch 200 train Loss 5.3024 test Loss 5.3859 with MSE metric 7412.2681\n",
      "Time taken for 1 epoch: 21.755704164505005 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 5.3110 test Loss 5.4146 with MSE metric 7544.2021\n",
      "Epoch 37 batch 100 train Loss 5.3339 test Loss 5.4542 with MSE metric 7895.1904\n",
      "Epoch 37 batch 200 train Loss 5.2843 test Loss 5.3243 with MSE metric 7144.4219\n",
      "Time taken for 1 epoch: 21.74980092048645 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 5.2387 test Loss 5.3930 with MSE metric 6520.2090\n",
      "Epoch 38 batch 100 train Loss 5.1880 test Loss 5.4151 with MSE metric 5874.2983\n",
      "Epoch 38 batch 200 train Loss 5.2448 test Loss 5.3518 with MSE metric 6599.0132\n",
      "Time taken for 1 epoch: 21.669992923736572 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 5.2988 test Loss 5.3300 with MSE metric 7364.4185\n",
      "Epoch 39 batch 100 train Loss 5.2048 test Loss 5.4117 with MSE metric 6060.4443\n",
      "Epoch 39 batch 200 train Loss 5.3709 test Loss 5.4090 with MSE metric 8427.3242\n",
      "Time taken for 1 epoch: 21.73150897026062 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 5.3102 test Loss 5.3322 with MSE metric 7534.3906\n",
      "Epoch 40 batch 100 train Loss 5.3368 test Loss 5.3947 with MSE metric 7936.4336\n",
      "Epoch 40 batch 200 train Loss 5.2669 test Loss 5.3565 with MSE metric 6890.3301\n",
      "Time taken for 1 epoch: 21.74260401725769 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 5.2717 test Loss 5.3825 with MSE metric 6917.7090\n",
      "Epoch 41 batch 100 train Loss 5.2726 test Loss 5.3957 with MSE metric 6987.9253\n",
      "Epoch 41 batch 200 train Loss 5.2639 test Loss 5.4414 with MSE metric 6867.4683\n",
      "Time taken for 1 epoch: 21.685012102127075 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 5.3495 test Loss 5.3132 with MSE metric 8140.5352\n",
      "Epoch 42 batch 100 train Loss 5.2320 test Loss 5.4886 with MSE metric 6387.1895\n",
      "Epoch 42 batch 200 train Loss 5.2854 test Loss 5.4429 with MSE metric 7093.2432\n",
      "Time taken for 1 epoch: 21.71090579032898 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 5.1839 test Loss 5.3632 with MSE metric 5659.1045\n",
      "Epoch 43 batch 100 train Loss 5.2178 test Loss 5.3692 with MSE metric 6150.1670\n",
      "Epoch 43 batch 200 train Loss 5.2386 test Loss 5.3624 with MSE metric 6455.0322\n",
      "Time taken for 1 epoch: 21.91931414604187 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 5.3257 test Loss 5.3579 with MSE metric 7760.2769\n",
      "Epoch 44 batch 100 train Loss 5.2396 test Loss 5.2838 with MSE metric 6513.7930\n",
      "Epoch 44 batch 200 train Loss 5.3545 test Loss 5.3432 with MSE metric 8227.8262\n",
      "Time taken for 1 epoch: 23.261245012283325 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 5.3068 test Loss 5.3656 with MSE metric 7473.1846\n",
      "Epoch 45 batch 100 train Loss 5.3249 test Loss 5.2872 with MSE metric 7728.1533\n",
      "Epoch 45 batch 200 train Loss 5.2165 test Loss 5.2809 with MSE metric 5968.7476\n",
      "Time taken for 1 epoch: 23.50497007369995 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 5.2502 test Loss 5.3757 with MSE metric 6675.9160\n",
      "Epoch 46 batch 100 train Loss 5.3186 test Loss 5.3046 with MSE metric 7629.7646\n",
      "Epoch 46 batch 200 train Loss 5.2129 test Loss 5.4196 with MSE metric 6130.5029\n",
      "Time taken for 1 epoch: 25.137787103652954 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 5.3108 test Loss 5.3312 with MSE metric 7537.2173\n",
      "Epoch 47 batch 100 train Loss 5.2950 test Loss 5.3489 with MSE metric 7308.4976\n",
      "Epoch 47 batch 200 train Loss 5.3677 test Loss 5.3729 with MSE metric 8381.5840\n",
      "Time taken for 1 epoch: 22.622912883758545 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 5.2386 test Loss 5.3089 with MSE metric 6474.9961\n",
      "Epoch 48 batch 100 train Loss 5.3475 test Loss 5.3139 with MSE metric 8022.8730\n",
      "Epoch 48 batch 200 train Loss 5.2689 test Loss 5.2983 with MSE metric 6903.5693\n",
      "Time taken for 1 epoch: 24.153971910476685 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 5.2947 test Loss 5.3788 with MSE metric 7300.8135\n",
      "Epoch 49 batch 100 train Loss 5.2223 test Loss 5.2753 with MSE metric 6273.1484\n",
      "Epoch 49 batch 200 train Loss 5.2563 test Loss 5.3726 with MSE metric 6686.4658\n",
      "Time taken for 1 epoch: 22.46193838119507 secs\n",
      "\n",
      "Epoch 50 batch 0 train Loss 5.2773 test Loss 5.3111 with MSE metric 7036.7910\n",
      "Epoch 50 batch 100 train Loss 5.2355 test Loss 5.2998 with MSE metric 6446.1768\n",
      "Epoch 50 batch 200 train Loss 5.3346 test Loss 5.3970 with MSE metric 7886.0103\n",
      "Time taken for 1 epoch: 22.940948963165283 secs\n",
      "\n",
      "Epoch 51 batch 0 train Loss 5.3352 test Loss 5.3445 with MSE metric 7914.6890\n",
      "Epoch 51 batch 100 train Loss 5.2781 test Loss 5.3172 with MSE metric 7038.6772\n",
      "Epoch 51 batch 200 train Loss 5.2617 test Loss 5.3307 with MSE metric 6818.5908\n",
      "Time taken for 1 epoch: 22.41071605682373 secs\n",
      "\n",
      "Epoch 52 batch 0 train Loss 5.3526 test Loss 5.2645 with MSE metric 8188.9209\n",
      "Epoch 52 batch 100 train Loss 5.2109 test Loss 5.3836 with MSE metric 6110.4678\n",
      "Epoch 52 batch 200 train Loss 5.3637 test Loss 5.2542 with MSE metric 8317.2432\n",
      "Time taken for 1 epoch: 22.815463304519653 secs\n",
      "\n",
      "Epoch 53 batch 0 train Loss 5.3561 test Loss 5.3383 with MSE metric 8207.7773\n",
      "Epoch 53 batch 100 train Loss 5.3357 test Loss 5.4345 with MSE metric 7897.5562\n",
      "Epoch 53 batch 200 train Loss 5.3423 test Loss 5.4454 with MSE metric 8033.8257\n",
      "Time taken for 1 epoch: 23.38223910331726 secs\n",
      "\n",
      "Epoch 54 batch 0 train Loss 5.3319 test Loss 5.2496 with MSE metric 7846.5664\n",
      "Epoch 54 batch 100 train Loss 5.3197 test Loss 5.3931 with MSE metric 7676.0166\n",
      "Epoch 54 batch 200 train Loss 5.2595 test Loss 5.3749 with MSE metric 6778.0059\n",
      "Time taken for 1 epoch: 22.93171501159668 secs\n",
      "\n",
      "Epoch 55 batch 0 train Loss 5.3538 test Loss 5.3418 with MSE metric 8187.7080\n",
      "Epoch 55 batch 100 train Loss 5.3164 test Loss 5.3899 with MSE metric 7611.4326\n",
      "Epoch 55 batch 200 train Loss 5.4278 test Loss 5.3810 with MSE metric 9116.5918\n",
      "Time taken for 1 epoch: 23.543291091918945 secs\n",
      "\n",
      "Epoch 56 batch 0 train Loss 5.3550 test Loss 5.3865 with MSE metric 8097.0996\n",
      "Epoch 56 batch 100 train Loss 5.3623 test Loss 5.3630 with MSE metric 8281.0723\n",
      "Epoch 56 batch 200 train Loss 5.3341 test Loss 5.2972 with MSE metric 7880.1133\n",
      "Time taken for 1 epoch: 23.54235291481018 secs\n",
      "\n",
      "Epoch 57 batch 0 train Loss 5.3023 test Loss 5.4498 with MSE metric 7415.8047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 batch 100 train Loss 5.3342 test Loss 5.2860 with MSE metric 7903.9614\n",
      "Epoch 57 batch 200 train Loss 5.2155 test Loss 5.3844 with MSE metric 6177.2295\n",
      "Time taken for 1 epoch: 23.81431818008423 secs\n",
      "\n",
      "Epoch 58 batch 0 train Loss 5.3856 test Loss 5.3280 with MSE metric 8585.5605\n",
      "Epoch 58 batch 100 train Loss 5.3562 test Loss 5.3146 with MSE metric 8199.5322\n",
      "Epoch 58 batch 200 train Loss 5.3329 test Loss 5.4534 with MSE metric 7877.0811\n",
      "Time taken for 1 epoch: 23.219605922698975 secs\n",
      "\n",
      "Epoch 59 batch 0 train Loss 5.3535 test Loss 5.3049 with MSE metric 8191.1978\n",
      "Epoch 59 batch 100 train Loss 5.3660 test Loss 5.4579 with MSE metric 8412.7725\n",
      "Epoch 59 batch 200 train Loss 5.2728 test Loss 5.3766 with MSE metric 6985.8491\n",
      "Time taken for 1 epoch: 23.743059873580933 secs\n",
      "\n",
      "Epoch 60 batch 0 train Loss 5.3475 test Loss 5.3626 with MSE metric 8085.1426\n",
      "Epoch 60 batch 100 train Loss 5.3652 test Loss 5.4105 with MSE metric 8365.5898\n",
      "Epoch 60 batch 200 train Loss 5.2795 test Loss 5.2826 with MSE metric 6982.2563\n",
      "Time taken for 1 epoch: 23.414714097976685 secs\n",
      "\n",
      "Epoch 61 batch 0 train Loss 5.2677 test Loss 5.3739 with MSE metric 6911.4707\n",
      "Epoch 61 batch 100 train Loss 5.2544 test Loss 5.4710 with MSE metric 6706.5391\n",
      "Epoch 61 batch 200 train Loss 5.1879 test Loss 5.4169 with MSE metric 5752.3701\n",
      "Time taken for 1 epoch: 23.739200830459595 secs\n",
      "\n",
      "Epoch 62 batch 0 train Loss 5.3029 test Loss 5.3743 with MSE metric 7392.2769\n",
      "Epoch 62 batch 100 train Loss 5.2626 test Loss 5.3926 with MSE metric 6813.1616\n",
      "Epoch 62 batch 200 train Loss 5.3772 test Loss 5.2935 with MSE metric 8513.1865\n",
      "Time taken for 1 epoch: 23.68641209602356 secs\n",
      "\n",
      "Epoch 63 batch 0 train Loss 5.2557 test Loss 5.3502 with MSE metric 6709.4707\n",
      "Epoch 63 batch 100 train Loss 5.3273 test Loss 5.4792 with MSE metric 7757.3643\n",
      "Epoch 63 batch 200 train Loss 5.2333 test Loss 5.3262 with MSE metric 6406.0649\n",
      "Time taken for 1 epoch: 23.71346092224121 secs\n",
      "\n",
      "Epoch 64 batch 0 train Loss 5.3625 test Loss 5.4262 with MSE metric 8306.4375\n",
      "Epoch 64 batch 100 train Loss 5.2911 test Loss 5.2814 with MSE metric 7165.0518\n",
      "Epoch 64 batch 200 train Loss 5.3504 test Loss 5.2944 with MSE metric 8140.7852\n",
      "Time taken for 1 epoch: 24.178110122680664 secs\n",
      "\n",
      "Epoch 65 batch 0 train Loss 5.3545 test Loss 5.3819 with MSE metric 8174.2725\n",
      "Epoch 65 batch 100 train Loss 5.2780 test Loss 5.2505 with MSE metric 7059.0063\n",
      "Epoch 65 batch 200 train Loss 5.3349 test Loss 5.5651 with MSE metric 7903.1318\n",
      "Time taken for 1 epoch: 23.665320873260498 secs\n",
      "\n",
      "Epoch 66 batch 0 train Loss 5.2207 test Loss 5.3702 with MSE metric 6256.1895\n",
      "Epoch 66 batch 100 train Loss 5.3241 test Loss 5.3509 with MSE metric 7746.4463\n",
      "Epoch 66 batch 200 train Loss 5.2869 test Loss 5.3488 with MSE metric 7178.3701\n",
      "Time taken for 1 epoch: 24.02789831161499 secs\n",
      "\n",
      "Epoch 67 batch 0 train Loss 5.2626 test Loss 5.3514 with MSE metric 6771.6812\n",
      "Epoch 67 batch 100 train Loss 5.2395 test Loss 5.3654 with MSE metric 6500.5210\n",
      "Epoch 67 batch 200 train Loss 5.2961 test Loss 5.3855 with MSE metric 7323.8594\n",
      "Time taken for 1 epoch: 24.507275819778442 secs\n",
      "\n",
      "Epoch 68 batch 0 train Loss 5.2957 test Loss 5.2868 with MSE metric 7317.3984\n",
      "Epoch 68 batch 100 train Loss 5.3357 test Loss 5.4291 with MSE metric 7928.4238\n",
      "Epoch 68 batch 200 train Loss 5.2183 test Loss 5.3478 with MSE metric 6108.9004\n",
      "Time taken for 1 epoch: 26.029540061950684 secs\n",
      "\n",
      "Epoch 69 batch 0 train Loss 5.2006 test Loss 5.3849 with MSE metric 5887.1689\n",
      "Epoch 69 batch 100 train Loss 5.2142 test Loss 5.3555 with MSE metric 6115.0303\n",
      "Epoch 69 batch 200 train Loss 5.2663 test Loss 5.2711 with MSE metric 6897.4829\n",
      "Time taken for 1 epoch: 23.56401491165161 secs\n",
      "\n",
      "Epoch 70 batch 0 train Loss 5.3724 test Loss 5.3896 with MSE metric 8369.9219\n",
      "Epoch 70 batch 100 train Loss 5.2179 test Loss 5.3816 with MSE metric 6152.1548\n",
      "Epoch 70 batch 200 train Loss 5.3622 test Loss 5.4035 with MSE metric 8271.4707\n",
      "Time taken for 1 epoch: 23.299368143081665 secs\n",
      "\n",
      "Epoch 71 batch 0 train Loss 5.3229 test Loss 5.3431 with MSE metric 7714.1865\n",
      "Epoch 71 batch 100 train Loss 5.3706 test Loss 5.3884 with MSE metric 8434.0518\n",
      "Epoch 71 batch 200 train Loss 5.2140 test Loss 5.4449 with MSE metric 6130.3438\n",
      "Time taken for 1 epoch: 23.06098198890686 secs\n",
      "\n",
      "Epoch 72 batch 0 train Loss 5.3227 test Loss 5.3507 with MSE metric 7720.7207\n",
      "Epoch 72 batch 100 train Loss 5.3061 test Loss 5.3691 with MSE metric 7472.8721\n",
      "Epoch 72 batch 200 train Loss 5.2245 test Loss 5.3672 with MSE metric 6308.3330\n",
      "Time taken for 1 epoch: 24.4458429813385 secs\n",
      "\n",
      "Epoch 73 batch 0 train Loss 5.2776 test Loss 5.2418 with MSE metric 7045.3242\n",
      "Epoch 73 batch 100 train Loss 5.3334 test Loss 5.4291 with MSE metric 7891.0933\n",
      "Epoch 73 batch 200 train Loss 5.3072 test Loss 5.3112 with MSE metric 7474.4385\n",
      "Time taken for 1 epoch: 24.205458879470825 secs\n",
      "\n",
      "Epoch 74 batch 0 train Loss 5.3557 test Loss 5.3547 with MSE metric 8171.8979\n",
      "Epoch 74 batch 100 train Loss 5.3426 test Loss 5.3826 with MSE metric 8021.8594\n",
      "Epoch 74 batch 200 train Loss 5.2922 test Loss 5.3038 with MSE metric 7253.1113\n",
      "Time taken for 1 epoch: 23.999562978744507 secs\n",
      "\n",
      "Epoch 75 batch 0 train Loss 5.2007 test Loss 5.4192 with MSE metric 5908.2090\n",
      "Epoch 75 batch 100 train Loss 5.2964 test Loss 5.3631 with MSE metric 7327.9678\n",
      "Epoch 75 batch 200 train Loss 5.1856 test Loss 5.4486 with MSE metric 5618.7832\n",
      "Time taken for 1 epoch: 23.197250843048096 secs\n",
      "\n",
      "Epoch 76 batch 0 train Loss 5.2876 test Loss 5.3537 with MSE metric 7131.1533\n",
      "Epoch 76 batch 100 train Loss 5.2829 test Loss 5.3044 with MSE metric 7127.0098\n",
      "Epoch 76 batch 200 train Loss 5.2807 test Loss 5.3448 with MSE metric 7101.8149\n",
      "Time taken for 1 epoch: 23.25148320198059 secs\n",
      "\n",
      "Epoch 77 batch 0 train Loss 5.3794 test Loss 5.3653 with MSE metric 8504.9375\n",
      "Epoch 77 batch 100 train Loss 5.3599 test Loss 5.3517 with MSE metric 8224.9268\n",
      "Epoch 77 batch 200 train Loss 5.2412 test Loss 5.2968 with MSE metric 6523.2197\n",
      "Time taken for 1 epoch: 23.36046004295349 secs\n",
      "\n",
      "Epoch 78 batch 0 train Loss 5.3684 test Loss 5.4526 with MSE metric 8404.1768\n",
      "Epoch 78 batch 100 train Loss 5.3404 test Loss 5.3752 with MSE metric 7985.8804\n",
      "Epoch 78 batch 200 train Loss 5.1835 test Loss 5.3926 with MSE metric 5669.9624\n",
      "Time taken for 1 epoch: 23.912135124206543 secs\n",
      "\n",
      "Epoch 79 batch 0 train Loss 5.2621 test Loss 5.3759 with MSE metric 6841.8091\n",
      "Epoch 79 batch 100 train Loss 5.3369 test Loss 5.3500 with MSE metric 7892.6953\n",
      "Epoch 79 batch 200 train Loss 5.2175 test Loss 5.3033 with MSE metric 6186.4443\n",
      "Time taken for 1 epoch: 23.330199718475342 secs\n",
      "\n",
      "Epoch 80 batch 0 train Loss 5.3061 test Loss 5.4405 with MSE metric 7471.6938\n",
      "Epoch 80 batch 100 train Loss 5.3432 test Loss 5.3604 with MSE metric 8041.6836\n",
      "Epoch 80 batch 200 train Loss 5.2689 test Loss 5.4700 with MSE metric 6920.6792\n",
      "Time taken for 1 epoch: 23.28314995765686 secs\n",
      "\n",
      "Epoch 81 batch 0 train Loss 5.2902 test Loss 5.3202 with MSE metric 7234.6387\n",
      "Epoch 81 batch 100 train Loss 5.2441 test Loss 5.3095 with MSE metric 6590.1216\n",
      "Epoch 81 batch 200 train Loss 5.2082 test Loss 5.3859 with MSE metric 6061.5562\n",
      "Time taken for 1 epoch: 23.32364511489868 secs\n",
      "\n",
      "Epoch 82 batch 0 train Loss 5.3637 test Loss 5.3566 with MSE metric 8317.5771\n",
      "Epoch 82 batch 100 train Loss 5.3341 test Loss 5.2829 with MSE metric 7871.2676\n",
      "Epoch 82 batch 200 train Loss 5.3226 test Loss 5.3367 with MSE metric 7722.7358\n",
      "Time taken for 1 epoch: 24.57183074951172 secs\n",
      "\n",
      "Epoch 83 batch 0 train Loss 5.3606 test Loss 5.3038 with MSE metric 8301.9297\n",
      "Epoch 83 batch 100 train Loss 5.3383 test Loss 5.4575 with MSE metric 7927.5728\n",
      "Epoch 83 batch 200 train Loss 5.3323 test Loss 5.3397 with MSE metric 7833.0596\n",
      "Time taken for 1 epoch: 24.290632963180542 secs\n",
      "\n",
      "Epoch 84 batch 0 train Loss 5.2707 test Loss 5.3205 with MSE metric 6936.1641\n",
      "Epoch 84 batch 100 train Loss 5.2382 test Loss 5.3757 with MSE metric 6504.2568\n",
      "Epoch 84 batch 200 train Loss 5.3297 test Loss 5.2924 with MSE metric 7819.2520\n",
      "Time taken for 1 epoch: 23.580042123794556 secs\n",
      "\n",
      "Epoch 85 batch 0 train Loss 5.3529 test Loss 5.2623 with MSE metric 8194.5957\n",
      "Epoch 85 batch 100 train Loss 5.2637 test Loss 5.4702 with MSE metric 6850.9932\n",
      "Epoch 85 batch 200 train Loss 5.3368 test Loss 5.3363 with MSE metric 7900.4263\n",
      "Time taken for 1 epoch: 23.821041107177734 secs\n",
      "\n",
      "Epoch 86 batch 0 train Loss 5.3003 test Loss 5.3933 with MSE metric 7385.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 batch 100 train Loss 5.2901 test Loss 5.4448 with MSE metric 7237.1523\n",
      "Epoch 86 batch 200 train Loss 5.2517 test Loss 5.2787 with MSE metric 6608.3613\n",
      "Time taken for 1 epoch: 23.277854204177856 secs\n",
      "\n",
      "Epoch 87 batch 0 train Loss 5.2427 test Loss 5.5196 with MSE metric 6507.5483\n",
      "Epoch 87 batch 100 train Loss 5.3727 test Loss 5.2662 with MSE metric 8434.9941\n",
      "Epoch 87 batch 200 train Loss 5.3417 test Loss 5.3215 with MSE metric 7992.1055\n",
      "Time taken for 1 epoch: 23.48769998550415 secs\n",
      "\n",
      "Epoch 88 batch 0 train Loss 5.2513 test Loss 5.3184 with MSE metric 6648.8486\n",
      "Epoch 88 batch 100 train Loss 5.3053 test Loss 5.3465 with MSE metric 7449.3945\n",
      "Epoch 88 batch 200 train Loss 5.3224 test Loss 5.4345 with MSE metric 7720.5034\n",
      "Time taken for 1 epoch: 23.442229986190796 secs\n",
      "\n",
      "Epoch 89 batch 0 train Loss 5.2239 test Loss 5.3646 with MSE metric 6305.1846\n",
      "Epoch 89 batch 100 train Loss 5.3448 test Loss 5.3968 with MSE metric 8065.4351\n",
      "Epoch 89 batch 200 train Loss 5.3694 test Loss 5.3304 with MSE metric 8413.4570\n",
      "Time taken for 1 epoch: 23.88442301750183 secs\n",
      "\n",
      "Epoch 90 batch 0 train Loss 5.2239 test Loss 5.3520 with MSE metric 6317.2729\n",
      "Epoch 90 batch 100 train Loss 5.3788 test Loss 5.4290 with MSE metric 8533.6074\n",
      "Epoch 90 batch 200 train Loss 5.3015 test Loss 5.4529 with MSE metric 7357.9829\n",
      "Time taken for 1 epoch: 23.628491163253784 secs\n",
      "\n",
      "Epoch 91 batch 0 train Loss 5.3473 test Loss 5.3334 with MSE metric 8076.0859\n",
      "Epoch 91 batch 100 train Loss 5.2428 test Loss 5.3855 with MSE metric 6546.5000\n",
      "Epoch 91 batch 200 train Loss 5.2553 test Loss 5.2545 with MSE metric 6743.4512\n",
      "Time taken for 1 epoch: 23.86954689025879 secs\n",
      "\n",
      "Epoch 92 batch 0 train Loss 5.3715 test Loss 5.3067 with MSE metric 8309.0010\n",
      "Epoch 92 batch 100 train Loss 5.2568 test Loss 5.4357 with MSE metric 6730.6167\n",
      "Epoch 92 batch 200 train Loss 5.3521 test Loss 5.3198 with MSE metric 8166.6558\n",
      "Time taken for 1 epoch: 24.014246940612793 secs\n",
      "\n",
      "Epoch 93 batch 0 train Loss 5.2722 test Loss 5.2780 with MSE metric 6932.1104\n",
      "Epoch 93 batch 100 train Loss 5.3154 test Loss 5.3618 with MSE metric 7584.9741\n",
      "Epoch 93 batch 200 train Loss 5.3092 test Loss 5.3459 with MSE metric 7517.8203\n",
      "Time taken for 1 epoch: 23.329252243041992 secs\n",
      "\n",
      "Epoch 94 batch 0 train Loss 5.3336 test Loss 5.3180 with MSE metric 7895.5088\n",
      "Epoch 94 batch 100 train Loss 5.3127 test Loss 5.3590 with MSE metric 7553.3682\n",
      "Epoch 94 batch 200 train Loss 5.3499 test Loss 5.3995 with MSE metric 8137.1982\n",
      "Time taken for 1 epoch: 23.28516125679016 secs\n",
      "\n",
      "Epoch 95 batch 0 train Loss 5.3259 test Loss 5.3076 with MSE metric 7739.3735\n",
      "Epoch 95 batch 100 train Loss 5.3251 test Loss 5.2675 with MSE metric 7729.7061\n",
      "Epoch 95 batch 200 train Loss 5.3011 test Loss 5.3469 with MSE metric 7385.3843\n",
      "Time taken for 1 epoch: 23.364455938339233 secs\n",
      "\n",
      "Epoch 96 batch 0 train Loss 5.2818 test Loss 5.4209 with MSE metric 7113.7017\n",
      "Epoch 96 batch 100 train Loss 5.3233 test Loss 5.4253 with MSE metric 7672.2339\n",
      "Epoch 96 batch 200 train Loss 5.3378 test Loss 5.3955 with MSE metric 7959.8940\n",
      "Time taken for 1 epoch: 24.82601284980774 secs\n",
      "\n",
      "Epoch 97 batch 0 train Loss 5.3199 test Loss 5.3185 with MSE metric 7679.1016\n",
      "Epoch 97 batch 100 train Loss 5.2844 test Loss 5.3437 with MSE metric 7154.8159\n",
      "Epoch 97 batch 200 train Loss 5.3124 test Loss 5.3332 with MSE metric 7551.8457\n",
      "Time taken for 1 epoch: 23.931888103485107 secs\n",
      "\n",
      "Epoch 98 batch 0 train Loss 5.3136 test Loss 5.4496 with MSE metric 7515.4062\n",
      "Epoch 98 batch 100 train Loss 5.2373 test Loss 5.3333 with MSE metric 6452.3555\n",
      "Epoch 98 batch 200 train Loss 5.2651 test Loss 5.4038 with MSE metric 6881.8325\n",
      "Time taken for 1 epoch: 24.77019429206848 secs\n",
      "\n",
      "Epoch 99 batch 0 train Loss 5.3144 test Loss 5.3342 with MSE metric 7595.5928\n",
      "Epoch 99 batch 100 train Loss 5.2845 test Loss 5.3110 with MSE metric 7156.3936\n",
      "Epoch 99 batch 200 train Loss 5.3579 test Loss 5.4514 with MSE metric 8167.8467\n",
      "Time taken for 1 epoch: 24.840869188308716 secs\n",
      "\n",
      "Epoch 100 batch 0 train Loss 5.2738 test Loss 5.3963 with MSE metric 6980.2124\n",
      "Epoch 100 batch 100 train Loss 5.3270 test Loss 5.3811 with MSE metric 7788.5840\n",
      "Epoch 100 batch 200 train Loss 5.3478 test Loss 5.3771 with MSE metric 8074.9907\n",
      "Time taken for 1 epoch: 24.670477867126465 secs\n",
      "\n",
      "Epoch 101 batch 0 train Loss 5.3095 test Loss 5.3917 with MSE metric 7514.3926\n",
      "Epoch 101 batch 100 train Loss 5.3492 test Loss 5.2537 with MSE metric 8139.2461\n",
      "Epoch 101 batch 200 train Loss 5.2927 test Loss 5.3001 with MSE metric 7258.9312\n",
      "Time taken for 1 epoch: 24.34620499610901 secs\n",
      "\n",
      "Epoch 102 batch 0 train Loss 5.3830 test Loss 5.3261 with MSE metric 8526.4307\n",
      "Epoch 102 batch 100 train Loss 5.3647 test Loss 5.4400 with MSE metric 8292.4395\n",
      "Epoch 102 batch 200 train Loss 5.3324 test Loss 5.3752 with MSE metric 7866.3955\n",
      "Time taken for 1 epoch: 23.106362104415894 secs\n",
      "\n",
      "Epoch 103 batch 0 train Loss 5.3354 test Loss 5.3844 with MSE metric 7845.6982\n",
      "Epoch 103 batch 100 train Loss 5.3551 test Loss 5.3143 with MSE metric 8130.1089\n",
      "Epoch 103 batch 200 train Loss 5.2726 test Loss 5.2554 with MSE metric 6977.3179\n",
      "Time taken for 1 epoch: 23.02522110939026 secs\n",
      "\n",
      "Epoch 104 batch 0 train Loss 5.3923 test Loss 5.2834 with MSE metric 8741.5938\n",
      "Epoch 104 batch 100 train Loss 5.2405 test Loss 5.3257 with MSE metric 6438.3706\n",
      "Epoch 104 batch 200 train Loss 5.2603 test Loss 5.2977 with MSE metric 6811.3584\n",
      "Time taken for 1 epoch: 23.07357406616211 secs\n",
      "\n",
      "Epoch 105 batch 0 train Loss 5.2712 test Loss 5.4032 with MSE metric 6956.1934\n",
      "Epoch 105 batch 100 train Loss 5.3767 test Loss 5.3305 with MSE metric 8448.3965\n",
      "Epoch 105 batch 200 train Loss 5.3414 test Loss 5.3687 with MSE metric 7988.4688\n",
      "Time taken for 1 epoch: 23.082002878189087 secs\n",
      "\n",
      "Epoch 106 batch 0 train Loss 5.3813 test Loss 5.3903 with MSE metric 8659.6172\n",
      "Epoch 106 batch 100 train Loss 5.3374 test Loss 5.4457 with MSE metric 7930.9810\n",
      "Epoch 106 batch 200 train Loss 5.3400 test Loss 5.4094 with MSE metric 7935.5859\n",
      "Time taken for 1 epoch: 23.161396265029907 secs\n",
      "\n",
      "Epoch 107 batch 0 train Loss 5.2533 test Loss 5.3115 with MSE metric 6654.7251\n",
      "Epoch 107 batch 100 train Loss 5.2098 test Loss 5.4630 with MSE metric 6096.5615\n",
      "Epoch 107 batch 200 train Loss 5.4430 test Loss 5.3855 with MSE metric 9454.2021\n",
      "Time taken for 1 epoch: 23.287630081176758 secs\n",
      "\n",
      "Epoch 108 batch 0 train Loss 5.2279 test Loss 5.3806 with MSE metric 6348.6709\n",
      "Epoch 108 batch 100 train Loss 5.2122 test Loss 5.2259 with MSE metric 6155.5981\n",
      "Epoch 108 batch 200 train Loss 5.2698 test Loss 5.2937 with MSE metric 6923.3369\n",
      "Time taken for 1 epoch: 23.233280897140503 secs\n",
      "\n",
      "Epoch 109 batch 0 train Loss 5.3879 test Loss 5.4979 with MSE metric 8666.0098\n",
      "Epoch 109 batch 100 train Loss 5.3289 test Loss 5.3545 with MSE metric 7811.5859\n",
      "Epoch 109 batch 200 train Loss 5.2271 test Loss 5.3423 with MSE metric 6236.0879\n",
      "Time taken for 1 epoch: 23.19429612159729 secs\n",
      "\n",
      "Epoch 110 batch 0 train Loss 5.3611 test Loss 5.3405 with MSE metric 8300.3623\n",
      "Epoch 110 batch 100 train Loss 5.3420 test Loss 5.3540 with MSE metric 8006.2939\n",
      "Epoch 110 batch 200 train Loss 5.2432 test Loss 5.3398 with MSE metric 6570.0908\n",
      "Time taken for 1 epoch: 23.19232487678528 secs\n",
      "\n",
      "Epoch 111 batch 0 train Loss 5.2962 test Loss 5.3556 with MSE metric 7311.6226\n",
      "Epoch 111 batch 100 train Loss 5.3770 test Loss 5.3286 with MSE metric 8465.1016\n",
      "Epoch 111 batch 200 train Loss 5.2881 test Loss 5.4685 with MSE metric 7187.0874\n",
      "Time taken for 1 epoch: 23.146777868270874 secs\n",
      "\n",
      "Epoch 112 batch 0 train Loss 5.3350 test Loss 5.4303 with MSE metric 7906.8774\n",
      "Epoch 112 batch 100 train Loss 5.3141 test Loss 5.3674 with MSE metric 7537.5552\n",
      "Epoch 112 batch 200 train Loss 5.2618 test Loss 5.4489 with MSE metric 6830.2559\n",
      "Time taken for 1 epoch: 23.22182607650757 secs\n",
      "\n",
      "Epoch 113 batch 0 train Loss 5.3614 test Loss 5.3106 with MSE metric 8333.4629\n",
      "Epoch 113 batch 100 train Loss 5.3645 test Loss 5.3959 with MSE metric 8378.9492\n",
      "Epoch 113 batch 200 train Loss 5.3437 test Loss 5.3448 with MSE metric 8041.8496\n",
      "Time taken for 1 epoch: 23.28025484085083 secs\n",
      "\n",
      "Epoch 114 batch 0 train Loss 5.3030 test Loss 5.3190 with MSE metric 7415.5557\n",
      "Epoch 114 batch 100 train Loss 5.2658 test Loss 5.2990 with MSE metric 6868.3994\n",
      "Epoch 114 batch 200 train Loss 5.2319 test Loss 5.3789 with MSE metric 6351.1562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 23.206897974014282 secs\n",
      "\n",
      "Epoch 115 batch 0 train Loss 5.2154 test Loss 5.3015 with MSE metric 6104.0439\n",
      "Epoch 115 batch 100 train Loss 5.2819 test Loss 5.3382 with MSE metric 7111.0859\n",
      "Epoch 115 batch 200 train Loss 5.2716 test Loss 5.3270 with MSE metric 6941.3291\n",
      "Time taken for 1 epoch: 23.27065372467041 secs\n",
      "\n",
      "Epoch 116 batch 0 train Loss 5.2793 test Loss 5.2684 with MSE metric 7060.0068\n",
      "Epoch 116 batch 100 train Loss 5.2640 test Loss 5.4067 with MSE metric 6846.7974\n",
      "Epoch 116 batch 200 train Loss 5.2673 test Loss 5.4256 with MSE metric 6899.2773\n",
      "Time taken for 1 epoch: 23.151429891586304 secs\n",
      "\n",
      "Epoch 117 batch 0 train Loss 5.2641 test Loss 5.3882 with MSE metric 6866.2373\n",
      "Epoch 117 batch 100 train Loss 5.3817 test Loss 5.3644 with MSE metric 8585.2480\n",
      "Epoch 117 batch 200 train Loss 5.2342 test Loss 5.3776 with MSE metric 6416.8447\n",
      "Time taken for 1 epoch: 23.17644691467285 secs\n",
      "\n",
      "Epoch 118 batch 0 train Loss 5.2789 test Loss 5.2980 with MSE metric 7023.0371\n",
      "Epoch 118 batch 100 train Loss 5.3191 test Loss 5.4437 with MSE metric 7661.1650\n",
      "Epoch 118 batch 200 train Loss 5.2952 test Loss 5.3977 with MSE metric 7295.6982\n",
      "Time taken for 1 epoch: 23.176111936569214 secs\n",
      "\n",
      "Epoch 119 batch 0 train Loss 5.2915 test Loss 5.4041 with MSE metric 7257.5400\n",
      "Epoch 119 batch 100 train Loss 5.2882 test Loss 5.2897 with MSE metric 7209.7612\n",
      "Epoch 119 batch 200 train Loss 5.3409 test Loss 5.3156 with MSE metric 7982.7295\n",
      "Time taken for 1 epoch: 23.14805793762207 secs\n",
      "\n",
      "Epoch 120 batch 0 train Loss 5.3379 test Loss 5.2960 with MSE metric 7943.1338\n",
      "Epoch 120 batch 100 train Loss 5.3334 test Loss 5.3436 with MSE metric 7886.7686\n",
      "Epoch 120 batch 200 train Loss 5.3569 test Loss 5.3333 with MSE metric 8271.5605\n",
      "Time taken for 1 epoch: 23.241904258728027 secs\n",
      "\n",
      "Epoch 121 batch 0 train Loss 5.2903 test Loss 5.4475 with MSE metric 7239.5215\n",
      "Epoch 121 batch 100 train Loss 5.3326 test Loss 5.3368 with MSE metric 7822.1626\n",
      "Epoch 121 batch 200 train Loss 5.2738 test Loss 5.3477 with MSE metric 7005.4878\n",
      "Time taken for 1 epoch: 23.14391779899597 secs\n",
      "\n",
      "Epoch 122 batch 0 train Loss 5.2827 test Loss 5.4669 with MSE metric 7128.1538\n",
      "Epoch 122 batch 100 train Loss 5.2600 test Loss 5.3996 with MSE metric 6771.4517\n",
      "Epoch 122 batch 200 train Loss 5.2856 test Loss 5.3512 with MSE metric 7157.2476\n",
      "Time taken for 1 epoch: 23.16968011856079 secs\n",
      "\n",
      "Epoch 123 batch 0 train Loss 5.1842 test Loss 5.3776 with MSE metric 5748.4248\n",
      "Epoch 123 batch 100 train Loss 5.2154 test Loss 5.3893 with MSE metric 6175.5771\n",
      "Epoch 123 batch 200 train Loss 5.2976 test Loss 5.3713 with MSE metric 7344.8789\n",
      "Time taken for 1 epoch: 23.159570932388306 secs\n",
      "\n",
      "Epoch 124 batch 0 train Loss 5.2777 test Loss 5.4301 with MSE metric 7057.5127\n",
      "Epoch 124 batch 100 train Loss 5.3331 test Loss 5.3804 with MSE metric 7809.1406\n",
      "Epoch 124 batch 200 train Loss 5.3242 test Loss 5.3983 with MSE metric 7724.4233\n",
      "Time taken for 1 epoch: 23.193727016448975 secs\n",
      "\n",
      "Epoch 125 batch 0 train Loss 5.2520 test Loss 5.3078 with MSE metric 6669.3154\n",
      "Epoch 125 batch 100 train Loss 5.3240 test Loss 5.3784 with MSE metric 7737.3247\n",
      "Epoch 125 batch 200 train Loss 5.2821 test Loss 5.4347 with MSE metric 7119.8931\n",
      "Time taken for 1 epoch: 23.1975519657135 secs\n",
      "\n",
      "Epoch 126 batch 0 train Loss 5.2925 test Loss 5.3852 with MSE metric 7272.6147\n",
      "Epoch 126 batch 100 train Loss 5.2988 test Loss 5.3558 with MSE metric 7352.5864\n",
      "Epoch 126 batch 200 train Loss 5.2669 test Loss 5.2777 with MSE metric 6804.0225\n",
      "Time taken for 1 epoch: 23.181429862976074 secs\n",
      "\n",
      "Epoch 127 batch 0 train Loss 5.2666 test Loss 5.4027 with MSE metric 6896.5586\n",
      "Epoch 127 batch 100 train Loss 5.3109 test Loss 5.3182 with MSE metric 7538.4087\n",
      "Epoch 127 batch 200 train Loss 5.3644 test Loss 5.3991 with MSE metric 8320.0918\n",
      "Time taken for 1 epoch: 23.20514988899231 secs\n",
      "\n",
      "Epoch 128 batch 0 train Loss 5.1865 test Loss 5.4344 with MSE metric 5698.4854\n",
      "Epoch 128 batch 100 train Loss 5.3925 test Loss 5.2540 with MSE metric 8548.4932\n",
      "Epoch 128 batch 200 train Loss 5.3199 test Loss 5.3501 with MSE metric 7644.8809\n",
      "Time taken for 1 epoch: 23.092421770095825 secs\n",
      "\n",
      "Epoch 129 batch 0 train Loss 5.2737 test Loss 5.2733 with MSE metric 6938.4717\n",
      "Epoch 129 batch 100 train Loss 5.1703 test Loss 5.3825 with MSE metric 5442.7725\n",
      "Epoch 129 batch 200 train Loss 5.3407 test Loss 5.3066 with MSE metric 7933.9844\n",
      "Time taken for 1 epoch: 23.141690969467163 secs\n",
      "\n",
      "Epoch 130 batch 0 train Loss 5.2559 test Loss 5.3873 with MSE metric 6755.3091\n",
      "Epoch 130 batch 100 train Loss 5.2186 test Loss 5.4496 with MSE metric 6224.5098\n",
      "Epoch 130 batch 200 train Loss 5.2590 test Loss 5.2871 with MSE metric 6798.2021\n",
      "Time taken for 1 epoch: 23.16045880317688 secs\n",
      "\n",
      "Epoch 131 batch 0 train Loss 5.3200 test Loss 5.3665 with MSE metric 7681.0498\n",
      "Epoch 131 batch 100 train Loss 5.2677 test Loss 5.4012 with MSE metric 6904.4912\n",
      "Epoch 131 batch 200 train Loss 5.1766 test Loss 5.4343 with MSE metric 5700.2529\n",
      "Time taken for 1 epoch: 23.10430383682251 secs\n",
      "\n",
      "Epoch 132 batch 0 train Loss 5.2577 test Loss 5.3371 with MSE metric 6777.8359\n",
      "Epoch 132 batch 100 train Loss 5.2605 test Loss 5.3640 with MSE metric 6804.9062\n",
      "Epoch 132 batch 200 train Loss 5.3272 test Loss 5.3925 with MSE metric 7793.5386\n",
      "Time taken for 1 epoch: 23.279419898986816 secs\n",
      "\n",
      "Epoch 133 batch 0 train Loss 5.2462 test Loss 5.4257 with MSE metric 6610.4019\n",
      "Epoch 133 batch 100 train Loss 5.2369 test Loss 5.3069 with MSE metric 6433.4087\n",
      "Epoch 133 batch 200 train Loss 5.3156 test Loss 5.3371 with MSE metric 7605.3984\n",
      "Time taken for 1 epoch: 23.14233684539795 secs\n",
      "\n",
      "Epoch 134 batch 0 train Loss 5.3192 test Loss 5.1864 with MSE metric 7668.1343\n",
      "Epoch 134 batch 100 train Loss 5.2289 test Loss 5.3700 with MSE metric 6359.2354\n",
      "Epoch 134 batch 200 train Loss 5.2706 test Loss 5.3643 with MSE metric 6939.9067\n",
      "Time taken for 1 epoch: 23.220124006271362 secs\n",
      "\n",
      "Epoch 135 batch 0 train Loss 5.2128 test Loss 5.3300 with MSE metric 6064.0674\n",
      "Epoch 135 batch 100 train Loss 5.3814 test Loss 5.3240 with MSE metric 8489.1279\n",
      "Epoch 135 batch 200 train Loss 5.3093 test Loss 5.3780 with MSE metric 7512.6558\n",
      "Time taken for 1 epoch: 23.196876764297485 secs\n",
      "\n",
      "Epoch 136 batch 0 train Loss 5.3512 test Loss 5.3228 with MSE metric 8063.8379\n",
      "Epoch 136 batch 100 train Loss 5.3035 test Loss 5.3185 with MSE metric 7423.7412\n",
      "Epoch 136 batch 200 train Loss 5.2781 test Loss 5.4514 with MSE metric 7066.2773\n",
      "Time taken for 1 epoch: 23.20364809036255 secs\n",
      "\n",
      "Epoch 137 batch 0 train Loss 5.3414 test Loss 5.4470 with MSE metric 8015.8252\n",
      "Epoch 137 batch 100 train Loss 5.4035 test Loss 5.3266 with MSE metric 8855.1895\n",
      "Epoch 137 batch 200 train Loss 5.2677 test Loss 5.3829 with MSE metric 6853.5371\n",
      "Time taken for 1 epoch: 23.189673900604248 secs\n",
      "\n",
      "Epoch 138 batch 0 train Loss 5.3115 test Loss 5.4173 with MSE metric 7550.6719\n",
      "Epoch 138 batch 100 train Loss 5.3059 test Loss 5.3773 with MSE metric 7468.9189\n",
      "Epoch 138 batch 200 train Loss 5.2037 test Loss 5.3814 with MSE metric 6000.0586\n",
      "Time taken for 1 epoch: 23.17587685585022 secs\n",
      "\n",
      "Epoch 139 batch 0 train Loss 5.3580 test Loss 5.3501 with MSE metric 8256.7363\n",
      "Epoch 139 batch 100 train Loss 5.2491 test Loss 5.2755 with MSE metric 6665.2393\n",
      "Epoch 139 batch 200 train Loss 5.2867 test Loss 5.3892 with MSE metric 7165.2529\n",
      "Time taken for 1 epoch: 23.136191844940186 secs\n",
      "\n",
      "Epoch 140 batch 0 train Loss 5.2619 test Loss 5.3517 with MSE metric 6813.4473\n",
      "Epoch 140 batch 100 train Loss 5.3287 test Loss 5.3388 with MSE metric 7801.3691\n",
      "Epoch 140 batch 200 train Loss 5.3526 test Loss 5.4022 with MSE metric 8191.2534\n",
      "Time taken for 1 epoch: 23.196182012557983 secs\n",
      "\n",
      "Epoch 141 batch 0 train Loss 5.3535 test Loss 5.2878 with MSE metric 8161.7451\n",
      "Epoch 141 batch 100 train Loss 5.3001 test Loss 5.3466 with MSE metric 7378.6685\n",
      "Epoch 141 batch 200 train Loss 5.2774 test Loss 5.3811 with MSE metric 7046.9512\n",
      "Time taken for 1 epoch: 23.174263954162598 secs\n",
      "\n",
      "Epoch 142 batch 0 train Loss 5.2930 test Loss 5.4018 with MSE metric 7278.5293\n",
      "Epoch 142 batch 100 train Loss 5.2525 test Loss 5.4663 with MSE metric 6703.4570\n",
      "Epoch 142 batch 200 train Loss 5.2835 test Loss 5.2854 with MSE metric 7141.8389\n",
      "Time taken for 1 epoch: 23.20914101600647 secs\n",
      "\n",
      "Epoch 143 batch 0 train Loss 5.3197 test Loss 5.3346 with MSE metric 7679.2881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143 batch 100 train Loss 5.3219 test Loss 5.2635 with MSE metric 7711.9082\n",
      "Epoch 143 batch 200 train Loss 5.3401 test Loss 5.4013 with MSE metric 7995.9277\n",
      "Time taken for 1 epoch: 23.22757387161255 secs\n",
      "\n",
      "Epoch 144 batch 0 train Loss 5.3254 test Loss 5.3161 with MSE metric 7719.2896\n",
      "Epoch 144 batch 100 train Loss 5.2390 test Loss 5.3684 with MSE metric 6460.1323\n",
      "Epoch 144 batch 200 train Loss 5.2058 test Loss 5.4766 with MSE metric 5988.9785\n",
      "Time taken for 1 epoch: 23.297497749328613 secs\n",
      "\n",
      "Epoch 145 batch 0 train Loss 5.3439 test Loss 5.3616 with MSE metric 7993.4629\n",
      "Epoch 145 batch 100 train Loss 5.2741 test Loss 5.3108 with MSE metric 7009.1665\n",
      "Epoch 145 batch 200 train Loss 5.2705 test Loss 5.3562 with MSE metric 6953.6553\n",
      "Time taken for 1 epoch: 23.13878297805786 secs\n",
      "\n",
      "Epoch 146 batch 0 train Loss 5.3156 test Loss 5.4584 with MSE metric 7615.5674\n",
      "Epoch 146 batch 100 train Loss 5.2726 test Loss 5.2525 with MSE metric 6963.5381\n",
      "Epoch 146 batch 200 train Loss 5.3878 test Loss 5.4317 with MSE metric 8708.7783\n",
      "Time taken for 1 epoch: 23.21329092979431 secs\n",
      "\n",
      "Epoch 147 batch 0 train Loss 5.1907 test Loss 5.3708 with MSE metric 5820.1675\n",
      "Epoch 147 batch 100 train Loss 5.3626 test Loss 5.3451 with MSE metric 8310.3555\n",
      "Epoch 147 batch 200 train Loss 5.2554 test Loss 5.3113 with MSE metric 6673.3125\n",
      "Time taken for 1 epoch: 23.1849582195282 secs\n",
      "\n",
      "Epoch 148 batch 0 train Loss 5.3491 test Loss 5.3288 with MSE metric 8143.3633\n",
      "Epoch 148 batch 100 train Loss 5.2715 test Loss 5.3764 with MSE metric 6970.0303\n",
      "Epoch 148 batch 200 train Loss 5.3392 test Loss 5.2620 with MSE metric 7984.0928\n",
      "Time taken for 1 epoch: 23.182377815246582 secs\n",
      "\n",
      "Epoch 149 batch 0 train Loss 5.2630 test Loss 5.3851 with MSE metric 6849.5142\n",
      "Epoch 149 batch 100 train Loss 5.3646 test Loss 5.3317 with MSE metric 8367.6094\n",
      "Epoch 149 batch 200 train Loss 5.2988 test Loss 5.3572 with MSE metric 7354.1602\n",
      "Time taken for 1 epoch: 23.22522807121277 secs\n",
      "\n",
      "Epoch 150 batch 0 train Loss 5.2476 test Loss 5.3571 with MSE metric 6477.8521\n",
      "Epoch 150 batch 100 train Loss 5.2759 test Loss 5.4392 with MSE metric 7033.3818\n",
      "Epoch 150 batch 200 train Loss 5.3314 test Loss 5.3310 with MSE metric 7857.2607\n",
      "Time taken for 1 epoch: 23.178833961486816 secs\n",
      "\n",
      "Epoch 151 batch 0 train Loss 5.2337 test Loss 5.4049 with MSE metric 6421.4048\n",
      "Epoch 151 batch 100 train Loss 5.3837 test Loss 5.2674 with MSE metric 8651.5293\n",
      "Epoch 151 batch 200 train Loss 5.3536 test Loss 5.3104 with MSE metric 8159.2671\n",
      "Time taken for 1 epoch: 23.169513940811157 secs\n",
      "\n",
      "Epoch 152 batch 0 train Loss 5.3293 test Loss 5.3406 with MSE metric 7825.8691\n",
      "Epoch 152 batch 100 train Loss 5.2560 test Loss 5.3337 with MSE metric 6736.0874\n",
      "Epoch 152 batch 200 train Loss 5.3441 test Loss 5.3347 with MSE metric 8036.6226\n",
      "Time taken for 1 epoch: 23.18959617614746 secs\n",
      "\n",
      "Epoch 153 batch 0 train Loss 5.2311 test Loss 5.3727 with MSE metric 6369.2393\n",
      "Epoch 153 batch 100 train Loss 5.2198 test Loss 5.3339 with MSE metric 6202.0786\n",
      "Epoch 153 batch 200 train Loss 5.3583 test Loss 5.3836 with MSE metric 8254.8730\n",
      "Time taken for 1 epoch: 23.153591871261597 secs\n",
      "\n",
      "Epoch 154 batch 0 train Loss 5.3190 test Loss 5.2969 with MSE metric 7666.9585\n",
      "Epoch 154 batch 100 train Loss 5.3340 test Loss 5.3537 with MSE metric 7867.5835\n",
      "Epoch 154 batch 200 train Loss 5.3253 test Loss 5.3741 with MSE metric 7739.8281\n",
      "Time taken for 1 epoch: 23.19751811027527 secs\n",
      "\n",
      "Epoch 155 batch 0 train Loss 5.3594 test Loss 5.3569 with MSE metric 8200.6914\n",
      "Epoch 155 batch 100 train Loss 5.2103 test Loss 5.4783 with MSE metric 6089.7007\n",
      "Epoch 155 batch 200 train Loss 5.2929 test Loss 5.3289 with MSE metric 7277.2822\n",
      "Time taken for 1 epoch: 23.14857506752014 secs\n",
      "\n",
      "Epoch 156 batch 0 train Loss 5.2446 test Loss 5.4349 with MSE metric 6564.1934\n",
      "Epoch 156 batch 100 train Loss 5.3439 test Loss 5.3800 with MSE metric 8030.8115\n",
      "Epoch 156 batch 200 train Loss 5.2098 test Loss 5.3945 with MSE metric 6118.1553\n",
      "Time taken for 1 epoch: 23.147571086883545 secs\n",
      "\n",
      "Epoch 157 batch 0 train Loss 5.2887 test Loss 5.3656 with MSE metric 7214.4209\n",
      "Epoch 157 batch 100 train Loss 5.2984 test Loss 5.3950 with MSE metric 7346.5269\n",
      "Epoch 157 batch 200 train Loss 5.2684 test Loss 5.3099 with MSE metric 6845.0571\n",
      "Time taken for 1 epoch: 23.213645935058594 secs\n",
      "\n",
      "Epoch 158 batch 0 train Loss 5.3112 test Loss 5.2843 with MSE metric 7525.3970\n",
      "Epoch 158 batch 100 train Loss 5.2492 test Loss 5.2945 with MSE metric 6654.6489\n",
      "Epoch 158 batch 200 train Loss 5.2996 test Loss 5.3293 with MSE metric 7369.8633\n",
      "Time taken for 1 epoch: 23.175829648971558 secs\n",
      "\n",
      "Epoch 159 batch 0 train Loss 5.4388 test Loss 5.3311 with MSE metric 9462.8896\n",
      "Epoch 159 batch 100 train Loss 5.4099 test Loss 5.3139 with MSE metric 8942.1445\n",
      "Epoch 159 batch 200 train Loss 5.3898 test Loss 5.3128 with MSE metric 8594.0801\n",
      "Time taken for 1 epoch: 23.209961891174316 secs\n",
      "\n",
      "Epoch 160 batch 0 train Loss 5.2576 test Loss 5.2440 with MSE metric 6763.3701\n",
      "Epoch 160 batch 100 train Loss 5.3566 test Loss 5.4374 with MSE metric 8223.1270\n",
      "Epoch 160 batch 200 train Loss 5.2519 test Loss 5.3664 with MSE metric 6678.6567\n",
      "Time taken for 1 epoch: 23.165812969207764 secs\n",
      "\n",
      "Epoch 161 batch 0 train Loss 5.3414 test Loss 5.5348 with MSE metric 7992.1475\n",
      "Epoch 161 batch 100 train Loss 5.3819 test Loss 5.4340 with MSE metric 8627.1738\n",
      "Epoch 161 batch 200 train Loss 5.2529 test Loss 5.3707 with MSE metric 6628.4995\n",
      "Time taken for 1 epoch: 23.228271007537842 secs\n",
      "\n",
      "Epoch 162 batch 0 train Loss 5.3536 test Loss 5.3539 with MSE metric 8169.4751\n",
      "Epoch 162 batch 100 train Loss 5.2661 test Loss 5.3256 with MSE metric 6894.4106\n",
      "Epoch 162 batch 200 train Loss 5.2395 test Loss 5.3008 with MSE metric 6417.9985\n",
      "Time taken for 1 epoch: 23.148143768310547 secs\n",
      "\n",
      "Epoch 163 batch 0 train Loss 5.2551 test Loss 5.4761 with MSE metric 6735.5225\n",
      "Epoch 163 batch 100 train Loss 5.3451 test Loss 5.2914 with MSE metric 8062.0498\n",
      "Epoch 163 batch 200 train Loss 5.2680 test Loss 5.3449 with MSE metric 6902.8760\n",
      "Time taken for 1 epoch: 23.190598964691162 secs\n",
      "\n",
      "Epoch 164 batch 0 train Loss 5.3423 test Loss 5.3112 with MSE metric 8033.9238\n",
      "Epoch 164 batch 100 train Loss 5.2496 test Loss 5.4452 with MSE metric 6666.2139\n",
      "Epoch 164 batch 200 train Loss 5.2520 test Loss 5.4132 with MSE metric 6683.4087\n",
      "Time taken for 1 epoch: 23.17171287536621 secs\n",
      "\n",
      "Epoch 165 batch 0 train Loss 5.2285 test Loss 5.3455 with MSE metric 6168.1274\n",
      "Epoch 165 batch 100 train Loss 5.3186 test Loss 5.3766 with MSE metric 7661.1895\n",
      "Epoch 165 batch 200 train Loss 5.2164 test Loss 5.3956 with MSE metric 6190.1045\n",
      "Time taken for 1 epoch: 23.201739072799683 secs\n",
      "\n",
      "Epoch 166 batch 0 train Loss 5.2812 test Loss 5.3574 with MSE metric 7061.9893\n",
      "Epoch 166 batch 100 train Loss 5.1854 test Loss 5.3083 with MSE metric 5616.0454\n",
      "Epoch 166 batch 200 train Loss 5.2372 test Loss 5.2261 with MSE metric 6431.7310\n",
      "Time taken for 1 epoch: 23.3021137714386 secs\n",
      "\n",
      "Epoch 167 batch 0 train Loss 5.3760 test Loss 5.3252 with MSE metric 8495.6768\n",
      "Epoch 167 batch 100 train Loss 5.2556 test Loss 5.3871 with MSE metric 6724.4863\n",
      "Epoch 167 batch 200 train Loss 5.2688 test Loss 5.3340 with MSE metric 6925.4097\n",
      "Time taken for 1 epoch: 23.20051407814026 secs\n",
      "\n",
      "Epoch 168 batch 0 train Loss 5.2233 test Loss 5.4079 with MSE metric 6247.6699\n",
      "Epoch 168 batch 100 train Loss 5.2816 test Loss 5.3260 with MSE metric 7094.0420\n",
      "Epoch 168 batch 200 train Loss 5.3219 test Loss 5.3848 with MSE metric 7706.3252\n",
      "Time taken for 1 epoch: 23.179542064666748 secs\n",
      "\n",
      "Epoch 169 batch 0 train Loss 5.2106 test Loss 5.2910 with MSE metric 6054.6445\n",
      "Epoch 169 batch 100 train Loss 5.3449 test Loss 5.3275 with MSE metric 8070.0166\n",
      "Epoch 169 batch 200 train Loss 5.3862 test Loss 5.3175 with MSE metric 8728.1504\n",
      "Time taken for 1 epoch: 23.123862981796265 secs\n",
      "\n",
      "Epoch 170 batch 0 train Loss 5.3477 test Loss 5.3471 with MSE metric 8046.5967\n",
      "Epoch 170 batch 100 train Loss 5.3117 test Loss 5.2924 with MSE metric 7554.8271\n",
      "Epoch 170 batch 200 train Loss 5.2607 test Loss 5.3269 with MSE metric 6718.5596\n",
      "Time taken for 1 epoch: 23.15374493598938 secs\n",
      "\n",
      "Epoch 171 batch 0 train Loss 5.2897 test Loss 5.3518 with MSE metric 7199.5200\n",
      "Epoch 171 batch 100 train Loss 5.3273 test Loss 5.3668 with MSE metric 7722.2798\n",
      "Epoch 171 batch 200 train Loss 5.2959 test Loss 5.2741 with MSE metric 7321.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 23.138854026794434 secs\n",
      "\n",
      "Epoch 172 batch 0 train Loss 5.2857 test Loss 5.3372 with MSE metric 7174.3252\n",
      "Epoch 172 batch 100 train Loss 5.3211 test Loss 5.3749 with MSE metric 7664.9048\n",
      "Epoch 172 batch 200 train Loss 5.2774 test Loss 5.3775 with MSE metric 7017.4629\n",
      "Time taken for 1 epoch: 23.182334899902344 secs\n",
      "\n",
      "Epoch 173 batch 0 train Loss 5.2562 test Loss 5.3889 with MSE metric 6746.5288\n",
      "Epoch 173 batch 100 train Loss 5.3068 test Loss 5.4005 with MSE metric 7483.3169\n",
      "Epoch 173 batch 200 train Loss 5.2766 test Loss 5.2920 with MSE metric 7044.3916\n",
      "Time taken for 1 epoch: 23.117932081222534 secs\n",
      "\n",
      "Epoch 174 batch 0 train Loss 5.2056 test Loss 5.3429 with MSE metric 5899.1836\n",
      "Epoch 174 batch 100 train Loss 5.3817 test Loss 5.3993 with MSE metric 8503.8291\n",
      "Epoch 174 batch 200 train Loss 5.3276 test Loss 5.4037 with MSE metric 7799.4849\n",
      "Time taken for 1 epoch: 23.16606903076172 secs\n",
      "\n",
      "Epoch 175 batch 0 train Loss 5.3503 test Loss 5.3246 with MSE metric 8137.5537\n",
      "Epoch 175 batch 100 train Loss 5.3219 test Loss 5.3164 with MSE metric 7709.7671\n",
      "Epoch 175 batch 200 train Loss 5.3149 test Loss 5.3508 with MSE metric 7598.3701\n",
      "Time taken for 1 epoch: 23.181347846984863 secs\n",
      "\n",
      "Epoch 176 batch 0 train Loss 5.3285 test Loss 5.4386 with MSE metric 7800.2559\n",
      "Epoch 176 batch 100 train Loss 5.4042 test Loss 5.4957 with MSE metric 8944.3604\n",
      "Epoch 176 batch 200 train Loss 5.2743 test Loss 5.4241 with MSE metric 7010.1328\n",
      "Time taken for 1 epoch: 23.180548906326294 secs\n",
      "\n",
      "Epoch 177 batch 0 train Loss 5.2968 test Loss 5.4295 with MSE metric 7329.3535\n",
      "Epoch 177 batch 100 train Loss 5.3706 test Loss 5.3711 with MSE metric 8411.4082\n",
      "Epoch 177 batch 200 train Loss 5.3621 test Loss 5.2488 with MSE metric 8246.8906\n",
      "Time taken for 1 epoch: 23.146233797073364 secs\n",
      "\n",
      "Epoch 178 batch 0 train Loss 5.1580 test Loss 5.3607 with MSE metric 5400.8042\n",
      "Epoch 178 batch 100 train Loss 5.3796 test Loss 5.3084 with MSE metric 8473.1348\n",
      "Epoch 178 batch 200 train Loss 5.3342 test Loss 5.3240 with MSE metric 7871.5273\n",
      "Time taken for 1 epoch: 23.176212787628174 secs\n",
      "\n",
      "Epoch 179 batch 0 train Loss 5.3134 test Loss 5.3582 with MSE metric 7572.1187\n",
      "Epoch 179 batch 100 train Loss 5.3533 test Loss 5.3431 with MSE metric 8177.4272\n",
      "Epoch 179 batch 200 train Loss 5.3243 test Loss 5.3832 with MSE metric 7722.1777\n",
      "Time taken for 1 epoch: 23.13131284713745 secs\n",
      "\n",
      "Epoch 180 batch 0 train Loss 5.3171 test Loss 5.3138 with MSE metric 7631.0791\n",
      "Epoch 180 batch 100 train Loss 5.2920 test Loss 5.3707 with MSE metric 7264.7002\n",
      "Epoch 180 batch 200 train Loss 5.2782 test Loss 5.3945 with MSE metric 7067.4482\n",
      "Time taken for 1 epoch: 23.149540901184082 secs\n",
      "\n",
      "Epoch 181 batch 0 train Loss 5.3739 test Loss 5.3407 with MSE metric 8493.2139\n",
      "Epoch 181 batch 100 train Loss 5.2717 test Loss 5.4120 with MSE metric 6940.3301\n",
      "Epoch 181 batch 200 train Loss 5.2105 test Loss 5.4053 with MSE metric 6112.4648\n",
      "Time taken for 1 epoch: 23.144611120224 secs\n",
      "\n",
      "Epoch 182 batch 0 train Loss 5.2489 test Loss 5.3661 with MSE metric 6625.7227\n",
      "Epoch 182 batch 100 train Loss 5.2149 test Loss 5.3917 with MSE metric 6181.1709\n",
      "Epoch 182 batch 200 train Loss 5.3386 test Loss 5.4519 with MSE metric 7906.1826\n",
      "Time taken for 1 epoch: 23.111021041870117 secs\n",
      "\n",
      "Epoch 183 batch 0 train Loss 5.2577 test Loss 5.3319 with MSE metric 6711.0649\n",
      "Epoch 183 batch 100 train Loss 5.3289 test Loss 5.3698 with MSE metric 7792.8545\n",
      "Epoch 183 batch 200 train Loss 5.2821 test Loss 5.3266 with MSE metric 7092.1260\n",
      "Time taken for 1 epoch: 23.152945041656494 secs\n",
      "\n",
      "Epoch 184 batch 0 train Loss 5.2493 test Loss 5.3076 with MSE metric 6668.9922\n",
      "Epoch 184 batch 100 train Loss 5.3067 test Loss 5.3750 with MSE metric 7479.4766\n",
      "Epoch 184 batch 200 train Loss 5.3901 test Loss 5.4037 with MSE metric 8771.4727\n",
      "Time taken for 1 epoch: 23.21075701713562 secs\n",
      "\n",
      "Epoch 185 batch 0 train Loss 5.4739 test Loss 5.3339 with MSE metric 9948.5068\n",
      "Epoch 185 batch 100 train Loss 5.3815 test Loss 5.3593 with MSE metric 8538.6445\n",
      "Epoch 185 batch 200 train Loss 5.2722 test Loss 5.3089 with MSE metric 6978.2168\n",
      "Time taken for 1 epoch: 23.093585968017578 secs\n",
      "\n",
      "Epoch 186 batch 0 train Loss 5.3688 test Loss 5.4071 with MSE metric 8416.1953\n",
      "Epoch 186 batch 100 train Loss 5.3124 test Loss 5.4145 with MSE metric 7564.3501\n",
      "Epoch 186 batch 200 train Loss 5.2717 test Loss 5.3335 with MSE metric 6949.8921\n",
      "Time taken for 1 epoch: 23.109516143798828 secs\n",
      "\n",
      "Epoch 187 batch 0 train Loss 5.3375 test Loss 5.2636 with MSE metric 7939.0137\n",
      "Epoch 187 batch 100 train Loss 5.3239 test Loss 5.3726 with MSE metric 7737.2871\n",
      "Epoch 187 batch 200 train Loss 5.3415 test Loss 5.5260 with MSE metric 7977.6504\n",
      "Time taken for 1 epoch: 23.139412879943848 secs\n",
      "\n",
      "Epoch 188 batch 0 train Loss 5.3773 test Loss 5.3227 with MSE metric 8538.1377\n",
      "Epoch 188 batch 100 train Loss 5.4254 test Loss 5.3146 with MSE metric 9191.9336\n",
      "Epoch 188 batch 200 train Loss 5.3023 test Loss 5.3388 with MSE metric 7415.5107\n",
      "Time taken for 1 epoch: 23.141735315322876 secs\n",
      "\n",
      "Epoch 189 batch 0 train Loss 5.2620 test Loss 5.2793 with MSE metric 6834.0771\n",
      "Epoch 189 batch 100 train Loss 5.2424 test Loss 5.4684 with MSE metric 6570.0068\n",
      "Epoch 189 batch 200 train Loss 5.1981 test Loss 5.4175 with MSE metric 5820.2207\n",
      "Time taken for 1 epoch: 23.128227949142456 secs\n",
      "\n",
      "Epoch 190 batch 0 train Loss 5.2922 test Loss 5.3784 with MSE metric 7259.6016\n",
      "Epoch 190 batch 100 train Loss 5.2769 test Loss 5.2955 with MSE metric 7024.8633\n",
      "Epoch 190 batch 200 train Loss 5.3341 test Loss 5.4291 with MSE metric 7901.3984\n",
      "Time taken for 1 epoch: 23.140756130218506 secs\n",
      "\n",
      "Epoch 191 batch 0 train Loss 5.2371 test Loss 5.4286 with MSE metric 6456.4229\n",
      "Epoch 191 batch 100 train Loss 5.2976 test Loss 5.3502 with MSE metric 7343.5459\n",
      "Epoch 191 batch 200 train Loss 5.2806 test Loss 5.3129 with MSE metric 7099.8281\n",
      "Time taken for 1 epoch: 23.13196611404419 secs\n",
      "\n",
      "Epoch 192 batch 0 train Loss 5.2591 test Loss 5.4030 with MSE metric 6788.5547\n",
      "Epoch 192 batch 100 train Loss 5.3637 test Loss 5.3964 with MSE metric 8292.9482\n",
      "Epoch 192 batch 200 train Loss 5.1998 test Loss 5.2237 with MSE metric 5838.2446\n",
      "Time taken for 1 epoch: 23.179978847503662 secs\n",
      "\n",
      "Epoch 193 batch 0 train Loss 5.2716 test Loss 5.3618 with MSE metric 6960.9385\n",
      "Epoch 193 batch 100 train Loss 5.1316 test Loss 5.3660 with MSE metric 4906.5332\n",
      "Epoch 193 batch 200 train Loss 5.3195 test Loss 5.4180 with MSE metric 7663.9775\n",
      "Time taken for 1 epoch: 23.054007053375244 secs\n",
      "\n",
      "Epoch 194 batch 0 train Loss 5.3257 test Loss 5.3335 with MSE metric 7768.0083\n",
      "Epoch 194 batch 100 train Loss 5.1976 test Loss 5.3808 with MSE metric 5955.4756\n",
      "Epoch 194 batch 200 train Loss 5.3816 test Loss 5.3777 with MSE metric 8592.3291\n",
      "Time taken for 1 epoch: 23.145601987838745 secs\n",
      "\n",
      "Epoch 195 batch 0 train Loss 5.3280 test Loss 5.4254 with MSE metric 7800.8496\n",
      "Epoch 195 batch 100 train Loss 5.2263 test Loss 5.3845 with MSE metric 6337.1782\n",
      "Epoch 195 batch 200 train Loss 5.3781 test Loss 5.4306 with MSE metric 8509.3936\n",
      "Time taken for 1 epoch: 23.164068937301636 secs\n",
      "\n",
      "Epoch 196 batch 0 train Loss 5.2678 test Loss 5.4100 with MSE metric 6888.6123\n",
      "Epoch 196 batch 100 train Loss 5.2307 test Loss 5.3467 with MSE metric 6399.5732\n",
      "Epoch 196 batch 200 train Loss 5.1858 test Loss 5.3586 with MSE metric 5753.4116\n",
      "Time taken for 1 epoch: 23.139734983444214 secs\n",
      "\n",
      "Epoch 197 batch 0 train Loss 5.2627 test Loss 5.3585 with MSE metric 6851.4961\n",
      "Epoch 197 batch 100 train Loss 5.2857 test Loss 5.3043 with MSE metric 7136.6104\n",
      "Epoch 197 batch 200 train Loss 5.2344 test Loss 5.4296 with MSE metric 6358.7363\n",
      "Time taken for 1 epoch: 23.12525510787964 secs\n",
      "\n",
      "Epoch 198 batch 0 train Loss 5.2847 test Loss 5.4176 with MSE metric 7159.0068\n",
      "Epoch 198 batch 100 train Loss 5.3634 test Loss 5.2602 with MSE metric 8280.9971\n",
      "Epoch 198 batch 200 train Loss 5.2249 test Loss 5.3953 with MSE metric 6289.5967\n",
      "Time taken for 1 epoch: 23.101137161254883 secs\n",
      "\n",
      "Epoch 199 batch 0 train Loss 5.3564 test Loss 5.3567 with MSE metric 8203.3926\n",
      "Epoch 199 batch 100 train Loss 5.3266 test Loss 5.3240 with MSE metric 7782.9878\n",
      "Epoch 199 batch 200 train Loss 5.4043 test Loss 5.4198 with MSE metric 8927.3359\n",
      "Time taken for 1 epoch: 23.132044792175293 secs\n",
      "\n",
      "Epoch 200 batch 0 train Loss 5.2718 test Loss 5.3944 with MSE metric 6961.4590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200 batch 100 train Loss 5.3792 test Loss 5.2195 with MSE metric 8545.7305\n",
      "Epoch 200 batch 200 train Loss 5.2558 test Loss 5.3860 with MSE metric 6749.5581\n",
      "Time taken for 1 epoch: 23.1619770526886 secs\n",
      "\n",
      "Epoch 201 batch 0 train Loss 5.3328 test Loss 5.3139 with MSE metric 7874.7744\n",
      "Epoch 201 batch 100 train Loss 5.2607 test Loss 5.4684 with MSE metric 6800.9507\n",
      "Epoch 201 batch 200 train Loss 5.2394 test Loss 5.4694 with MSE metric 6481.2373\n",
      "Time taken for 1 epoch: 23.21681523323059 secs\n",
      "\n",
      "Epoch 202 batch 0 train Loss 5.2444 test Loss 5.4497 with MSE metric 6593.9434\n",
      "Epoch 202 batch 100 train Loss 5.3186 test Loss 5.3427 with MSE metric 7648.4727\n",
      "Epoch 202 batch 200 train Loss 5.3118 test Loss 5.4130 with MSE metric 7556.6787\n",
      "Time taken for 1 epoch: 23.158123016357422 secs\n",
      "\n",
      "Epoch 203 batch 0 train Loss 5.2467 test Loss 5.4002 with MSE metric 6583.0615\n",
      "Epoch 203 batch 100 train Loss 5.3193 test Loss 5.4411 with MSE metric 7661.5645\n",
      "Epoch 203 batch 200 train Loss 5.3304 test Loss 5.3160 with MSE metric 7830.5322\n",
      "Time taken for 1 epoch: 23.19593906402588 secs\n",
      "\n",
      "Epoch 204 batch 0 train Loss 5.3147 test Loss 5.3994 with MSE metric 7600.9893\n",
      "Epoch 204 batch 100 train Loss 5.2638 test Loss 5.3833 with MSE metric 6862.0615\n",
      "Epoch 204 batch 200 train Loss 5.2934 test Loss 5.4141 with MSE metric 7282.1733\n",
      "Time taken for 1 epoch: 23.20179295539856 secs\n",
      "\n",
      "Epoch 205 batch 0 train Loss 5.2347 test Loss 5.2289 with MSE metric 6441.3149\n",
      "Epoch 205 batch 100 train Loss 5.3400 test Loss 5.2869 with MSE metric 7992.0352\n",
      "Epoch 205 batch 200 train Loss 5.2147 test Loss 5.3869 with MSE metric 6123.1562\n",
      "Time taken for 1 epoch: 23.1618971824646 secs\n",
      "\n",
      "Epoch 206 batch 0 train Loss 5.2875 test Loss 5.3653 with MSE metric 7189.9141\n",
      "Epoch 206 batch 100 train Loss 5.2001 test Loss 5.3467 with MSE metric 5938.4683\n",
      "Epoch 206 batch 200 train Loss 5.2703 test Loss 5.2852 with MSE metric 6846.1255\n",
      "Time taken for 1 epoch: 23.11921000480652 secs\n",
      "\n",
      "Epoch 207 batch 0 train Loss 5.2917 test Loss 5.3157 with MSE metric 7252.6406\n",
      "Epoch 207 batch 100 train Loss 5.3023 test Loss 5.4059 with MSE metric 7415.8057\n",
      "Epoch 207 batch 200 train Loss 5.2697 test Loss 5.3867 with MSE metric 6917.8486\n",
      "Time taken for 1 epoch: 23.124219179153442 secs\n",
      "\n",
      "Epoch 208 batch 0 train Loss 5.3778 test Loss 5.3511 with MSE metric 8538.8691\n",
      "Epoch 208 batch 100 train Loss 5.3474 test Loss 5.2859 with MSE metric 8059.2935\n",
      "Epoch 208 batch 200 train Loss 5.3368 test Loss 5.3443 with MSE metric 7917.5771\n",
      "Time taken for 1 epoch: 23.176658153533936 secs\n",
      "\n",
      "Epoch 209 batch 0 train Loss 5.2799 test Loss 5.3832 with MSE metric 7090.5479\n",
      "Epoch 209 batch 100 train Loss 5.3505 test Loss 5.2645 with MSE metric 8085.5566\n",
      "Epoch 209 batch 200 train Loss 5.2692 test Loss 5.3814 with MSE metric 6938.2070\n",
      "Time taken for 1 epoch: 23.1230571269989 secs\n",
      "\n",
      "Epoch 210 batch 0 train Loss 5.2587 test Loss 5.4316 with MSE metric 6690.9111\n",
      "Epoch 210 batch 100 train Loss 5.4186 test Loss 5.5055 with MSE metric 9125.6475\n",
      "Epoch 210 batch 200 train Loss 5.3579 test Loss 5.4084 with MSE metric 8222.2666\n",
      "Time taken for 1 epoch: 23.20250964164734 secs\n",
      "\n",
      "Epoch 211 batch 0 train Loss 5.3435 test Loss 5.3860 with MSE metric 8053.2031\n",
      "Epoch 211 batch 100 train Loss 5.3339 test Loss 5.2960 with MSE metric 7857.7671\n",
      "Epoch 211 batch 200 train Loss 5.3410 test Loss 5.3728 with MSE metric 7981.0981\n",
      "Time taken for 1 epoch: 23.150204181671143 secs\n",
      "\n",
      "Epoch 212 batch 0 train Loss 5.3776 test Loss 5.3851 with MSE metric 8484.1914\n",
      "Epoch 212 batch 100 train Loss 5.3190 test Loss 5.3317 with MSE metric 7667.1250\n",
      "Epoch 212 batch 200 train Loss 5.3200 test Loss 5.2531 with MSE metric 7675.8447\n",
      "Time taken for 1 epoch: 23.11766791343689 secs\n",
      "\n",
      "Epoch 213 batch 0 train Loss 5.1337 test Loss 5.3524 with MSE metric 5001.5391\n",
      "Epoch 213 batch 100 train Loss 5.2801 test Loss 5.3497 with MSE metric 7078.4023\n",
      "Epoch 213 batch 200 train Loss 5.3411 test Loss 5.3815 with MSE metric 8002.1313\n",
      "Time taken for 1 epoch: 23.206400156021118 secs\n",
      "\n",
      "Epoch 214 batch 0 train Loss 5.2520 test Loss 5.3186 with MSE metric 6705.3730\n",
      "Epoch 214 batch 100 train Loss 5.3239 test Loss 5.3749 with MSE metric 7740.9189\n",
      "Epoch 214 batch 200 train Loss 5.3431 test Loss 5.3305 with MSE metric 8038.8574\n",
      "Time taken for 1 epoch: 23.07711911201477 secs\n",
      "\n",
      "Epoch 215 batch 0 train Loss 5.3178 test Loss 5.3371 with MSE metric 7612.1606\n",
      "Epoch 215 batch 100 train Loss 5.3447 test Loss 5.4309 with MSE metric 8062.3457\n",
      "Epoch 215 batch 200 train Loss 5.2892 test Loss 5.3869 with MSE metric 7224.1875\n",
      "Time taken for 1 epoch: 23.168735027313232 secs\n",
      "\n",
      "Epoch 216 batch 0 train Loss 5.2188 test Loss 5.2969 with MSE metric 6195.9111\n",
      "Epoch 216 batch 100 train Loss 5.3484 test Loss 5.3167 with MSE metric 8128.0625\n",
      "Epoch 216 batch 200 train Loss 5.2295 test Loss 5.3440 with MSE metric 6352.9434\n",
      "Time taken for 1 epoch: 23.18545889854431 secs\n",
      "\n",
      "Epoch 217 batch 0 train Loss 5.3447 test Loss 5.3122 with MSE metric 8051.4399\n",
      "Epoch 217 batch 100 train Loss 5.2541 test Loss 5.3878 with MSE metric 6664.7246\n",
      "Epoch 217 batch 200 train Loss 5.2274 test Loss 5.3259 with MSE metric 6297.3774\n",
      "Time taken for 1 epoch: 23.17436909675598 secs\n",
      "\n",
      "Epoch 218 batch 0 train Loss 5.3476 test Loss 5.3935 with MSE metric 8103.6206\n",
      "Epoch 218 batch 100 train Loss 5.3722 test Loss 5.3851 with MSE metric 8468.5000\n",
      "Epoch 218 batch 200 train Loss 5.2612 test Loss 5.3817 with MSE metric 6821.6992\n",
      "Time taken for 1 epoch: 23.211515188217163 secs\n",
      "\n",
      "Epoch 219 batch 0 train Loss 5.2891 test Loss 5.4822 with MSE metric 7191.6406\n",
      "Epoch 219 batch 100 train Loss 5.2354 test Loss 5.3139 with MSE metric 6467.6543\n",
      "Epoch 219 batch 200 train Loss 5.4046 test Loss 5.4016 with MSE metric 8971.9531\n",
      "Time taken for 1 epoch: 23.199743032455444 secs\n",
      "\n",
      "Epoch 220 batch 0 train Loss 5.2540 test Loss 5.4588 with MSE metric 6733.5107\n",
      "Epoch 220 batch 100 train Loss 5.3172 test Loss 5.2869 with MSE metric 7634.8838\n",
      "Epoch 220 batch 200 train Loss 5.3954 test Loss 5.3216 with MSE metric 8894.8770\n",
      "Time taken for 1 epoch: 23.174028873443604 secs\n",
      "\n",
      "Epoch 221 batch 0 train Loss 5.2918 test Loss 5.3497 with MSE metric 7261.1348\n",
      "Epoch 221 batch 100 train Loss 5.3739 test Loss 5.3716 with MSE metric 8439.9043\n",
      "Epoch 221 batch 200 train Loss 5.3020 test Loss 5.4048 with MSE metric 7407.9521\n",
      "Time taken for 1 epoch: 23.135229110717773 secs\n",
      "\n",
      "Epoch 222 batch 0 train Loss 5.2202 test Loss 5.3325 with MSE metric 6252.0752\n",
      "Epoch 222 batch 100 train Loss 5.2505 test Loss 5.3075 with MSE metric 6607.4844\n",
      "Epoch 222 batch 200 train Loss 5.3074 test Loss 5.3415 with MSE metric 7484.6191\n",
      "Time taken for 1 epoch: 23.151283979415894 secs\n",
      "\n",
      "Epoch 223 batch 0 train Loss 5.4026 test Loss 5.3416 with MSE metric 8936.3818\n",
      "Epoch 223 batch 100 train Loss 5.2943 test Loss 5.4136 with MSE metric 7296.7681\n",
      "Epoch 223 batch 200 train Loss 5.3567 test Loss 5.4770 with MSE metric 8267.9648\n",
      "Time taken for 1 epoch: 23.164669036865234 secs\n",
      "\n",
      "Epoch 224 batch 0 train Loss 5.2779 test Loss 5.2796 with MSE metric 7062.3184\n",
      "Epoch 224 batch 100 train Loss 5.3274 test Loss 5.3211 with MSE metric 7776.3867\n",
      "Epoch 224 batch 200 train Loss 5.3668 test Loss 5.3518 with MSE metric 8341.2021\n",
      "Time taken for 1 epoch: 23.163715839385986 secs\n",
      "\n",
      "Epoch 225 batch 0 train Loss 5.3144 test Loss 5.3093 with MSE metric 7597.6260\n",
      "Epoch 225 batch 100 train Loss 5.2781 test Loss 5.3838 with MSE metric 7054.7686\n",
      "Epoch 225 batch 200 train Loss 5.3173 test Loss 5.3801 with MSE metric 7632.0303\n",
      "Time taken for 1 epoch: 23.19006323814392 secs\n",
      "\n",
      "Epoch 226 batch 0 train Loss 5.3358 test Loss 5.4256 with MSE metric 7910.9463\n",
      "Epoch 226 batch 100 train Loss 5.2767 test Loss 5.3737 with MSE metric 7040.0278\n",
      "Epoch 226 batch 200 train Loss 5.2585 test Loss 5.2866 with MSE metric 6722.7705\n",
      "Time taken for 1 epoch: 23.14802885055542 secs\n",
      "\n",
      "Epoch 227 batch 0 train Loss 5.2897 test Loss 5.3597 with MSE metric 7231.1768\n",
      "Epoch 227 batch 100 train Loss 5.3635 test Loss 5.3577 with MSE metric 8337.5469\n",
      "Epoch 227 batch 200 train Loss 5.2618 test Loss 5.3193 with MSE metric 6831.5381\n",
      "Time taken for 1 epoch: 23.134503841400146 secs\n",
      "\n",
      "Epoch 228 batch 0 train Loss 5.3097 test Loss 5.4613 with MSE metric 7523.3911\n",
      "Epoch 228 batch 100 train Loss 5.4002 test Loss 5.4160 with MSE metric 8881.1260\n",
      "Epoch 228 batch 200 train Loss 5.2831 test Loss 5.4529 with MSE metric 7135.5493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 23.17144203186035 secs\n",
      "\n",
      "Epoch 229 batch 0 train Loss 5.2336 test Loss 5.4316 with MSE metric 6433.0605\n",
      "Epoch 229 batch 100 train Loss 5.3067 test Loss 5.2953 with MSE metric 7473.4419\n",
      "Epoch 229 batch 200 train Loss 5.3649 test Loss 5.3475 with MSE metric 8391.4287\n",
      "Time taken for 1 epoch: 23.123114109039307 secs\n",
      "\n",
      "Epoch 230 batch 0 train Loss 5.3778 test Loss 5.3744 with MSE metric 8471.8984\n",
      "Epoch 230 batch 100 train Loss 5.3367 test Loss 5.3845 with MSE metric 7938.9873\n",
      "Epoch 230 batch 200 train Loss 5.3299 test Loss 5.2565 with MSE metric 7831.7021\n",
      "Time taken for 1 epoch: 23.15724515914917 secs\n",
      "\n",
      "Epoch 231 batch 0 train Loss 5.3169 test Loss 5.2395 with MSE metric 7631.6162\n",
      "Epoch 231 batch 100 train Loss 5.4722 test Loss 5.2108 with MSE metric 9875.9082\n",
      "Epoch 231 batch 200 train Loss 5.2741 test Loss 5.3246 with MSE metric 7006.8857\n",
      "Time taken for 1 epoch: 23.162497997283936 secs\n",
      "\n",
      "Epoch 232 batch 0 train Loss 5.3449 test Loss 5.2904 with MSE metric 8018.5669\n",
      "Epoch 232 batch 100 train Loss 5.3249 test Loss 5.4084 with MSE metric 7758.3057\n",
      "Epoch 232 batch 200 train Loss 5.0879 test Loss 5.4190 with MSE metric 4465.9180\n",
      "Time taken for 1 epoch: 23.260327100753784 secs\n",
      "\n",
      "Epoch 233 batch 0 train Loss 5.2973 test Loss 5.5561 with MSE metric 7324.1748\n",
      "Epoch 233 batch 100 train Loss 5.3664 test Loss 5.3379 with MSE metric 8377.1973\n",
      "Epoch 233 batch 200 train Loss 5.2900 test Loss 5.2813 with MSE metric 7223.0366\n",
      "Time taken for 1 epoch: 23.155609846115112 secs\n",
      "\n",
      "Epoch 234 batch 0 train Loss 5.2477 test Loss 5.2866 with MSE metric 6574.2314\n",
      "Epoch 234 batch 100 train Loss 5.2493 test Loss 5.2583 with MSE metric 6651.3364\n",
      "Epoch 234 batch 200 train Loss 5.4179 test Loss 5.2213 with MSE metric 9079.4434\n",
      "Time taken for 1 epoch: 23.210586071014404 secs\n",
      "\n",
      "Epoch 235 batch 0 train Loss 5.2784 test Loss 5.4672 with MSE metric 7060.3218\n",
      "Epoch 235 batch 100 train Loss 5.3429 test Loss 5.3701 with MSE metric 8026.2637\n",
      "Epoch 235 batch 200 train Loss 5.2233 test Loss 5.3297 with MSE metric 6292.3496\n",
      "Time taken for 1 epoch: 23.291099071502686 secs\n",
      "\n",
      "Epoch 236 batch 0 train Loss 5.2465 test Loss 5.3647 with MSE metric 6630.5781\n",
      "Epoch 236 batch 100 train Loss 5.1859 test Loss 5.3005 with MSE metric 5714.4946\n",
      "Epoch 236 batch 200 train Loss 5.3592 test Loss 5.4409 with MSE metric 8266.4150\n",
      "Time taken for 1 epoch: 23.046234846115112 secs\n",
      "\n",
      "Epoch 237 batch 0 train Loss 5.2905 test Loss 5.4436 with MSE metric 7229.2197\n",
      "Epoch 237 batch 100 train Loss 5.2693 test Loss 5.3111 with MSE metric 6927.9712\n",
      "Epoch 237 batch 200 train Loss 5.2789 test Loss 5.3784 with MSE metric 7073.0952\n",
      "Time taken for 1 epoch: 23.13445806503296 secs\n",
      "\n",
      "Epoch 238 batch 0 train Loss 5.3226 test Loss 5.4426 with MSE metric 7718.0571\n",
      "Epoch 238 batch 100 train Loss 5.2398 test Loss 5.2766 with MSE metric 6502.4961\n",
      "Epoch 238 batch 200 train Loss 5.2370 test Loss 5.3384 with MSE metric 6453.2681\n",
      "Time taken for 1 epoch: 23.21030879020691 secs\n",
      "\n",
      "Epoch 239 batch 0 train Loss 5.3122 test Loss 5.4114 with MSE metric 7563.0654\n",
      "Epoch 239 batch 100 train Loss 5.3176 test Loss 5.3634 with MSE metric 7645.7764\n",
      "Epoch 239 batch 200 train Loss 5.2974 test Loss 5.4050 with MSE metric 7311.9961\n",
      "Time taken for 1 epoch: 23.239398956298828 secs\n",
      "\n",
      "Epoch 240 batch 0 train Loss 5.2027 test Loss 5.3393 with MSE metric 5909.2017\n",
      "Epoch 240 batch 100 train Loss 5.3329 test Loss 5.3994 with MSE metric 7883.7256\n",
      "Epoch 240 batch 200 train Loss 5.3927 test Loss 5.3658 with MSE metric 8729.4102\n",
      "Time taken for 1 epoch: 23.13715887069702 secs\n",
      "\n",
      "Epoch 241 batch 0 train Loss 5.3389 test Loss 5.4212 with MSE metric 7973.3735\n",
      "Epoch 241 batch 100 train Loss 5.2303 test Loss 5.3433 with MSE metric 6356.0361\n",
      "Epoch 241 batch 200 train Loss 5.3403 test Loss 5.3695 with MSE metric 7933.2188\n",
      "Time taken for 1 epoch: 23.166405200958252 secs\n",
      "\n",
      "Epoch 242 batch 0 train Loss 5.3025 test Loss 5.2763 with MSE metric 7399.4902\n",
      "Epoch 242 batch 100 train Loss 5.2600 test Loss 5.3084 with MSE metric 6712.5957\n",
      "Epoch 242 batch 200 train Loss 5.3284 test Loss 5.4278 with MSE metric 7804.6016\n",
      "Time taken for 1 epoch: 23.180660009384155 secs\n",
      "\n",
      "Epoch 243 batch 0 train Loss 5.3506 test Loss 5.3715 with MSE metric 8143.9121\n",
      "Epoch 243 batch 100 train Loss 5.3333 test Loss 5.3769 with MSE metric 7857.9658\n",
      "Epoch 243 batch 200 train Loss 5.2635 test Loss 5.3503 with MSE metric 6862.2852\n",
      "Time taken for 1 epoch: 23.201040744781494 secs\n",
      "\n",
      "Epoch 244 batch 0 train Loss 5.3274 test Loss 5.2561 with MSE metric 7792.8623\n",
      "Epoch 244 batch 100 train Loss 5.2644 test Loss 5.4268 with MSE metric 6845.1689\n",
      "Epoch 244 batch 200 train Loss 5.3104 test Loss 5.3005 with MSE metric 7526.7441\n",
      "Time taken for 1 epoch: 23.10721206665039 secs\n",
      "\n",
      "Epoch 245 batch 0 train Loss 5.3105 test Loss 5.2796 with MSE metric 7534.4072\n",
      "Epoch 245 batch 100 train Loss 5.2998 test Loss 5.3233 with MSE metric 7379.0605\n",
      "Epoch 245 batch 200 train Loss 5.3077 test Loss 5.3491 with MSE metric 7495.6646\n",
      "Time taken for 1 epoch: 23.199326992034912 secs\n",
      "\n",
      "Epoch 246 batch 0 train Loss 5.3191 test Loss 5.2569 with MSE metric 7664.6113\n",
      "Epoch 246 batch 100 train Loss 5.3183 test Loss 5.3819 with MSE metric 7639.5234\n",
      "Epoch 246 batch 200 train Loss 5.3438 test Loss 5.3488 with MSE metric 8034.2578\n",
      "Time taken for 1 epoch: 23.21575093269348 secs\n",
      "\n",
      "Epoch 247 batch 0 train Loss 5.3210 test Loss 5.4114 with MSE metric 7691.3809\n",
      "Epoch 247 batch 100 train Loss 5.3213 test Loss 5.4225 with MSE metric 7695.4790\n",
      "Epoch 247 batch 200 train Loss 5.2195 test Loss 5.2635 with MSE metric 6123.4229\n",
      "Time taken for 1 epoch: 23.689433813095093 secs\n",
      "\n",
      "Epoch 248 batch 0 train Loss 5.3602 test Loss 5.4336 with MSE metric 8227.4648\n",
      "Epoch 248 batch 100 train Loss 5.2866 test Loss 5.3390 with MSE metric 7177.1191\n",
      "Epoch 248 batch 200 train Loss 5.3304 test Loss 5.4837 with MSE metric 7800.6221\n",
      "Time taken for 1 epoch: 23.757781982421875 secs\n",
      "\n",
      "Epoch 249 batch 0 train Loss 5.3428 test Loss 5.2831 with MSE metric 8033.1572\n",
      "Epoch 249 batch 100 train Loss 5.3607 test Loss 5.4671 with MSE metric 8288.2422\n",
      "Epoch 249 batch 200 train Loss 5.3516 test Loss 5.4293 with MSE metric 8137.3086\n",
      "Time taken for 1 epoch: 23.125438928604126 secs\n",
      "\n",
      "Epoch 250 batch 0 train Loss 5.2264 test Loss 5.2332 with MSE metric 6292.9229\n",
      "Epoch 250 batch 100 train Loss 5.2965 test Loss 5.3914 with MSE metric 7327.7031\n",
      "Epoch 250 batch 200 train Loss 5.3459 test Loss 5.3560 with MSE metric 8069.0996\n",
      "Time taken for 1 epoch: 23.4638991355896 secs\n",
      "\n",
      "Epoch 251 batch 0 train Loss 5.4255 test Loss 5.3810 with MSE metric 9355.7354\n",
      "Epoch 251 batch 100 train Loss 5.3126 test Loss 5.3809 with MSE metric 7565.5908\n",
      "Epoch 251 batch 200 train Loss 5.2993 test Loss 5.3196 with MSE metric 7368.3584\n",
      "Time taken for 1 epoch: 23.26264500617981 secs\n",
      "\n",
      "Epoch 252 batch 0 train Loss 5.2877 test Loss 5.4214 with MSE metric 7202.4521\n",
      "Epoch 252 batch 100 train Loss 5.3367 test Loss 5.3227 with MSE metric 7845.1719\n",
      "Epoch 252 batch 200 train Loss 5.1939 test Loss 5.3520 with MSE metric 5861.1602\n",
      "Time taken for 1 epoch: 23.215415000915527 secs\n",
      "\n",
      "Epoch 253 batch 0 train Loss 5.1918 test Loss 5.3086 with MSE metric 5785.6533\n",
      "Epoch 253 batch 100 train Loss 5.3007 test Loss 5.3237 with MSE metric 7392.1211\n",
      "Epoch 253 batch 200 train Loss 5.2042 test Loss 5.3944 with MSE metric 5854.3623\n",
      "Time taken for 1 epoch: 23.32692575454712 secs\n",
      "\n",
      "Epoch 254 batch 0 train Loss 5.3746 test Loss 5.3611 with MSE metric 8466.9062\n",
      "Epoch 254 batch 100 train Loss 5.3067 test Loss 5.3596 with MSE metric 7481.1982\n",
      "Epoch 254 batch 200 train Loss 5.3007 test Loss 5.4107 with MSE metric 7391.1362\n",
      "Time taken for 1 epoch: 23.196768045425415 secs\n",
      "\n",
      "Epoch 255 batch 0 train Loss 5.2036 test Loss 5.3602 with MSE metric 5968.1572\n",
      "Epoch 255 batch 100 train Loss 5.2637 test Loss 5.2286 with MSE metric 6857.6514\n",
      "Epoch 255 batch 200 train Loss 5.3648 test Loss 5.2933 with MSE metric 8387.3926\n",
      "Time taken for 1 epoch: 23.109934091567993 secs\n",
      "\n",
      "Epoch 256 batch 0 train Loss 5.3339 test Loss 5.3679 with MSE metric 7898.0645\n",
      "Epoch 256 batch 100 train Loss 5.2364 test Loss 5.4442 with MSE metric 6462.2573\n",
      "Epoch 256 batch 200 train Loss 5.3147 test Loss 5.3295 with MSE metric 7587.6914\n",
      "Time taken for 1 epoch: 23.142413854599 secs\n",
      "\n",
      "Epoch 257 batch 0 train Loss 5.2205 test Loss 5.3420 with MSE metric 6210.2090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 batch 100 train Loss 5.2753 test Loss 5.3443 with MSE metric 7002.5391\n",
      "Epoch 257 batch 200 train Loss 5.2746 test Loss 5.4446 with MSE metric 7009.8994\n",
      "Time taken for 1 epoch: 23.133349895477295 secs\n",
      "\n",
      "Epoch 258 batch 0 train Loss 5.3014 test Loss 5.3363 with MSE metric 7371.5186\n",
      "Epoch 258 batch 100 train Loss 5.3347 test Loss 5.3936 with MSE metric 7898.4062\n",
      "Epoch 258 batch 200 train Loss 5.3587 test Loss 5.3888 with MSE metric 8287.9590\n",
      "Time taken for 1 epoch: 23.10762119293213 secs\n",
      "\n",
      "Epoch 259 batch 0 train Loss 5.3163 test Loss 5.4252 with MSE metric 7624.1250\n",
      "Epoch 259 batch 100 train Loss 5.3224 test Loss 5.3389 with MSE metric 7710.8877\n",
      "Epoch 259 batch 200 train Loss 5.2472 test Loss 5.3509 with MSE metric 6556.4106\n",
      "Time taken for 1 epoch: 24.049879789352417 secs\n",
      "\n",
      "Epoch 260 batch 0 train Loss 5.3278 test Loss 5.3569 with MSE metric 7803.4951\n",
      "Epoch 260 batch 100 train Loss 5.3774 test Loss 5.3209 with MSE metric 8546.0918\n",
      "Epoch 260 batch 200 train Loss 5.2635 test Loss 5.4236 with MSE metric 6854.5977\n",
      "Time taken for 1 epoch: 25.76525378227234 secs\n",
      "\n",
      "Epoch 261 batch 0 train Loss 5.3362 test Loss 5.3394 with MSE metric 7916.4497\n",
      "Epoch 261 batch 100 train Loss 5.1971 test Loss 5.3324 with MSE metric 5778.6943\n",
      "Epoch 261 batch 200 train Loss 5.2903 test Loss 5.3434 with MSE metric 7225.7881\n",
      "Time taken for 1 epoch: 24.700669050216675 secs\n",
      "\n",
      "Epoch 262 batch 0 train Loss 5.2601 test Loss 5.3094 with MSE metric 6800.2046\n",
      "Epoch 262 batch 100 train Loss 5.3542 test Loss 5.3825 with MSE metric 8194.8145\n",
      "Epoch 262 batch 200 train Loss 5.2993 test Loss 5.3732 with MSE metric 7372.1865\n",
      "Time taken for 1 epoch: 23.9461350440979 secs\n",
      "\n",
      "Epoch 263 batch 0 train Loss 5.3488 test Loss 5.3438 with MSE metric 8089.5615\n",
      "Epoch 263 batch 100 train Loss 5.2329 test Loss 5.2666 with MSE metric 6364.7485\n",
      "Epoch 263 batch 200 train Loss 5.3557 test Loss 5.3549 with MSE metric 8234.8232\n",
      "Time taken for 1 epoch: 23.901084184646606 secs\n",
      "\n",
      "Epoch 264 batch 0 train Loss 5.3019 test Loss 5.5040 with MSE metric 7408.2188\n",
      "Epoch 264 batch 100 train Loss 5.3459 test Loss 5.3833 with MSE metric 8073.0645\n",
      "Epoch 264 batch 200 train Loss 5.3360 test Loss 5.3160 with MSE metric 7898.8267\n",
      "Time taken for 1 epoch: 23.930448055267334 secs\n",
      "\n",
      "Epoch 265 batch 0 train Loss 5.3188 test Loss 5.4167 with MSE metric 7642.9697\n",
      "Epoch 265 batch 100 train Loss 5.2528 test Loss 5.3639 with MSE metric 6635.0684\n",
      "Epoch 265 batch 200 train Loss 5.3337 test Loss 5.3751 with MSE metric 7894.5864\n",
      "Time taken for 1 epoch: 25.14027714729309 secs\n",
      "\n",
      "Epoch 266 batch 0 train Loss 5.2578 test Loss 5.3899 with MSE metric 6750.0581\n",
      "Epoch 266 batch 100 train Loss 5.2169 test Loss 5.4496 with MSE metric 6122.0308\n",
      "Epoch 266 batch 200 train Loss 5.2192 test Loss 5.3933 with MSE metric 6207.8164\n",
      "Time taken for 1 epoch: 25.18221116065979 secs\n",
      "\n",
      "Epoch 267 batch 0 train Loss 5.1812 test Loss 5.3823 with MSE metric 5722.1470\n",
      "Epoch 267 batch 100 train Loss 5.3247 test Loss 5.2089 with MSE metric 7744.9277\n",
      "Epoch 267 batch 200 train Loss 5.2477 test Loss 5.3097 with MSE metric 6611.3013\n",
      "Time taken for 1 epoch: 24.709428071975708 secs\n",
      "\n",
      "Epoch 268 batch 0 train Loss 5.2414 test Loss 5.4210 with MSE metric 6494.2920\n",
      "Epoch 268 batch 100 train Loss 5.2609 test Loss 5.2475 with MSE metric 6777.0884\n",
      "Epoch 268 batch 200 train Loss 5.3129 test Loss 5.4023 with MSE metric 7568.3501\n",
      "Time taken for 1 epoch: 24.23287010192871 secs\n",
      "\n",
      "Epoch 269 batch 0 train Loss 5.3052 test Loss 5.3169 with MSE metric 7455.5137\n",
      "Epoch 269 batch 100 train Loss 5.2408 test Loss 5.3339 with MSE metric 6446.1758\n",
      "Epoch 269 batch 200 train Loss 5.3597 test Loss 5.3057 with MSE metric 8248.0000\n",
      "Time taken for 1 epoch: 24.38880205154419 secs\n",
      "\n",
      "Epoch 270 batch 0 train Loss 5.3696 test Loss 5.4206 with MSE metric 8382.0381\n",
      "Epoch 270 batch 100 train Loss 5.2785 test Loss 5.3744 with MSE metric 7054.9277\n",
      "Epoch 270 batch 200 train Loss 5.3029 test Loss 5.3650 with MSE metric 7425.1533\n",
      "Time taken for 1 epoch: 24.027637004852295 secs\n",
      "\n",
      "Epoch 271 batch 0 train Loss 5.4019 test Loss 5.3790 with MSE metric 8806.2031\n",
      "Epoch 271 batch 100 train Loss 5.2593 test Loss 5.4657 with MSE metric 6747.3159\n",
      "Epoch 271 batch 200 train Loss 5.3563 test Loss 5.2584 with MSE metric 8194.9111\n",
      "Time taken for 1 epoch: 23.98897910118103 secs\n",
      "\n",
      "Epoch 272 batch 0 train Loss 5.3403 test Loss 5.4003 with MSE metric 7975.9053\n",
      "Epoch 272 batch 100 train Loss 5.2799 test Loss 5.4534 with MSE metric 7084.4976\n",
      "Epoch 272 batch 200 train Loss 5.3240 test Loss 5.4365 with MSE metric 7744.9692\n",
      "Time taken for 1 epoch: 24.696758031845093 secs\n",
      "\n",
      "Epoch 273 batch 0 train Loss 5.3322 test Loss 5.3099 with MSE metric 7773.1621\n",
      "Epoch 273 batch 100 train Loss 5.2221 test Loss 5.3088 with MSE metric 6163.9404\n",
      "Epoch 273 batch 200 train Loss 5.2584 test Loss 5.3386 with MSE metric 6784.8828\n",
      "Time taken for 1 epoch: 24.10861897468567 secs\n",
      "\n",
      "Epoch 274 batch 0 train Loss 5.3694 test Loss 5.3865 with MSE metric 8384.8516\n",
      "Epoch 274 batch 100 train Loss 5.2920 test Loss 5.2737 with MSE metric 7265.2275\n",
      "Epoch 274 batch 200 train Loss 5.3815 test Loss 5.3704 with MSE metric 8486.1641\n",
      "Time taken for 1 epoch: 23.297632932662964 secs\n",
      "\n",
      "Epoch 275 batch 0 train Loss 5.3236 test Loss 5.3590 with MSE metric 7738.6377\n",
      "Epoch 275 batch 100 train Loss 5.3430 test Loss 5.3724 with MSE metric 8031.3296\n",
      "Epoch 275 batch 200 train Loss 5.2171 test Loss 5.3565 with MSE metric 6123.0444\n",
      "Time taken for 1 epoch: 23.28272795677185 secs\n",
      "\n",
      "Epoch 276 batch 0 train Loss 5.2018 test Loss 5.4662 with MSE metric 5934.4932\n",
      "Epoch 276 batch 100 train Loss 5.2628 test Loss 5.3380 with MSE metric 6847.9009\n",
      "Epoch 276 batch 200 train Loss 5.2060 test Loss 5.3212 with MSE metric 5945.2432\n",
      "Time taken for 1 epoch: 23.289544343948364 secs\n",
      "\n",
      "Epoch 277 batch 0 train Loss 5.2537 test Loss 5.3602 with MSE metric 6689.9785\n",
      "Epoch 277 batch 100 train Loss 5.2561 test Loss 5.3319 with MSE metric 6653.9209\n",
      "Epoch 277 batch 200 train Loss 5.1831 test Loss 5.3314 with MSE metric 5728.1606\n",
      "Time taken for 1 epoch: 23.30477523803711 secs\n",
      "\n",
      "Epoch 278 batch 0 train Loss 5.2620 test Loss 5.2940 with MSE metric 6828.9561\n",
      "Epoch 278 batch 100 train Loss 5.3343 test Loss 5.4958 with MSE metric 7877.2163\n",
      "Epoch 278 batch 200 train Loss 5.2988 test Loss 5.2897 with MSE metric 7364.9941\n",
      "Time taken for 1 epoch: 23.173362970352173 secs\n",
      "\n",
      "Epoch 279 batch 0 train Loss 5.3422 test Loss 5.2871 with MSE metric 8019.3140\n",
      "Epoch 279 batch 100 train Loss 5.1888 test Loss 5.4020 with MSE metric 5774.3726\n",
      "Epoch 279 batch 200 train Loss 5.2759 test Loss 5.3288 with MSE metric 7027.5171\n",
      "Time taken for 1 epoch: 23.26933717727661 secs\n",
      "\n",
      "Epoch 280 batch 0 train Loss 5.1984 test Loss 5.3982 with MSE metric 5934.1001\n",
      "Epoch 280 batch 100 train Loss 5.2832 test Loss 5.4037 with MSE metric 7129.3560\n",
      "Epoch 280 batch 200 train Loss 5.2598 test Loss 5.2805 with MSE metric 6790.0181\n",
      "Time taken for 1 epoch: 23.34147882461548 secs\n",
      "\n",
      "Epoch 281 batch 0 train Loss 5.3504 test Loss 5.3214 with MSE metric 8089.0684\n",
      "Epoch 281 batch 100 train Loss 5.2827 test Loss 5.3919 with MSE metric 7124.9648\n",
      "Epoch 281 batch 200 train Loss 5.3133 test Loss 5.3578 with MSE metric 7570.9707\n",
      "Time taken for 1 epoch: 23.287298917770386 secs\n",
      "\n",
      "Epoch 282 batch 0 train Loss 5.2987 test Loss 5.2911 with MSE metric 7339.8315\n",
      "Epoch 282 batch 100 train Loss 5.2797 test Loss 5.4039 with MSE metric 7077.7764\n",
      "Epoch 282 batch 200 train Loss 5.2323 test Loss 5.2948 with MSE metric 6366.9678\n",
      "Time taken for 1 epoch: 23.221982955932617 secs\n",
      "\n",
      "Epoch 283 batch 0 train Loss 5.3351 test Loss 5.4678 with MSE metric 7896.0166\n",
      "Epoch 283 batch 100 train Loss 5.3039 test Loss 5.3773 with MSE metric 7424.3540\n",
      "Epoch 283 batch 200 train Loss 5.3235 test Loss 5.2862 with MSE metric 7723.2925\n",
      "Time taken for 1 epoch: 23.30935502052307 secs\n",
      "\n",
      "Epoch 284 batch 0 train Loss 5.2096 test Loss 5.3809 with MSE metric 6075.1333\n",
      "Epoch 284 batch 100 train Loss 5.3216 test Loss 5.3157 with MSE metric 7707.5034\n",
      "Epoch 284 batch 200 train Loss 5.3222 test Loss 5.3685 with MSE metric 7715.1572\n",
      "Time taken for 1 epoch: 23.29867696762085 secs\n",
      "\n",
      "Epoch 285 batch 0 train Loss 5.4176 test Loss 5.3263 with MSE metric 9081.1406\n",
      "Epoch 285 batch 100 train Loss 5.2809 test Loss 5.3258 with MSE metric 7100.7319\n",
      "Epoch 285 batch 200 train Loss 5.2432 test Loss 5.3927 with MSE metric 6520.7632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 23.158220052719116 secs\n",
      "\n",
      "Epoch 286 batch 0 train Loss 5.2621 test Loss 5.3532 with MSE metric 6797.2153\n",
      "Epoch 286 batch 100 train Loss 5.2740 test Loss 5.2791 with MSE metric 7007.1943\n",
      "Epoch 286 batch 200 train Loss 5.3781 test Loss 5.3059 with MSE metric 8497.1260\n",
      "Time taken for 1 epoch: 23.401750802993774 secs\n",
      "\n",
      "Epoch 287 batch 0 train Loss 5.3605 test Loss 5.3073 with MSE metric 8297.5811\n",
      "Epoch 287 batch 100 train Loss 5.2811 test Loss 5.3852 with MSE metric 7108.2197\n",
      "Epoch 287 batch 200 train Loss 5.2721 test Loss 5.5013 with MSE metric 6981.6021\n",
      "Time taken for 1 epoch: 23.21450114250183 secs\n",
      "\n",
      "Epoch 288 batch 0 train Loss 5.2322 test Loss 5.4484 with MSE metric 6363.2090\n",
      "Epoch 288 batch 100 train Loss 5.3554 test Loss 5.3508 with MSE metric 8149.1240\n",
      "Epoch 288 batch 200 train Loss 5.3184 test Loss 5.2972 with MSE metric 7657.6875\n",
      "Time taken for 1 epoch: 23.682854890823364 secs\n",
      "\n",
      "Epoch 289 batch 0 train Loss 5.3994 test Loss 5.3317 with MSE metric 8843.4238\n",
      "Epoch 289 batch 100 train Loss 5.2111 test Loss 5.3319 with MSE metric 6056.5601\n",
      "Epoch 289 batch 200 train Loss 5.3046 test Loss 5.4568 with MSE metric 7449.6250\n",
      "Time taken for 1 epoch: 23.357975006103516 secs\n",
      "\n",
      "Epoch 290 batch 0 train Loss 5.3869 test Loss 5.3636 with MSE metric 8543.2871\n",
      "Epoch 290 batch 100 train Loss 5.3094 test Loss 5.3038 with MSE metric 7521.3223\n",
      "Epoch 290 batch 200 train Loss 5.2424 test Loss 5.4500 with MSE metric 6553.2490\n",
      "Time taken for 1 epoch: 23.37054681777954 secs\n",
      "\n",
      "Epoch 291 batch 0 train Loss 5.3787 test Loss 5.3699 with MSE metric 8566.3848\n",
      "Epoch 291 batch 100 train Loss 5.3217 test Loss 5.3097 with MSE metric 7676.3203\n",
      "Epoch 291 batch 200 train Loss 5.2585 test Loss 5.3368 with MSE metric 6778.0054\n",
      "Time taken for 1 epoch: 23.42210102081299 secs\n",
      "\n",
      "Epoch 292 batch 0 train Loss 5.3665 test Loss 5.2977 with MSE metric 8420.3027\n",
      "Epoch 292 batch 100 train Loss 5.2567 test Loss 5.2490 with MSE metric 6726.5859\n",
      "Epoch 292 batch 200 train Loss 5.3008 test Loss 5.4185 with MSE metric 7392.1621\n",
      "Time taken for 1 epoch: 26.026148080825806 secs\n",
      "\n",
      "Epoch 293 batch 0 train Loss 5.2706 test Loss 5.3252 with MSE metric 6958.7617\n",
      "Epoch 293 batch 100 train Loss 5.3118 test Loss 5.4073 with MSE metric 7534.5894\n",
      "Epoch 293 batch 200 train Loss 5.3854 test Loss 5.3958 with MSE metric 8731.7070\n",
      "Time taken for 1 epoch: 24.771631956100464 secs\n",
      "\n",
      "Epoch 294 batch 0 train Loss 5.3173 test Loss 5.3427 with MSE metric 7630.7900\n",
      "Epoch 294 batch 100 train Loss 5.3092 test Loss 5.2417 with MSE metric 7518.6582\n",
      "Epoch 294 batch 200 train Loss 5.3825 test Loss 5.4654 with MSE metric 8618.2852\n",
      "Time taken for 1 epoch: 26.531928062438965 secs\n",
      "\n",
      "Epoch 295 batch 0 train Loss 5.3048 test Loss 5.3912 with MSE metric 7436.0918\n",
      "Epoch 295 batch 100 train Loss 5.3430 test Loss 5.4427 with MSE metric 8003.9521\n",
      "Epoch 295 batch 200 train Loss 5.2667 test Loss 5.3891 with MSE metric 6905.0664\n",
      "Time taken for 1 epoch: 24.380868911743164 secs\n",
      "\n",
      "Epoch 296 batch 0 train Loss 5.3146 test Loss 5.4079 with MSE metric 7599.2500\n",
      "Epoch 296 batch 100 train Loss 5.3402 test Loss 5.3194 with MSE metric 7962.5986\n",
      "Epoch 296 batch 200 train Loss 5.4307 test Loss 5.4412 with MSE metric 9297.5264\n",
      "Time taken for 1 epoch: 25.429244995117188 secs\n",
      "\n",
      "Epoch 297 batch 0 train Loss 5.2829 test Loss 5.3905 with MSE metric 7113.6885\n",
      "Epoch 297 batch 100 train Loss 5.3200 test Loss 5.3547 with MSE metric 7682.6875\n",
      "Epoch 297 batch 200 train Loss 5.3149 test Loss 5.3709 with MSE metric 7591.0840\n",
      "Time taken for 1 epoch: 24.79467797279358 secs\n",
      "\n",
      "Epoch 298 batch 0 train Loss 5.4246 test Loss 5.2901 with MSE metric 9098.5781\n",
      "Epoch 298 batch 100 train Loss 5.3105 test Loss 5.2803 with MSE metric 7537.7949\n",
      "Epoch 298 batch 200 train Loss 5.3061 test Loss 5.3027 with MSE metric 7468.6924\n",
      "Time taken for 1 epoch: 26.983388900756836 secs\n",
      "\n",
      "Epoch 299 batch 0 train Loss 5.2986 test Loss 5.3693 with MSE metric 7345.7144\n",
      "Epoch 299 batch 100 train Loss 5.2664 test Loss 5.3761 with MSE metric 6875.0371\n",
      "Epoch 299 batch 200 train Loss 5.2995 test Loss 5.3001 with MSE metric 7370.6309\n",
      "Time taken for 1 epoch: 25.535714149475098 secs\n",
      "\n",
      "Epoch 300 batch 0 train Loss 5.3254 test Loss 5.3699 with MSE metric 7686.2998\n",
      "Epoch 300 batch 100 train Loss 5.2803 test Loss 5.2991 with MSE metric 7038.2559\n",
      "Epoch 300 batch 200 train Loss 5.3215 test Loss 5.4733 with MSE metric 7698.0732\n",
      "Time taken for 1 epoch: 26.183008193969727 secs\n",
      "\n",
      "Epoch 301 batch 0 train Loss 5.2486 test Loss 5.3358 with MSE metric 6617.2598\n",
      "Epoch 301 batch 100 train Loss 5.2687 test Loss 5.3426 with MSE metric 6895.7109\n",
      "Epoch 301 batch 200 train Loss 5.2616 test Loss 5.3102 with MSE metric 6822.0527\n",
      "Time taken for 1 epoch: 27.219812870025635 secs\n",
      "\n",
      "Epoch 302 batch 0 train Loss 5.3626 test Loss 5.3011 with MSE metric 8291.3633\n",
      "Epoch 302 batch 100 train Loss 5.2934 test Loss 5.4582 with MSE metric 7285.6411\n",
      "Epoch 302 batch 200 train Loss 5.3626 test Loss 5.3573 with MSE metric 8351.6074\n",
      "Time taken for 1 epoch: 26.279377937316895 secs\n",
      "\n",
      "Epoch 303 batch 0 train Loss 5.3127 test Loss 5.4093 with MSE metric 7563.2051\n",
      "Epoch 303 batch 100 train Loss 5.2389 test Loss 5.4040 with MSE metric 6480.4053\n",
      "Epoch 303 batch 200 train Loss 5.2548 test Loss 5.4163 with MSE metric 6738.7778\n",
      "Time taken for 1 epoch: 25.79403281211853 secs\n",
      "\n",
      "Epoch 304 batch 0 train Loss 5.2651 test Loss 5.3883 with MSE metric 6883.8076\n",
      "Epoch 304 batch 100 train Loss 5.4102 test Loss 5.4172 with MSE metric 9069.0254\n",
      "Epoch 304 batch 200 train Loss 5.2488 test Loss 5.4184 with MSE metric 6657.7090\n",
      "Time taken for 1 epoch: 27.314255952835083 secs\n",
      "\n",
      "Epoch 305 batch 0 train Loss 5.3457 test Loss 5.3153 with MSE metric 8086.4326\n",
      "Epoch 305 batch 100 train Loss 5.3393 test Loss 5.3854 with MSE metric 7952.3105\n",
      "Epoch 305 batch 200 train Loss 5.3312 test Loss 5.3395 with MSE metric 7792.9521\n",
      "Time taken for 1 epoch: 25.156715869903564 secs\n",
      "\n",
      "Epoch 306 batch 0 train Loss 5.1985 test Loss 5.4545 with MSE metric 5993.3882\n",
      "Epoch 306 batch 100 train Loss 5.2646 test Loss 5.3315 with MSE metric 6876.4346\n",
      "Epoch 306 batch 200 train Loss 5.2344 test Loss 5.3055 with MSE metric 6406.3545\n",
      "Time taken for 1 epoch: 24.442389011383057 secs\n",
      "\n",
      "Epoch 307 batch 0 train Loss 5.3036 test Loss 5.4333 with MSE metric 7432.2793\n",
      "Epoch 307 batch 100 train Loss 5.3436 test Loss 5.3940 with MSE metric 7975.6636\n",
      "Epoch 307 batch 200 train Loss 5.3028 test Loss 5.3444 with MSE metric 7423.7036\n",
      "Time taken for 1 epoch: 26.444589138031006 secs\n",
      "\n",
      "Epoch 308 batch 0 train Loss 5.3491 test Loss 5.3468 with MSE metric 8118.1514\n",
      "Epoch 308 batch 100 train Loss 5.3029 test Loss 5.4446 with MSE metric 7412.1118\n",
      "Epoch 308 batch 200 train Loss 5.2512 test Loss 5.3498 with MSE metric 6598.6094\n",
      "Time taken for 1 epoch: 25.643372058868408 secs\n",
      "\n",
      "Epoch 309 batch 0 train Loss 5.3374 test Loss 5.3838 with MSE metric 7937.3638\n",
      "Epoch 309 batch 100 train Loss 5.3248 test Loss 5.2693 with MSE metric 7755.7129\n",
      "Epoch 309 batch 200 train Loss 5.2886 test Loss 5.3925 with MSE metric 7209.9932\n",
      "Time taken for 1 epoch: 25.68454599380493 secs\n",
      "\n",
      "Epoch 310 batch 0 train Loss 5.3621 test Loss 5.3785 with MSE metric 8294.0254\n",
      "Epoch 310 batch 100 train Loss 5.1836 test Loss 5.4820 with MSE metric 5637.9355\n",
      "Epoch 310 batch 200 train Loss 5.3335 test Loss 5.3401 with MSE metric 7892.4341\n",
      "Time taken for 1 epoch: 25.595473051071167 secs\n",
      "\n",
      "Epoch 311 batch 0 train Loss 5.2920 test Loss 5.3584 with MSE metric 7198.6392\n",
      "Epoch 311 batch 100 train Loss 5.3136 test Loss 5.2611 with MSE metric 7582.9932\n",
      "Epoch 311 batch 200 train Loss 5.2651 test Loss 5.4336 with MSE metric 6871.2754\n",
      "Time taken for 1 epoch: 25.9497230052948 secs\n",
      "\n",
      "Epoch 312 batch 0 train Loss 5.3747 test Loss 5.3190 with MSE metric 8546.8652\n",
      "Epoch 312 batch 100 train Loss 5.2938 test Loss 5.3659 with MSE metric 7290.6348\n",
      "Epoch 312 batch 200 train Loss 5.2833 test Loss 5.3432 with MSE metric 7131.4922\n",
      "Time taken for 1 epoch: 26.734102964401245 secs\n",
      "\n",
      "Epoch 313 batch 0 train Loss 5.2479 test Loss 5.4179 with MSE metric 6600.2314\n",
      "Epoch 313 batch 100 train Loss 5.2722 test Loss 5.3799 with MSE metric 6970.3086\n",
      "Epoch 313 batch 200 train Loss 5.3517 test Loss 5.3904 with MSE metric 8102.8823\n",
      "Time taken for 1 epoch: 25.70307183265686 secs\n",
      "\n",
      "Epoch 314 batch 0 train Loss 5.3654 test Loss 5.3115 with MSE metric 8395.1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 314 batch 100 train Loss 5.2249 test Loss 5.3394 with MSE metric 6270.8174\n",
      "Epoch 314 batch 200 train Loss 5.2644 test Loss 5.3912 with MSE metric 6855.1045\n",
      "Time taken for 1 epoch: 25.49135184288025 secs\n",
      "\n",
      "Epoch 315 batch 0 train Loss 5.2546 test Loss 5.4186 with MSE metric 6726.6338\n",
      "Epoch 315 batch 100 train Loss 5.3881 test Loss 5.3996 with MSE metric 8720.2383\n",
      "Epoch 315 batch 200 train Loss 5.2951 test Loss 5.4013 with MSE metric 7309.1191\n",
      "Time taken for 1 epoch: 25.489909887313843 secs\n",
      "\n",
      "Epoch 316 batch 0 train Loss 5.2721 test Loss 5.4397 with MSE metric 6978.5449\n",
      "Epoch 316 batch 100 train Loss 5.4258 test Loss 5.4291 with MSE metric 9312.4912\n",
      "Epoch 316 batch 200 train Loss 5.2311 test Loss 5.4304 with MSE metric 6389.9893\n",
      "Time taken for 1 epoch: 26.36717987060547 secs\n",
      "\n",
      "Epoch 317 batch 0 train Loss 5.3539 test Loss 5.4343 with MSE metric 8198.8340\n",
      "Epoch 317 batch 100 train Loss 5.3456 test Loss 5.3194 with MSE metric 8026.5459\n",
      "Epoch 317 batch 200 train Loss 5.2576 test Loss 5.3962 with MSE metric 6775.9189\n",
      "Time taken for 1 epoch: 24.776522874832153 secs\n",
      "\n",
      "Epoch 318 batch 0 train Loss 5.2993 test Loss 5.2835 with MSE metric 7371.8193\n",
      "Epoch 318 batch 100 train Loss 5.3011 test Loss 5.3648 with MSE metric 7398.2925\n",
      "Epoch 318 batch 200 train Loss 5.3542 test Loss 5.4080 with MSE metric 8168.9854\n",
      "Time taken for 1 epoch: 25.53447675704956 secs\n",
      "\n",
      "Epoch 319 batch 0 train Loss 5.2543 test Loss 5.4117 with MSE metric 6699.3848\n",
      "Epoch 319 batch 100 train Loss 5.2846 test Loss 5.3511 with MSE metric 7153.5273\n",
      "Epoch 319 batch 200 train Loss 5.3487 test Loss 5.3196 with MSE metric 8097.7939\n",
      "Time taken for 1 epoch: 25.687873125076294 secs\n",
      "\n",
      "Epoch 320 batch 0 train Loss 5.3420 test Loss 5.3393 with MSE metric 7977.0835\n",
      "Epoch 320 batch 100 train Loss 5.2634 test Loss 5.2808 with MSE metric 6811.9766\n",
      "Epoch 320 batch 200 train Loss 5.2787 test Loss 5.3713 with MSE metric 7072.1045\n",
      "Time taken for 1 epoch: 24.458600282669067 secs\n",
      "\n",
      "Epoch 321 batch 0 train Loss 5.2460 test Loss 5.3891 with MSE metric 6612.8330\n",
      "Epoch 321 batch 100 train Loss 5.2795 test Loss 5.4080 with MSE metric 7083.2363\n",
      "Epoch 321 batch 200 train Loss 5.2750 test Loss 5.3187 with MSE metric 7010.7368\n",
      "Time taken for 1 epoch: 24.875263929367065 secs\n",
      "\n",
      "Epoch 322 batch 0 train Loss 5.2737 test Loss 5.4428 with MSE metric 7002.5312\n",
      "Epoch 322 batch 100 train Loss 5.3157 test Loss 5.2939 with MSE metric 7615.0723\n",
      "Epoch 322 batch 200 train Loss 5.3089 test Loss 5.3932 with MSE metric 7514.0117\n",
      "Time taken for 1 epoch: 24.24926495552063 secs\n",
      "\n",
      "Epoch 323 batch 0 train Loss 5.3873 test Loss 5.3453 with MSE metric 8675.4707\n",
      "Epoch 323 batch 100 train Loss 5.3069 test Loss 5.3467 with MSE metric 7480.6694\n",
      "Epoch 323 batch 200 train Loss 5.2571 test Loss 5.4259 with MSE metric 6744.6045\n",
      "Time taken for 1 epoch: 24.749390125274658 secs\n",
      "\n",
      "Epoch 324 batch 0 train Loss 5.3234 test Loss 5.3642 with MSE metric 7717.4990\n",
      "Epoch 324 batch 100 train Loss 5.3301 test Loss 5.3648 with MSE metric 7830.1162\n",
      "Epoch 324 batch 200 train Loss 5.2991 test Loss 5.2600 with MSE metric 7367.8613\n",
      "Time taken for 1 epoch: 24.39974093437195 secs\n",
      "\n",
      "Epoch 325 batch 0 train Loss 5.3755 test Loss 5.3309 with MSE metric 8511.5137\n",
      "Epoch 325 batch 100 train Loss 5.2725 test Loss 5.3639 with MSE metric 6978.6172\n",
      "Epoch 325 batch 200 train Loss 5.3450 test Loss 5.4064 with MSE metric 8036.9434\n",
      "Time taken for 1 epoch: 24.162583112716675 secs\n",
      "\n",
      "Epoch 326 batch 0 train Loss 5.3005 test Loss 5.4250 with MSE metric 7388.8193\n",
      "Epoch 326 batch 100 train Loss 5.2147 test Loss 5.3086 with MSE metric 6127.6729\n",
      "Epoch 326 batch 200 train Loss 5.1937 test Loss 5.3340 with MSE metric 5884.7104\n",
      "Time taken for 1 epoch: 26.14844512939453 secs\n",
      "\n",
      "Epoch 327 batch 0 train Loss 5.2392 test Loss 5.4047 with MSE metric 6519.3848\n",
      "Epoch 327 batch 100 train Loss 5.2748 test Loss 5.4171 with MSE metric 7014.5645\n",
      "Epoch 327 batch 200 train Loss 5.4125 test Loss 5.3487 with MSE metric 9102.5176\n",
      "Time taken for 1 epoch: 25.105028867721558 secs\n",
      "\n",
      "Epoch 328 batch 0 train Loss 5.2789 test Loss 5.2500 with MSE metric 7057.4385\n",
      "Epoch 328 batch 100 train Loss 5.3477 test Loss 5.3164 with MSE metric 8075.8569\n",
      "Epoch 328 batch 200 train Loss 5.2976 test Loss 5.3829 with MSE metric 7343.2095\n",
      "Time taken for 1 epoch: 24.274150848388672 secs\n",
      "\n",
      "Epoch 329 batch 0 train Loss 5.2634 test Loss 5.4595 with MSE metric 6837.0576\n",
      "Epoch 329 batch 100 train Loss 5.2650 test Loss 5.3679 with MSE metric 6827.2305\n",
      "Epoch 329 batch 200 train Loss 5.2730 test Loss 5.3869 with MSE metric 6987.3271\n",
      "Time taken for 1 epoch: 23.49274492263794 secs\n",
      "\n",
      "Epoch 330 batch 0 train Loss 5.2757 test Loss 5.2763 with MSE metric 7010.1230\n",
      "Epoch 330 batch 100 train Loss 5.3019 test Loss 5.1945 with MSE metric 7400.9170\n",
      "Epoch 330 batch 200 train Loss 5.3186 test Loss 5.3637 with MSE metric 7661.8916\n",
      "Time taken for 1 epoch: 25.740663051605225 secs\n",
      "\n",
      "Epoch 331 batch 0 train Loss 5.2302 test Loss 5.3786 with MSE metric 6392.7295\n",
      "Epoch 331 batch 100 train Loss 5.2648 test Loss 5.3447 with MSE metric 6850.5459\n",
      "Epoch 331 batch 200 train Loss 5.3101 test Loss 5.3670 with MSE metric 7523.5908\n",
      "Time taken for 1 epoch: 25.91596817970276 secs\n",
      "\n",
      "Epoch 332 batch 0 train Loss 5.3208 test Loss 5.3359 with MSE metric 7675.5479\n",
      "Epoch 332 batch 100 train Loss 5.3794 test Loss 5.3978 with MSE metric 8483.5703\n",
      "Epoch 332 batch 200 train Loss 5.2628 test Loss 5.3924 with MSE metric 6787.9102\n",
      "Time taken for 1 epoch: 25.47339916229248 secs\n",
      "\n",
      "Epoch 333 batch 0 train Loss 5.2191 test Loss 5.4408 with MSE metric 6216.6924\n",
      "Epoch 333 batch 100 train Loss 5.2651 test Loss 5.3640 with MSE metric 6878.4790\n",
      "Epoch 333 batch 200 train Loss 5.2696 test Loss 5.3683 with MSE metric 6938.7383\n",
      "Time taken for 1 epoch: 26.63666796684265 secs\n",
      "\n",
      "Epoch 334 batch 0 train Loss 5.2509 test Loss 5.3528 with MSE metric 6689.5874\n",
      "Epoch 334 batch 100 train Loss 5.2391 test Loss 5.3828 with MSE metric 6507.0576\n",
      "Epoch 334 batch 200 train Loss 5.2938 test Loss 5.3287 with MSE metric 7291.1992\n",
      "Time taken for 1 epoch: 25.028419733047485 secs\n",
      "\n",
      "Epoch 335 batch 0 train Loss 5.1659 test Loss 5.4726 with MSE metric 5419.3477\n",
      "Epoch 335 batch 100 train Loss 5.2977 test Loss 5.3636 with MSE metric 7346.4292\n",
      "Epoch 335 batch 200 train Loss 5.1685 test Loss 5.3627 with MSE metric 5502.3994\n",
      "Time taken for 1 epoch: 24.0334529876709 secs\n",
      "\n",
      "Epoch 336 batch 0 train Loss 5.3118 test Loss 5.3676 with MSE metric 7557.7354\n",
      "Epoch 336 batch 100 train Loss 5.3487 test Loss 5.4116 with MSE metric 8123.7764\n",
      "Epoch 336 batch 200 train Loss 5.3708 test Loss 5.3281 with MSE metric 8457.6680\n",
      "Time taken for 1 epoch: 23.979280948638916 secs\n",
      "\n",
      "Epoch 337 batch 0 train Loss 5.2858 test Loss 5.3052 with MSE metric 7170.8877\n",
      "Epoch 337 batch 100 train Loss 5.3574 test Loss 5.4428 with MSE metric 8251.1445\n",
      "Epoch 337 batch 200 train Loss 5.1591 test Loss 5.3681 with MSE metric 5439.3223\n",
      "Time taken for 1 epoch: 24.36706304550171 secs\n",
      "\n",
      "Epoch 338 batch 0 train Loss 5.3028 test Loss 5.3822 with MSE metric 7418.3071\n",
      "Epoch 338 batch 100 train Loss 5.3753 test Loss 5.3754 with MSE metric 8546.0127\n",
      "Epoch 338 batch 200 train Loss 5.2352 test Loss 5.3355 with MSE metric 6409.5371\n",
      "Time taken for 1 epoch: 22.72510600090027 secs\n",
      "\n",
      "Epoch 339 batch 0 train Loss 5.4263 test Loss 5.2829 with MSE metric 9210.3984\n",
      "Epoch 339 batch 100 train Loss 5.2827 test Loss 5.4154 with MSE metric 7130.5654\n",
      "Epoch 339 batch 200 train Loss 5.3661 test Loss 5.2846 with MSE metric 8335.3828\n",
      "Time taken for 1 epoch: 22.821187019348145 secs\n",
      "\n",
      "Epoch 340 batch 0 train Loss 5.3615 test Loss 5.3497 with MSE metric 8342.8740\n",
      "Epoch 340 batch 100 train Loss 5.3902 test Loss 5.4351 with MSE metric 8699.6221\n",
      "Epoch 340 batch 200 train Loss 5.2470 test Loss 5.4397 with MSE metric 6597.1865\n",
      "Time taken for 1 epoch: 22.830685138702393 secs\n",
      "\n",
      "Epoch 341 batch 0 train Loss 5.2060 test Loss 5.4413 with MSE metric 6006.8813\n",
      "Epoch 341 batch 100 train Loss 5.2218 test Loss 5.3828 with MSE metric 6242.3716\n",
      "Epoch 341 batch 200 train Loss 5.2723 test Loss 5.4970 with MSE metric 6980.0513\n",
      "Time taken for 1 epoch: 22.84486222267151 secs\n",
      "\n",
      "Epoch 342 batch 0 train Loss 5.3351 test Loss 5.4306 with MSE metric 7901.1641\n",
      "Epoch 342 batch 100 train Loss 5.4024 test Loss 5.3683 with MSE metric 8912.2432\n",
      "Epoch 342 batch 200 train Loss 5.2232 test Loss 5.3035 with MSE metric 6210.1816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 22.84672474861145 secs\n",
      "\n",
      "Epoch 343 batch 0 train Loss 5.4195 test Loss 5.3948 with MSE metric 9164.8086\n",
      "Epoch 343 batch 100 train Loss 5.2655 test Loss 5.4476 with MSE metric 6876.5801\n",
      "Epoch 343 batch 200 train Loss 5.3576 test Loss 5.3361 with MSE metric 8255.7959\n",
      "Time taken for 1 epoch: 22.86808204650879 secs\n",
      "\n",
      "Epoch 344 batch 0 train Loss 5.3453 test Loss 5.4019 with MSE metric 8005.4971\n",
      "Epoch 344 batch 100 train Loss 5.3289 test Loss 5.3852 with MSE metric 7797.6475\n",
      "Epoch 344 batch 200 train Loss 5.2083 test Loss 5.3140 with MSE metric 6119.9121\n",
      "Time taken for 1 epoch: 23.16726279258728 secs\n",
      "\n",
      "Epoch 345 batch 0 train Loss 5.3510 test Loss 5.3723 with MSE metric 8112.1309\n",
      "Epoch 345 batch 100 train Loss 5.2349 test Loss 5.3964 with MSE metric 6353.4131\n",
      "Epoch 345 batch 200 train Loss 5.3353 test Loss 5.2976 with MSE metric 7920.8594\n",
      "Time taken for 1 epoch: 23.15425181388855 secs\n",
      "\n",
      "Epoch 346 batch 0 train Loss 5.2567 test Loss 5.4012 with MSE metric 6756.2036\n",
      "Epoch 346 batch 100 train Loss 5.3330 test Loss 5.4477 with MSE metric 7862.9995\n",
      "Epoch 346 batch 200 train Loss 5.3255 test Loss 5.3371 with MSE metric 7751.2129\n",
      "Time taken for 1 epoch: 23.163814306259155 secs\n",
      "\n",
      "Epoch 347 batch 0 train Loss 5.2777 test Loss 5.3375 with MSE metric 7051.8799\n",
      "Epoch 347 batch 100 train Loss 5.2782 test Loss 5.3936 with MSE metric 7018.6270\n",
      "Epoch 347 batch 200 train Loss 5.2330 test Loss 5.2979 with MSE metric 6400.0996\n",
      "Time taken for 1 epoch: 24.001347064971924 secs\n",
      "\n",
      "Epoch 348 batch 0 train Loss 5.3241 test Loss 5.3994 with MSE metric 7732.4595\n",
      "Epoch 348 batch 100 train Loss 5.2751 test Loss 5.3958 with MSE metric 6992.9629\n",
      "Epoch 348 batch 200 train Loss 5.3813 test Loss 5.3354 with MSE metric 8513.3799\n",
      "Time taken for 1 epoch: 26.068027019500732 secs\n",
      "\n",
      "Epoch 349 batch 0 train Loss 5.3175 test Loss 5.3853 with MSE metric 7638.6074\n",
      "Epoch 349 batch 100 train Loss 5.3155 test Loss 5.3620 with MSE metric 7609.0498\n",
      "Epoch 349 batch 200 train Loss 5.2377 test Loss 5.2608 with MSE metric 6436.5610\n",
      "Time taken for 1 epoch: 25.8780038356781 secs\n",
      "\n",
      "Epoch 350 batch 0 train Loss 5.2386 test Loss 5.3084 with MSE metric 6513.2178\n",
      "Epoch 350 batch 100 train Loss 5.2200 test Loss 5.4023 with MSE metric 6211.7051\n",
      "Epoch 350 batch 200 train Loss 5.2747 test Loss 5.4212 with MSE metric 6979.1650\n",
      "Time taken for 1 epoch: 24.261945724487305 secs\n",
      "\n",
      "Epoch 351 batch 0 train Loss 5.2889 test Loss 5.3545 with MSE metric 7209.9297\n",
      "Epoch 351 batch 100 train Loss 5.2632 test Loss 5.3672 with MSE metric 6804.9980\n",
      "Epoch 351 batch 200 train Loss 5.3002 test Loss 5.4620 with MSE metric 7382.9580\n",
      "Time taken for 1 epoch: 23.2688729763031 secs\n",
      "\n",
      "Epoch 352 batch 0 train Loss 5.3216 test Loss 5.3948 with MSE metric 7707.6128\n",
      "Epoch 352 batch 100 train Loss 5.3221 test Loss 5.3141 with MSE metric 7704.7163\n",
      "Epoch 352 batch 200 train Loss 5.3455 test Loss 5.3385 with MSE metric 8077.9814\n",
      "Time taken for 1 epoch: 24.141342401504517 secs\n",
      "\n",
      "Epoch 353 batch 0 train Loss 5.2609 test Loss 5.3573 with MSE metric 6800.8350\n",
      "Epoch 353 batch 100 train Loss 5.2988 test Loss 5.3079 with MSE metric 7364.2383\n",
      "Epoch 353 batch 200 train Loss 5.3310 test Loss 5.4178 with MSE metric 7853.4512\n",
      "Time taken for 1 epoch: 24.86169719696045 secs\n",
      "\n",
      "Epoch 354 batch 0 train Loss 5.2194 test Loss 5.2188 with MSE metric 6127.3413\n",
      "Epoch 354 batch 100 train Loss 5.3237 test Loss 5.3855 with MSE metric 7738.4116\n",
      "Epoch 354 batch 200 train Loss 5.3654 test Loss 5.2732 with MSE metric 8357.9951\n",
      "Time taken for 1 epoch: 24.789199113845825 secs\n",
      "\n",
      "Epoch 355 batch 0 train Loss 5.2708 test Loss 5.2484 with MSE metric 6930.3560\n",
      "Epoch 355 batch 100 train Loss 5.3006 test Loss 5.4662 with MSE metric 7387.0879\n",
      "Epoch 355 batch 200 train Loss 5.3190 test Loss 5.3232 with MSE metric 7665.8359\n",
      "Time taken for 1 epoch: 24.658066987991333 secs\n",
      "\n",
      "Epoch 356 batch 0 train Loss 5.2107 test Loss 5.3522 with MSE metric 6074.8745\n",
      "Epoch 356 batch 100 train Loss 5.2858 test Loss 5.4945 with MSE metric 7168.6670\n",
      "Epoch 356 batch 200 train Loss 5.3416 test Loss 5.2273 with MSE metric 7974.2485\n",
      "Time taken for 1 epoch: 25.273353099822998 secs\n",
      "\n",
      "Epoch 357 batch 0 train Loss 5.3483 test Loss 5.2738 with MSE metric 8115.0605\n",
      "Epoch 357 batch 100 train Loss 5.2766 test Loss 5.3770 with MSE metric 7011.4600\n",
      "Epoch 357 batch 200 train Loss 5.2796 test Loss 5.4022 with MSE metric 7036.9971\n",
      "Time taken for 1 epoch: 25.546871185302734 secs\n",
      "\n",
      "Epoch 358 batch 0 train Loss 5.3155 test Loss 5.1943 with MSE metric 7595.0098\n",
      "Epoch 358 batch 100 train Loss 5.1615 test Loss 5.3259 with MSE metric 5388.8350\n",
      "Epoch 358 batch 200 train Loss 5.1952 test Loss 5.4487 with MSE metric 5767.9473\n",
      "Time taken for 1 epoch: 25.173816204071045 secs\n",
      "\n",
      "Epoch 359 batch 0 train Loss 5.3072 test Loss 5.3688 with MSE metric 7488.9619\n",
      "Epoch 359 batch 100 train Loss 5.3345 test Loss 5.3966 with MSE metric 7909.2207\n",
      "Epoch 359 batch 200 train Loss 5.2911 test Loss 5.4235 with MSE metric 7250.4619\n",
      "Time taken for 1 epoch: 24.59239387512207 secs\n",
      "\n",
      "Epoch 360 batch 0 train Loss 5.2847 test Loss 5.3293 with MSE metric 7147.5430\n",
      "Epoch 360 batch 100 train Loss 5.3928 test Loss 5.4640 with MSE metric 8723.4893\n",
      "Epoch 360 batch 200 train Loss 5.2686 test Loss 5.4221 with MSE metric 6924.8564\n",
      "Time taken for 1 epoch: 24.011812210083008 secs\n",
      "\n",
      "Epoch 361 batch 0 train Loss 5.2957 test Loss 5.3723 with MSE metric 7309.7314\n",
      "Epoch 361 batch 100 train Loss 5.3799 test Loss 5.4073 with MSE metric 8584.4844\n",
      "Epoch 361 batch 200 train Loss 5.3324 test Loss 5.2666 with MSE metric 7873.7822\n",
      "Time taken for 1 epoch: 23.47088098526001 secs\n",
      "\n",
      "Epoch 362 batch 0 train Loss 5.2556 test Loss 5.3461 with MSE metric 6724.3926\n",
      "Epoch 362 batch 100 train Loss 5.2620 test Loss 5.3673 with MSE metric 6717.3623\n",
      "Epoch 362 batch 200 train Loss 5.2972 test Loss 5.3877 with MSE metric 7336.1021\n",
      "Time taken for 1 epoch: 23.73099112510681 secs\n",
      "\n",
      "Epoch 363 batch 0 train Loss 5.2178 test Loss 5.3876 with MSE metric 6147.2622\n",
      "Epoch 363 batch 100 train Loss 5.2564 test Loss 5.3708 with MSE metric 6750.4951\n",
      "Epoch 363 batch 200 train Loss 5.2173 test Loss 5.3276 with MSE metric 6143.8828\n",
      "Time taken for 1 epoch: 24.37417197227478 secs\n",
      "\n",
      "Epoch 364 batch 0 train Loss 5.3085 test Loss 5.3119 with MSE metric 7508.4912\n",
      "Epoch 364 batch 100 train Loss 5.3438 test Loss 5.3507 with MSE metric 8058.5249\n",
      "Epoch 364 batch 200 train Loss 5.3107 test Loss 5.3905 with MSE metric 7539.9858\n",
      "Time taken for 1 epoch: 24.87558388710022 secs\n",
      "\n",
      "Epoch 365 batch 0 train Loss 5.3531 test Loss 5.4266 with MSE metric 8099.6689\n",
      "Epoch 365 batch 100 train Loss 5.3136 test Loss 5.3869 with MSE metric 7572.4307\n",
      "Epoch 365 batch 200 train Loss 5.3416 test Loss 5.3547 with MSE metric 7941.9497\n",
      "Time taken for 1 epoch: 24.51623511314392 secs\n",
      "\n",
      "Epoch 366 batch 0 train Loss 5.1893 test Loss 5.3853 with MSE metric 5687.9004\n",
      "Epoch 366 batch 100 train Loss 5.2615 test Loss 5.3437 with MSE metric 6812.7051\n",
      "Epoch 366 batch 200 train Loss 5.1781 test Loss 5.3568 with MSE metric 5564.1377\n",
      "Time taken for 1 epoch: 27.16364598274231 secs\n",
      "\n",
      "Epoch 367 batch 0 train Loss 5.3929 test Loss 5.3129 with MSE metric 8612.1465\n",
      "Epoch 367 batch 100 train Loss 5.3563 test Loss 5.4468 with MSE metric 8235.2217\n",
      "Epoch 367 batch 200 train Loss 5.1838 test Loss 5.3288 with MSE metric 5768.3169\n",
      "Time taken for 1 epoch: 24.681644916534424 secs\n",
      "\n",
      "Epoch 368 batch 0 train Loss 5.2864 test Loss 5.3600 with MSE metric 7163.0771\n",
      "Epoch 368 batch 100 train Loss 5.3322 test Loss 5.4075 with MSE metric 7873.3828\n",
      "Epoch 368 batch 200 train Loss 5.3843 test Loss 5.2838 with MSE metric 8710.7129\n",
      "Time taken for 1 epoch: 24.736150979995728 secs\n",
      "\n",
      "Epoch 369 batch 0 train Loss 5.3111 test Loss 5.3325 with MSE metric 7540.5630\n",
      "Epoch 369 batch 100 train Loss 5.3720 test Loss 5.3567 with MSE metric 8448.4512\n",
      "Epoch 369 batch 200 train Loss 5.3978 test Loss 5.4685 with MSE metric 8661.0664\n",
      "Time taken for 1 epoch: 24.83041214942932 secs\n",
      "\n",
      "Epoch 370 batch 0 train Loss 5.4029 test Loss 5.2896 with MSE metric 8813.2754\n",
      "Epoch 370 batch 100 train Loss 5.4864 test Loss 5.2965 with MSE metric 9994.8564\n",
      "Epoch 370 batch 200 train Loss 5.2981 test Loss 5.4337 with MSE metric 7345.3496\n",
      "Time taken for 1 epoch: 24.819039344787598 secs\n",
      "\n",
      "Epoch 371 batch 0 train Loss 5.2694 test Loss 5.3147 with MSE metric 6842.7715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371 batch 100 train Loss 5.1537 test Loss 5.3436 with MSE metric 5340.6006\n",
      "Epoch 371 batch 200 train Loss 5.3264 test Loss 5.3698 with MSE metric 7767.5127\n",
      "Time taken for 1 epoch: 25.350380182266235 secs\n",
      "\n",
      "Epoch 372 batch 0 train Loss 5.2954 test Loss 5.4055 with MSE metric 7312.4443\n",
      "Epoch 372 batch 100 train Loss 5.2692 test Loss 5.4548 with MSE metric 6921.8779\n",
      "Epoch 372 batch 200 train Loss 5.3266 test Loss 5.3399 with MSE metric 7765.9302\n",
      "Time taken for 1 epoch: 25.07077980041504 secs\n",
      "\n",
      "Epoch 373 batch 0 train Loss 5.3163 test Loss 5.3610 with MSE metric 7620.5410\n",
      "Epoch 373 batch 100 train Loss 5.3389 test Loss 5.4638 with MSE metric 7940.6387\n",
      "Epoch 373 batch 200 train Loss 5.1829 test Loss 5.3542 with MSE metric 5669.5483\n",
      "Time taken for 1 epoch: 24.41229486465454 secs\n",
      "\n",
      "Epoch 374 batch 0 train Loss 5.4059 test Loss 5.3442 with MSE metric 8928.5938\n",
      "Epoch 374 batch 100 train Loss 5.2125 test Loss 5.4817 with MSE metric 5956.4644\n",
      "Epoch 374 batch 200 train Loss 5.2305 test Loss 5.4745 with MSE metric 6329.9995\n",
      "Time taken for 1 epoch: 24.535000801086426 secs\n",
      "\n",
      "Epoch 375 batch 0 train Loss 5.2806 test Loss 5.3735 with MSE metric 7073.3213\n",
      "Epoch 375 batch 100 train Loss 5.3391 test Loss 5.3562 with MSE metric 7975.9941\n",
      "Epoch 375 batch 200 train Loss 5.2572 test Loss 5.3304 with MSE metric 6691.8252\n",
      "Time taken for 1 epoch: 24.39974284172058 secs\n",
      "\n",
      "Epoch 376 batch 0 train Loss 5.3503 test Loss 5.3888 with MSE metric 8053.6973\n",
      "Epoch 376 batch 100 train Loss 5.2754 test Loss 5.3508 with MSE metric 6997.8770\n",
      "Epoch 376 batch 200 train Loss 5.1599 test Loss 5.4209 with MSE metric 5181.2783\n",
      "Time taken for 1 epoch: 24.20171284675598 secs\n",
      "\n",
      "Epoch 377 batch 0 train Loss 5.3030 test Loss 5.3217 with MSE metric 7418.7354\n",
      "Epoch 377 batch 100 train Loss 5.2759 test Loss 5.3373 with MSE metric 7005.4326\n",
      "Epoch 377 batch 200 train Loss 5.3338 test Loss 5.3989 with MSE metric 7874.0957\n",
      "Time taken for 1 epoch: 23.212068796157837 secs\n",
      "\n",
      "Epoch 378 batch 0 train Loss 5.2973 test Loss 5.4122 with MSE metric 7337.9053\n",
      "Epoch 378 batch 100 train Loss 5.2308 test Loss 5.3528 with MSE metric 6395.1123\n",
      "Epoch 378 batch 200 train Loss 5.3174 test Loss 5.2542 with MSE metric 7628.6377\n",
      "Time taken for 1 epoch: 23.727728128433228 secs\n",
      "\n",
      "Epoch 379 batch 0 train Loss 5.2672 test Loss 5.3815 with MSE metric 6913.4214\n",
      "Epoch 379 batch 100 train Loss 5.2854 test Loss 5.1815 with MSE metric 7157.6553\n",
      "Epoch 379 batch 200 train Loss 5.3790 test Loss 5.3963 with MSE metric 8580.6543\n",
      "Time taken for 1 epoch: 24.328709840774536 secs\n",
      "\n",
      "Epoch 380 batch 0 train Loss 5.3285 test Loss 5.3406 with MSE metric 7772.6436\n",
      "Epoch 380 batch 100 train Loss 5.1710 test Loss 5.2884 with MSE metric 5387.9180\n",
      "Epoch 380 batch 200 train Loss 5.3097 test Loss 5.3797 with MSE metric 7518.9888\n",
      "Time taken for 1 epoch: 23.383727073669434 secs\n",
      "\n",
      "Epoch 381 batch 0 train Loss 5.2376 test Loss 5.3580 with MSE metric 6505.9438\n",
      "Epoch 381 batch 100 train Loss 5.1778 test Loss 5.2685 with MSE metric 5590.6396\n",
      "Epoch 381 batch 200 train Loss 5.2966 test Loss 5.3956 with MSE metric 7331.8467\n",
      "Time taken for 1 epoch: 24.208624839782715 secs\n",
      "\n",
      "Epoch 382 batch 0 train Loss 5.2675 test Loss 5.3532 with MSE metric 6889.5952\n",
      "Epoch 382 batch 100 train Loss 5.3535 test Loss 5.3529 with MSE metric 8211.3965\n",
      "Epoch 382 batch 200 train Loss 5.2923 test Loss 5.3484 with MSE metric 7268.6460\n",
      "Time taken for 1 epoch: 24.38543391227722 secs\n",
      "\n",
      "Epoch 383 batch 0 train Loss 5.2801 test Loss 5.3150 with MSE metric 7082.6147\n",
      "Epoch 383 batch 100 train Loss 5.2686 test Loss 5.2863 with MSE metric 6930.5552\n",
      "Epoch 383 batch 200 train Loss 5.3549 test Loss 5.3043 with MSE metric 8192.2188\n",
      "Time taken for 1 epoch: 23.080844163894653 secs\n",
      "\n",
      "Epoch 384 batch 0 train Loss 5.3093 test Loss 5.4369 with MSE metric 7512.5518\n",
      "Epoch 384 batch 100 train Loss 5.2435 test Loss 5.3635 with MSE metric 6533.6133\n",
      "Epoch 384 batch 200 train Loss 5.2681 test Loss 5.3985 with MSE metric 6924.3721\n",
      "Time taken for 1 epoch: 23.137085914611816 secs\n",
      "\n",
      "Epoch 385 batch 0 train Loss 5.3143 test Loss 5.3173 with MSE metric 7594.7393\n",
      "Epoch 385 batch 100 train Loss 5.3124 test Loss 5.2934 with MSE metric 7546.3789\n",
      "Epoch 385 batch 200 train Loss 5.2132 test Loss 5.3533 with MSE metric 6094.3721\n",
      "Time taken for 1 epoch: 23.135124921798706 secs\n",
      "\n",
      "Epoch 386 batch 0 train Loss 5.3664 test Loss 5.4210 with MSE metric 8298.1289\n",
      "Epoch 386 batch 100 train Loss 5.1868 test Loss 5.4188 with MSE metric 5681.4082\n",
      "Epoch 386 batch 200 train Loss 5.3406 test Loss 5.3205 with MSE metric 8002.1133\n",
      "Time taken for 1 epoch: 23.10806918144226 secs\n",
      "\n",
      "Epoch 387 batch 0 train Loss 5.3338 test Loss 5.3358 with MSE metric 7897.7949\n",
      "Epoch 387 batch 100 train Loss 5.2548 test Loss 5.3471 with MSE metric 6700.8838\n",
      "Epoch 387 batch 200 train Loss 5.2451 test Loss 5.3269 with MSE metric 6459.8408\n",
      "Time taken for 1 epoch: 23.09574818611145 secs\n",
      "\n",
      "Epoch 388 batch 0 train Loss 5.2551 test Loss 5.3756 with MSE metric 6708.9272\n",
      "Epoch 388 batch 100 train Loss 5.2492 test Loss 5.3105 with MSE metric 6663.4751\n",
      "Epoch 388 batch 200 train Loss 5.2860 test Loss 5.3934 with MSE metric 7159.1934\n",
      "Time taken for 1 epoch: 23.203075885772705 secs\n",
      "\n",
      "Epoch 389 batch 0 train Loss 5.2844 test Loss 5.2940 with MSE metric 7155.5044\n",
      "Epoch 389 batch 100 train Loss 5.3679 test Loss 5.1851 with MSE metric 8335.7900\n",
      "Epoch 389 batch 200 train Loss 5.2476 test Loss 5.4718 with MSE metric 6599.9033\n",
      "Time taken for 1 epoch: 23.1627459526062 secs\n",
      "\n",
      "Epoch 390 batch 0 train Loss 5.2783 test Loss 5.4579 with MSE metric 7058.2798\n",
      "Epoch 390 batch 100 train Loss 5.3202 test Loss 5.1965 with MSE metric 7683.9561\n",
      "Epoch 390 batch 200 train Loss 5.3665 test Loss 5.3645 with MSE metric 8382.7207\n",
      "Time taken for 1 epoch: 23.23610496520996 secs\n",
      "\n",
      "Epoch 391 batch 0 train Loss 5.2891 test Loss 5.3897 with MSE metric 7195.4199\n",
      "Epoch 391 batch 100 train Loss 5.3307 test Loss 5.3829 with MSE metric 7809.0508\n",
      "Epoch 391 batch 200 train Loss 5.3338 test Loss 5.3262 with MSE metric 7896.9844\n",
      "Time taken for 1 epoch: 23.168976068496704 secs\n",
      "\n",
      "Epoch 392 batch 0 train Loss 5.2332 test Loss 5.2992 with MSE metric 6381.5518\n",
      "Epoch 392 batch 100 train Loss 5.2999 test Loss 5.3091 with MSE metric 7379.6929\n",
      "Epoch 392 batch 200 train Loss 5.2908 test Loss 5.3448 with MSE metric 7247.3589\n",
      "Time taken for 1 epoch: 23.1199471950531 secs\n",
      "\n",
      "Epoch 393 batch 0 train Loss 5.3505 test Loss 5.3334 with MSE metric 8116.7881\n",
      "Epoch 393 batch 100 train Loss 5.2895 test Loss 5.4523 with MSE metric 7227.0098\n",
      "Epoch 393 batch 200 train Loss 5.2806 test Loss 5.3454 with MSE metric 7096.0879\n",
      "Time taken for 1 epoch: 23.110283136367798 secs\n",
      "\n",
      "Epoch 394 batch 0 train Loss 5.2212 test Loss 5.3993 with MSE metric 6262.8564\n",
      "Epoch 394 batch 100 train Loss 5.2644 test Loss 5.4123 with MSE metric 6824.2837\n",
      "Epoch 394 batch 200 train Loss 5.3099 test Loss 5.2952 with MSE metric 7529.5322\n",
      "Time taken for 1 epoch: 23.18998384475708 secs\n",
      "\n",
      "Epoch 395 batch 0 train Loss 5.3728 test Loss 5.3524 with MSE metric 8379.1875\n",
      "Epoch 395 batch 100 train Loss 5.2624 test Loss 5.4590 with MSE metric 6764.6797\n",
      "Epoch 395 batch 200 train Loss 5.3522 test Loss 5.3545 with MSE metric 8168.8721\n",
      "Time taken for 1 epoch: 23.18059515953064 secs\n",
      "\n",
      "Epoch 396 batch 0 train Loss 5.3873 test Loss 5.3366 with MSE metric 8692.0059\n",
      "Epoch 396 batch 100 train Loss 5.3086 test Loss 5.4354 with MSE metric 7501.5898\n",
      "Epoch 396 batch 200 train Loss 5.3963 test Loss 5.3428 with MSE metric 8788.4531\n",
      "Time taken for 1 epoch: 23.06361985206604 secs\n",
      "\n",
      "Epoch 397 batch 0 train Loss 5.3389 test Loss 5.3807 with MSE metric 7970.5166\n",
      "Epoch 397 batch 100 train Loss 5.2268 test Loss 5.3693 with MSE metric 6258.1899\n",
      "Epoch 397 batch 200 train Loss 5.2792 test Loss 5.3715 with MSE metric 7080.6909\n",
      "Time taken for 1 epoch: 23.13952398300171 secs\n",
      "\n",
      "Epoch 398 batch 0 train Loss 5.1789 test Loss 5.3088 with MSE metric 5686.9316\n",
      "Epoch 398 batch 100 train Loss 5.2546 test Loss 5.3165 with MSE metric 6679.9219\n",
      "Epoch 398 batch 200 train Loss 5.2873 test Loss 5.4245 with MSE metric 7196.3740\n",
      "Time taken for 1 epoch: 23.168001174926758 secs\n",
      "\n",
      "Epoch 399 batch 0 train Loss 5.3063 test Loss 5.3697 with MSE metric 7462.2188\n",
      "Epoch 399 batch 100 train Loss 5.2783 test Loss 5.5190 with MSE metric 7059.4780\n",
      "Epoch 399 batch 200 train Loss 5.3919 test Loss 5.3519 with MSE metric 8759.2422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 23.1499502658844 secs\n",
      "\n",
      "Epoch 400 batch 0 train Loss 5.2708 test Loss 5.3669 with MSE metric 6960.3936\n",
      "Epoch 400 batch 100 train Loss 5.2873 test Loss 5.4256 with MSE metric 7196.8188\n",
      "Epoch 400 batch 200 train Loss 5.2844 test Loss 5.4202 with MSE metric 7153.6162\n",
      "Time taken for 1 epoch: 23.256110191345215 secs\n",
      "\n",
      "Epoch 401 batch 0 train Loss 5.2638 test Loss 5.4288 with MSE metric 6847.5273\n",
      "Epoch 401 batch 100 train Loss 5.3126 test Loss 5.3164 with MSE metric 7555.6792\n",
      "Epoch 401 batch 200 train Loss 5.2346 test Loss 5.2912 with MSE metric 6391.2100\n",
      "Time taken for 1 epoch: 22.849128007888794 secs\n",
      "\n",
      "Epoch 402 batch 0 train Loss 5.2644 test Loss 5.3122 with MSE metric 6857.4209\n",
      "Epoch 402 batch 100 train Loss 5.2632 test Loss 5.2986 with MSE metric 6835.0308\n",
      "Epoch 402 batch 200 train Loss 5.3600 test Loss 5.3494 with MSE metric 8234.2891\n",
      "Time taken for 1 epoch: 21.740886926651 secs\n",
      "\n",
      "Epoch 403 batch 0 train Loss 5.2986 test Loss 5.4309 with MSE metric 7361.7207\n",
      "Epoch 403 batch 100 train Loss 5.1709 test Loss 5.3136 with MSE metric 5355.0820\n",
      "Epoch 403 batch 200 train Loss 5.2790 test Loss 5.2171 with MSE metric 7077.4502\n",
      "Time taken for 1 epoch: 21.716185092926025 secs\n",
      "\n",
      "Epoch 404 batch 0 train Loss 5.3757 test Loss 5.3216 with MSE metric 8506.5986\n",
      "Epoch 404 batch 100 train Loss 5.2081 test Loss 5.4115 with MSE metric 6093.3301\n",
      "Epoch 404 batch 200 train Loss 5.3363 test Loss 5.3620 with MSE metric 7938.1572\n",
      "Time taken for 1 epoch: 21.673666954040527 secs\n",
      "\n",
      "Epoch 405 batch 0 train Loss 5.3665 test Loss 5.3981 with MSE metric 8372.2871\n",
      "Epoch 405 batch 100 train Loss 5.3324 test Loss 5.3587 with MSE metric 7823.7451\n",
      "Epoch 405 batch 200 train Loss 5.3070 test Loss 5.3168 with MSE metric 7486.3901\n",
      "Time taken for 1 epoch: 21.6754150390625 secs\n",
      "\n",
      "Epoch 406 batch 0 train Loss 5.3561 test Loss 5.2573 with MSE metric 8229.2520\n",
      "Epoch 406 batch 100 train Loss 5.3289 test Loss 5.2847 with MSE metric 7819.6187\n",
      "Epoch 406 batch 200 train Loss 5.2976 test Loss 5.2956 with MSE metric 7346.1582\n",
      "Time taken for 1 epoch: 21.733410120010376 secs\n",
      "\n",
      "Epoch 407 batch 0 train Loss 5.3736 test Loss 5.3356 with MSE metric 8460.0938\n",
      "Epoch 407 batch 100 train Loss 5.4027 test Loss 5.3117 with MSE metric 8885.0195\n",
      "Epoch 407 batch 200 train Loss 5.2839 test Loss 5.3671 with MSE metric 7140.9727\n",
      "Time taken for 1 epoch: 21.779106855392456 secs\n",
      "\n",
      "Epoch 408 batch 0 train Loss 5.2290 test Loss 5.4029 with MSE metric 6345.3506\n",
      "Epoch 408 batch 100 train Loss 5.3077 test Loss 5.3232 with MSE metric 7494.8740\n",
      "Epoch 408 batch 200 train Loss 5.2526 test Loss 5.4330 with MSE metric 6675.4248\n",
      "Time taken for 1 epoch: 21.66909432411194 secs\n",
      "\n",
      "Epoch 409 batch 0 train Loss 5.3881 test Loss 5.3480 with MSE metric 8696.0039\n",
      "Epoch 409 batch 100 train Loss 5.4124 test Loss 5.3052 with MSE metric 9196.4336\n",
      "Epoch 409 batch 200 train Loss 5.3302 test Loss 5.4395 with MSE metric 7832.2407\n",
      "Time taken for 1 epoch: 21.733630895614624 secs\n",
      "\n",
      "Epoch 410 batch 0 train Loss 5.2175 test Loss 5.3461 with MSE metric 6170.0972\n",
      "Epoch 410 batch 100 train Loss 5.2477 test Loss 5.3037 with MSE metric 6624.0176\n",
      "Epoch 410 batch 200 train Loss 5.4083 test Loss 5.3336 with MSE metric 8916.5605\n",
      "Time taken for 1 epoch: 21.76729702949524 secs\n",
      "\n",
      "Epoch 411 batch 0 train Loss 5.2085 test Loss 5.3850 with MSE metric 5983.9727\n",
      "Epoch 411 batch 100 train Loss 5.2642 test Loss 5.3854 with MSE metric 6869.6045\n",
      "Epoch 411 batch 200 train Loss 5.3086 test Loss 5.4636 with MSE metric 7507.4902\n",
      "Time taken for 1 epoch: 21.731549978256226 secs\n",
      "\n",
      "Epoch 412 batch 0 train Loss 5.2628 test Loss 5.2768 with MSE metric 6838.7871\n",
      "Epoch 412 batch 100 train Loss 5.3544 test Loss 5.5017 with MSE metric 8189.0479\n",
      "Epoch 412 batch 200 train Loss 5.2718 test Loss 5.3563 with MSE metric 6957.0332\n",
      "Time taken for 1 epoch: 21.696667909622192 secs\n",
      "\n",
      "Epoch 413 batch 0 train Loss 5.2223 test Loss 5.3061 with MSE metric 6200.6797\n",
      "Epoch 413 batch 100 train Loss 5.3353 test Loss 5.4865 with MSE metric 7920.2520\n",
      "Epoch 413 batch 200 train Loss 5.3479 test Loss 5.4081 with MSE metric 8046.6055\n",
      "Time taken for 1 epoch: 21.75439715385437 secs\n",
      "\n",
      "Epoch 414 batch 0 train Loss 5.2258 test Loss 5.3456 with MSE metric 6334.4580\n",
      "Epoch 414 batch 100 train Loss 5.2575 test Loss 5.4670 with MSE metric 6751.5415\n",
      "Epoch 414 batch 200 train Loss 5.2316 test Loss 5.3336 with MSE metric 6351.6641\n",
      "Time taken for 1 epoch: 21.678242921829224 secs\n",
      "\n",
      "Epoch 415 batch 0 train Loss 5.3389 test Loss 5.2271 with MSE metric 7918.1299\n",
      "Epoch 415 batch 100 train Loss 5.2688 test Loss 5.4415 with MSE metric 6902.5234\n",
      "Epoch 415 batch 200 train Loss 5.2909 test Loss 5.3421 with MSE metric 7220.1074\n",
      "Time taken for 1 epoch: 21.749764919281006 secs\n",
      "\n",
      "Epoch 416 batch 0 train Loss 5.2818 test Loss 5.3707 with MSE metric 7114.7881\n",
      "Epoch 416 batch 100 train Loss 5.2175 test Loss 5.3589 with MSE metric 6200.8057\n",
      "Epoch 416 batch 200 train Loss 5.3889 test Loss 5.3324 with MSE metric 8644.6006\n",
      "Time taken for 1 epoch: 21.786334991455078 secs\n",
      "\n",
      "Epoch 417 batch 0 train Loss 5.3240 test Loss 5.3254 with MSE metric 7745.7090\n",
      "Epoch 417 batch 100 train Loss 5.3471 test Loss 5.4343 with MSE metric 8065.8447\n",
      "Epoch 417 batch 200 train Loss 5.1396 test Loss 5.3193 with MSE metric 5003.5718\n",
      "Time taken for 1 epoch: 21.74135398864746 secs\n",
      "\n",
      "Epoch 418 batch 0 train Loss 5.3903 test Loss 5.3500 with MSE metric 8761.1797\n",
      "Epoch 418 batch 100 train Loss 5.1533 test Loss 5.3333 with MSE metric 5126.9287\n",
      "Epoch 418 batch 200 train Loss 5.3309 test Loss 5.3982 with MSE metric 7841.0244\n",
      "Time taken for 1 epoch: 21.7345609664917 secs\n",
      "\n",
      "Epoch 419 batch 0 train Loss 5.3473 test Loss 5.3914 with MSE metric 8053.7500\n",
      "Epoch 419 batch 100 train Loss 5.2815 test Loss 5.3682 with MSE metric 7096.0566\n",
      "Epoch 419 batch 200 train Loss 5.3252 test Loss 5.5031 with MSE metric 7763.6675\n",
      "Time taken for 1 epoch: 21.774595737457275 secs\n",
      "\n",
      "Epoch 420 batch 0 train Loss 5.2674 test Loss 5.3040 with MSE metric 6909.7012\n",
      "Epoch 420 batch 100 train Loss 5.3309 test Loss 5.3501 with MSE metric 7842.1274\n",
      "Epoch 420 batch 200 train Loss 5.1819 test Loss 5.1678 with MSE metric 5583.7876\n",
      "Time taken for 1 epoch: 21.80823016166687 secs\n",
      "\n",
      "Epoch 421 batch 0 train Loss 5.3084 test Loss 5.4169 with MSE metric 7506.1182\n",
      "Epoch 421 batch 100 train Loss 5.2283 test Loss 5.4321 with MSE metric 6356.7837\n",
      "Epoch 421 batch 200 train Loss 5.2759 test Loss 5.3712 with MSE metric 7019.3574\n",
      "Time taken for 1 epoch: 21.744935989379883 secs\n",
      "\n",
      "Epoch 422 batch 0 train Loss 5.3080 test Loss 5.3164 with MSE metric 7495.6943\n",
      "Epoch 422 batch 100 train Loss 5.2706 test Loss 5.2934 with MSE metric 6960.1973\n",
      "Epoch 422 batch 200 train Loss 5.3865 test Loss 5.3262 with MSE metric 8508.9961\n",
      "Time taken for 1 epoch: 21.830559253692627 secs\n",
      "\n",
      "Epoch 423 batch 0 train Loss 5.3344 test Loss 5.3228 with MSE metric 7892.6831\n",
      "Epoch 423 batch 100 train Loss 5.3170 test Loss 5.3953 with MSE metric 7632.7090\n",
      "Epoch 423 batch 200 train Loss 5.1827 test Loss 5.4996 with MSE metric 5552.6738\n",
      "Time taken for 1 epoch: 21.930640935897827 secs\n",
      "\n",
      "Epoch 424 batch 0 train Loss 5.3906 test Loss 5.3178 with MSE metric 8747.5889\n",
      "Epoch 424 batch 100 train Loss 5.3305 test Loss 5.3835 with MSE metric 7788.1909\n",
      "Epoch 424 batch 200 train Loss 5.4593 test Loss 5.3572 with MSE metric 9665.8828\n",
      "Time taken for 1 epoch: 21.981529712677002 secs\n",
      "\n",
      "Epoch 425 batch 0 train Loss 5.3237 test Loss 5.2790 with MSE metric 7740.2285\n",
      "Epoch 425 batch 100 train Loss 5.3458 test Loss 5.4100 with MSE metric 8029.4082\n",
      "Epoch 425 batch 200 train Loss 5.3467 test Loss 5.2716 with MSE metric 8089.8125\n",
      "Time taken for 1 epoch: 22.046859979629517 secs\n",
      "\n",
      "Epoch 426 batch 0 train Loss 5.2839 test Loss 5.3529 with MSE metric 7148.2227\n",
      "Epoch 426 batch 100 train Loss 5.2887 test Loss 5.4516 with MSE metric 7214.3032\n",
      "Epoch 426 batch 200 train Loss 5.2452 test Loss 5.2555 with MSE metric 6591.0723\n",
      "Time taken for 1 epoch: 22.004553079605103 secs\n",
      "\n",
      "Epoch 427 batch 0 train Loss 5.3063 test Loss 5.3566 with MSE metric 7473.3682\n",
      "Epoch 427 batch 100 train Loss 5.1998 test Loss 5.3096 with MSE metric 5868.6309\n",
      "Epoch 427 batch 200 train Loss 5.2819 test Loss 5.3596 with MSE metric 7067.9360\n",
      "Time taken for 1 epoch: 22.07785701751709 secs\n",
      "\n",
      "Epoch 428 batch 0 train Loss 5.3408 test Loss 5.3899 with MSE metric 7997.6240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428 batch 100 train Loss 5.3037 test Loss 5.4034 with MSE metric 7436.4175\n",
      "Epoch 428 batch 200 train Loss 5.3621 test Loss 5.4122 with MSE metric 8282.3311\n",
      "Time taken for 1 epoch: 21.982819080352783 secs\n",
      "\n",
      "Epoch 429 batch 0 train Loss 5.3030 test Loss 5.4197 with MSE metric 7404.0791\n",
      "Epoch 429 batch 100 train Loss 5.3763 test Loss 5.3437 with MSE metric 8509.5859\n",
      "Epoch 429 batch 200 train Loss 5.2939 test Loss 5.3400 with MSE metric 7286.3491\n",
      "Time taken for 1 epoch: 21.892422199249268 secs\n",
      "\n",
      "Epoch 430 batch 0 train Loss 5.3348 test Loss 5.3775 with MSE metric 7908.3203\n",
      "Epoch 430 batch 100 train Loss 5.2188 test Loss 5.3692 with MSE metric 6053.9482\n",
      "Epoch 430 batch 200 train Loss 5.2344 test Loss 5.2814 with MSE metric 6380.1357\n",
      "Time taken for 1 epoch: 21.946483850479126 secs\n",
      "\n",
      "Epoch 431 batch 0 train Loss 5.3339 test Loss 5.3367 with MSE metric 7900.1079\n",
      "Epoch 431 batch 100 train Loss 5.2586 test Loss 5.3749 with MSE metric 6719.3960\n",
      "Epoch 431 batch 200 train Loss 5.2638 test Loss 5.3790 with MSE metric 6866.5176\n",
      "Time taken for 1 epoch: 21.9883451461792 secs\n",
      "\n",
      "Epoch 432 batch 0 train Loss 5.2525 test Loss 5.3465 with MSE metric 6666.6914\n",
      "Epoch 432 batch 100 train Loss 5.2916 test Loss 5.3724 with MSE metric 7236.6450\n",
      "Epoch 432 batch 200 train Loss 5.2547 test Loss 5.3634 with MSE metric 6729.6045\n",
      "Time taken for 1 epoch: 21.984666109085083 secs\n",
      "\n",
      "Epoch 433 batch 0 train Loss 5.3052 test Loss 5.4450 with MSE metric 7451.3877\n",
      "Epoch 433 batch 100 train Loss 5.3008 test Loss 5.3421 with MSE metric 7387.1133\n",
      "Epoch 433 batch 200 train Loss 5.4177 test Loss 5.3449 with MSE metric 8886.2656\n",
      "Time taken for 1 epoch: 22.192760944366455 secs\n",
      "\n",
      "Epoch 434 batch 0 train Loss 5.3972 test Loss 5.4136 with MSE metric 8861.0068\n",
      "Epoch 434 batch 100 train Loss 5.3016 test Loss 5.5341 with MSE metric 7405.8926\n",
      "Epoch 434 batch 200 train Loss 5.3177 test Loss 5.3432 with MSE metric 7619.4624\n",
      "Time taken for 1 epoch: 22.083411931991577 secs\n",
      "\n",
      "Epoch 435 batch 0 train Loss 5.2709 test Loss 5.3696 with MSE metric 6938.7021\n",
      "Epoch 435 batch 100 train Loss 5.1909 test Loss 5.3863 with MSE metric 5655.1831\n",
      "Epoch 435 batch 200 train Loss 5.2973 test Loss 5.4032 with MSE metric 7331.7319\n",
      "Time taken for 1 epoch: 22.230645895004272 secs\n",
      "\n",
      "Epoch 436 batch 0 train Loss 5.3092 test Loss 5.3260 with MSE metric 7517.1367\n",
      "Epoch 436 batch 100 train Loss 5.3916 test Loss 5.3639 with MSE metric 8700.8096\n",
      "Epoch 436 batch 200 train Loss 5.4454 test Loss 5.3505 with MSE metric 9477.3916\n",
      "Time taken for 1 epoch: 22.384729862213135 secs\n",
      "\n",
      "Epoch 437 batch 0 train Loss 5.2322 test Loss 5.2977 with MSE metric 6339.0317\n",
      "Epoch 437 batch 100 train Loss 5.2669 test Loss 5.3587 with MSE metric 6887.8057\n",
      "Epoch 437 batch 200 train Loss 5.3178 test Loss 5.3307 with MSE metric 7625.6348\n",
      "Time taken for 1 epoch: 22.371367931365967 secs\n",
      "\n",
      "Epoch 438 batch 0 train Loss 5.3339 test Loss 5.2899 with MSE metric 7857.4912\n",
      "Epoch 438 batch 100 train Loss 5.3066 test Loss 5.4059 with MSE metric 7463.9463\n",
      "Epoch 438 batch 200 train Loss 5.3327 test Loss 5.4361 with MSE metric 7861.3105\n",
      "Time taken for 1 epoch: 22.30488896369934 secs\n",
      "\n",
      "Epoch 439 batch 0 train Loss 5.2778 test Loss 5.4232 with MSE metric 7060.1440\n",
      "Epoch 439 batch 100 train Loss 5.3749 test Loss 5.3194 with MSE metric 8513.4609\n",
      "Epoch 439 batch 200 train Loss 5.2164 test Loss 5.5088 with MSE metric 6100.3945\n",
      "Time taken for 1 epoch: 22.282495975494385 secs\n",
      "\n",
      "Epoch 440 batch 0 train Loss 5.2622 test Loss 5.4327 with MSE metric 6823.5850\n",
      "Epoch 440 batch 100 train Loss 5.2275 test Loss 5.3970 with MSE metric 6307.8125\n",
      "Epoch 440 batch 200 train Loss 5.2863 test Loss 5.3113 with MSE metric 7165.1299\n",
      "Time taken for 1 epoch: 22.341489791870117 secs\n",
      "\n",
      "Epoch 441 batch 0 train Loss 5.3666 test Loss 5.4106 with MSE metric 8377.7178\n",
      "Epoch 441 batch 100 train Loss 5.3828 test Loss 5.3205 with MSE metric 8550.4443\n",
      "Epoch 441 batch 200 train Loss 5.3546 test Loss 5.3629 with MSE metric 8179.6006\n",
      "Time taken for 1 epoch: 22.230465173721313 secs\n",
      "\n",
      "Epoch 442 batch 0 train Loss 5.3193 test Loss 5.4165 with MSE metric 7672.3164\n",
      "Epoch 442 batch 100 train Loss 5.2738 test Loss 5.2991 with MSE metric 6913.4844\n",
      "Epoch 442 batch 200 train Loss 5.3519 test Loss 5.2752 with MSE metric 8154.0684\n",
      "Time taken for 1 epoch: 22.350779056549072 secs\n",
      "\n",
      "Epoch 443 batch 0 train Loss 5.2463 test Loss 5.1692 with MSE metric 6622.9668\n",
      "Epoch 443 batch 100 train Loss 5.2606 test Loss 5.3523 with MSE metric 6736.4971\n",
      "Epoch 443 batch 200 train Loss 5.3300 test Loss 5.2583 with MSE metric 7796.5425\n",
      "Time taken for 1 epoch: 22.2591712474823 secs\n",
      "\n",
      "Epoch 444 batch 0 train Loss 5.3505 test Loss 5.3918 with MSE metric 8164.4170\n",
      "Epoch 444 batch 100 train Loss 5.3440 test Loss 5.3795 with MSE metric 8020.7100\n",
      "Epoch 444 batch 200 train Loss 5.2920 test Loss 5.3335 with MSE metric 7264.5278\n",
      "Time taken for 1 epoch: 22.397417068481445 secs\n",
      "\n",
      "Epoch 445 batch 0 train Loss 5.3443 test Loss 5.3371 with MSE metric 8063.6387\n",
      "Epoch 445 batch 100 train Loss 5.1862 test Loss 5.3061 with MSE metric 5642.1221\n",
      "Epoch 445 batch 200 train Loss 5.3410 test Loss 5.3716 with MSE metric 8010.0625\n",
      "Time taken for 1 epoch: 22.54958987236023 secs\n",
      "\n",
      "Epoch 446 batch 0 train Loss 5.2729 test Loss 5.3959 with MSE metric 6991.9434\n",
      "Epoch 446 batch 100 train Loss 5.3430 test Loss 5.4367 with MSE metric 8003.9824\n",
      "Epoch 446 batch 200 train Loss 5.3398 test Loss 5.3776 with MSE metric 7965.9517\n",
      "Time taken for 1 epoch: 22.428622007369995 secs\n",
      "\n",
      "Epoch 447 batch 0 train Loss 5.2933 test Loss 5.4547 with MSE metric 7266.0415\n",
      "Epoch 447 batch 100 train Loss 5.3885 test Loss 5.4106 with MSE metric 8760.1895\n",
      "Epoch 447 batch 200 train Loss 5.3135 test Loss 5.3647 with MSE metric 7538.2158\n",
      "Time taken for 1 epoch: 22.412544012069702 secs\n",
      "\n",
      "Epoch 448 batch 0 train Loss 5.2997 test Loss 5.3157 with MSE metric 7365.3486\n",
      "Epoch 448 batch 100 train Loss 5.2870 test Loss 5.3872 with MSE metric 7184.3818\n",
      "Epoch 448 batch 200 train Loss 5.2608 test Loss 5.2837 with MSE metric 6808.0361\n",
      "Time taken for 1 epoch: 22.224581241607666 secs\n",
      "\n",
      "Epoch 449 batch 0 train Loss 5.1706 test Loss 5.4581 with MSE metric 5548.4688\n",
      "Epoch 449 batch 100 train Loss 5.2644 test Loss 5.3722 with MSE metric 6848.3682\n",
      "Epoch 449 batch 200 train Loss 5.3549 test Loss 5.4338 with MSE metric 8238.0039\n",
      "Time taken for 1 epoch: 22.504530906677246 secs\n",
      "\n",
      "Epoch 450 batch 0 train Loss 5.2454 test Loss 5.4219 with MSE metric 6591.6226\n",
      "Epoch 450 batch 100 train Loss 5.2797 test Loss 5.3204 with MSE metric 7014.4351\n",
      "Epoch 450 batch 200 train Loss 5.3813 test Loss 5.3979 with MSE metric 8581.4609\n",
      "Time taken for 1 epoch: 22.59902000427246 secs\n",
      "\n",
      "Epoch 451 batch 0 train Loss 5.2165 test Loss 5.3616 with MSE metric 6218.8623\n",
      "Epoch 451 batch 100 train Loss 5.2549 test Loss 5.4162 with MSE metric 6673.9150\n",
      "Epoch 451 batch 200 train Loss 5.3237 test Loss 5.3833 with MSE metric 7739.0127\n",
      "Time taken for 1 epoch: 22.531920909881592 secs\n",
      "\n",
      "Epoch 452 batch 0 train Loss 5.3676 test Loss 5.4040 with MSE metric 8390.4102\n",
      "Epoch 452 batch 100 train Loss 5.2392 test Loss 5.3867 with MSE metric 6466.0850\n",
      "Epoch 452 batch 200 train Loss 5.2587 test Loss 5.4270 with MSE metric 6723.2148\n",
      "Time taken for 1 epoch: 22.333605766296387 secs\n",
      "\n",
      "Epoch 453 batch 0 train Loss 5.3524 test Loss 5.4093 with MSE metric 8194.5635\n",
      "Epoch 453 batch 100 train Loss 5.2452 test Loss 5.3933 with MSE metric 6587.4912\n",
      "Epoch 453 batch 200 train Loss 5.3205 test Loss 5.3306 with MSE metric 7690.1519\n",
      "Time taken for 1 epoch: 22.27282190322876 secs\n",
      "\n",
      "Epoch 454 batch 0 train Loss 5.3257 test Loss 5.3154 with MSE metric 7764.3301\n",
      "Epoch 454 batch 100 train Loss 5.3459 test Loss 5.5025 with MSE metric 8030.5977\n",
      "Epoch 454 batch 200 train Loss 5.2945 test Loss 5.3397 with MSE metric 7299.7441\n",
      "Time taken for 1 epoch: 22.514400959014893 secs\n",
      "\n",
      "Epoch 455 batch 0 train Loss 5.3213 test Loss 5.3005 with MSE metric 7668.6328\n",
      "Epoch 455 batch 100 train Loss 5.2579 test Loss 5.2988 with MSE metric 6751.6157\n",
      "Epoch 455 batch 200 train Loss 5.2759 test Loss 5.4043 with MSE metric 7034.7490\n",
      "Time taken for 1 epoch: 22.37731695175171 secs\n",
      "\n",
      "Epoch 456 batch 0 train Loss 5.3562 test Loss 5.2978 with MSE metric 8181.1787\n",
      "Epoch 456 batch 100 train Loss 5.3589 test Loss 5.4353 with MSE metric 8195.6152\n",
      "Epoch 456 batch 200 train Loss 5.1995 test Loss 5.3813 with MSE metric 5946.6274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 22.623358964920044 secs\n",
      "\n",
      "Epoch 457 batch 0 train Loss 5.2890 test Loss 5.3879 with MSE metric 7216.9902\n",
      "Epoch 457 batch 100 train Loss 5.2575 test Loss 5.2655 with MSE metric 6775.1909\n",
      "Epoch 457 batch 200 train Loss 5.3038 test Loss 5.3903 with MSE metric 7437.0264\n",
      "Time taken for 1 epoch: 22.657464027404785 secs\n",
      "\n",
      "Epoch 458 batch 0 train Loss 5.3164 test Loss 5.3650 with MSE metric 7616.2485\n",
      "Epoch 458 batch 100 train Loss 5.3368 test Loss 5.3445 with MSE metric 7934.5234\n",
      "Epoch 458 batch 200 train Loss 5.3325 test Loss 5.3318 with MSE metric 7877.6504\n",
      "Time taken for 1 epoch: 22.585384845733643 secs\n",
      "\n",
      "Epoch 459 batch 0 train Loss 5.3203 test Loss 5.4038 with MSE metric 7664.2690\n",
      "Epoch 459 batch 100 train Loss 5.3001 test Loss 5.3857 with MSE metric 7362.0967\n",
      "Epoch 459 batch 200 train Loss 5.2889 test Loss 5.2733 with MSE metric 7200.3452\n",
      "Time taken for 1 epoch: 22.57805585861206 secs\n",
      "\n",
      "Epoch 460 batch 0 train Loss 5.2433 test Loss 5.4268 with MSE metric 6561.9819\n",
      "Epoch 460 batch 100 train Loss 5.3237 test Loss 5.3554 with MSE metric 7740.3701\n",
      "Epoch 460 batch 200 train Loss 5.2610 test Loss 5.4015 with MSE metric 6826.8960\n",
      "Time taken for 1 epoch: 22.63233995437622 secs\n",
      "\n",
      "Epoch 461 batch 0 train Loss 5.2802 test Loss 5.2794 with MSE metric 7082.2061\n",
      "Epoch 461 batch 100 train Loss 5.3116 test Loss 5.4199 with MSE metric 7548.7456\n",
      "Epoch 461 batch 200 train Loss 5.3900 test Loss 5.3676 with MSE metric 8759.0156\n",
      "Time taken for 1 epoch: 22.588239669799805 secs\n",
      "\n",
      "Epoch 462 batch 0 train Loss 5.2994 test Loss 5.3131 with MSE metric 7352.8154\n",
      "Epoch 462 batch 100 train Loss 5.2432 test Loss 5.3759 with MSE metric 6553.7891\n",
      "Epoch 462 batch 200 train Loss 5.2581 test Loss 5.3764 with MSE metric 6771.2314\n",
      "Time taken for 1 epoch: 22.512952089309692 secs\n",
      "\n",
      "Epoch 463 batch 0 train Loss 5.3435 test Loss 5.3644 with MSE metric 8028.2803\n",
      "Epoch 463 batch 100 train Loss 5.2020 test Loss 5.3728 with MSE metric 5978.3198\n",
      "Epoch 463 batch 200 train Loss 5.3263 test Loss 5.3855 with MSE metric 7750.6479\n",
      "Time taken for 1 epoch: 22.547253131866455 secs\n",
      "\n",
      "Epoch 464 batch 0 train Loss 5.2626 test Loss 5.3784 with MSE metric 6834.0137\n",
      "Epoch 464 batch 100 train Loss 5.3351 test Loss 5.2276 with MSE metric 7892.1245\n",
      "Epoch 464 batch 200 train Loss 5.3595 test Loss 5.3390 with MSE metric 8227.9922\n",
      "Time taken for 1 epoch: 22.582736015319824 secs\n",
      "\n",
      "Epoch 465 batch 0 train Loss 5.2099 test Loss 5.3310 with MSE metric 6070.4272\n",
      "Epoch 465 batch 100 train Loss 5.2782 test Loss 5.3270 with MSE metric 7053.6763\n",
      "Epoch 465 batch 200 train Loss 5.2649 test Loss 5.3054 with MSE metric 6864.0879\n",
      "Time taken for 1 epoch: 22.593013763427734 secs\n",
      "\n",
      "Epoch 466 batch 0 train Loss 5.2977 test Loss 5.4377 with MSE metric 7346.2910\n",
      "Epoch 466 batch 100 train Loss 5.2485 test Loss 5.3790 with MSE metric 6578.6489\n",
      "Epoch 466 batch 200 train Loss 5.3328 test Loss 5.3311 with MSE metric 7838.0269\n",
      "Time taken for 1 epoch: 22.63333511352539 secs\n",
      "\n",
      "Epoch 467 batch 0 train Loss 5.3026 test Loss 5.4135 with MSE metric 7415.6465\n",
      "Epoch 467 batch 100 train Loss 5.3124 test Loss 5.3923 with MSE metric 7566.8887\n",
      "Epoch 467 batch 200 train Loss 5.2277 test Loss 5.3202 with MSE metric 6328.8115\n",
      "Time taken for 1 epoch: 22.635918855667114 secs\n",
      "\n",
      "Epoch 468 batch 0 train Loss 5.3789 test Loss 5.3043 with MSE metric 8642.3018\n",
      "Epoch 468 batch 100 train Loss 5.3246 test Loss 5.3263 with MSE metric 7625.8442\n",
      "Epoch 468 batch 200 train Loss 5.3027 test Loss 5.3068 with MSE metric 7422.1602\n",
      "Time taken for 1 epoch: 22.502104997634888 secs\n",
      "\n",
      "Epoch 469 batch 0 train Loss 5.2719 test Loss 5.4653 with MSE metric 6972.5942\n",
      "Epoch 469 batch 100 train Loss 5.4074 test Loss 5.2686 with MSE metric 9059.6484\n",
      "Epoch 469 batch 200 train Loss 5.2561 test Loss 5.4060 with MSE metric 6761.9268\n",
      "Time taken for 1 epoch: 22.40719985961914 secs\n",
      "\n",
      "Epoch 470 batch 0 train Loss 5.2735 test Loss 5.4382 with MSE metric 6991.4946\n",
      "Epoch 470 batch 100 train Loss 5.2603 test Loss 5.2920 with MSE metric 6755.2510\n",
      "Epoch 470 batch 200 train Loss 5.3860 test Loss 5.3365 with MSE metric 8706.0684\n",
      "Time taken for 1 epoch: 22.43846297264099 secs\n",
      "\n",
      "Epoch 471 batch 0 train Loss 5.4030 test Loss 5.3211 with MSE metric 8912.0703\n",
      "Epoch 471 batch 100 train Loss 5.3389 test Loss 5.3650 with MSE metric 7968.8311\n",
      "Epoch 471 batch 200 train Loss 5.2162 test Loss 5.4911 with MSE metric 6171.8647\n",
      "Time taken for 1 epoch: 22.581636905670166 secs\n",
      "\n",
      "Epoch 472 batch 0 train Loss 5.2109 test Loss 5.2567 with MSE metric 6125.6201\n",
      "Epoch 472 batch 100 train Loss 5.4064 test Loss 5.2934 with MSE metric 9028.0195\n",
      "Epoch 472 batch 200 train Loss 5.2612 test Loss 5.3512 with MSE metric 6830.5249\n",
      "Time taken for 1 epoch: 22.526593923568726 secs\n",
      "\n",
      "Epoch 473 batch 0 train Loss 5.3039 test Loss 5.3526 with MSE metric 7439.4951\n",
      "Epoch 473 batch 100 train Loss 5.3411 test Loss 5.4081 with MSE metric 8011.9824\n",
      "Epoch 473 batch 200 train Loss 5.2979 test Loss 5.2909 with MSE metric 7350.5244\n",
      "Time taken for 1 epoch: 22.479572296142578 secs\n",
      "\n",
      "Epoch 474 batch 0 train Loss 5.3889 test Loss 5.3628 with MSE metric 8743.2197\n",
      "Epoch 474 batch 100 train Loss 5.3538 test Loss 5.3762 with MSE metric 8118.4014\n",
      "Epoch 474 batch 200 train Loss 5.3557 test Loss 5.3107 with MSE metric 8250.6514\n",
      "Time taken for 1 epoch: 22.406260013580322 secs\n",
      "\n",
      "Epoch 475 batch 0 train Loss 5.2775 test Loss 5.4011 with MSE metric 7057.0361\n",
      "Epoch 475 batch 100 train Loss 5.1947 test Loss 5.3659 with MSE metric 5779.0166\n",
      "Epoch 475 batch 200 train Loss 5.3412 test Loss 5.3220 with MSE metric 7930.5361\n",
      "Time taken for 1 epoch: 22.45692014694214 secs\n",
      "\n",
      "Epoch 476 batch 0 train Loss 5.3140 test Loss 5.3943 with MSE metric 7591.9868\n",
      "Epoch 476 batch 100 train Loss 5.3503 test Loss 5.3606 with MSE metric 8138.6270\n",
      "Epoch 476 batch 200 train Loss 5.3392 test Loss 5.3356 with MSE metric 7982.2939\n",
      "Time taken for 1 epoch: 22.460530281066895 secs\n",
      "\n",
      "Epoch 477 batch 0 train Loss 5.2634 test Loss 5.3572 with MSE metric 6842.4561\n",
      "Epoch 477 batch 100 train Loss 5.2890 test Loss 5.2583 with MSE metric 7202.5371\n",
      "Epoch 477 batch 200 train Loss 5.1791 test Loss 5.3734 with MSE metric 5666.4209\n",
      "Time taken for 1 epoch: 22.433043956756592 secs\n",
      "\n",
      "Epoch 478 batch 0 train Loss 5.2552 test Loss 5.3041 with MSE metric 6712.5630\n",
      "Epoch 478 batch 100 train Loss 5.3432 test Loss 5.3222 with MSE metric 7979.0703\n",
      "Epoch 478 batch 200 train Loss 5.3243 test Loss 5.3918 with MSE metric 7721.6318\n",
      "Time taken for 1 epoch: 22.402808904647827 secs\n",
      "\n",
      "Epoch 479 batch 0 train Loss 5.2583 test Loss 5.4911 with MSE metric 6761.9639\n",
      "Epoch 479 batch 100 train Loss 5.3803 test Loss 5.2557 with MSE metric 8647.5000\n",
      "Epoch 479 batch 200 train Loss 5.2373 test Loss 5.4803 with MSE metric 6501.9961\n",
      "Time taken for 1 epoch: 22.413750886917114 secs\n",
      "\n",
      "Epoch 480 batch 0 train Loss 5.3147 test Loss 5.2936 with MSE metric 7601.8896\n",
      "Epoch 480 batch 100 train Loss 5.3175 test Loss 5.3023 with MSE metric 7644.6226\n",
      "Epoch 480 batch 200 train Loss 5.3148 test Loss 5.4437 with MSE metric 7597.3867\n",
      "Time taken for 1 epoch: 22.381476163864136 secs\n",
      "\n",
      "Epoch 481 batch 0 train Loss 5.3390 test Loss 5.3312 with MSE metric 7900.8525\n",
      "Epoch 481 batch 100 train Loss 5.3157 test Loss 5.2177 with MSE metric 7615.0898\n",
      "Epoch 481 batch 200 train Loss 5.2911 test Loss 5.3923 with MSE metric 7216.5928\n",
      "Time taken for 1 epoch: 22.535982131958008 secs\n",
      "\n",
      "Epoch 482 batch 0 train Loss 5.3287 test Loss 5.2838 with MSE metric 7818.1606\n",
      "Epoch 482 batch 100 train Loss 5.3086 test Loss 5.2518 with MSE metric 7504.2314\n",
      "Epoch 482 batch 200 train Loss 5.2788 test Loss 5.4413 with MSE metric 7053.8667\n",
      "Time taken for 1 epoch: 22.358999729156494 secs\n",
      "\n",
      "Epoch 483 batch 0 train Loss 5.2360 test Loss 5.3295 with MSE metric 6388.5635\n",
      "Epoch 483 batch 100 train Loss 5.2796 test Loss 5.3865 with MSE metric 7087.4854\n",
      "Epoch 483 batch 200 train Loss 5.2526 test Loss 5.4054 with MSE metric 6654.9004\n",
      "Time taken for 1 epoch: 22.476372957229614 secs\n",
      "\n",
      "Epoch 484 batch 0 train Loss 5.2725 test Loss 5.4129 with MSE metric 6984.8027\n",
      "Epoch 484 batch 100 train Loss 5.2620 test Loss 5.2448 with MSE metric 6813.3955\n",
      "Epoch 484 batch 200 train Loss 5.3148 test Loss 5.3958 with MSE metric 7600.1255\n",
      "Time taken for 1 epoch: 22.490341901779175 secs\n",
      "\n",
      "Epoch 485 batch 0 train Loss 5.3884 test Loss 5.3568 with MSE metric 8701.3057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485 batch 100 train Loss 5.3225 test Loss 5.3692 with MSE metric 7721.7979\n",
      "Epoch 485 batch 200 train Loss 5.2944 test Loss 5.3463 with MSE metric 7300.1924\n",
      "Time taken for 1 epoch: 22.402196884155273 secs\n",
      "\n",
      "Epoch 486 batch 0 train Loss 5.3204 test Loss 5.4723 with MSE metric 7689.2246\n",
      "Epoch 486 batch 100 train Loss 5.2746 test Loss 5.4246 with MSE metric 7010.4839\n",
      "Epoch 486 batch 200 train Loss 5.1548 test Loss 5.3722 with MSE metric 5309.9536\n",
      "Time taken for 1 epoch: 22.49403977394104 secs\n",
      "\n",
      "Epoch 487 batch 0 train Loss 5.3312 test Loss 5.4040 with MSE metric 7845.3115\n",
      "Epoch 487 batch 100 train Loss 5.3836 test Loss 5.2669 with MSE metric 8672.8652\n",
      "Epoch 487 batch 200 train Loss 5.3648 test Loss 5.4243 with MSE metric 8343.5762\n",
      "Time taken for 1 epoch: 22.40848207473755 secs\n",
      "\n",
      "Epoch 488 batch 0 train Loss 5.2059 test Loss 5.2147 with MSE metric 6065.7637\n",
      "Epoch 488 batch 100 train Loss 5.2934 test Loss 5.4158 with MSE metric 7270.4302\n",
      "Epoch 488 batch 200 train Loss 5.3087 test Loss 5.3821 with MSE metric 7491.2891\n",
      "Time taken for 1 epoch: 22.577558040618896 secs\n",
      "\n",
      "Epoch 489 batch 0 train Loss 5.2012 test Loss 5.2392 with MSE metric 5930.7568\n",
      "Epoch 489 batch 100 train Loss 5.3476 test Loss 5.4383 with MSE metric 8094.1807\n",
      "Epoch 489 batch 200 train Loss 5.1960 test Loss 5.4753 with MSE metric 5781.4360\n",
      "Time taken for 1 epoch: 22.5941801071167 secs\n",
      "\n",
      "Epoch 490 batch 0 train Loss 5.3174 test Loss 5.2196 with MSE metric 7633.3955\n",
      "Epoch 490 batch 100 train Loss 5.1825 test Loss 5.4195 with MSE metric 5616.5542\n",
      "Epoch 490 batch 200 train Loss 5.3121 test Loss 5.3233 with MSE metric 7538.8086\n",
      "Time taken for 1 epoch: 22.323669910430908 secs\n",
      "\n",
      "Epoch 491 batch 0 train Loss 5.4043 test Loss 5.4670 with MSE metric 8956.9746\n",
      "Epoch 491 batch 100 train Loss 5.3026 test Loss 5.4226 with MSE metric 7419.9663\n",
      "Epoch 491 batch 200 train Loss 5.3483 test Loss 5.3023 with MSE metric 8100.8516\n",
      "Time taken for 1 epoch: 22.456486701965332 secs\n",
      "\n",
      "Epoch 492 batch 0 train Loss 5.2424 test Loss 5.3438 with MSE metric 6545.4819\n",
      "Epoch 492 batch 100 train Loss 5.3608 test Loss 5.3409 with MSE metric 8299.7266\n",
      "Epoch 492 batch 200 train Loss 5.2443 test Loss 5.3467 with MSE metric 6539.9419\n",
      "Time taken for 1 epoch: 22.364535093307495 secs\n",
      "\n",
      "Epoch 493 batch 0 train Loss 5.3411 test Loss 5.3390 with MSE metric 7970.1230\n",
      "Epoch 493 batch 100 train Loss 5.3557 test Loss 5.3941 with MSE metric 8191.4961\n",
      "Epoch 493 batch 200 train Loss 5.3206 test Loss 5.3032 with MSE metric 7691.1514\n",
      "Time taken for 1 epoch: 22.594536066055298 secs\n",
      "\n",
      "Epoch 494 batch 0 train Loss 5.2290 test Loss 5.3411 with MSE metric 6187.0054\n",
      "Epoch 494 batch 100 train Loss 5.3032 test Loss 5.3751 with MSE metric 7423.4834\n",
      "Epoch 494 batch 200 train Loss 5.3138 test Loss 5.3759 with MSE metric 7580.8789\n",
      "Time taken for 1 epoch: 22.368724822998047 secs\n",
      "\n",
      "Epoch 495 batch 0 train Loss 5.2179 test Loss 5.3794 with MSE metric 6135.4985\n",
      "Epoch 495 batch 100 train Loss 5.2317 test Loss 5.3152 with MSE metric 6397.7510\n",
      "Epoch 495 batch 200 train Loss 5.2302 test Loss 5.3168 with MSE metric 6391.3975\n",
      "Time taken for 1 epoch: 22.496071815490723 secs\n",
      "\n",
      "Epoch 496 batch 0 train Loss 5.2824 test Loss 5.3404 with MSE metric 7122.5527\n",
      "Epoch 496 batch 100 train Loss 5.2298 test Loss 5.3442 with MSE metric 6378.0889\n",
      "Epoch 496 batch 200 train Loss 5.2107 test Loss 5.3827 with MSE metric 6101.9126\n",
      "Time taken for 1 epoch: 22.39377474784851 secs\n",
      "\n",
      "Epoch 497 batch 0 train Loss 5.2481 test Loss 5.4563 with MSE metric 6608.4565\n",
      "Epoch 497 batch 100 train Loss 5.2475 test Loss 5.3295 with MSE metric 6642.5000\n",
      "Epoch 497 batch 200 train Loss 5.2445 test Loss 5.3712 with MSE metric 6575.3960\n",
      "Time taken for 1 epoch: 22.420281171798706 secs\n",
      "\n",
      "Epoch 498 batch 0 train Loss 5.3153 test Loss 5.4060 with MSE metric 7610.7524\n",
      "Epoch 498 batch 100 train Loss 5.2609 test Loss 5.2940 with MSE metric 6800.9009\n",
      "Epoch 498 batch 200 train Loss 5.2477 test Loss 5.3831 with MSE metric 6539.4849\n",
      "Time taken for 1 epoch: 22.539700984954834 secs\n",
      "\n",
      "Epoch 499 batch 0 train Loss 5.3215 test Loss 5.4060 with MSE metric 7690.2012\n",
      "Epoch 499 batch 100 train Loss 5.3346 test Loss 5.3625 with MSE metric 7905.1274\n",
      "Epoch 499 batch 200 train Loss 5.3698 test Loss 5.4227 with MSE metric 8343.3320\n",
      "Time taken for 1 epoch: 22.46050477027893 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam(0.1)\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s=32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 100 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s= 32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float64, numpy=\n",
       "array([277.4391 , 277.1047 , 280.83145, 277.54877, 276.43674, 280.73575,\n",
       "       279.6379 , 278.9441 , 280.72787, 279.9052 , 276.1898 , 278.17764,\n",
       "       276.66025, 277.8398 , 279.3794 , 277.32608, 279.0153 , 276.279  ,\n",
       "       277.99994, 276.37442, 280.85577, 279.12204, 280.74197, 277.27045,\n",
       "       278.04602,   0.     ,   0.     ,   0.     ,   0.     ,   0.     ,\n",
       "         0.     ,   0.     ,   0.     ,   0.     ,   0.     ,   0.     ,\n",
       "         0.     ,   0.     ,   0.     ])>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_real_te[10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float64, numpy=\n",
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_mask[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float64, numpy=\n",
       "array([305.5369064 , 305.53690675, 305.53690678, 305.53690751,\n",
       "       305.53690744, 305.79655562, 305.79655595, 305.79655609,\n",
       "       305.84506049, 305.84506066, 305.84506075, 310.90365222,\n",
       "       310.90365221, 310.90365213, 105.75508275, 105.75508295,\n",
       "       105.75508276, 105.75508288, 105.75508255, 105.75508251,\n",
       "       105.75508224, 105.75508261, 105.75508272, 105.75508303,\n",
       "       105.75508289, 105.75508286, 105.75508286, 105.75508286,\n",
       "       105.75508286, 105.75508286, 105.75508286, 105.75508286,\n",
       "       105.75508286, 105.75508286, 105.75508286, 105.75508286,\n",
       "       105.75508286, 105.75508286, 105.75508286])>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_te[10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float64, numpy=\n",
       "array([  28.0978064 ,   28.43220675,   24.70545678,   27.98813751,\n",
       "         29.10016744,   25.06080562,   26.15865595,   26.85245609,\n",
       "         25.11719049,   25.93986066,   29.65526075,   32.72601222,\n",
       "         34.24340221,   33.06385213, -173.62431725, -171.57099705,\n",
       "       -173.26021724, -170.52391712, -172.24485745, -170.61933749,\n",
       "       -175.10068776, -173.36695739, -174.98688728, -171.51536697,\n",
       "       -172.29093711,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ])>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_te[10, :] * t_mask[10, :] ) - tar_real_te[10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
