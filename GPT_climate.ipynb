{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, combine_climate_data, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, t, token = combine_climate_data.climate_data_to_model_input('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_log_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         print(pred_log_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_log_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_log_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_log_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_log_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_log_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Restored from /Users/omernivron/Downloads/GPT_climate/ckpt/check_0/ckpt-1\n",
      "pos_attn1 : Tensor(\"decoder_2/layer_normalization_8/batchnorm/add_1:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "nl_qk:  Tensor(\"decoder_2/nl_qk:0\", shape=(32, 39, 39), dtype=float64)\n",
      "tar_attn1 : Tensor(\"decoder_2/layer_normalization_9/batchnorm/add_1:0\", shape=(32, 39, 16), dtype=float64)\n",
      "tar_attn1 : Tensor(\"decoder_2/strided_slice_10:0\", shape=(32, 39, 16, 1), dtype=float64)\n",
      "tar1 : Tensor(\"decoder_2/B/BiasAdd:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "L: Tensor(\"decoder_2/layer_normalization_10/batchnorm/add_1:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "pos_attn1 : Tensor(\"decoder_2/layer_normalization_8/batchnorm/add_1:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "nl_qk:  Tensor(\"decoder_2/nl_qk:0\", shape=(32, 39, 39), dtype=float64)\n",
      "tar_attn1 : Tensor(\"decoder_2/layer_normalization_9/batchnorm/add_1:0\", shape=(32, 39, 16), dtype=float64)\n",
      "tar_attn1 : Tensor(\"decoder_2/strided_slice_10:0\", shape=(32, 39, 16, 1), dtype=float64)\n",
      "tar1 : Tensor(\"decoder_2/B/BiasAdd:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "L: Tensor(\"decoder_2/layer_normalization_10/batchnorm/add_1:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "pos_attn1 : Tensor(\"decoder_2/layer_normalization_8/batchnorm/add_1:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "nl_qk:  Tensor(\"decoder_2/nl_qk:0\", shape=(32, 39, 39), dtype=float64)\n",
      "tar_attn1 : Tensor(\"decoder_2/layer_normalization_9/batchnorm/add_1:0\", shape=(32, 39, 16), dtype=float64)\n",
      "tar_attn1 : Tensor(\"decoder_2/strided_slice_10:0\", shape=(32, 39, 16, 1), dtype=float64)\n",
      "tar1 : Tensor(\"decoder_2/B/BiasAdd:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "L: Tensor(\"decoder_2/layer_normalization_10/batchnorm/add_1:0\", shape=(32, 39, 16, 16), dtype=float64)\n",
      "Epoch 0 batch 0 train Loss 233125.4441 test Loss 8515.4648 with MSE metric 45661.1172\n",
      "Epoch 0 batch 100 train Loss 10504.1994 test Loss 16.7482 with MSE metric 44684.2305\n",
      "Epoch 0 batch 200 train Loss 4261.5355 test Loss 7.3275 with MSE metric 47605.1641\n",
      "Time taken for 1 epoch: 26.726120948791504 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 1849.5509 test Loss 6.7267 with MSE metric 46456.9609\n",
      "Epoch 1 batch 100 train Loss 1452.2287 test Loss 6.2632 with MSE metric 43969.1289\n",
      "Epoch 1 batch 200 train Loss 877.7212 test Loss 6.2408 with MSE metric 39575.9609\n",
      "Time taken for 1 epoch: 25.302170991897583 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 706.4574 test Loss 6.3485 with MSE metric 47144.0391\n",
      "Epoch 2 batch 100 train Loss 1339.3673 test Loss 6.4890 with MSE metric 41880.1562\n",
      "Epoch 2 batch 200 train Loss 665.8042 test Loss 6.6690 with MSE metric 45874.6172\n",
      "Time taken for 1 epoch: 25.629388093948364 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 520.1801 test Loss 6.7287 with MSE metric 47608.2578\n",
      "Epoch 3 batch 100 train Loss 319.0009 test Loss 6.9111 with MSE metric 50120.7969\n",
      "Epoch 3 batch 200 train Loss 321.7925 test Loss 7.0574 with MSE metric 44000.4805\n",
      "Time taken for 1 epoch: 24.602123022079468 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 541.1232 test Loss 7.1449 with MSE metric 46197.5547\n",
      "Epoch 4 batch 100 train Loss 260.9095 test Loss 7.2646 with MSE metric 44511.4492\n",
      "Epoch 4 batch 200 train Loss 581.6248 test Loss 7.3878 with MSE metric 47416.2461\n",
      "Time taken for 1 epoch: 24.78939986228943 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 265.0294 test Loss 7.4637 with MSE metric 47210.2578\n",
      "Epoch 5 batch 100 train Loss 156.9124 test Loss 7.5574 with MSE metric 46417.1562\n",
      "Epoch 5 batch 200 train Loss 195.0880 test Loss 7.6912 with MSE metric 44812.3359\n",
      "Time taken for 1 epoch: 25.309730052947998 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 317.0964 test Loss 7.7417 with MSE metric 46654.1875\n",
      "Epoch 6 batch 100 train Loss 103.6637 test Loss 7.8427 with MSE metric 43121.7422\n",
      "Epoch 6 batch 200 train Loss 149.2612 test Loss 7.9280 with MSE metric 41537.9062\n",
      "Time taken for 1 epoch: 24.9136381149292 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 90.6246 test Loss 8.0246 with MSE metric 42297.9492\n",
      "Epoch 7 batch 100 train Loss 98.9079 test Loss 8.0743 with MSE metric 42503.6484\n",
      "Epoch 7 batch 200 train Loss 57.8769 test Loss 8.1301 with MSE metric 44832.0391\n",
      "Time taken for 1 epoch: 23.358913898468018 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 61.8000 test Loss 8.1966 with MSE metric 44756.5078\n",
      "Epoch 8 batch 100 train Loss 92.8136 test Loss 8.2975 with MSE metric 45139.8125\n",
      "Epoch 8 batch 200 train Loss 61.9992 test Loss 8.3629 with MSE metric 44236.1797\n",
      "Time taken for 1 epoch: 24.068099975585938 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 60.2243 test Loss 8.4345 with MSE metric 47251.9375\n",
      "Epoch 9 batch 100 train Loss 60.0882 test Loss 8.5184 with MSE metric 44832.9688\n",
      "Epoch 9 batch 200 train Loss 79.6144 test Loss 8.6079 with MSE metric 44969.2969\n",
      "Time taken for 1 epoch: 24.54100465774536 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 71.6504 test Loss 8.6238 with MSE metric 46690.3633\n",
      "Epoch 10 batch 100 train Loss 70.1075 test Loss 8.6911 with MSE metric 46130.9766\n",
      "Epoch 10 batch 200 train Loss 40.5564 test Loss 8.7658 with MSE metric 46483.6445\n",
      "Time taken for 1 epoch: 25.53331184387207 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 42.9141 test Loss 8.8247 with MSE metric 46278.2812\n",
      "Epoch 11 batch 100 train Loss 28.9779 test Loss 8.8968 with MSE metric 43144.7188\n",
      "Epoch 11 batch 200 train Loss 45.3005 test Loss 8.9225 with MSE metric 42429.5156\n",
      "Time taken for 1 epoch: 23.139477968215942 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 42.1937 test Loss 9.0115 with MSE metric 42076.2930\n",
      "Epoch 12 batch 100 train Loss 46.8641 test Loss 9.0617 with MSE metric 45505.1172\n",
      "Epoch 12 batch 200 train Loss 90.0170 test Loss 9.1556 with MSE metric 42499.4961\n",
      "Time taken for 1 epoch: 23.996575355529785 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 95.0551 test Loss 9.1184 with MSE metric 44015.0156\n",
      "Epoch 13 batch 100 train Loss 24.0243 test Loss 9.2057 with MSE metric 40367.7188\n",
      "Epoch 13 batch 200 train Loss 28.2805 test Loss 9.2325 with MSE metric 41902.6758\n",
      "Time taken for 1 epoch: 25.257325172424316 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 23.2983 test Loss 9.2532 with MSE metric 45543.6562\n",
      "Epoch 14 batch 100 train Loss 23.3358 test Loss 9.3188 with MSE metric 42915.8672\n",
      "Epoch 14 batch 200 train Loss 25.8875 test Loss 9.3683 with MSE metric 45136.7734\n",
      "Time taken for 1 epoch: 24.064512968063354 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 17.3049 test Loss 9.4250 with MSE metric 40495.1133\n",
      "Epoch 15 batch 100 train Loss 12.6525 test Loss 9.4525 with MSE metric 44458.4297\n",
      "Epoch 15 batch 200 train Loss 19.8713 test Loss 9.5041 with MSE metric 48623.6367\n",
      "Time taken for 1 epoch: 22.79522395133972 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 17.6385 test Loss 9.5320 with MSE metric 44155.5469\n",
      "Epoch 16 batch 100 train Loss 16.4601 test Loss 9.6031 with MSE metric 42172.7031\n",
      "Epoch 16 batch 200 train Loss 16.2943 test Loss 9.6792 with MSE metric 43470.0352\n",
      "Time taken for 1 epoch: 30.12485098838806 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 20.3227 test Loss 9.6836 with MSE metric 44588.0312\n",
      "Epoch 17 batch 100 train Loss 15.0665 test Loss 9.7238 with MSE metric 47084.5781\n",
      "Epoch 17 batch 200 train Loss 14.6027 test Loss 9.7952 with MSE metric 46875.8906\n",
      "Time taken for 1 epoch: 33.910003900527954 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 13.2645 test Loss 9.8430 with MSE metric 47446.5391\n",
      "Epoch 18 batch 100 train Loss 47.9213 test Loss 9.8450 with MSE metric 44484.7500\n",
      "Epoch 18 batch 200 train Loss 26.2678 test Loss 9.9562 with MSE metric 42392.8906\n",
      "Time taken for 1 epoch: 32.82413196563721 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 11.2167 test Loss 9.9524 with MSE metric 43311.8516\n",
      "Epoch 19 batch 100 train Loss 18.7976 test Loss 9.9968 with MSE metric 42085.8008\n",
      "Epoch 19 batch 200 train Loss 16.7685 test Loss 10.0320 with MSE metric 45813.5898\n",
      "Time taken for 1 epoch: 35.49372577667236 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 13.5443 test Loss 10.0528 with MSE metric 43281.0156\n",
      "Epoch 20 batch 100 train Loss 14.6196 test Loss 10.0896 with MSE metric 40898.2383\n",
      "Epoch 20 batch 200 train Loss 12.9984 test Loss 10.1673 with MSE metric 45523.4531\n",
      "Time taken for 1 epoch: 34.8403160572052 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 12.4070 test Loss 10.1851 with MSE metric 45440.8555\n",
      "Epoch 21 batch 100 train Loss 10.6048 test Loss 10.2142 with MSE metric 45455.7656\n",
      "Epoch 21 batch 200 train Loss 11.6628 test Loss 10.2939 with MSE metric 41287.5859\n",
      "Time taken for 1 epoch: 35.20834302902222 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 12.2445 test Loss 10.2396 with MSE metric 42350.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 batch 100 train Loss 9.9242 test Loss 10.3269 with MSE metric 43710.7656\n",
      "Epoch 22 batch 200 train Loss 11.9578 test Loss 10.3801 with MSE metric 41930.0234\n",
      "Time taken for 1 epoch: 34.18447422981262 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 11.5907 test Loss 10.3751 with MSE metric 43569.6641\n",
      "Epoch 23 batch 100 train Loss 9.9155 test Loss 10.4583 with MSE metric 46815.4062\n",
      "Epoch 23 batch 200 train Loss 10.9187 test Loss 10.4915 with MSE metric 43928.6328\n",
      "Time taken for 1 epoch: 33.83740496635437 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 20.5475 test Loss 10.4760 with MSE metric 43128.4609\n",
      "Epoch 24 batch 100 train Loss 8.9766 test Loss 10.5061 with MSE metric 47437.9688\n",
      "Epoch 24 batch 200 train Loss 12.2728 test Loss 10.5966 with MSE metric 49487.0586\n",
      "Time taken for 1 epoch: 33.538349866867065 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 8.6775 test Loss 10.6303 with MSE metric 42106.1406\n",
      "Epoch 25 batch 100 train Loss 122.3714 test Loss 10.6512 with MSE metric 45200.2266\n",
      "Epoch 25 batch 200 train Loss 17.3881 test Loss 10.7230 with MSE metric 45166.2383\n",
      "Time taken for 1 epoch: 34.4494206905365 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 18.8778 test Loss 10.7737 with MSE metric 43960.7266\n",
      "Epoch 26 batch 100 train Loss 9.0806 test Loss 10.7496 with MSE metric 44988.0625\n",
      "Epoch 26 batch 200 train Loss 8.5021 test Loss 10.8190 with MSE metric 42693.5703\n",
      "Time taken for 1 epoch: 34.782405853271484 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 7.9588 test Loss 10.8407 with MSE metric 45903.3711\n",
      "Epoch 27 batch 100 train Loss 8.4869 test Loss 10.8866 with MSE metric 43578.1836\n",
      "Epoch 27 batch 200 train Loss 8.7714 test Loss 10.9531 with MSE metric 42771.4844\n",
      "Time taken for 1 epoch: 34.98838710784912 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 8.3447 test Loss 10.9294 with MSE metric 41667.3906\n",
      "Epoch 28 batch 100 train Loss 7.9046 test Loss 11.0023 with MSE metric 43312.0703\n",
      "Epoch 28 batch 200 train Loss 7.8344 test Loss 11.0354 with MSE metric 40252.3711\n",
      "Time taken for 1 epoch: 33.370272159576416 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 8.1994 test Loss 11.0577 with MSE metric 44456.2305\n",
      "Epoch 29 batch 100 train Loss 8.3996 test Loss 11.0941 with MSE metric 44594.4805\n",
      "Epoch 29 batch 200 train Loss 11.4719 test Loss 11.0946 with MSE metric 41392.8672\n",
      "Time taken for 1 epoch: 34.37622594833374 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 11.1018 test Loss 11.1062 with MSE metric 40585.3516\n",
      "Epoch 30 batch 100 train Loss 9.3796 test Loss 11.1343 with MSE metric 44267.9922\n",
      "Epoch 30 batch 200 train Loss 7.8190 test Loss 11.1986 with MSE metric 42722.1797\n",
      "Time taken for 1 epoch: 32.716623306274414 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 8.0927 test Loss 11.2335 with MSE metric 48160.0938\n",
      "Epoch 31 batch 100 train Loss 8.0121 test Loss 11.2241 with MSE metric 42024.3359\n",
      "Epoch 31 batch 200 train Loss 7.5338 test Loss 11.2722 with MSE metric 43173.3281\n",
      "Time taken for 1 epoch: 32.43119692802429 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 8.3302 test Loss 11.2985 with MSE metric 45118.1016\n",
      "Epoch 32 batch 100 train Loss 8.9002 test Loss 11.3289 with MSE metric 46253.9414\n",
      "Epoch 32 batch 200 train Loss 8.6502 test Loss 11.3284 with MSE metric 45705.1797\n",
      "Time taken for 1 epoch: 32.34538912773132 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 8.8076 test Loss 11.3071 with MSE metric 44815.4297\n",
      "Epoch 33 batch 100 train Loss 7.9253 test Loss 11.3871 with MSE metric 41509.9141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2b774f8b228d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mbatch_pos_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tok_pos_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mtar_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_log_sig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tok_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tim_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tar_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_n\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam(0.0003)\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s = 32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_log_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 100 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s = 32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_log_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
