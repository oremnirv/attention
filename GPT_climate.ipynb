{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, data_combine, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, t, token = data_combine.climate_data_to_model_input('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Initializing from scratch.\n",
      "Epoch 0 batch 0 train Loss 276635.7271 test Loss 92122.0270 with MSE metric 45551.5664\n",
      "Epoch 0 batch 10 train Loss 147707.3831 test Loss 46122.4647 with MSE metric 46528.5883\n",
      "Epoch 0 batch 20 train Loss 88084.5578 test Loss 30754.6986 with MSE metric 46206.7365\n",
      "Epoch 0 batch 30 train Loss 62429.0003 test Loss 23068.2816 with MSE metric 45921.8479\n",
      "Epoch 0 batch 40 train Loss 48274.3682 test Loss 18456.1269 with MSE metric 46211.5025\n",
      "Epoch 0 batch 50 train Loss 39442.4337 test Loss 15381.2496 with MSE metric 46455.9027\n",
      "Epoch 0 batch 60 train Loss 33508.3575 test Loss 13184.8707 with MSE metric 46346.1084\n",
      "Epoch 0 batch 70 train Loss 29167.5388 test Loss 11537.5780 with MSE metric 46335.6242\n",
      "Epoch 0 batch 80 train Loss 25797.9345 test Loss 10256.3357 with MSE metric 46273.0657\n",
      "Epoch 0 batch 90 train Loss 23146.2318 test Loss 9231.3473 with MSE metric 46227.9679\n",
      "Epoch 0 batch 100 train Loss 21060.0951 test Loss 8392.7229 with MSE metric 46165.1620\n",
      "Epoch 0 batch 110 train Loss 19309.1080 test Loss 7693.8508 with MSE metric 46050.8190\n",
      "Epoch 0 batch 120 train Loss 17813.1833 test Loss 7102.5077 with MSE metric 46037.1066\n",
      "Epoch 0 batch 130 train Loss 16545.6666 test Loss 6595.6374 with MSE metric 45987.3798\n",
      "Epoch 0 batch 140 train Loss 15436.1051 test Loss 6156.3540 with MSE metric 45965.9638\n",
      "Epoch 0 batch 150 train Loss 14490.9189 test Loss 5771.9818 with MSE metric 45837.2313\n",
      "Epoch 0 batch 160 train Loss 13648.2320 test Loss 5432.8268 with MSE metric 45738.6189\n",
      "Epoch 0 batch 170 train Loss 12894.9693 test Loss 5131.3574 with MSE metric 45722.8218\n",
      "Epoch 0 batch 180 train Loss 12231.2676 test Loss 4861.6208 with MSE metric 45704.8512\n",
      "Epoch 0 batch 190 train Loss 11666.5050 test Loss 4618.8589 with MSE metric 45681.4122\n",
      "Epoch 0 batch 200 train Loss 11118.5316 test Loss 4399.2165 with MSE metric 45638.4006\n",
      "Epoch 0 batch 210 train Loss 10623.2994 test Loss 4199.5465 with MSE metric 45616.0264\n",
      "Epoch 0 batch 220 train Loss 10173.6591 test Loss 4017.2419 with MSE metric 45623.0627\n",
      "Epoch 0 batch 230 train Loss 9873.5946 test Loss 3850.1275 with MSE metric 45636.4663\n",
      "Epoch 0 batch 240 train Loss 9480.7455 test Loss 3696.3865 with MSE metric 45682.9595\n",
      "Time taken for 1 epoch: 29.80279779434204 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 9124.2280 test Loss 3554.4753 with MSE metric 45660.4712\n",
      "Epoch 1 batch 10 train Loss 8791.8516 test Loss 3423.0775 with MSE metric 45631.3107\n",
      "Epoch 1 batch 20 train Loss 8483.4931 test Loss 3301.0655 with MSE metric 45624.8822\n",
      "Epoch 1 batch 30 train Loss 8197.5235 test Loss 3187.4693 with MSE metric 45595.5737\n",
      "Epoch 1 batch 40 train Loss 7926.3725 test Loss 3081.4480 with MSE metric 45558.1109\n",
      "Epoch 1 batch 50 train Loss 7672.9370 test Loss 2982.2665 with MSE metric 45564.8212\n",
      "Epoch 1 batch 60 train Loss 7435.2876 test Loss 2889.2848 with MSE metric 45548.5692\n",
      "Epoch 1 batch 70 train Loss 7216.6531 test Loss 2801.9395 with MSE metric 45565.6623\n",
      "Epoch 1 batch 80 train Loss 7013.3297 test Loss 2719.7330 with MSE metric 45543.4595\n",
      "Epoch 1 batch 90 train Loss 6815.8027 test Loss 2642.2251 with MSE metric 45544.6597\n",
      "Epoch 1 batch 100 train Loss 6630.0385 test Loss 2569.0271 with MSE metric 45511.0475\n",
      "Epoch 1 batch 110 train Loss 6454.1176 test Loss 2499.7850 with MSE metric 45552.1871\n",
      "Epoch 1 batch 120 train Loss 6286.4269 test Loss 2434.1891 with MSE metric 45541.7238\n",
      "Epoch 1 batch 130 train Loss 6127.5972 test Loss 2371.9559 with MSE metric 45516.7344\n",
      "Epoch 1 batch 140 train Loss 5977.9277 test Loss 2312.8366 with MSE metric 45525.2810\n",
      "Epoch 1 batch 150 train Loss 5834.2226 test Loss 2256.6018 with MSE metric 45501.9900\n",
      "Epoch 1 batch 160 train Loss 5696.9706 test Loss 2203.0444 with MSE metric 45504.5150\n",
      "Epoch 1 batch 170 train Loss 5567.8414 test Loss 2151.9788 with MSE metric 45511.1050\n",
      "Epoch 1 batch 180 train Loss 5444.3873 test Loss 2103.2348 with MSE metric 45505.7568\n",
      "Epoch 1 batch 190 train Loss 5327.1111 test Loss 2056.6562 with MSE metric 45486.8465\n",
      "Epoch 1 batch 200 train Loss 5215.3255 test Loss 2012.1051 with MSE metric 45456.5968\n",
      "Epoch 1 batch 210 train Loss 5107.2656 test Loss 1969.4498 with MSE metric 45438.7616\n",
      "Epoch 1 batch 220 train Loss 5004.6703 test Loss 1928.5730 with MSE metric 45416.6755\n",
      "Epoch 1 batch 230 train Loss 4904.7037 test Loss 1889.3640 with MSE metric 45411.7511\n",
      "Epoch 1 batch 240 train Loss 4808.9931 test Loss 1851.7242 with MSE metric 45396.3384\n",
      "Time taken for 1 epoch: 30.204960107803345 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 4715.8155 test Loss 1815.5617 with MSE metric 45372.3463\n",
      "Epoch 2 batch 10 train Loss 4626.4286 test Loss 1780.7894 with MSE metric 45407.1597\n",
      "Epoch 2 batch 20 train Loss 4540.6385 test Loss 1747.3304 with MSE metric 45395.9634\n",
      "Epoch 2 batch 30 train Loss 4458.9864 test Loss 1715.1112 with MSE metric 45368.7647\n",
      "Epoch 2 batch 40 train Loss 4378.9445 test Loss 1684.0633 with MSE metric 45358.9707\n",
      "Epoch 2 batch 50 train Loss 4301.6845 test Loss 1654.1259 with MSE metric 45358.0889\n",
      "Epoch 2 batch 60 train Loss 4226.7513 test Loss 1625.2389 with MSE metric 45356.1640\n",
      "Epoch 2 batch 70 train Loss 4155.5887 test Loss 1597.3483 with MSE metric 45337.4026\n",
      "Epoch 2 batch 80 train Loss 4085.8513 test Loss 1570.4030 with MSE metric 45320.4106\n",
      "Epoch 2 batch 90 train Loss 4018.8824 test Loss 1544.3560 with MSE metric 45301.5502\n",
      "Epoch 2 batch 100 train Loss 3953.5983 test Loss 1519.1634 with MSE metric 45312.4614\n",
      "Epoch 2 batch 110 train Loss 3907.1685 test Loss 1494.7844 with MSE metric 45320.0233\n",
      "Epoch 2 batch 120 train Loss 3846.0601 test Loss 1471.1793 with MSE metric 45321.6546\n",
      "Epoch 2 batch 130 train Loss 3786.8848 test Loss 1448.3114 with MSE metric 45310.0291\n",
      "Epoch 2 batch 140 train Loss 3730.0600 test Loss 1426.1475 with MSE metric 45307.0465\n",
      "Epoch 2 batch 150 train Loss 3674.7688 test Loss 1404.6558 with MSE metric 45301.0975\n",
      "Epoch 2 batch 160 train Loss 3622.3668 test Loss 1383.8053 with MSE metric 45284.5422\n",
      "Epoch 2 batch 170 train Loss 3570.4887 test Loss 1363.5687 with MSE metric 45264.1534\n",
      "Epoch 2 batch 180 train Loss 3519.4906 test Loss 1343.9183 with MSE metric 45255.2268\n",
      "Epoch 2 batch 190 train Loss 3469.9788 test Loss 1324.8305 with MSE metric 45242.3446\n",
      "Epoch 2 batch 200 train Loss 3421.7977 test Loss 1306.2798 with MSE metric 45234.8210\n",
      "Epoch 2 batch 210 train Loss 3374.9198 test Loss 1288.2458 with MSE metric 45216.6375\n",
      "Epoch 2 batch 220 train Loss 3329.2466 test Loss 1270.7057 with MSE metric 45224.7367\n",
      "Epoch 2 batch 230 train Loss 3290.2829 test Loss 1253.6399 with MSE metric 45212.4944\n",
      "Epoch 2 batch 240 train Loss 3247.0155 test Loss 1237.0290 with MSE metric 45213.0627\n",
      "Time taken for 1 epoch: 31.725919008255005 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 3204.6864 test Loss 1220.8559 with MSE metric 45215.3255\n",
      "Epoch 3 batch 10 train Loss 3164.4331 test Loss 1205.1031 with MSE metric 45203.2527\n",
      "Epoch 3 batch 20 train Loss 3124.5183 test Loss 1189.7540 with MSE metric 45187.2403\n",
      "Epoch 3 batch 30 train Loss 3085.7194 test Loss 1174.7932 with MSE metric 45186.5168\n",
      "Epoch 3 batch 40 train Loss 3047.8729 test Loss 1160.2082 with MSE metric 45193.8979\n",
      "Epoch 3 batch 50 train Loss 3010.8848 test Loss 1145.9832 with MSE metric 45201.4190\n",
      "Epoch 3 batch 60 train Loss 2974.5083 test Loss 1132.1055 with MSE metric 45197.5738\n",
      "Epoch 3 batch 70 train Loss 2939.4955 test Loss 1118.5618 with MSE metric 45179.5291\n",
      "Epoch 3 batch 80 train Loss 2904.8209 test Loss 1105.3416 with MSE metric 45177.5222\n",
      "Epoch 3 batch 90 train Loss 2871.3609 test Loss 1092.4312 with MSE metric 45154.4694\n",
      "Epoch 3 batch 100 train Loss 2838.4304 test Loss 1079.8221 with MSE metric 45135.9870\n",
      "Epoch 3 batch 110 train Loss 2806.0842 test Loss 1067.5027 with MSE metric 45127.8282\n",
      "Epoch 3 batch 120 train Loss 2774.9793 test Loss 1055.4634 with MSE metric 45137.2418\n",
      "Epoch 3 batch 130 train Loss 2744.1656 test Loss 1043.6953 with MSE metric 45140.2590\n",
      "Epoch 3 batch 140 train Loss 2714.0698 test Loss 1032.1884 with MSE metric 45125.9614\n",
      "Epoch 3 batch 150 train Loss 2684.6101 test Loss 1020.9345 with MSE metric 45127.0517\n",
      "Epoch 3 batch 160 train Loss 2655.7088 test Loss 1009.9257 with MSE metric 45106.0034\n",
      "Epoch 3 batch 170 train Loss 2627.6455 test Loss 999.1540 with MSE metric 45100.1974\n",
      "Epoch 3 batch 180 train Loss 2600.1196 test Loss 988.6107 with MSE metric 45096.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 batch 190 train Loss 2573.7582 test Loss 978.2907 with MSE metric 45097.1019\n",
      "Epoch 3 batch 200 train Loss 2547.3261 test Loss 968.1856 with MSE metric 45086.2424\n",
      "Epoch 3 batch 210 train Loss 2521.5919 test Loss 958.2892 with MSE metric 45083.5910\n",
      "Epoch 3 batch 220 train Loss 2496.2625 test Loss 948.5948 with MSE metric 45077.8469\n",
      "Epoch 3 batch 230 train Loss 2471.7906 test Loss 939.0961 with MSE metric 45075.7935\n",
      "Epoch 3 batch 240 train Loss 2447.3417 test Loss 929.7878 with MSE metric 45074.1080\n",
      "Time taken for 1 epoch: 30.357666015625 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 2423.4229 test Loss 920.6635 with MSE metric 45070.6918\n",
      "Epoch 4 batch 10 train Loss 2399.9280 test Loss 911.7182 with MSE metric 45057.1591\n",
      "Epoch 4 batch 20 train Loss 2376.9605 test Loss 902.9480 with MSE metric 45060.1760\n",
      "Epoch 4 batch 30 train Loss 2354.4834 test Loss 894.3457 with MSE metric 45053.5382\n",
      "Epoch 4 batch 40 train Loss 2332.3808 test Loss 885.9081 with MSE metric 45056.1516\n",
      "Epoch 4 batch 50 train Loss 2310.8005 test Loss 877.6294 with MSE metric 45048.3208\n",
      "Epoch 4 batch 60 train Loss 2289.5329 test Loss 869.5059 with MSE metric 45050.3329\n",
      "Epoch 4 batch 70 train Loss 2268.5803 test Loss 861.5328 with MSE metric 45049.0500\n",
      "Epoch 4 batch 80 train Loss 2248.0512 test Loss 853.7055 with MSE metric 45050.0251\n",
      "Epoch 4 batch 90 train Loss 2228.1946 test Loss 846.0206 with MSE metric 45040.2955\n",
      "Epoch 4 batch 100 train Loss 2208.3677 test Loss 838.4739 with MSE metric 45038.7982\n",
      "Epoch 4 batch 110 train Loss 2189.0255 test Loss 831.0628 with MSE metric 45039.2967\n",
      "Epoch 4 batch 120 train Loss 2169.8542 test Loss 823.7827 with MSE metric 45040.0957\n",
      "Epoch 4 batch 130 train Loss 2151.1224 test Loss 816.6308 with MSE metric 45031.2315\n",
      "Epoch 4 batch 140 train Loss 2132.6633 test Loss 809.6035 with MSE metric 45028.1465\n",
      "Epoch 4 batch 150 train Loss 2114.6679 test Loss 802.6972 with MSE metric 45029.7922\n",
      "Epoch 4 batch 160 train Loss 2096.8272 test Loss 795.9090 with MSE metric 45009.7464\n",
      "Epoch 4 batch 170 train Loss 2079.2895 test Loss 789.2361 with MSE metric 45003.7349\n",
      "Epoch 4 batch 180 train Loss 2062.1005 test Loss 782.6755 with MSE metric 45006.3125\n",
      "Epoch 4 batch 190 train Loss 2045.0445 test Loss 776.2245 with MSE metric 45009.7786\n",
      "Epoch 4 batch 200 train Loss 2028.3304 test Loss 769.8799 with MSE metric 45013.1025\n",
      "Epoch 4 batch 210 train Loss 2012.2038 test Loss 763.6393 with MSE metric 45015.9939\n",
      "Epoch 4 batch 220 train Loss 1996.0434 test Loss 757.4997 with MSE metric 45015.8314\n",
      "Epoch 4 batch 230 train Loss 1980.1839 test Loss 751.4598 with MSE metric 45021.4358\n",
      "Epoch 4 batch 240 train Loss 1964.5745 test Loss 745.5168 with MSE metric 45019.8027\n",
      "Time taken for 1 epoch: 26.902130365371704 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 1949.3237 test Loss 739.6682 with MSE metric 45015.0101\n",
      "Epoch 5 batch 10 train Loss 1934.1405 test Loss 733.9112 with MSE metric 45005.8931\n",
      "Epoch 5 batch 20 train Loss 1919.2996 test Loss 728.2447 with MSE metric 44992.6769\n",
      "Epoch 5 batch 30 train Loss 1904.5790 test Loss 722.6661 with MSE metric 44985.9718\n",
      "Epoch 5 batch 40 train Loss 1890.1015 test Loss 717.1732 with MSE metric 44980.7272\n",
      "Epoch 5 batch 50 train Loss 1875.8885 test Loss 711.7641 with MSE metric 44976.6084\n",
      "Epoch 5 batch 60 train Loss 1861.7850 test Loss 706.4370 with MSE metric 44972.1892\n",
      "Epoch 5 batch 70 train Loss 1848.0188 test Loss 701.1902 with MSE metric 44966.2768\n",
      "Epoch 5 batch 80 train Loss 1834.3533 test Loss 696.0219 with MSE metric 44961.8813\n",
      "Epoch 5 batch 90 train Loss 1820.9810 test Loss 690.9305 with MSE metric 44958.3249\n",
      "Epoch 5 batch 100 train Loss 1807.7856 test Loss 685.9142 with MSE metric 44957.5026\n",
      "Epoch 5 batch 110 train Loss 1794.7100 test Loss 680.9711 with MSE metric 44954.3753\n",
      "Epoch 5 batch 120 train Loss 1781.8545 test Loss 676.0997 with MSE metric 44950.0262\n",
      "Epoch 5 batch 130 train Loss 1769.5992 test Loss 671.2974 with MSE metric 44953.6941\n",
      "Epoch 5 batch 140 train Loss 1757.0611 test Loss 666.5647 with MSE metric 44950.4413\n",
      "Epoch 5 batch 150 train Loss 1744.7342 test Loss 661.8990 with MSE metric 44945.1746\n",
      "Epoch 5 batch 160 train Loss 1732.5453 test Loss 657.2992 with MSE metric 44942.7267\n",
      "Epoch 5 batch 170 train Loss 1720.5524 test Loss 652.7641 with MSE metric 44939.7389\n",
      "Epoch 5 batch 180 train Loss 1708.8043 test Loss 648.2915 with MSE metric 44932.7510\n",
      "Epoch 5 batch 190 train Loss 1697.1367 test Loss 643.8807 with MSE metric 44930.0002\n",
      "Epoch 5 batch 200 train Loss 1685.6918 test Loss 639.5310 with MSE metric 44923.7291\n",
      "Epoch 5 batch 210 train Loss 1674.3916 test Loss 635.2398 with MSE metric 44923.1230\n",
      "Epoch 5 batch 220 train Loss 1663.1726 test Loss 631.0065 with MSE metric 44920.7822\n",
      "Epoch 5 batch 230 train Loss 1652.1218 test Loss 626.8302 with MSE metric 44918.4070\n",
      "Epoch 5 batch 240 train Loss 1641.2626 test Loss 622.7098 with MSE metric 44907.1517\n",
      "Time taken for 1 epoch: 26.943596124649048 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 1630.5116 test Loss 618.6443 with MSE metric 44908.3888\n",
      "Epoch 6 batch 10 train Loss 1619.9084 test Loss 614.6322 with MSE metric 44907.9875\n",
      "Epoch 6 batch 20 train Loss 1609.4370 test Loss 610.6725 with MSE metric 44904.9789\n",
      "Epoch 6 batch 30 train Loss 1599.1753 test Loss 606.7645 with MSE metric 44898.2220\n",
      "Epoch 6 batch 40 train Loss 1589.4027 test Loss 602.9062 with MSE metric 44898.4453\n",
      "Epoch 6 batch 50 train Loss 1579.8025 test Loss 599.0980 with MSE metric 44895.3265\n",
      "Epoch 6 batch 60 train Loss 1570.1713 test Loss 595.3385 with MSE metric 44895.1295\n",
      "Epoch 6 batch 70 train Loss 1560.3152 test Loss 591.6264 with MSE metric 44889.9835\n",
      "Epoch 6 batch 80 train Loss 1550.6231 test Loss 587.9608 with MSE metric 44877.7429\n",
      "Epoch 6 batch 90 train Loss 1540.9968 test Loss 584.3415 with MSE metric 44868.1489\n",
      "Epoch 6 batch 100 train Loss 1531.5152 test Loss 580.7673 with MSE metric 44863.2595\n",
      "Epoch 6 batch 110 train Loss 1522.2312 test Loss 577.2369 with MSE metric 44860.4943\n",
      "Epoch 6 batch 120 train Loss 1512.9760 test Loss 573.7503 with MSE metric 44855.4648\n",
      "Epoch 6 batch 130 train Loss 1503.8501 test Loss 570.3058 with MSE metric 44853.5367\n",
      "Epoch 6 batch 140 train Loss 1494.8289 test Loss 566.9034 with MSE metric 44852.7956\n",
      "Epoch 6 batch 150 train Loss 1485.9243 test Loss 563.5417 with MSE metric 44845.7155\n",
      "Epoch 6 batch 160 train Loss 1477.0828 test Loss 560.2209 with MSE metric 44849.0325\n",
      "Epoch 6 batch 170 train Loss 1468.3799 test Loss 556.9394 with MSE metric 44841.9069\n",
      "Epoch 6 batch 180 train Loss 1459.7514 test Loss 553.6971 with MSE metric 44841.4974\n",
      "Epoch 6 batch 190 train Loss 1451.2333 test Loss 550.4925 with MSE metric 44835.1646\n",
      "Epoch 6 batch 200 train Loss 1442.8199 test Loss 547.3256 with MSE metric 44837.8007\n",
      "Epoch 6 batch 210 train Loss 1434.5088 test Loss 544.1954 with MSE metric 44837.0209\n",
      "Epoch 6 batch 220 train Loss 1426.3004 test Loss 541.1014 with MSE metric 44837.4252\n",
      "Epoch 6 batch 230 train Loss 1418.2052 test Loss 538.0432 with MSE metric 44834.3945\n",
      "Epoch 6 batch 240 train Loss 1410.2322 test Loss 535.0199 with MSE metric 44837.7895\n",
      "Time taken for 1 epoch: 28.42843222618103 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 1402.3128 test Loss 532.0310 with MSE metric 44838.7347\n",
      "Epoch 7 batch 10 train Loss 1394.8938 test Loss 529.0759 with MSE metric 44837.8097\n",
      "Epoch 7 batch 20 train Loss 1387.1896 test Loss 526.1541 with MSE metric 44836.9873\n",
      "Epoch 7 batch 30 train Loss 1379.5406 test Loss 523.2653 with MSE metric 44838.1247\n",
      "Epoch 7 batch 40 train Loss 1371.9384 test Loss 520.4084 with MSE metric 44842.7618\n",
      "Epoch 7 batch 50 train Loss 1364.4487 test Loss 517.5830 with MSE metric 44842.7576\n",
      "Epoch 7 batch 60 train Loss 1357.0505 test Loss 514.7889 with MSE metric 44840.7958\n",
      "Epoch 7 batch 70 train Loss 1349.6931 test Loss 512.0254 with MSE metric 44841.6104\n",
      "Epoch 7 batch 80 train Loss 1342.4208 test Loss 509.2917 with MSE metric 44838.3105\n",
      "Epoch 7 batch 90 train Loss 1335.2635 test Loss 506.5879 with MSE metric 44837.1282\n",
      "Epoch 7 batch 100 train Loss 1328.2923 test Loss 503.9134 with MSE metric 44828.0234\n",
      "Epoch 7 batch 110 train Loss 1321.3661 test Loss 501.2670 with MSE metric 44822.9268\n",
      "Epoch 7 batch 120 train Loss 1314.4068 test Loss 498.6491 with MSE metric 44821.2390\n",
      "Epoch 7 batch 130 train Loss 1307.5278 test Loss 496.0590 with MSE metric 44819.3020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 batch 140 train Loss 1300.7095 test Loss 493.4961 with MSE metric 44818.2943\n",
      "Epoch 7 batch 150 train Loss 1294.0040 test Loss 490.9602 with MSE metric 44817.6849\n",
      "Epoch 7 batch 160 train Loss 1287.3511 test Loss 488.4506 with MSE metric 44813.3637\n",
      "Epoch 7 batch 170 train Loss 1280.7618 test Loss 485.9669 with MSE metric 44810.6646\n",
      "Epoch 7 batch 180 train Loss 1274.3010 test Loss 483.5091 with MSE metric 44811.2059\n",
      "Epoch 7 batch 190 train Loss 1267.8261 test Loss 481.0765 with MSE metric 44810.5494\n",
      "Epoch 7 batch 200 train Loss 1261.4127 test Loss 478.6689 with MSE metric 44809.8589\n",
      "Epoch 7 batch 210 train Loss 1255.0965 test Loss 476.2854 with MSE metric 44813.5021\n",
      "Epoch 7 batch 220 train Loss 1248.8292 test Loss 473.9266 with MSE metric 44812.7825\n",
      "Epoch 7 batch 230 train Loss 1242.6034 test Loss 471.5913 with MSE metric 44808.9969\n",
      "Epoch 7 batch 240 train Loss 1236.4422 test Loss 469.2792 with MSE metric 44808.6132\n",
      "Time taken for 1 epoch: 29.421195030212402 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 1230.3906 test Loss 466.9904 with MSE metric 44808.7149\n",
      "Epoch 8 batch 10 train Loss 1224.4285 test Loss 464.7240 with MSE metric 44806.7802\n",
      "Epoch 8 batch 20 train Loss 1218.6171 test Loss 462.4802 with MSE metric 44807.5621\n",
      "Epoch 8 batch 30 train Loss 1212.7419 test Loss 460.2585 with MSE metric 44804.1905\n",
      "Epoch 8 batch 40 train Loss 1206.9594 test Loss 458.0585 with MSE metric 44804.8762\n",
      "Epoch 8 batch 50 train Loss 1201.1744 test Loss 455.8799 with MSE metric 44804.1438\n",
      "Epoch 8 batch 60 train Loss 1195.4953 test Loss 453.7222 with MSE metric 44800.6782\n",
      "Epoch 8 batch 70 train Loss 1189.8997 test Loss 451.5857 with MSE metric 44800.6376\n",
      "Epoch 8 batch 80 train Loss 1184.2786 test Loss 449.4694 with MSE metric 44794.7038\n",
      "Epoch 8 batch 90 train Loss 1178.7679 test Loss 447.3732 with MSE metric 44790.2379\n",
      "Epoch 8 batch 100 train Loss 1173.2369 test Loss 445.2970 with MSE metric 44784.2181\n",
      "Epoch 8 batch 110 train Loss 1167.7877 test Loss 443.2406 with MSE metric 44779.5893\n",
      "Epoch 8 batch 120 train Loss 1162.3806 test Loss 441.2032 with MSE metric 44783.8443\n",
      "Epoch 8 batch 130 train Loss 1156.9951 test Loss 439.1849 with MSE metric 44780.4207\n",
      "Epoch 8 batch 140 train Loss 1151.6675 test Loss 437.1854 with MSE metric 44780.1581\n",
      "Epoch 8 batch 150 train Loss 1146.3919 test Loss 435.2046 with MSE metric 44781.6780\n",
      "Epoch 8 batch 160 train Loss 1141.1671 test Loss 433.2420 with MSE metric 44780.8104\n",
      "Epoch 8 batch 170 train Loss 1135.9960 test Loss 431.2975 with MSE metric 44782.0080\n",
      "Epoch 8 batch 180 train Loss 1130.8632 test Loss 429.3708 with MSE metric 44782.8237\n",
      "Epoch 8 batch 190 train Loss 1125.7662 test Loss 427.4617 with MSE metric 44784.3824\n",
      "Epoch 8 batch 200 train Loss 1120.7611 test Loss 425.5697 with MSE metric 44780.5057\n",
      "Epoch 8 batch 210 train Loss 1115.8185 test Loss 423.6950 with MSE metric 44778.1294\n",
      "Epoch 8 batch 220 train Loss 1110.8698 test Loss 421.8371 with MSE metric 44776.9149\n",
      "Epoch 8 batch 230 train Loss 1105.9861 test Loss 419.9959 with MSE metric 44780.6554\n",
      "Epoch 8 batch 240 train Loss 1101.1204 test Loss 418.1710 with MSE metric 44781.0386\n",
      "Time taken for 1 epoch: 28.709784269332886 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 1096.3074 test Loss 416.3621 with MSE metric 44782.9400\n",
      "Epoch 9 batch 10 train Loss 1091.5239 test Loss 414.5694 with MSE metric 44785.1685\n",
      "Epoch 9 batch 20 train Loss 1086.7956 test Loss 412.7924 with MSE metric 44780.8820\n",
      "Epoch 9 batch 30 train Loss 1082.1020 test Loss 411.0307 with MSE metric 44778.6129\n",
      "Epoch 9 batch 40 train Loss 1077.4411 test Loss 409.2844 with MSE metric 44776.2438\n",
      "Epoch 9 batch 50 train Loss 1072.8193 test Loss 407.5533 with MSE metric 44781.0837\n",
      "Epoch 9 batch 60 train Loss 1068.2353 test Loss 405.8372 with MSE metric 44782.0508\n",
      "Epoch 9 batch 70 train Loss 1063.6886 test Loss 404.1360 with MSE metric 44780.8341\n",
      "Epoch 9 batch 80 train Loss 1059.1905 test Loss 402.4494 with MSE metric 44781.4039\n",
      "Epoch 9 batch 90 train Loss 1054.7699 test Loss 400.7771 with MSE metric 44787.7362\n",
      "Epoch 9 batch 100 train Loss 1050.3931 test Loss 399.1191 with MSE metric 44785.3247\n",
      "Epoch 9 batch 110 train Loss 1046.0192 test Loss 397.4750 with MSE metric 44784.5071\n",
      "Epoch 9 batch 120 train Loss 1041.6685 test Loss 395.8446 with MSE metric 44786.4044\n",
      "Epoch 9 batch 130 train Loss 1037.3602 test Loss 394.2278 with MSE metric 44781.6735\n",
      "Epoch 9 batch 140 train Loss 1033.0749 test Loss 392.6245 with MSE metric 44776.6956\n",
      "Epoch 9 batch 150 train Loss 1028.8282 test Loss 391.0346 with MSE metric 44776.2279\n",
      "Epoch 9 batch 160 train Loss 1024.6156 test Loss 389.4578 with MSE metric 44767.4653\n",
      "Epoch 9 batch 170 train Loss 1020.4697 test Loss 387.8941 with MSE metric 44768.6716\n",
      "Epoch 9 batch 180 train Loss 1016.3315 test Loss 386.3434 with MSE metric 44766.8421\n",
      "Epoch 9 batch 190 train Loss 1012.2648 test Loss 384.8051 with MSE metric 44765.5127\n",
      "Epoch 9 batch 200 train Loss 1008.1914 test Loss 383.2797 with MSE metric 44766.9645\n",
      "Epoch 9 batch 210 train Loss 1004.1462 test Loss 381.7665 with MSE metric 44762.0403\n",
      "Epoch 9 batch 220 train Loss 1000.1362 test Loss 380.2652 with MSE metric 44756.9332\n",
      "Epoch 9 batch 230 train Loss 996.1593 test Loss 378.7764 with MSE metric 44755.8938\n",
      "Epoch 9 batch 240 train Loss 992.2195 test Loss 377.2995 with MSE metric 44757.9395\n",
      "Time taken for 1 epoch: 26.26639413833618 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 988.3106 test Loss 375.8343 with MSE metric 44752.6474\n",
      "Epoch 10 batch 10 train Loss 984.4457 test Loss 374.3808 with MSE metric 44752.1910\n",
      "Epoch 10 batch 20 train Loss 980.5911 test Loss 372.9387 with MSE metric 44749.4991\n",
      "Epoch 10 batch 30 train Loss 976.7615 test Loss 371.5080 with MSE metric 44745.4841\n",
      "Epoch 10 batch 40 train Loss 972.9814 test Loss 370.0887 with MSE metric 44743.7243\n",
      "Epoch 10 batch 50 train Loss 969.2190 test Loss 368.6804 with MSE metric 44738.4643\n",
      "Epoch 10 batch 60 train Loss 965.4848 test Loss 367.2831 with MSE metric 44735.0319\n",
      "Epoch 10 batch 70 train Loss 961.7912 test Loss 365.8969 with MSE metric 44737.4210\n",
      "Epoch 10 batch 80 train Loss 958.1134 test Loss 364.5213 with MSE metric 44735.4580\n",
      "Epoch 10 batch 90 train Loss 954.4746 test Loss 363.1560 with MSE metric 44734.0153\n",
      "Epoch 10 batch 100 train Loss 951.0032 test Loss 361.8011 with MSE metric 44734.3537\n",
      "Epoch 10 batch 110 train Loss 947.4074 test Loss 360.4568 with MSE metric 44735.1766\n",
      "Epoch 10 batch 120 train Loss 943.8441 test Loss 359.1229 with MSE metric 44730.9161\n",
      "Epoch 10 batch 130 train Loss 940.3111 test Loss 357.7991 with MSE metric 44727.8943\n",
      "Epoch 10 batch 140 train Loss 936.8113 test Loss 356.4852 with MSE metric 44727.6135\n",
      "Epoch 10 batch 150 train Loss 933.3255 test Loss 355.1812 with MSE metric 44725.6370\n",
      "Epoch 10 batch 160 train Loss 929.8681 test Loss 353.8870 with MSE metric 44724.3893\n",
      "Epoch 10 batch 170 train Loss 926.4301 test Loss 352.6024 with MSE metric 44724.1716\n",
      "Epoch 10 batch 180 train Loss 923.0297 test Loss 351.3273 with MSE metric 44721.5506\n",
      "Epoch 10 batch 190 train Loss 919.7079 test Loss 350.0621 with MSE metric 44718.6745\n",
      "Epoch 10 batch 200 train Loss 916.3466 test Loss 348.8059 with MSE metric 44717.5225\n",
      "Epoch 10 batch 210 train Loss 913.0072 test Loss 347.5592 with MSE metric 44715.3538\n",
      "Epoch 10 batch 220 train Loss 909.6953 test Loss 346.3217 with MSE metric 44713.4314\n",
      "Epoch 10 batch 230 train Loss 906.4107 test Loss 345.0931 with MSE metric 44709.1637\n",
      "Epoch 10 batch 240 train Loss 903.1823 test Loss 343.8733 with MSE metric 44708.9765\n",
      "Time taken for 1 epoch: 28.26937174797058 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 899.9468 test Loss 342.6624 with MSE metric 44709.2745\n",
      "Epoch 11 batch 10 train Loss 896.7564 test Loss 341.4603 with MSE metric 44710.2116\n",
      "Epoch 11 batch 20 train Loss 893.5612 test Loss 340.2668 with MSE metric 44711.2036\n",
      "Epoch 11 batch 30 train Loss 890.3962 test Loss 339.0820 with MSE metric 44711.3188\n",
      "Epoch 11 batch 40 train Loss 887.2443 test Loss 337.9054 with MSE metric 44713.8833\n",
      "Epoch 11 batch 50 train Loss 884.1199 test Loss 336.7374 with MSE metric 44711.1275\n",
      "Epoch 11 batch 60 train Loss 881.0252 test Loss 335.5779 with MSE metric 44708.5708\n",
      "Epoch 11 batch 70 train Loss 877.9422 test Loss 334.4263 with MSE metric 44707.4125\n",
      "Epoch 11 batch 80 train Loss 874.8821 test Loss 333.2829 with MSE metric 44706.8448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 batch 90 train Loss 871.8436 test Loss 332.1477 with MSE metric 44707.9802\n",
      "Epoch 11 batch 100 train Loss 868.8273 test Loss 331.0205 with MSE metric 44706.4711\n",
      "Epoch 11 batch 110 train Loss 865.8288 test Loss 329.9011 with MSE metric 44702.7393\n",
      "Epoch 11 batch 120 train Loss 862.8777 test Loss 328.7893 with MSE metric 44698.5043\n",
      "Epoch 11 batch 130 train Loss 859.9492 test Loss 327.6854 with MSE metric 44698.4619\n",
      "Epoch 11 batch 140 train Loss 857.0172 test Loss 326.5892 with MSE metric 44696.4532\n",
      "Epoch 11 batch 150 train Loss 854.1239 test Loss 325.5003 with MSE metric 44692.4952\n",
      "Epoch 11 batch 160 train Loss 851.2339 test Loss 324.4191 with MSE metric 44690.6223\n",
      "Epoch 11 batch 170 train Loss 848.3575 test Loss 323.3452 with MSE metric 44689.6701\n",
      "Epoch 11 batch 180 train Loss 845.5189 test Loss 322.2788 with MSE metric 44688.6152\n",
      "Epoch 11 batch 190 train Loss 842.7060 test Loss 321.2197 with MSE metric 44688.2442\n",
      "Epoch 11 batch 200 train Loss 839.8990 test Loss 320.1676 with MSE metric 44689.1026\n",
      "Epoch 11 batch 210 train Loss 837.1020 test Loss 319.1224 with MSE metric 44685.8370\n",
      "Epoch 11 batch 220 train Loss 834.3277 test Loss 318.0846 with MSE metric 44685.0183\n",
      "Epoch 11 batch 230 train Loss 831.5774 test Loss 317.0537 with MSE metric 44688.6831\n",
      "Epoch 11 batch 240 train Loss 828.8410 test Loss 316.0297 with MSE metric 44687.6819\n",
      "Time taken for 1 epoch: 27.862637758255005 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 826.1139 test Loss 315.0125 with MSE metric 44686.7473\n",
      "Epoch 12 batch 10 train Loss 823.4161 test Loss 314.0018 with MSE metric 44686.8018\n",
      "Epoch 12 batch 20 train Loss 820.7248 test Loss 312.9979 with MSE metric 44684.6282\n",
      "Epoch 12 batch 30 train Loss 818.0516 test Loss 312.0006 with MSE metric 44682.8346\n",
      "Epoch 12 batch 40 train Loss 815.3965 test Loss 311.0100 with MSE metric 44678.7482\n",
      "Epoch 12 batch 50 train Loss 812.7663 test Loss 310.0258 with MSE metric 44680.0629\n",
      "Epoch 12 batch 60 train Loss 810.1732 test Loss 309.0481 with MSE metric 44682.5152\n",
      "Epoch 12 batch 70 train Loss 807.5722 test Loss 308.0768 with MSE metric 44683.3552\n",
      "Epoch 12 batch 80 train Loss 804.9882 test Loss 307.1117 with MSE metric 44681.0805\n",
      "Epoch 12 batch 90 train Loss 802.4322 test Loss 306.1529 with MSE metric 44679.0264\n",
      "Epoch 12 batch 100 train Loss 799.8885 test Loss 305.2003 with MSE metric 44684.1487\n",
      "Epoch 12 batch 110 train Loss 797.3700 test Loss 304.2537 with MSE metric 44682.2568\n",
      "Epoch 12 batch 120 train Loss 794.8503 test Loss 303.3133 with MSE metric 44679.5334\n",
      "Epoch 12 batch 130 train Loss 792.3456 test Loss 302.3790 with MSE metric 44675.8627\n",
      "Epoch 12 batch 140 train Loss 789.8730 test Loss 301.4506 with MSE metric 44672.1712\n",
      "Epoch 12 batch 150 train Loss 787.4012 test Loss 300.5279 with MSE metric 44672.5554\n",
      "Epoch 12 batch 160 train Loss 784.9648 test Loss 299.6110 with MSE metric 44675.9904\n",
      "Epoch 12 batch 170 train Loss 782.5254 test Loss 298.7000 with MSE metric 44681.6546\n",
      "Epoch 12 batch 180 train Loss 780.1048 test Loss 297.7949 with MSE metric 44680.5267\n",
      "Epoch 12 batch 190 train Loss 777.6986 test Loss 296.8954 with MSE metric 44677.7837\n",
      "Epoch 12 batch 200 train Loss 775.3045 test Loss 296.0015 with MSE metric 44678.0041\n",
      "Epoch 12 batch 210 train Loss 772.9262 test Loss 295.1132 with MSE metric 44675.5806\n",
      "Epoch 12 batch 220 train Loss 770.5724 test Loss 294.2304 with MSE metric 44677.7565\n",
      "Epoch 12 batch 230 train Loss 768.2180 test Loss 293.3529 with MSE metric 44679.8394\n",
      "Epoch 12 batch 240 train Loss 765.8847 test Loss 292.4810 with MSE metric 44678.7118\n",
      "Time taken for 1 epoch: 28.55362367630005 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 763.5676 test Loss 291.6143 with MSE metric 44672.9973\n",
      "Epoch 13 batch 10 train Loss 761.3746 test Loss 290.7530 with MSE metric 44671.4468\n",
      "Epoch 13 batch 20 train Loss 759.0766 test Loss 289.8969 with MSE metric 44671.0782\n",
      "Epoch 13 batch 30 train Loss 756.7954 test Loss 289.0460 with MSE metric 44669.3588\n",
      "Epoch 13 batch 40 train Loss 754.5238 test Loss 288.2005 with MSE metric 44671.2155\n",
      "Epoch 13 batch 50 train Loss 752.2770 test Loss 287.3601 with MSE metric 44674.3512\n",
      "Epoch 13 batch 60 train Loss 750.0392 test Loss 286.5248 with MSE metric 44676.4519\n",
      "Epoch 13 batch 70 train Loss 747.8237 test Loss 285.6944 with MSE metric 44676.9571\n",
      "Epoch 13 batch 80 train Loss 745.6089 test Loss 284.8692 with MSE metric 44676.2174\n",
      "Epoch 13 batch 90 train Loss 743.4107 test Loss 284.0488 with MSE metric 44676.2114\n",
      "Epoch 13 batch 100 train Loss 741.2214 test Loss 283.2334 with MSE metric 44674.6699\n",
      "Epoch 13 batch 110 train Loss 739.0465 test Loss 282.4228 with MSE metric 44671.0623\n",
      "Epoch 13 batch 120 train Loss 736.8860 test Loss 281.6169 with MSE metric 44669.2436\n",
      "Epoch 13 batch 130 train Loss 734.7384 test Loss 280.8158 with MSE metric 44667.1430\n",
      "Epoch 13 batch 140 train Loss 732.6059 test Loss 280.0193 with MSE metric 44662.0977\n",
      "Epoch 13 batch 150 train Loss 730.4834 test Loss 279.2276 with MSE metric 44665.6378\n",
      "Epoch 13 batch 160 train Loss 728.3706 test Loss 278.4405 with MSE metric 44665.4503\n",
      "Epoch 13 batch 170 train Loss 726.3468 test Loss 277.6581 with MSE metric 44665.1478\n",
      "Epoch 13 batch 180 train Loss 724.2588 test Loss 276.8801 with MSE metric 44661.0026\n",
      "Epoch 13 batch 190 train Loss 722.1830 test Loss 276.1065 with MSE metric 44664.2451\n",
      "Epoch 13 batch 200 train Loss 720.1283 test Loss 275.3376 with MSE metric 44665.0625\n",
      "Epoch 13 batch 210 train Loss 718.0773 test Loss 274.5731 with MSE metric 44665.2590\n",
      "Epoch 13 batch 220 train Loss 716.0477 test Loss 273.8132 with MSE metric 44666.0939\n",
      "Epoch 13 batch 230 train Loss 714.0193 test Loss 273.0575 with MSE metric 44669.2406\n",
      "Epoch 13 batch 240 train Loss 712.0108 test Loss 272.3061 with MSE metric 44670.3363\n",
      "Time taken for 1 epoch: 25.893537998199463 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 710.0040 test Loss 271.5590 with MSE metric 44667.5588\n",
      "Epoch 14 batch 10 train Loss 708.0099 test Loss 270.8161 with MSE metric 44664.6253\n",
      "Epoch 14 batch 20 train Loss 706.0275 test Loss 270.0775 with MSE metric 44659.6701\n",
      "Epoch 14 batch 30 train Loss 704.0543 test Loss 269.3430 with MSE metric 44658.5434\n",
      "Epoch 14 batch 40 train Loss 702.0951 test Loss 268.6127 with MSE metric 44659.1883\n",
      "Epoch 14 batch 50 train Loss 700.1510 test Loss 267.8866 with MSE metric 44656.3263\n",
      "Epoch 14 batch 60 train Loss 698.2208 test Loss 267.1644 with MSE metric 44656.3787\n",
      "Epoch 14 batch 70 train Loss 696.2933 test Loss 266.4465 with MSE metric 44655.9660\n",
      "Epoch 14 batch 80 train Loss 694.3830 test Loss 265.7326 with MSE metric 44656.6759\n",
      "Epoch 14 batch 90 train Loss 692.4755 test Loss 265.0223 with MSE metric 44658.0571\n",
      "Epoch 14 batch 100 train Loss 690.5795 test Loss 264.3162 with MSE metric 44656.7533\n",
      "Epoch 14 batch 110 train Loss 688.6938 test Loss 263.6140 with MSE metric 44659.1272\n",
      "Epoch 14 batch 120 train Loss 686.8213 test Loss 262.9157 with MSE metric 44659.9702\n",
      "Epoch 14 batch 130 train Loss 684.9554 test Loss 262.2213 with MSE metric 44656.1318\n",
      "Epoch 14 batch 140 train Loss 683.1003 test Loss 261.5305 with MSE metric 44652.9989\n",
      "Epoch 14 batch 150 train Loss 681.2574 test Loss 260.8438 with MSE metric 44651.5726\n",
      "Epoch 14 batch 160 train Loss 679.4220 test Loss 260.1608 with MSE metric 44650.6180\n",
      "Epoch 14 batch 170 train Loss 677.6114 test Loss 259.4814 with MSE metric 44650.9449\n",
      "Epoch 14 batch 180 train Loss 675.8012 test Loss 258.8057 with MSE metric 44646.6062\n",
      "Epoch 14 batch 190 train Loss 673.9959 test Loss 258.1337 with MSE metric 44644.9843\n",
      "Epoch 14 batch 200 train Loss 672.2022 test Loss 257.4655 with MSE metric 44643.9647\n",
      "Epoch 14 batch 210 train Loss 670.4183 test Loss 256.8008 with MSE metric 44645.5900\n",
      "Epoch 14 batch 220 train Loss 668.6418 test Loss 256.1397 with MSE metric 44644.3464\n",
      "Epoch 14 batch 230 train Loss 666.8784 test Loss 255.4822 with MSE metric 44644.5990\n",
      "Epoch 14 batch 240 train Loss 665.1224 test Loss 254.8280 with MSE metric 44645.2015\n",
      "Time taken for 1 epoch: 28.45941400527954 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 663.3777 test Loss 254.1773 with MSE metric 44646.0120\n",
      "Epoch 15 batch 10 train Loss 661.6410 test Loss 253.5301 with MSE metric 44645.7942\n",
      "Epoch 15 batch 20 train Loss 659.9102 test Loss 252.8864 with MSE metric 44646.8515\n",
      "Epoch 15 batch 30 train Loss 658.2015 test Loss 252.2461 with MSE metric 44648.8572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 batch 40 train Loss 656.4897 test Loss 251.6091 with MSE metric 44649.6013\n",
      "Epoch 15 batch 50 train Loss 654.7873 test Loss 250.9754 with MSE metric 44650.0012\n",
      "Epoch 15 batch 60 train Loss 653.1027 test Loss 250.3452 with MSE metric 44650.0421\n",
      "Epoch 15 batch 70 train Loss 651.4181 test Loss 249.7183 with MSE metric 44649.1121\n",
      "Epoch 15 batch 80 train Loss 649.7408 test Loss 249.0947 with MSE metric 44647.7067\n",
      "Epoch 15 batch 90 train Loss 648.0725 test Loss 248.4744 with MSE metric 44646.1725\n",
      "Epoch 15 batch 100 train Loss 646.4165 test Loss 247.8572 with MSE metric 44645.2851\n",
      "Epoch 15 batch 110 train Loss 644.7653 test Loss 247.2429 with MSE metric 44644.8591\n",
      "Epoch 15 batch 120 train Loss 643.1229 test Loss 246.6321 with MSE metric 44643.1733\n",
      "Epoch 15 batch 130 train Loss 641.4899 test Loss 246.0245 with MSE metric 44641.2036\n",
      "Epoch 15 batch 140 train Loss 639.8646 test Loss 245.4198 with MSE metric 44639.2454\n",
      "Epoch 15 batch 150 train Loss 638.2462 test Loss 244.8183 with MSE metric 44637.1294\n",
      "Epoch 15 batch 160 train Loss 636.6366 test Loss 244.2200 with MSE metric 44636.2056\n",
      "Epoch 15 batch 170 train Loss 635.0384 test Loss 243.6246 with MSE metric 44633.5415\n",
      "Epoch 15 batch 180 train Loss 633.4459 test Loss 243.0323 with MSE metric 44634.9891\n",
      "Epoch 15 batch 190 train Loss 631.8672 test Loss 242.4429 with MSE metric 44632.7174\n",
      "Epoch 15 batch 200 train Loss 630.2952 test Loss 241.8566 with MSE metric 44630.0086\n",
      "Epoch 15 batch 210 train Loss 628.7249 test Loss 241.2731 with MSE metric 44627.8200\n",
      "Epoch 15 batch 220 train Loss 627.1637 test Loss 240.6927 with MSE metric 44626.1303\n",
      "Epoch 15 batch 230 train Loss 625.6177 test Loss 240.1152 with MSE metric 44621.7435\n",
      "Epoch 15 batch 240 train Loss 624.0721 test Loss 239.5405 with MSE metric 44619.6470\n",
      "Time taken for 1 epoch: 28.90988326072693 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 622.5375 test Loss 238.9688 with MSE metric 44619.5518\n",
      "Epoch 16 batch 10 train Loss 621.0085 test Loss 238.3998 with MSE metric 44616.2723\n",
      "Epoch 16 batch 20 train Loss 619.4871 test Loss 237.8339 with MSE metric 44612.1733\n",
      "Epoch 16 batch 30 train Loss 617.9728 test Loss 237.2707 with MSE metric 44613.5428\n",
      "Epoch 16 batch 40 train Loss 616.4645 test Loss 236.7103 with MSE metric 44615.1343\n",
      "Epoch 16 batch 50 train Loss 614.9680 test Loss 236.1526 with MSE metric 44615.7748\n",
      "Epoch 16 batch 60 train Loss 613.4840 test Loss 235.5977 with MSE metric 44617.3193\n",
      "Epoch 16 batch 70 train Loss 611.9978 test Loss 235.0456 with MSE metric 44615.2659\n",
      "Epoch 16 batch 80 train Loss 610.5251 test Loss 234.4961 with MSE metric 44617.2815\n",
      "Epoch 16 batch 90 train Loss 609.0752 test Loss 233.9494 with MSE metric 44619.3489\n",
      "Epoch 16 batch 100 train Loss 607.6105 test Loss 233.4053 with MSE metric 44616.7754\n",
      "Epoch 16 batch 110 train Loss 606.1545 test Loss 232.8640 with MSE metric 44617.7027\n",
      "Epoch 16 batch 120 train Loss 604.7073 test Loss 232.3252 with MSE metric 44618.3692\n",
      "Epoch 16 batch 130 train Loss 603.2639 test Loss 231.7892 with MSE metric 44620.2422\n",
      "Epoch 16 batch 140 train Loss 601.8282 test Loss 231.2557 with MSE metric 44621.7492\n",
      "Epoch 16 batch 150 train Loss 600.3996 test Loss 230.7247 with MSE metric 44622.8676\n",
      "Epoch 16 batch 160 train Loss 598.9808 test Loss 230.1964 with MSE metric 44621.7593\n",
      "Epoch 16 batch 170 train Loss 597.5665 test Loss 229.6704 with MSE metric 44623.5806\n",
      "Epoch 16 batch 180 train Loss 596.1589 test Loss 229.1471 with MSE metric 44622.9014\n",
      "Epoch 16 batch 190 train Loss 594.7965 test Loss 228.6263 with MSE metric 44623.5280\n",
      "Epoch 16 batch 200 train Loss 593.4124 test Loss 228.1079 with MSE metric 44621.6565\n",
      "Epoch 16 batch 210 train Loss 592.0234 test Loss 227.5920 with MSE metric 44622.1929\n",
      "Epoch 16 batch 220 train Loss 590.6417 test Loss 227.0783 with MSE metric 44622.4778\n",
      "Epoch 16 batch 230 train Loss 589.2674 test Loss 226.5673 with MSE metric 44622.3663\n",
      "Epoch 16 batch 240 train Loss 587.9002 test Loss 226.0586 with MSE metric 44621.4502\n",
      "Time taken for 1 epoch: 28.559781074523926 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 586.5374 test Loss 225.5524 with MSE metric 44619.4639\n",
      "Epoch 17 batch 10 train Loss 585.1842 test Loss 225.0485 with MSE metric 44618.4106\n",
      "Epoch 17 batch 20 train Loss 583.8353 test Loss 224.5471 with MSE metric 44619.4588\n",
      "Epoch 17 batch 30 train Loss 582.4959 test Loss 224.0481 with MSE metric 44618.4556\n",
      "Epoch 17 batch 40 train Loss 581.1578 test Loss 223.5512 with MSE metric 44617.4340\n",
      "Epoch 17 batch 50 train Loss 579.8283 test Loss 223.0567 with MSE metric 44616.7769\n",
      "Epoch 17 batch 60 train Loss 578.5057 test Loss 222.5645 with MSE metric 44616.3809\n",
      "Epoch 17 batch 70 train Loss 577.1856 test Loss 222.0746 with MSE metric 44616.3480\n",
      "Epoch 17 batch 80 train Loss 575.8727 test Loss 221.5867 with MSE metric 44614.7490\n",
      "Epoch 17 batch 90 train Loss 574.5689 test Loss 221.1013 with MSE metric 44618.5417\n",
      "Epoch 17 batch 100 train Loss 573.2682 test Loss 220.6182 with MSE metric 44617.8802\n",
      "Epoch 17 batch 110 train Loss 571.9757 test Loss 220.1373 with MSE metric 44617.5010\n",
      "Epoch 17 batch 120 train Loss 570.6888 test Loss 219.6584 with MSE metric 44616.9643\n",
      "Epoch 17 batch 130 train Loss 569.4081 test Loss 219.1819 with MSE metric 44616.1957\n",
      "Epoch 17 batch 140 train Loss 568.1315 test Loss 218.7074 with MSE metric 44614.6499\n",
      "Epoch 17 batch 150 train Loss 566.8612 test Loss 218.2352 with MSE metric 44612.1384\n",
      "Epoch 17 batch 160 train Loss 565.5968 test Loss 217.7651 with MSE metric 44609.1376\n",
      "Epoch 17 batch 170 train Loss 564.3373 test Loss 217.2971 with MSE metric 44609.7966\n",
      "Epoch 17 batch 180 train Loss 563.0825 test Loss 216.8313 with MSE metric 44610.3200\n",
      "Epoch 17 batch 190 train Loss 561.8392 test Loss 216.3677 with MSE metric 44609.1782\n",
      "Epoch 17 batch 200 train Loss 560.5953 test Loss 215.9061 with MSE metric 44609.8612\n",
      "Epoch 17 batch 210 train Loss 559.3602 test Loss 215.4465 with MSE metric 44609.8614\n",
      "Epoch 17 batch 220 train Loss 558.1292 test Loss 214.9890 with MSE metric 44608.8580\n",
      "Epoch 17 batch 230 train Loss 556.9032 test Loss 214.5336 with MSE metric 44610.3394\n",
      "Epoch 17 batch 240 train Loss 555.6828 test Loss 214.0800 with MSE metric 44611.4293\n",
      "Time taken for 1 epoch: 29.99758791923523 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 554.4664 test Loss 213.6286 with MSE metric 44611.7451\n",
      "Epoch 18 batch 10 train Loss 553.2573 test Loss 213.1791 with MSE metric 44610.5758\n",
      "Epoch 18 batch 20 train Loss 552.0523 test Loss 212.7318 with MSE metric 44608.1104\n",
      "Epoch 18 batch 30 train Loss 550.8527 test Loss 212.2865 with MSE metric 44605.2050\n",
      "Epoch 18 batch 40 train Loss 549.6578 test Loss 211.8430 with MSE metric 44601.0230\n",
      "Epoch 18 batch 50 train Loss 548.4685 test Loss 211.4016 with MSE metric 44599.6690\n",
      "Epoch 18 batch 60 train Loss 547.2831 test Loss 210.9620 with MSE metric 44600.2956\n",
      "Epoch 18 batch 70 train Loss 546.1045 test Loss 210.5242 with MSE metric 44598.7428\n",
      "Epoch 18 batch 80 train Loss 544.9328 test Loss 210.0884 with MSE metric 44598.1984\n",
      "Epoch 18 batch 90 train Loss 543.7638 test Loss 209.6547 with MSE metric 44597.9782\n",
      "Epoch 18 batch 100 train Loss 542.5996 test Loss 209.2227 with MSE metric 44596.8771\n",
      "Epoch 18 batch 110 train Loss 541.4404 test Loss 208.7927 with MSE metric 44594.8061\n",
      "Epoch 18 batch 120 train Loss 540.2863 test Loss 208.3646 with MSE metric 44591.9276\n",
      "Epoch 18 batch 130 train Loss 539.1372 test Loss 207.9383 with MSE metric 44591.9650\n",
      "Epoch 18 batch 140 train Loss 537.9936 test Loss 207.5138 with MSE metric 44591.0258\n",
      "Epoch 18 batch 150 train Loss 536.8629 test Loss 207.0911 with MSE metric 44591.6491\n",
      "Epoch 18 batch 160 train Loss 535.7331 test Loss 206.6703 with MSE metric 44590.9638\n",
      "Epoch 18 batch 170 train Loss 534.6034 test Loss 206.2512 with MSE metric 44589.4598\n",
      "Epoch 18 batch 180 train Loss 533.4796 test Loss 205.8339 with MSE metric 44588.8982\n",
      "Epoch 18 batch 190 train Loss 532.3612 test Loss 205.4184 with MSE metric 44590.7852\n",
      "Epoch 18 batch 200 train Loss 531.2461 test Loss 205.0045 with MSE metric 44591.5490\n",
      "Epoch 18 batch 210 train Loss 530.1370 test Loss 204.5927 with MSE metric 44589.9326\n",
      "Epoch 18 batch 220 train Loss 529.0327 test Loss 204.1825 with MSE metric 44587.4440\n",
      "Epoch 18 batch 230 train Loss 527.9343 test Loss 203.7739 with MSE metric 44586.7431\n",
      "Epoch 18 batch 240 train Loss 526.8394 test Loss 203.3671 with MSE metric 44585.4931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 29.84224796295166 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 525.7473 test Loss 202.9620 with MSE metric 44585.1595\n",
      "Epoch 19 batch 10 train Loss 524.6610 test Loss 202.5586 with MSE metric 44583.3717\n",
      "Epoch 19 batch 20 train Loss 523.5810 test Loss 202.1569 with MSE metric 44583.4698\n",
      "Epoch 19 batch 30 train Loss 522.5030 test Loss 201.7569 with MSE metric 44582.7458\n",
      "Epoch 19 batch 40 train Loss 521.4332 test Loss 201.3587 with MSE metric 44581.1580\n",
      "Epoch 19 batch 50 train Loss 520.3638 test Loss 200.9622 with MSE metric 44579.6533\n",
      "Epoch 19 batch 60 train Loss 519.2989 test Loss 200.5672 with MSE metric 44577.5454\n",
      "Epoch 19 batch 70 train Loss 518.2389 test Loss 200.1739 with MSE metric 44574.7417\n",
      "Epoch 19 batch 80 train Loss 517.1837 test Loss 199.7822 with MSE metric 44573.0875\n",
      "Epoch 19 batch 90 train Loss 516.1344 test Loss 199.3921 with MSE metric 44571.1268\n",
      "Epoch 19 batch 100 train Loss 515.0874 test Loss 199.0036 with MSE metric 44570.0729\n",
      "Epoch 19 batch 110 train Loss 514.0465 test Loss 198.6167 with MSE metric 44570.3090\n",
      "Epoch 19 batch 120 train Loss 513.0080 test Loss 198.2315 with MSE metric 44567.8395\n",
      "Epoch 19 batch 130 train Loss 511.9804 test Loss 197.8478 with MSE metric 44565.7494\n",
      "Epoch 19 batch 140 train Loss 510.9505 test Loss 197.4656 with MSE metric 44564.7961\n",
      "Epoch 19 batch 150 train Loss 509.9253 test Loss 197.0852 with MSE metric 44566.4279\n",
      "Epoch 19 batch 160 train Loss 508.9041 test Loss 196.7063 with MSE metric 44564.1533\n",
      "Epoch 19 batch 170 train Loss 507.8868 test Loss 196.3288 with MSE metric 44562.3490\n",
      "Epoch 19 batch 180 train Loss 506.8751 test Loss 195.9528 with MSE metric 44562.4334\n",
      "Epoch 19 batch 190 train Loss 505.8698 test Loss 195.5784 with MSE metric 44564.2516\n",
      "Epoch 19 batch 200 train Loss 504.8644 test Loss 195.2056 with MSE metric 44563.3392\n",
      "Epoch 19 batch 210 train Loss 503.8771 test Loss 194.8343 with MSE metric 44563.3639\n",
      "Epoch 19 batch 220 train Loss 502.8801 test Loss 194.4645 with MSE metric 44562.4938\n",
      "Epoch 19 batch 230 train Loss 501.8873 test Loss 194.0961 with MSE metric 44560.2809\n",
      "Epoch 19 batch 240 train Loss 500.8983 test Loss 193.7293 with MSE metric 44559.0701\n",
      "Time taken for 1 epoch: 30.568185091018677 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 499.9135 test Loss 193.3640 with MSE metric 44559.5486\n",
      "Epoch 20 batch 10 train Loss 498.9328 test Loss 193.0001 with MSE metric 44559.9785\n",
      "Epoch 20 batch 20 train Loss 497.9550 test Loss 192.6377 with MSE metric 44560.1646\n",
      "Epoch 20 batch 30 train Loss 496.9813 test Loss 192.2765 with MSE metric 44557.6269\n",
      "Epoch 20 batch 40 train Loss 496.0113 test Loss 191.9170 with MSE metric 44555.8084\n",
      "Epoch 20 batch 50 train Loss 495.0456 test Loss 191.5588 with MSE metric 44556.9033\n",
      "Epoch 20 batch 60 train Loss 494.0829 test Loss 191.2020 with MSE metric 44556.1448\n",
      "Epoch 20 batch 70 train Loss 493.1243 test Loss 190.8467 with MSE metric 44554.2765\n",
      "Epoch 20 batch 80 train Loss 492.1821 test Loss 190.4927 with MSE metric 44553.2407\n",
      "Epoch 20 batch 90 train Loss 491.2308 test Loss 190.1401 with MSE metric 44552.6936\n",
      "Epoch 20 batch 100 train Loss 490.2838 test Loss 189.7890 with MSE metric 44552.7122\n",
      "Epoch 20 batch 110 train Loss 489.3421 test Loss 189.4391 with MSE metric 44552.6955\n",
      "Epoch 20 batch 120 train Loss 488.4034 test Loss 189.0906 with MSE metric 44551.7263\n",
      "Epoch 20 batch 130 train Loss 487.4682 test Loss 188.7435 with MSE metric 44550.7526\n",
      "Epoch 20 batch 140 train Loss 486.5357 test Loss 188.3978 with MSE metric 44549.9307\n",
      "Epoch 20 batch 150 train Loss 485.6121 test Loss 188.0535 with MSE metric 44547.1567\n",
      "Epoch 20 batch 160 train Loss 484.6869 test Loss 187.7105 with MSE metric 44545.5417\n",
      "Epoch 20 batch 170 train Loss 483.7676 test Loss 187.3688 with MSE metric 44545.3842\n",
      "Epoch 20 batch 180 train Loss 482.8502 test Loss 187.0285 with MSE metric 44544.4560\n",
      "Epoch 20 batch 190 train Loss 481.9357 test Loss 186.6895 with MSE metric 44544.5368\n",
      "Epoch 20 batch 200 train Loss 481.0239 test Loss 186.3518 with MSE metric 44544.0892\n",
      "Epoch 20 batch 210 train Loss 480.1160 test Loss 186.0154 with MSE metric 44543.4862\n",
      "Epoch 20 batch 220 train Loss 479.2114 test Loss 185.6802 with MSE metric 44541.3666\n",
      "Epoch 20 batch 230 train Loss 478.3118 test Loss 185.3463 with MSE metric 44539.3565\n",
      "Epoch 20 batch 240 train Loss 477.4146 test Loss 185.0136 with MSE metric 44538.8495\n",
      "Time taken for 1 epoch: 30.16786217689514 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 476.5206 test Loss 184.6823 with MSE metric 44541.2643\n",
      "Epoch 21 batch 10 train Loss 475.6377 test Loss 184.3524 with MSE metric 44540.3382\n",
      "Epoch 21 batch 20 train Loss 474.7501 test Loss 184.0235 with MSE metric 44540.6327\n",
      "Epoch 21 batch 30 train Loss 473.8670 test Loss 183.6959 with MSE metric 44539.9980\n",
      "Epoch 21 batch 40 train Loss 472.9864 test Loss 183.3695 with MSE metric 44539.7680\n",
      "Epoch 21 batch 50 train Loss 472.1109 test Loss 183.0446 with MSE metric 44538.6627\n",
      "Epoch 21 batch 60 train Loss 471.2366 test Loss 182.7208 with MSE metric 44539.1137\n",
      "Epoch 21 batch 70 train Loss 470.3668 test Loss 182.3981 with MSE metric 44538.3571\n",
      "Epoch 21 batch 80 train Loss 469.5000 test Loss 182.0767 with MSE metric 44539.5117\n",
      "Epoch 21 batch 90 train Loss 468.6358 test Loss 181.7566 with MSE metric 44537.4115\n",
      "Epoch 21 batch 100 train Loss 467.7763 test Loss 181.4376 with MSE metric 44536.6927\n",
      "Epoch 21 batch 110 train Loss 466.9180 test Loss 181.1198 with MSE metric 44533.3931\n",
      "Epoch 21 batch 120 train Loss 466.0633 test Loss 180.8031 with MSE metric 44531.6526\n",
      "Epoch 21 batch 130 train Loss 465.2125 test Loss 180.4877 with MSE metric 44532.1801\n",
      "Epoch 21 batch 140 train Loss 464.3646 test Loss 180.1734 with MSE metric 44532.1076\n",
      "Epoch 21 batch 150 train Loss 463.5209 test Loss 179.8604 with MSE metric 44531.6240\n",
      "Epoch 21 batch 160 train Loss 462.6793 test Loss 179.5484 with MSE metric 44530.1659\n",
      "Epoch 21 batch 170 train Loss 461.8398 test Loss 179.2376 with MSE metric 44531.7636\n",
      "Epoch 21 batch 180 train Loss 461.0042 test Loss 178.9280 with MSE metric 44530.2022\n",
      "Epoch 21 batch 190 train Loss 460.1713 test Loss 178.6195 with MSE metric 44529.8319\n",
      "Epoch 21 batch 200 train Loss 459.3415 test Loss 178.3120 with MSE metric 44528.6794\n",
      "Epoch 21 batch 210 train Loss 458.5150 test Loss 178.0057 with MSE metric 44528.1707\n",
      "Epoch 21 batch 220 train Loss 457.6914 test Loss 177.7007 with MSE metric 44528.0325\n",
      "Epoch 21 batch 230 train Loss 456.8706 test Loss 177.3966 with MSE metric 44526.8091\n",
      "Epoch 21 batch 240 train Loss 456.0535 test Loss 177.0936 with MSE metric 44526.0047\n",
      "Time taken for 1 epoch: 27.78231692314148 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 455.2401 test Loss 176.7919 with MSE metric 44524.9635\n",
      "Epoch 22 batch 10 train Loss 454.4282 test Loss 176.4912 with MSE metric 44524.3901\n",
      "Epoch 22 batch 20 train Loss 453.6195 test Loss 176.1915 with MSE metric 44526.1289\n",
      "Epoch 22 batch 30 train Loss 452.8161 test Loss 175.8930 with MSE metric 44526.2864\n",
      "Epoch 22 batch 40 train Loss 452.0447 test Loss 175.5956 with MSE metric 44527.5907\n",
      "Epoch 22 batch 50 train Loss 451.2441 test Loss 175.2994 with MSE metric 44526.3286\n",
      "Epoch 22 batch 60 train Loss 450.4469 test Loss 175.0039 with MSE metric 44527.5434\n",
      "Epoch 22 batch 70 train Loss 449.6531 test Loss 174.7099 with MSE metric 44526.3519\n",
      "Epoch 22 batch 80 train Loss 448.8614 test Loss 174.4166 with MSE metric 44526.2595\n",
      "Epoch 22 batch 90 train Loss 448.0724 test Loss 174.1245 with MSE metric 44525.1485\n",
      "Epoch 22 batch 100 train Loss 447.2866 test Loss 173.8336 with MSE metric 44523.5136\n",
      "Epoch 22 batch 110 train Loss 446.5031 test Loss 173.5437 with MSE metric 44522.2833\n",
      "Epoch 22 batch 120 train Loss 445.7228 test Loss 173.2547 with MSE metric 44521.4576\n",
      "Epoch 22 batch 130 train Loss 444.9455 test Loss 172.9668 with MSE metric 44519.5376\n",
      "Epoch 22 batch 140 train Loss 444.1720 test Loss 172.6798 with MSE metric 44519.5327\n",
      "Epoch 22 batch 150 train Loss 443.4007 test Loss 172.3939 with MSE metric 44518.4265\n",
      "Epoch 22 batch 160 train Loss 442.6321 test Loss 172.1090 with MSE metric 44518.5569\n",
      "Epoch 22 batch 170 train Loss 441.8662 test Loss 171.8251 with MSE metric 44517.7236\n",
      "Epoch 22 batch 180 train Loss 441.1028 test Loss 171.5422 with MSE metric 44517.9067\n",
      "Epoch 22 batch 190 train Loss 440.3419 test Loss 171.2603 with MSE metric 44516.4745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 batch 200 train Loss 439.5845 test Loss 170.9795 with MSE metric 44517.1503\n",
      "Epoch 22 batch 210 train Loss 438.8287 test Loss 170.6996 with MSE metric 44518.5449\n",
      "Epoch 22 batch 220 train Loss 438.0748 test Loss 170.4206 with MSE metric 44515.3944\n",
      "Epoch 22 batch 230 train Loss 437.3248 test Loss 170.1427 with MSE metric 44514.9178\n",
      "Epoch 22 batch 240 train Loss 436.5766 test Loss 169.8657 with MSE metric 44516.0122\n",
      "Time taken for 1 epoch: 30.260584354400635 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 435.8310 test Loss 169.5898 with MSE metric 44513.3356\n",
      "Epoch 23 batch 10 train Loss 435.0883 test Loss 169.3148 with MSE metric 44512.0308\n",
      "Epoch 23 batch 20 train Loss 434.3486 test Loss 169.0408 with MSE metric 44513.1384\n",
      "Epoch 23 batch 30 train Loss 433.6202 test Loss 168.7675 with MSE metric 44513.4129\n",
      "Epoch 23 batch 40 train Loss 432.8849 test Loss 168.4954 with MSE metric 44511.8111\n",
      "Epoch 23 batch 50 train Loss 432.1534 test Loss 168.2243 with MSE metric 44511.1223\n",
      "Epoch 23 batch 60 train Loss 431.4247 test Loss 167.9540 with MSE metric 44508.3982\n",
      "Epoch 23 batch 70 train Loss 430.6979 test Loss 167.6847 with MSE metric 44507.5394\n",
      "Epoch 23 batch 80 train Loss 429.9726 test Loss 167.4163 with MSE metric 44506.6466\n",
      "Epoch 23 batch 90 train Loss 429.2497 test Loss 167.1489 with MSE metric 44506.2492\n",
      "Epoch 23 batch 100 train Loss 428.5315 test Loss 166.8823 with MSE metric 44506.8506\n",
      "Epoch 23 batch 110 train Loss 427.8139 test Loss 166.6166 with MSE metric 44506.0082\n",
      "Epoch 23 batch 120 train Loss 427.0996 test Loss 166.3517 with MSE metric 44505.4590\n",
      "Epoch 23 batch 130 train Loss 426.3896 test Loss 166.0879 with MSE metric 44505.9764\n",
      "Epoch 23 batch 140 train Loss 425.6798 test Loss 165.8250 with MSE metric 44505.8162\n",
      "Epoch 23 batch 150 train Loss 424.9717 test Loss 165.5628 with MSE metric 44505.3034\n",
      "Epoch 23 batch 160 train Loss 424.2654 test Loss 165.3016 with MSE metric 44504.3922\n",
      "Epoch 23 batch 170 train Loss 423.5618 test Loss 165.0414 with MSE metric 44504.5068\n",
      "Epoch 23 batch 180 train Loss 422.8617 test Loss 164.7820 with MSE metric 44504.6756\n",
      "Epoch 23 batch 190 train Loss 422.1629 test Loss 164.5235 with MSE metric 44502.3542\n",
      "Epoch 23 batch 200 train Loss 421.4668 test Loss 164.2659 with MSE metric 44503.2639\n",
      "Epoch 23 batch 210 train Loss 420.7730 test Loss 164.0091 with MSE metric 44502.4669\n",
      "Epoch 23 batch 220 train Loss 420.0812 test Loss 163.7533 with MSE metric 44503.5011\n",
      "Epoch 23 batch 230 train Loss 419.3917 test Loss 163.4983 with MSE metric 44503.7991\n",
      "Epoch 23 batch 240 train Loss 418.7049 test Loss 163.2442 with MSE metric 44501.7502\n",
      "Time taken for 1 epoch: 29.787312030792236 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 418.0205 test Loss 162.9907 with MSE metric 44502.2103\n",
      "Epoch 24 batch 10 train Loss 417.3378 test Loss 162.7382 with MSE metric 44502.9575\n",
      "Epoch 24 batch 20 train Loss 416.6598 test Loss 162.4864 with MSE metric 44503.3249\n",
      "Epoch 24 batch 30 train Loss 415.9820 test Loss 162.2356 with MSE metric 44503.9373\n",
      "Epoch 24 batch 40 train Loss 415.3063 test Loss 161.9856 with MSE metric 44502.4237\n",
      "Epoch 24 batch 50 train Loss 414.6334 test Loss 161.7364 with MSE metric 44502.3820\n",
      "Epoch 24 batch 60 train Loss 413.9619 test Loss 161.4880 with MSE metric 44501.7777\n",
      "Epoch 24 batch 70 train Loss 413.2925 test Loss 161.2404 with MSE metric 44500.2765\n",
      "Epoch 24 batch 80 train Loss 412.6270 test Loss 160.9937 with MSE metric 44500.1991\n",
      "Epoch 24 batch 90 train Loss 411.9636 test Loss 160.7476 with MSE metric 44500.1292\n",
      "Epoch 24 batch 100 train Loss 411.3011 test Loss 160.5025 with MSE metric 44499.8309\n",
      "Epoch 24 batch 110 train Loss 410.6409 test Loss 160.2583 with MSE metric 44499.1573\n",
      "Epoch 24 batch 120 train Loss 409.9837 test Loss 160.0147 with MSE metric 44498.7194\n",
      "Epoch 24 batch 130 train Loss 409.3280 test Loss 159.7720 with MSE metric 44497.9833\n",
      "Epoch 24 batch 140 train Loss 408.6762 test Loss 159.5301 with MSE metric 44496.5639\n",
      "Epoch 24 batch 150 train Loss 408.0267 test Loss 159.2889 with MSE metric 44495.7882\n",
      "Epoch 24 batch 160 train Loss 407.3770 test Loss 159.0485 with MSE metric 44494.3233\n",
      "Epoch 24 batch 170 train Loss 406.7300 test Loss 158.8089 with MSE metric 44493.5046\n",
      "Epoch 24 batch 180 train Loss 406.0849 test Loss 158.5701 with MSE metric 44491.4073\n",
      "Epoch 24 batch 190 train Loss 405.4413 test Loss 158.3321 with MSE metric 44492.0621\n",
      "Epoch 24 batch 200 train Loss 404.8007 test Loss 158.0948 with MSE metric 44491.4432\n",
      "Epoch 24 batch 210 train Loss 404.1613 test Loss 157.8582 with MSE metric 44489.7749\n",
      "Epoch 24 batch 220 train Loss 403.5257 test Loss 157.6225 with MSE metric 44489.0644\n",
      "Epoch 24 batch 230 train Loss 402.8906 test Loss 157.3874 with MSE metric 44490.8394\n",
      "Epoch 24 batch 240 train Loss 402.2578 test Loss 157.1532 with MSE metric 44490.8048\n",
      "Time taken for 1 epoch: 31.27457094192505 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 401.6268 test Loss 156.9197 with MSE metric 44488.0306\n",
      "Epoch 25 batch 10 train Loss 400.9975 test Loss 156.6870 with MSE metric 44488.0853\n",
      "Epoch 25 batch 20 train Loss 400.3706 test Loss 156.4550 with MSE metric 44485.9134\n",
      "Epoch 25 batch 30 train Loss 399.7454 test Loss 156.2236 with MSE metric 44486.0850\n",
      "Epoch 25 batch 40 train Loss 399.1220 test Loss 155.9931 with MSE metric 44484.2154\n",
      "Epoch 25 batch 50 train Loss 398.5028 test Loss 155.7634 with MSE metric 44482.0640\n",
      "Epoch 25 batch 60 train Loss 397.8842 test Loss 155.5344 with MSE metric 44481.5086\n",
      "Epoch 25 batch 70 train Loss 397.2674 test Loss 155.3061 with MSE metric 44480.6203\n",
      "Epoch 25 batch 80 train Loss 396.6529 test Loss 155.0785 with MSE metric 44481.1450\n",
      "Epoch 25 batch 90 train Loss 396.0489 test Loss 154.8516 with MSE metric 44478.6523\n",
      "Epoch 25 batch 100 train Loss 395.4383 test Loss 154.6256 with MSE metric 44477.8937\n",
      "Epoch 25 batch 110 train Loss 394.8297 test Loss 154.4001 with MSE metric 44478.1594\n",
      "Epoch 25 batch 120 train Loss 394.2217 test Loss 154.1754 with MSE metric 44479.8721\n",
      "Epoch 25 batch 130 train Loss 393.6163 test Loss 153.9514 with MSE metric 44479.5171\n",
      "Epoch 25 batch 140 train Loss 393.0127 test Loss 153.7280 with MSE metric 44478.3759\n",
      "Epoch 25 batch 150 train Loss 392.4116 test Loss 153.5055 with MSE metric 44476.9842\n",
      "Epoch 25 batch 160 train Loss 391.8122 test Loss 153.2835 with MSE metric 44477.0386\n",
      "Epoch 25 batch 170 train Loss 391.2140 test Loss 153.0623 with MSE metric 44476.7864\n",
      "Epoch 25 batch 180 train Loss 390.6300 test Loss 152.8417 with MSE metric 44474.1866\n",
      "Epoch 25 batch 190 train Loss 390.0355 test Loss 152.6218 with MSE metric 44472.6137\n",
      "Epoch 25 batch 200 train Loss 389.4442 test Loss 152.4028 with MSE metric 44473.8797\n",
      "Epoch 25 batch 210 train Loss 388.8541 test Loss 152.1844 with MSE metric 44472.8964\n",
      "Epoch 25 batch 220 train Loss 388.2657 test Loss 151.9665 with MSE metric 44469.5572\n",
      "Epoch 25 batch 230 train Loss 387.6784 test Loss 151.7494 with MSE metric 44467.5549\n",
      "Epoch 25 batch 240 train Loss 387.0934 test Loss 151.5329 with MSE metric 44465.7508\n",
      "Time taken for 1 epoch: 31.710872888565063 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 386.5099 test Loss 151.3171 with MSE metric 44464.5398\n",
      "Epoch 26 batch 10 train Loss 385.9280 test Loss 151.1020 with MSE metric 44464.5045\n",
      "Epoch 26 batch 20 train Loss 385.3493 test Loss 150.8875 with MSE metric 44464.3144\n",
      "Epoch 26 batch 30 train Loss 384.7712 test Loss 150.6738 with MSE metric 44464.6405\n",
      "Epoch 26 batch 40 train Loss 384.1945 test Loss 150.4606 with MSE metric 44461.8838\n",
      "Epoch 26 batch 50 train Loss 383.6199 test Loss 150.2481 with MSE metric 44462.6456\n",
      "Epoch 26 batch 60 train Loss 383.0496 test Loss 150.0362 with MSE metric 44461.7897\n",
      "Epoch 26 batch 70 train Loss 382.4783 test Loss 149.8249 with MSE metric 44461.0442\n",
      "Epoch 26 batch 80 train Loss 381.9085 test Loss 149.6143 with MSE metric 44457.9318\n",
      "Epoch 26 batch 90 train Loss 381.3410 test Loss 149.4044 with MSE metric 44456.8822\n",
      "Epoch 26 batch 100 train Loss 380.7748 test Loss 149.1950 with MSE metric 44456.3727\n",
      "Epoch 26 batch 110 train Loss 380.2101 test Loss 148.9863 with MSE metric 44457.0918\n",
      "Epoch 26 batch 120 train Loss 379.6481 test Loss 148.7783 with MSE metric 44456.4343\n",
      "Epoch 26 batch 130 train Loss 379.0873 test Loss 148.5710 with MSE metric 44454.6530\n",
      "Epoch 26 batch 140 train Loss 378.5311 test Loss 148.3642 with MSE metric 44454.9664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 batch 150 train Loss 377.9733 test Loss 148.1580 with MSE metric 44454.7234\n",
      "Epoch 26 batch 160 train Loss 377.4174 test Loss 147.9525 with MSE metric 44453.3625\n",
      "Epoch 26 batch 170 train Loss 376.8630 test Loss 147.7476 with MSE metric 44453.6740\n",
      "Epoch 26 batch 180 train Loss 376.3102 test Loss 147.5433 with MSE metric 44453.7684\n",
      "Epoch 26 batch 190 train Loss 375.7591 test Loss 147.3397 with MSE metric 44451.9496\n",
      "Epoch 26 batch 200 train Loss 375.2098 test Loss 147.1366 with MSE metric 44450.7005\n",
      "Epoch 26 batch 210 train Loss 374.6619 test Loss 146.9341 with MSE metric 44452.3522\n",
      "Epoch 26 batch 220 train Loss 374.1156 test Loss 146.7323 with MSE metric 44451.1834\n",
      "Epoch 26 batch 230 train Loss 373.5711 test Loss 146.5309 with MSE metric 44450.5299\n",
      "Epoch 26 batch 240 train Loss 373.0280 test Loss 146.3302 with MSE metric 44451.0870\n",
      "Time taken for 1 epoch: 29.952781915664673 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 372.4866 test Loss 146.1301 with MSE metric 44450.3651\n",
      "Epoch 27 batch 10 train Loss 371.9473 test Loss 145.9306 with MSE metric 44449.2412\n",
      "Epoch 27 batch 20 train Loss 371.4093 test Loss 145.7317 with MSE metric 44448.9084\n",
      "Epoch 27 batch 30 train Loss 370.8729 test Loss 145.5333 with MSE metric 44449.6830\n",
      "Epoch 27 batch 40 train Loss 370.3386 test Loss 145.3356 with MSE metric 44448.7742\n",
      "Epoch 27 batch 50 train Loss 369.8053 test Loss 145.1385 with MSE metric 44447.6842\n",
      "Epoch 27 batch 60 train Loss 369.2740 test Loss 144.9418 with MSE metric 44446.6652\n",
      "Epoch 27 batch 70 train Loss 368.7440 test Loss 144.7458 with MSE metric 44447.4786\n",
      "Epoch 27 batch 80 train Loss 368.2153 test Loss 144.5504 with MSE metric 44446.3417\n",
      "Epoch 27 batch 90 train Loss 367.6882 test Loss 144.3555 with MSE metric 44445.6922\n",
      "Epoch 27 batch 100 train Loss 367.1627 test Loss 144.1611 with MSE metric 44444.4540\n",
      "Epoch 27 batch 110 train Loss 366.6407 test Loss 143.9674 with MSE metric 44444.1631\n",
      "Epoch 27 batch 120 train Loss 366.1184 test Loss 143.7743 with MSE metric 44443.1881\n",
      "Epoch 27 batch 130 train Loss 365.5988 test Loss 143.5816 with MSE metric 44443.2500\n",
      "Epoch 27 batch 140 train Loss 365.0795 test Loss 143.3895 with MSE metric 44443.1479\n",
      "Epoch 27 batch 150 train Loss 364.5614 test Loss 143.1980 with MSE metric 44442.3447\n",
      "Epoch 27 batch 160 train Loss 364.0450 test Loss 143.0071 with MSE metric 44442.1274\n",
      "Epoch 27 batch 170 train Loss 363.5299 test Loss 142.8167 with MSE metric 44441.4744\n",
      "Epoch 27 batch 180 train Loss 363.0175 test Loss 142.6269 with MSE metric 44441.3993\n",
      "Epoch 27 batch 190 train Loss 362.5054 test Loss 142.4376 with MSE metric 44440.4337\n",
      "Epoch 27 batch 200 train Loss 361.9948 test Loss 142.2489 with MSE metric 44441.4784\n",
      "Epoch 27 batch 210 train Loss 361.4859 test Loss 142.0607 with MSE metric 44439.3951\n",
      "Epoch 27 batch 220 train Loss 360.9829 test Loss 141.8731 with MSE metric 44438.3368\n",
      "Epoch 27 batch 230 train Loss 360.4770 test Loss 141.6860 with MSE metric 44437.3609\n",
      "Epoch 27 batch 240 train Loss 359.9724 test Loss 141.4994 with MSE metric 44436.7279\n",
      "Time taken for 1 epoch: 30.433131217956543 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 359.4695 test Loss 141.3133 with MSE metric 44438.7372\n",
      "Epoch 28 batch 10 train Loss 358.9675 test Loss 141.1279 with MSE metric 44437.4755\n",
      "Epoch 28 batch 20 train Loss 358.4671 test Loss 140.9429 with MSE metric 44437.5265\n",
      "Epoch 28 batch 30 train Loss 357.9683 test Loss 140.7585 with MSE metric 44436.9023\n",
      "Epoch 28 batch 40 train Loss 357.4705 test Loss 140.5746 with MSE metric 44436.6041\n",
      "Epoch 28 batch 50 train Loss 356.9742 test Loss 140.3913 with MSE metric 44437.2828\n",
      "Epoch 28 batch 60 train Loss 356.4793 test Loss 140.2083 with MSE metric 44437.3487\n",
      "Epoch 28 batch 70 train Loss 355.9859 test Loss 140.0260 with MSE metric 44436.1055\n",
      "Epoch 28 batch 80 train Loss 355.4940 test Loss 139.8441 with MSE metric 44435.6005\n",
      "Epoch 28 batch 90 train Loss 355.0031 test Loss 139.6627 with MSE metric 44433.3865\n",
      "Epoch 28 batch 100 train Loss 354.5138 test Loss 139.4819 with MSE metric 44432.6426\n",
      "Epoch 28 batch 110 train Loss 354.0259 test Loss 139.3016 with MSE metric 44432.1475\n",
      "Epoch 28 batch 120 train Loss 353.5395 test Loss 139.1218 with MSE metric 44432.6995\n",
      "Epoch 28 batch 130 train Loss 353.0546 test Loss 138.9425 with MSE metric 44432.1447\n",
      "Epoch 28 batch 140 train Loss 352.5710 test Loss 138.7637 with MSE metric 44432.0027\n",
      "Epoch 28 batch 150 train Loss 352.0897 test Loss 138.5854 with MSE metric 44432.4883\n",
      "Epoch 28 batch 160 train Loss 351.6086 test Loss 138.4076 with MSE metric 44431.3688\n",
      "Epoch 28 batch 170 train Loss 351.1289 test Loss 138.2303 with MSE metric 44430.2415\n",
      "Epoch 28 batch 180 train Loss 350.6507 test Loss 138.0534 with MSE metric 44429.4506\n",
      "Epoch 28 batch 190 train Loss 350.1737 test Loss 137.8771 with MSE metric 44428.0734\n",
      "Epoch 28 batch 200 train Loss 349.6987 test Loss 137.7013 with MSE metric 44426.6721\n",
      "Epoch 28 batch 210 train Loss 349.2244 test Loss 137.5259 with MSE metric 44425.8820\n",
      "Epoch 28 batch 220 train Loss 348.7517 test Loss 137.3510 with MSE metric 44425.3358\n",
      "Epoch 28 batch 230 train Loss 348.2798 test Loss 137.1767 with MSE metric 44424.6874\n",
      "Epoch 28 batch 240 train Loss 347.8092 test Loss 137.0027 with MSE metric 44425.2455\n",
      "Time taken for 1 epoch: 29.607370853424072 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 347.3403 test Loss 136.8293 with MSE metric 44425.7740\n",
      "Epoch 29 batch 10 train Loss 346.8725 test Loss 136.6563 with MSE metric 44425.5950\n",
      "Epoch 29 batch 20 train Loss 346.4059 test Loss 136.4839 with MSE metric 44424.5724\n",
      "Epoch 29 batch 30 train Loss 345.9405 test Loss 136.3119 with MSE metric 44423.6686\n",
      "Epoch 29 batch 40 train Loss 345.4763 test Loss 136.1404 with MSE metric 44422.7239\n",
      "Epoch 29 batch 50 train Loss 345.0135 test Loss 135.9693 with MSE metric 44422.0632\n",
      "Epoch 29 batch 60 train Loss 344.5517 test Loss 135.7987 with MSE metric 44420.0858\n",
      "Epoch 29 batch 70 train Loss 344.0917 test Loss 135.6285 with MSE metric 44422.2485\n",
      "Epoch 29 batch 80 train Loss 343.6329 test Loss 135.4588 with MSE metric 44421.4307\n",
      "Epoch 29 batch 90 train Loss 343.1751 test Loss 135.2896 with MSE metric 44422.5613\n",
      "Epoch 29 batch 100 train Loss 342.7184 test Loss 135.1208 with MSE metric 44421.2323\n",
      "Epoch 29 batch 110 train Loss 342.2631 test Loss 134.9525 with MSE metric 44421.0077\n",
      "Epoch 29 batch 120 train Loss 341.8093 test Loss 134.7848 with MSE metric 44421.0726\n",
      "Epoch 29 batch 130 train Loss 341.3565 test Loss 134.6173 with MSE metric 44420.3308\n",
      "Epoch 29 batch 140 train Loss 340.9048 test Loss 134.4504 with MSE metric 44421.6821\n",
      "Epoch 29 batch 150 train Loss 340.4570 test Loss 134.2839 with MSE metric 44421.8485\n",
      "Epoch 29 batch 160 train Loss 340.0078 test Loss 134.1179 with MSE metric 44420.3539\n",
      "Epoch 29 batch 170 train Loss 339.5603 test Loss 133.9523 with MSE metric 44418.9651\n",
      "Epoch 29 batch 180 train Loss 339.1134 test Loss 133.7871 with MSE metric 44417.8854\n",
      "Epoch 29 batch 190 train Loss 338.6681 test Loss 133.6224 with MSE metric 44417.0549\n",
      "Epoch 29 batch 200 train Loss 338.2237 test Loss 133.4581 with MSE metric 44414.8476\n",
      "Epoch 29 batch 210 train Loss 337.7805 test Loss 133.2942 with MSE metric 44413.2550\n",
      "Epoch 29 batch 220 train Loss 337.3386 test Loss 133.1308 with MSE metric 44412.6882\n",
      "Epoch 29 batch 230 train Loss 336.8982 test Loss 132.9679 with MSE metric 44411.6667\n",
      "Epoch 29 batch 240 train Loss 336.4586 test Loss 132.8054 with MSE metric 44411.4782\n",
      "Time taken for 1 epoch: 30.93397092819214 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 336.0203 test Loss 132.6433 with MSE metric 44410.5437\n",
      "Epoch 30 batch 10 train Loss 335.5829 test Loss 132.4817 with MSE metric 44410.1456\n",
      "Epoch 30 batch 20 train Loss 335.1466 test Loss 132.3204 with MSE metric 44409.1879\n",
      "Epoch 30 batch 30 train Loss 334.7115 test Loss 132.1596 with MSE metric 44408.9858\n",
      "Epoch 30 batch 40 train Loss 334.2780 test Loss 131.9991 with MSE metric 44407.8582\n",
      "Epoch 30 batch 50 train Loss 333.8476 test Loss 131.8392 with MSE metric 44406.0127\n",
      "Epoch 30 batch 60 train Loss 333.4162 test Loss 131.6797 with MSE metric 44405.6028\n",
      "Epoch 30 batch 70 train Loss 332.9860 test Loss 131.5206 with MSE metric 44404.6121\n",
      "Epoch 30 batch 80 train Loss 332.5567 test Loss 131.3619 with MSE metric 44404.7620\n",
      "Epoch 30 batch 90 train Loss 332.1288 test Loss 131.2037 with MSE metric 44403.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 batch 100 train Loss 331.7017 test Loss 131.0458 with MSE metric 44402.5543\n",
      "Epoch 30 batch 110 train Loss 331.2757 test Loss 130.8884 with MSE metric 44401.7364\n",
      "Epoch 30 batch 120 train Loss 330.8509 test Loss 130.7314 with MSE metric 44401.1948\n",
      "Epoch 30 batch 130 train Loss 330.4272 test Loss 130.5748 with MSE metric 44402.4479\n",
      "Epoch 30 batch 140 train Loss 330.0047 test Loss 130.4186 with MSE metric 44402.8617\n",
      "Epoch 30 batch 150 train Loss 329.5861 test Loss 130.2628 with MSE metric 44401.9661\n",
      "Epoch 30 batch 160 train Loss 329.1658 test Loss 130.1074 with MSE metric 44401.9016\n",
      "Epoch 30 batch 170 train Loss 328.7467 test Loss 129.9523 with MSE metric 44402.1558\n",
      "Epoch 30 batch 180 train Loss 328.3287 test Loss 129.7978 with MSE metric 44402.1161\n",
      "Epoch 30 batch 190 train Loss 327.9115 test Loss 129.6436 with MSE metric 44402.0686\n",
      "Epoch 30 batch 200 train Loss 327.4956 test Loss 129.4898 with MSE metric 44400.9390\n",
      "Epoch 30 batch 210 train Loss 327.0813 test Loss 129.3364 with MSE metric 44400.0264\n",
      "Epoch 30 batch 220 train Loss 326.6677 test Loss 129.1834 with MSE metric 44399.6060\n",
      "Epoch 30 batch 230 train Loss 326.2550 test Loss 129.0307 with MSE metric 44398.4512\n",
      "Epoch 30 batch 240 train Loss 325.8431 test Loss 128.8785 with MSE metric 44397.8178\n",
      "Time taken for 1 epoch: 31.132457971572876 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 325.4323 test Loss 128.7267 with MSE metric 44395.2933\n",
      "Epoch 31 batch 10 train Loss 325.0231 test Loss 128.5753 with MSE metric 44394.3125\n",
      "Epoch 31 batch 20 train Loss 324.6146 test Loss 128.4243 with MSE metric 44392.9813\n",
      "Epoch 31 batch 30 train Loss 324.2071 test Loss 128.2736 with MSE metric 44392.2537\n",
      "Epoch 31 batch 40 train Loss 323.8006 test Loss 128.1233 with MSE metric 44392.2389\n",
      "Epoch 31 batch 50 train Loss 323.3963 test Loss 127.9734 with MSE metric 44391.1102\n",
      "Epoch 31 batch 60 train Loss 322.9918 test Loss 127.8238 with MSE metric 44391.0664\n",
      "Epoch 31 batch 70 train Loss 322.5884 test Loss 127.6747 with MSE metric 44390.2452\n",
      "Epoch 31 batch 80 train Loss 322.1888 test Loss 127.5259 with MSE metric 44389.5432\n",
      "Epoch 31 batch 90 train Loss 321.7874 test Loss 127.3775 with MSE metric 44388.6763\n",
      "Epoch 31 batch 100 train Loss 321.3870 test Loss 127.2295 with MSE metric 44389.5070\n",
      "Epoch 31 batch 110 train Loss 320.9879 test Loss 127.0819 with MSE metric 44388.3588\n",
      "Epoch 31 batch 120 train Loss 320.5909 test Loss 126.9346 with MSE metric 44387.7711\n",
      "Epoch 31 batch 130 train Loss 320.1937 test Loss 126.7877 with MSE metric 44388.2631\n",
      "Epoch 31 batch 140 train Loss 319.7977 test Loss 126.6412 with MSE metric 44387.2531\n",
      "Epoch 31 batch 150 train Loss 319.4026 test Loss 126.4950 with MSE metric 44387.2137\n",
      "Epoch 31 batch 160 train Loss 319.0084 test Loss 126.3492 with MSE metric 44388.2219\n",
      "Epoch 31 batch 170 train Loss 318.6172 test Loss 126.2038 with MSE metric 44387.2905\n",
      "Epoch 31 batch 180 train Loss 318.2248 test Loss 126.0588 with MSE metric 44385.9407\n",
      "Epoch 31 batch 190 train Loss 317.8335 test Loss 125.9141 with MSE metric 44384.2443\n",
      "Epoch 31 batch 200 train Loss 317.4435 test Loss 125.7698 with MSE metric 44383.4457\n",
      "Epoch 31 batch 210 train Loss 317.0541 test Loss 125.6258 with MSE metric 44383.4995\n",
      "Epoch 31 batch 220 train Loss 316.6658 test Loss 125.4822 with MSE metric 44383.6009\n",
      "Epoch 31 batch 230 train Loss 316.2797 test Loss 125.3389 with MSE metric 44384.1267\n",
      "Epoch 31 batch 240 train Loss 315.8934 test Loss 125.1960 with MSE metric 44384.4867\n",
      "Time taken for 1 epoch: 33.38770318031311 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 315.5080 test Loss 125.0535 with MSE metric 44383.2921\n",
      "Epoch 32 batch 10 train Loss 315.1234 test Loss 124.9113 with MSE metric 44382.3624\n",
      "Epoch 32 batch 20 train Loss 314.7398 test Loss 124.7695 with MSE metric 44381.7614\n",
      "Epoch 32 batch 30 train Loss 314.3573 test Loss 124.6280 with MSE metric 44381.2901\n",
      "Epoch 32 batch 40 train Loss 313.9757 test Loss 124.4869 with MSE metric 44380.4358\n",
      "Epoch 32 batch 50 train Loss 313.5950 test Loss 124.3461 with MSE metric 44379.5308\n",
      "Epoch 32 batch 60 train Loss 313.2153 test Loss 124.2056 with MSE metric 44379.0752\n",
      "Epoch 32 batch 70 train Loss 312.8365 test Loss 124.0654 with MSE metric 44379.4068\n",
      "Epoch 32 batch 80 train Loss 312.4586 test Loss 123.9257 with MSE metric 44378.3717\n",
      "Epoch 32 batch 90 train Loss 312.0819 test Loss 123.7863 with MSE metric 44378.1085\n",
      "Epoch 32 batch 100 train Loss 311.7059 test Loss 123.6472 with MSE metric 44378.0782\n",
      "Epoch 32 batch 110 train Loss 311.3366 test Loss 123.5085 with MSE metric 44377.8277\n",
      "Epoch 32 batch 120 train Loss 310.9623 test Loss 123.3701 with MSE metric 44376.7873\n",
      "Epoch 32 batch 130 train Loss 310.5890 test Loss 123.2322 with MSE metric 44375.8646\n",
      "Epoch 32 batch 140 train Loss 310.2166 test Loss 123.0945 with MSE metric 44374.4045\n",
      "Epoch 32 batch 150 train Loss 309.8452 test Loss 122.9572 with MSE metric 44372.9337\n",
      "Epoch 32 batch 160 train Loss 309.4761 test Loss 122.8203 with MSE metric 44372.9908\n",
      "Epoch 32 batch 170 train Loss 309.1064 test Loss 122.6836 with MSE metric 44372.3393\n",
      "Epoch 32 batch 180 train Loss 308.7377 test Loss 122.5472 with MSE metric 44370.9973\n",
      "Epoch 32 batch 190 train Loss 308.3701 test Loss 122.4112 with MSE metric 44370.9990\n",
      "Epoch 32 batch 200 train Loss 308.0032 test Loss 122.2756 with MSE metric 44368.7280\n",
      "Epoch 32 batch 210 train Loss 307.6378 test Loss 122.1403 with MSE metric 44366.9120\n",
      "Epoch 32 batch 220 train Loss 307.2744 test Loss 122.0053 with MSE metric 44365.4667\n",
      "Epoch 32 batch 230 train Loss 306.9102 test Loss 121.8707 with MSE metric 44364.8996\n",
      "Epoch 32 batch 240 train Loss 306.5470 test Loss 121.7363 with MSE metric 44365.4152\n",
      "Time taken for 1 epoch: 26.995386123657227 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 306.1863 test Loss 121.6023 with MSE metric 44364.5159\n",
      "Epoch 33 batch 10 train Loss 305.8247 test Loss 121.4686 with MSE metric 44364.8901\n",
      "Epoch 33 batch 20 train Loss 305.4642 test Loss 121.3353 with MSE metric 44363.4346\n",
      "Epoch 33 batch 30 train Loss 305.1044 test Loss 121.2023 with MSE metric 44364.1777\n",
      "Epoch 33 batch 40 train Loss 304.7454 test Loss 121.0696 with MSE metric 44362.8307\n",
      "Epoch 33 batch 50 train Loss 304.3874 test Loss 120.9372 with MSE metric 44360.8439\n",
      "Epoch 33 batch 60 train Loss 304.0304 test Loss 120.8051 with MSE metric 44359.1693\n",
      "Epoch 33 batch 70 train Loss 303.6739 test Loss 120.6733 with MSE metric 44358.7875\n",
      "Epoch 33 batch 80 train Loss 303.3184 test Loss 120.5418 with MSE metric 44357.1135\n",
      "Epoch 33 batch 90 train Loss 302.9639 test Loss 120.4106 with MSE metric 44355.8522\n",
      "Epoch 33 batch 100 train Loss 302.6099 test Loss 120.2798 with MSE metric 44355.5773\n",
      "Epoch 33 batch 110 train Loss 302.2571 test Loss 120.1493 with MSE metric 44355.6622\n",
      "Epoch 33 batch 120 train Loss 301.9050 test Loss 120.0191 with MSE metric 44355.2984\n",
      "Epoch 33 batch 130 train Loss 301.5537 test Loss 119.8892 with MSE metric 44354.3823\n",
      "Epoch 33 batch 140 train Loss 301.2033 test Loss 119.7596 with MSE metric 44352.2472\n",
      "Epoch 33 batch 150 train Loss 300.8537 test Loss 119.6303 with MSE metric 44351.3199\n",
      "Epoch 33 batch 160 train Loss 300.5051 test Loss 119.5013 with MSE metric 44350.6661\n",
      "Epoch 33 batch 170 train Loss 300.1571 test Loss 119.3726 with MSE metric 44350.3086\n",
      "Epoch 33 batch 180 train Loss 299.8100 test Loss 119.2442 with MSE metric 44349.4143\n",
      "Epoch 33 batch 190 train Loss 299.4637 test Loss 119.1161 with MSE metric 44349.9665\n",
      "Epoch 33 batch 200 train Loss 299.1181 test Loss 118.9882 with MSE metric 44349.2473\n",
      "Epoch 33 batch 210 train Loss 298.7734 test Loss 118.8607 with MSE metric 44348.5530\n",
      "Epoch 33 batch 220 train Loss 298.4296 test Loss 118.7335 with MSE metric 44347.2971\n",
      "Epoch 33 batch 230 train Loss 298.0864 test Loss 118.6066 with MSE metric 44345.7185\n",
      "Epoch 33 batch 240 train Loss 297.7441 test Loss 118.4800 with MSE metric 44345.2235\n",
      "Time taken for 1 epoch: 33.21414113044739 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 297.4025 test Loss 118.3536 with MSE metric 44344.1740\n",
      "Epoch 34 batch 10 train Loss 297.0620 test Loss 118.2276 with MSE metric 44343.7262\n",
      "Epoch 34 batch 20 train Loss 296.7221 test Loss 118.1018 with MSE metric 44343.7238\n",
      "Epoch 34 batch 30 train Loss 296.3831 test Loss 117.9763 with MSE metric 44343.3633\n",
      "Epoch 34 batch 40 train Loss 296.0451 test Loss 117.8511 with MSE metric 44343.0560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 batch 50 train Loss 295.7075 test Loss 117.7262 with MSE metric 44341.9728\n",
      "Epoch 34 batch 60 train Loss 295.3707 test Loss 117.6016 with MSE metric 44341.8949\n",
      "Epoch 34 batch 70 train Loss 295.0348 test Loss 117.4773 with MSE metric 44341.6695\n",
      "Epoch 34 batch 80 train Loss 294.7001 test Loss 117.3532 with MSE metric 44340.4276\n",
      "Epoch 34 batch 90 train Loss 294.3657 test Loss 117.2294 with MSE metric 44340.7906\n",
      "Epoch 34 batch 100 train Loss 294.0321 test Loss 117.1060 with MSE metric 44340.4896\n",
      "Epoch 34 batch 110 train Loss 293.6995 test Loss 116.9828 with MSE metric 44340.1295\n",
      "Epoch 34 batch 120 train Loss 293.3678 test Loss 116.8599 with MSE metric 44338.9561\n",
      "Epoch 34 batch 130 train Loss 293.0365 test Loss 116.7373 with MSE metric 44338.4681\n",
      "Epoch 34 batch 140 train Loss 292.7062 test Loss 116.6149 with MSE metric 44336.6368\n",
      "Epoch 34 batch 150 train Loss 292.3764 test Loss 116.4928 with MSE metric 44335.7128\n",
      "Epoch 34 batch 160 train Loss 292.0485 test Loss 116.3710 with MSE metric 44334.6346\n",
      "Epoch 34 batch 170 train Loss 291.7201 test Loss 116.2495 with MSE metric 44335.5717\n",
      "Epoch 34 batch 180 train Loss 291.3925 test Loss 116.1282 with MSE metric 44334.1412\n",
      "Epoch 34 batch 190 train Loss 291.0657 test Loss 116.0073 with MSE metric 44333.5644\n",
      "Epoch 34 batch 200 train Loss 290.7397 test Loss 115.8866 with MSE metric 44332.8458\n",
      "Epoch 34 batch 210 train Loss 290.4143 test Loss 115.7661 with MSE metric 44331.1580\n",
      "Epoch 34 batch 220 train Loss 290.0901 test Loss 115.6460 with MSE metric 44330.2315\n",
      "Epoch 34 batch 230 train Loss 289.7663 test Loss 115.5260 with MSE metric 44329.7985\n",
      "Epoch 34 batch 240 train Loss 289.4433 test Loss 115.4064 with MSE metric 44329.3568\n",
      "Time taken for 1 epoch: 31.681270122528076 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 289.1209 test Loss 115.2871 with MSE metric 44329.9038\n",
      "Epoch 35 batch 10 train Loss 288.7994 test Loss 115.1681 with MSE metric 44328.5986\n",
      "Epoch 35 batch 20 train Loss 288.4785 test Loss 115.0493 with MSE metric 44329.1525\n",
      "Epoch 35 batch 30 train Loss 288.1585 test Loss 114.9308 with MSE metric 44328.8736\n",
      "Epoch 35 batch 40 train Loss 287.8392 test Loss 114.8125 with MSE metric 44328.0815\n",
      "Epoch 35 batch 50 train Loss 287.5206 test Loss 114.6945 with MSE metric 44327.3490\n",
      "Epoch 35 batch 60 train Loss 287.2027 test Loss 114.5767 with MSE metric 44326.8305\n",
      "Epoch 35 batch 70 train Loss 286.8855 test Loss 114.4593 with MSE metric 44326.1724\n",
      "Epoch 35 batch 80 train Loss 286.5692 test Loss 114.3421 with MSE metric 44325.8708\n",
      "Epoch 35 batch 90 train Loss 286.2534 test Loss 114.2251 with MSE metric 44324.7114\n",
      "Epoch 35 batch 100 train Loss 285.9383 test Loss 114.1085 with MSE metric 44322.8426\n",
      "Epoch 35 batch 110 train Loss 285.6240 test Loss 113.9920 with MSE metric 44321.1611\n",
      "Epoch 35 batch 120 train Loss 285.3104 test Loss 113.8758 with MSE metric 44320.7945\n",
      "Epoch 35 batch 130 train Loss 284.9977 test Loss 113.7599 with MSE metric 44320.5615\n",
      "Epoch 35 batch 140 train Loss 284.6855 test Loss 113.6442 with MSE metric 44319.4807\n",
      "Epoch 35 batch 150 train Loss 284.3743 test Loss 113.5288 with MSE metric 44319.4863\n",
      "Epoch 35 batch 160 train Loss 284.0644 test Loss 113.4137 with MSE metric 44319.6822\n",
      "Epoch 35 batch 170 train Loss 283.7543 test Loss 113.2988 with MSE metric 44318.8634\n",
      "Epoch 35 batch 180 train Loss 283.4447 test Loss 113.1841 with MSE metric 44317.8515\n",
      "Epoch 35 batch 190 train Loss 283.1360 test Loss 113.0698 with MSE metric 44317.8409\n",
      "Epoch 35 batch 200 train Loss 282.8279 test Loss 112.9556 with MSE metric 44317.0318\n",
      "Epoch 35 batch 210 train Loss 282.5206 test Loss 112.8417 with MSE metric 44316.4663\n",
      "Epoch 35 batch 220 train Loss 282.2139 test Loss 112.7280 with MSE metric 44315.8152\n",
      "Epoch 35 batch 230 train Loss 281.9078 test Loss 112.6146 with MSE metric 44315.3655\n",
      "Epoch 35 batch 240 train Loss 281.6023 test Loss 112.5015 with MSE metric 44314.8838\n",
      "Time taken for 1 epoch: 30.756860971450806 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 281.2975 test Loss 112.3886 with MSE metric 44314.3072\n",
      "Epoch 36 batch 10 train Loss 280.9935 test Loss 112.2759 with MSE metric 44314.2527\n",
      "Epoch 36 batch 20 train Loss 280.6903 test Loss 112.1635 with MSE metric 44314.4038\n",
      "Epoch 36 batch 30 train Loss 280.3875 test Loss 112.0513 with MSE metric 44312.7680\n",
      "Epoch 36 batch 40 train Loss 280.0855 test Loss 111.9393 with MSE metric 44312.1401\n",
      "Epoch 36 batch 50 train Loss 279.7842 test Loss 111.8276 with MSE metric 44311.2905\n",
      "Epoch 36 batch 60 train Loss 279.4835 test Loss 111.7161 with MSE metric 44311.2226\n",
      "Epoch 36 batch 70 train Loss 279.1835 test Loss 111.6049 with MSE metric 44311.6194\n",
      "Epoch 36 batch 80 train Loss 278.8843 test Loss 111.4940 with MSE metric 44311.6559\n",
      "Epoch 36 batch 90 train Loss 278.5856 test Loss 111.3832 with MSE metric 44310.8827\n",
      "Epoch 36 batch 100 train Loss 278.2875 test Loss 111.2727 with MSE metric 44309.6556\n",
      "Epoch 36 batch 110 train Loss 277.9902 test Loss 111.1624 with MSE metric 44308.6758\n",
      "Epoch 36 batch 120 train Loss 277.6934 test Loss 111.0524 with MSE metric 44308.9942\n",
      "Epoch 36 batch 130 train Loss 277.3972 test Loss 110.9427 with MSE metric 44309.3514\n",
      "Epoch 36 batch 140 train Loss 277.1018 test Loss 110.8331 with MSE metric 44307.9419\n",
      "Epoch 36 batch 150 train Loss 276.8069 test Loss 110.7237 with MSE metric 44306.9068\n",
      "Epoch 36 batch 160 train Loss 276.5128 test Loss 110.6146 with MSE metric 44307.2725\n",
      "Epoch 36 batch 170 train Loss 276.2192 test Loss 110.5057 with MSE metric 44306.1406\n",
      "Epoch 36 batch 180 train Loss 275.9263 test Loss 110.3971 with MSE metric 44304.8760\n",
      "Epoch 36 batch 190 train Loss 275.6342 test Loss 110.2887 with MSE metric 44303.5625\n",
      "Epoch 36 batch 200 train Loss 275.3426 test Loss 110.1806 with MSE metric 44303.4807\n",
      "Epoch 36 batch 210 train Loss 275.0516 test Loss 110.0726 with MSE metric 44301.9333\n",
      "Epoch 36 batch 220 train Loss 274.7613 test Loss 109.9649 with MSE metric 44300.6103\n",
      "Epoch 36 batch 230 train Loss 274.4718 test Loss 109.8574 with MSE metric 44300.1058\n",
      "Epoch 36 batch 240 train Loss 274.1826 test Loss 109.7502 with MSE metric 44299.4267\n",
      "Time taken for 1 epoch: 29.93765091896057 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 273.8941 test Loss 109.6432 with MSE metric 44297.5856\n",
      "Epoch 37 batch 10 train Loss 273.6063 test Loss 109.5363 with MSE metric 44296.6035\n",
      "Epoch 37 batch 20 train Loss 273.3199 test Loss 109.4298 with MSE metric 44294.8407\n",
      "Epoch 37 batch 30 train Loss 273.0332 test Loss 109.3234 with MSE metric 44294.3430\n",
      "Epoch 37 batch 40 train Loss 272.7472 test Loss 109.2173 with MSE metric 44293.7683\n",
      "Epoch 37 batch 50 train Loss 272.4619 test Loss 109.1114 with MSE metric 44292.9573\n",
      "Epoch 37 batch 60 train Loss 272.1771 test Loss 109.0057 with MSE metric 44292.9156\n",
      "Epoch 37 batch 70 train Loss 271.8928 test Loss 108.9002 with MSE metric 44292.3996\n",
      "Epoch 37 batch 80 train Loss 271.6093 test Loss 108.7950 with MSE metric 44290.9496\n",
      "Epoch 37 batch 90 train Loss 271.3262 test Loss 108.6900 with MSE metric 44289.6007\n",
      "Epoch 37 batch 100 train Loss 271.0440 test Loss 108.5852 with MSE metric 44290.1560\n",
      "Epoch 37 batch 110 train Loss 270.7623 test Loss 108.4806 with MSE metric 44290.9327\n",
      "Epoch 37 batch 120 train Loss 270.4812 test Loss 108.3763 with MSE metric 44289.6274\n",
      "Epoch 37 batch 130 train Loss 270.2006 test Loss 108.2721 with MSE metric 44289.2790\n",
      "Epoch 37 batch 140 train Loss 269.9206 test Loss 108.1682 with MSE metric 44290.4066\n",
      "Epoch 37 batch 150 train Loss 269.6412 test Loss 108.0645 with MSE metric 44289.1958\n",
      "Epoch 37 batch 160 train Loss 269.3623 test Loss 107.9610 with MSE metric 44289.8178\n",
      "Epoch 37 batch 170 train Loss 269.0843 test Loss 107.8577 with MSE metric 44288.8699\n",
      "Epoch 37 batch 180 train Loss 268.8067 test Loss 107.7546 with MSE metric 44287.8802\n",
      "Epoch 37 batch 190 train Loss 268.5297 test Loss 107.6517 with MSE metric 44287.3426\n",
      "Epoch 37 batch 200 train Loss 268.2534 test Loss 107.5490 with MSE metric 44286.5428\n",
      "Epoch 37 batch 210 train Loss 267.9776 test Loss 107.4466 with MSE metric 44285.8722\n",
      "Epoch 37 batch 220 train Loss 267.7023 test Loss 107.3444 with MSE metric 44285.2446\n",
      "Epoch 37 batch 230 train Loss 267.4276 test Loss 107.2424 with MSE metric 44283.8042\n",
      "Epoch 37 batch 240 train Loss 267.1535 test Loss 107.1406 with MSE metric 44283.8780\n",
      "Time taken for 1 epoch: 32.63907194137573 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 266.8799 test Loss 107.0390 with MSE metric 44282.0343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 batch 10 train Loss 266.6070 test Loss 106.9376 with MSE metric 44281.3151\n",
      "Epoch 38 batch 20 train Loss 266.3348 test Loss 106.8364 with MSE metric 44281.0579\n",
      "Epoch 38 batch 30 train Loss 266.0629 test Loss 106.7355 with MSE metric 44280.2183\n",
      "Epoch 38 batch 40 train Loss 265.7916 test Loss 106.6347 with MSE metric 44279.0542\n",
      "Epoch 38 batch 50 train Loss 265.5209 test Loss 106.5341 with MSE metric 44279.6080\n",
      "Epoch 38 batch 60 train Loss 265.2507 test Loss 106.4338 with MSE metric 44278.0860\n",
      "Epoch 38 batch 70 train Loss 264.9811 test Loss 106.3336 with MSE metric 44276.4217\n",
      "Epoch 38 batch 80 train Loss 264.7121 test Loss 106.2337 with MSE metric 44275.2926\n",
      "Epoch 38 batch 90 train Loss 264.4436 test Loss 106.1339 with MSE metric 44274.6828\n",
      "Epoch 38 batch 100 train Loss 264.1757 test Loss 106.0344 with MSE metric 44273.8267\n",
      "Epoch 38 batch 110 train Loss 263.9085 test Loss 105.9350 with MSE metric 44273.1122\n",
      "Epoch 38 batch 120 train Loss 263.6417 test Loss 105.8359 with MSE metric 44271.9023\n",
      "Epoch 38 batch 130 train Loss 263.3755 test Loss 105.7369 with MSE metric 44271.1053\n",
      "Epoch 38 batch 140 train Loss 263.1103 test Loss 105.6382 with MSE metric 44270.4151\n",
      "Epoch 38 batch 150 train Loss 262.8453 test Loss 105.5396 with MSE metric 44269.8211\n",
      "Epoch 38 batch 160 train Loss 262.5807 test Loss 105.4413 with MSE metric 44268.6953\n",
      "Epoch 38 batch 170 train Loss 262.3167 test Loss 105.3431 with MSE metric 44269.0247\n",
      "Epoch 38 batch 180 train Loss 262.0532 test Loss 105.2452 with MSE metric 44268.7948\n",
      "Epoch 38 batch 190 train Loss 261.7902 test Loss 105.1474 with MSE metric 44266.9701\n",
      "Epoch 38 batch 200 train Loss 261.5278 test Loss 105.0498 with MSE metric 44266.0274\n",
      "Epoch 38 batch 210 train Loss 261.2660 test Loss 104.9525 with MSE metric 44265.5728\n",
      "Epoch 38 batch 220 train Loss 261.0047 test Loss 104.8553 with MSE metric 44264.4439\n",
      "Epoch 38 batch 230 train Loss 260.7439 test Loss 104.7583 with MSE metric 44263.9222\n",
      "Epoch 38 batch 240 train Loss 260.4837 test Loss 104.6616 with MSE metric 44264.8702\n",
      "Time taken for 1 epoch: 33.288737058639526 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 260.2241 test Loss 104.5650 with MSE metric 44264.4590\n",
      "Epoch 39 batch 10 train Loss 259.9653 test Loss 104.4686 with MSE metric 44265.1907\n",
      "Epoch 39 batch 20 train Loss 259.7066 test Loss 104.3724 with MSE metric 44264.0552\n",
      "Epoch 39 batch 30 train Loss 259.4485 test Loss 104.2764 with MSE metric 44263.9913\n",
      "Epoch 39 batch 40 train Loss 259.1912 test Loss 104.1806 with MSE metric 44262.8369\n",
      "Epoch 39 batch 50 train Loss 258.9342 test Loss 104.0850 with MSE metric 44261.9097\n",
      "Epoch 39 batch 60 train Loss 258.6777 test Loss 103.9896 with MSE metric 44261.1320\n",
      "Epoch 39 batch 70 train Loss 258.4216 test Loss 103.8944 with MSE metric 44260.6871\n",
      "Epoch 39 batch 80 train Loss 258.1661 test Loss 103.7993 with MSE metric 44259.5090\n",
      "Epoch 39 batch 90 train Loss 257.9115 test Loss 103.7045 with MSE metric 44258.0243\n",
      "Epoch 39 batch 100 train Loss 257.6570 test Loss 103.6098 with MSE metric 44256.1147\n",
      "Epoch 39 batch 110 train Loss 257.4031 test Loss 103.5153 with MSE metric 44255.8874\n",
      "Epoch 39 batch 120 train Loss 257.1496 test Loss 103.4211 with MSE metric 44254.0409\n",
      "Epoch 39 batch 130 train Loss 256.8967 test Loss 103.3270 with MSE metric 44252.3346\n",
      "Epoch 39 batch 140 train Loss 256.6443 test Loss 103.2331 with MSE metric 44250.7497\n",
      "Epoch 39 batch 150 train Loss 256.3923 test Loss 103.1394 with MSE metric 44249.3169\n",
      "Epoch 39 batch 160 train Loss 256.1410 test Loss 103.0458 with MSE metric 44247.8565\n",
      "Epoch 39 batch 170 train Loss 255.8900 test Loss 102.9524 with MSE metric 44247.7978\n",
      "Epoch 39 batch 180 train Loss 255.6396 test Loss 102.8593 with MSE metric 44246.6254\n",
      "Epoch 39 batch 190 train Loss 255.3904 test Loss 102.7663 with MSE metric 44246.0773\n",
      "Epoch 39 batch 200 train Loss 255.1415 test Loss 102.6735 with MSE metric 44244.7466\n",
      "Epoch 39 batch 210 train Loss 254.8927 test Loss 102.5809 with MSE metric 44244.7289\n",
      "Epoch 39 batch 220 train Loss 254.6442 test Loss 102.4885 with MSE metric 44244.2515\n",
      "Epoch 39 batch 230 train Loss 254.3963 test Loss 102.3962 with MSE metric 44243.2014\n",
      "Epoch 39 batch 240 train Loss 254.1489 test Loss 102.3042 with MSE metric 44242.6197\n",
      "Time taken for 1 epoch: 32.42719101905823 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 253.9020 test Loss 102.2123 with MSE metric 44241.3190\n",
      "Epoch 40 batch 10 train Loss 253.6557 test Loss 102.1206 with MSE metric 44239.6672\n",
      "Epoch 40 batch 20 train Loss 253.4099 test Loss 102.0291 with MSE metric 44238.6448\n",
      "Epoch 40 batch 30 train Loss 253.1644 test Loss 101.9377 with MSE metric 44237.8432\n",
      "Epoch 40 batch 40 train Loss 252.9196 test Loss 101.8466 with MSE metric 44236.8877\n",
      "Epoch 40 batch 50 train Loss 252.6751 test Loss 101.7556 with MSE metric 44236.7331\n",
      "Epoch 40 batch 60 train Loss 252.4312 test Loss 101.6648 with MSE metric 44236.9618\n",
      "Epoch 40 batch 70 train Loss 252.1877 test Loss 101.5742 with MSE metric 44235.4930\n",
      "Epoch 40 batch 80 train Loss 251.9448 test Loss 101.4837 with MSE metric 44234.6759\n",
      "Epoch 40 batch 90 train Loss 251.7022 test Loss 101.3934 with MSE metric 44232.7219\n",
      "Epoch 40 batch 100 train Loss 251.4601 test Loss 101.3033 with MSE metric 44232.2949\n",
      "Epoch 40 batch 110 train Loss 251.2185 test Loss 101.2134 with MSE metric 44231.5173\n",
      "Epoch 40 batch 120 train Loss 250.9774 test Loss 101.1236 with MSE metric 44230.1837\n",
      "Epoch 40 batch 130 train Loss 250.7368 test Loss 101.0340 with MSE metric 44230.0426\n",
      "Epoch 40 batch 140 train Loss 250.4966 test Loss 100.9446 with MSE metric 44228.8666\n",
      "Epoch 40 batch 150 train Loss 250.2569 test Loss 100.8554 with MSE metric 44228.0661\n",
      "Epoch 40 batch 160 train Loss 250.0178 test Loss 100.7663 with MSE metric 44228.3773\n",
      "Epoch 40 batch 170 train Loss 249.7791 test Loss 100.6774 with MSE metric 44228.0309\n",
      "Epoch 40 batch 180 train Loss 249.5409 test Loss 100.5887 with MSE metric 44227.2670\n",
      "Epoch 40 batch 190 train Loss 249.3030 test Loss 100.5001 with MSE metric 44225.0658\n",
      "Epoch 40 batch 200 train Loss 249.0657 test Loss 100.4117 with MSE metric 44224.5324\n",
      "Epoch 40 batch 210 train Loss 248.8288 test Loss 100.3235 with MSE metric 44223.6498\n",
      "Epoch 40 batch 220 train Loss 248.5924 test Loss 100.2354 with MSE metric 44222.9472\n",
      "Epoch 40 batch 230 train Loss 248.3564 test Loss 100.1475 with MSE metric 44222.1505\n",
      "Epoch 40 batch 240 train Loss 248.1209 test Loss 100.0598 with MSE metric 44221.2615\n",
      "Time taken for 1 epoch: 30.55947208404541 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 247.8859 test Loss 99.9723 with MSE metric 44220.8223\n",
      "Epoch 41 batch 10 train Loss 247.6513 test Loss 99.8849 with MSE metric 44219.3389\n",
      "Epoch 41 batch 20 train Loss 247.4171 test Loss 99.7976 with MSE metric 44218.4671\n",
      "Epoch 41 batch 30 train Loss 247.1835 test Loss 99.7106 with MSE metric 44217.6524\n",
      "Epoch 41 batch 40 train Loss 246.9503 test Loss 99.6237 with MSE metric 44216.3274\n",
      "Epoch 41 batch 50 train Loss 246.7175 test Loss 99.5369 with MSE metric 44216.1125\n",
      "Epoch 41 batch 60 train Loss 246.4854 test Loss 99.4504 with MSE metric 44215.7634\n",
      "Epoch 41 batch 70 train Loss 246.2536 test Loss 99.3640 with MSE metric 44215.1512\n",
      "Epoch 41 batch 80 train Loss 246.0220 test Loss 99.2777 with MSE metric 44213.7824\n",
      "Epoch 41 batch 90 train Loss 245.7911 test Loss 99.1916 with MSE metric 44212.5612\n",
      "Epoch 41 batch 100 train Loss 245.5606 test Loss 99.1057 with MSE metric 44210.3916\n",
      "Epoch 41 batch 110 train Loss 245.3305 test Loss 99.0199 with MSE metric 44209.9257\n",
      "Epoch 41 batch 120 train Loss 245.1010 test Loss 98.9343 with MSE metric 44210.2498\n",
      "Epoch 41 batch 130 train Loss 244.8718 test Loss 98.8489 with MSE metric 44209.8434\n",
      "Epoch 41 batch 140 train Loss 244.6429 test Loss 98.7636 with MSE metric 44208.3411\n",
      "Epoch 41 batch 150 train Loss 244.4146 test Loss 98.6785 with MSE metric 44206.9444\n",
      "Epoch 41 batch 160 train Loss 244.1867 test Loss 98.5935 with MSE metric 44206.3844\n",
      "Epoch 41 batch 170 train Loss 243.9591 test Loss 98.5087 with MSE metric 44205.4863\n",
      "Epoch 41 batch 180 train Loss 243.7321 test Loss 98.4241 with MSE metric 44204.6084\n",
      "Epoch 41 batch 190 train Loss 243.5056 test Loss 98.3396 with MSE metric 44202.9626\n",
      "Epoch 41 batch 200 train Loss 243.2794 test Loss 98.2553 with MSE metric 44201.6485\n",
      "Epoch 41 batch 210 train Loss 243.0537 test Loss 98.1711 with MSE metric 44201.2445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 batch 220 train Loss 242.8285 test Loss 98.0871 with MSE metric 44200.8859\n",
      "Epoch 41 batch 230 train Loss 242.6036 test Loss 98.0032 with MSE metric 44200.7658\n",
      "Epoch 41 batch 240 train Loss 242.3791 test Loss 97.9195 with MSE metric 44200.1878\n",
      "Time taken for 1 epoch: 33.621551752090454 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 242.1551 test Loss 97.8359 with MSE metric 44200.0555\n",
      "Epoch 42 batch 10 train Loss 241.9316 test Loss 97.7525 with MSE metric 44199.4589\n",
      "Epoch 42 batch 20 train Loss 241.7084 test Loss 97.6693 with MSE metric 44199.1145\n",
      "Epoch 42 batch 30 train Loss 241.4857 test Loss 97.5862 with MSE metric 44198.2599\n",
      "Epoch 42 batch 40 train Loss 241.2634 test Loss 97.5032 with MSE metric 44196.2663\n",
      "Epoch 42 batch 50 train Loss 241.0415 test Loss 97.4205 with MSE metric 44195.1520\n",
      "Epoch 42 batch 60 train Loss 240.8200 test Loss 97.3378 with MSE metric 44194.8486\n",
      "Epoch 42 batch 70 train Loss 240.5990 test Loss 97.2553 with MSE metric 44194.6064\n",
      "Epoch 42 batch 80 train Loss 240.3788 test Loss 97.1730 with MSE metric 44193.1305\n",
      "Epoch 42 batch 90 train Loss 240.1586 test Loss 97.0909 with MSE metric 44191.9936\n",
      "Epoch 42 batch 100 train Loss 239.9388 test Loss 97.0089 with MSE metric 44192.2105\n",
      "Epoch 42 batch 110 train Loss 239.7194 test Loss 96.9270 with MSE metric 44191.5003\n",
      "Epoch 42 batch 120 train Loss 239.5004 test Loss 96.8453 with MSE metric 44190.5800\n",
      "Epoch 42 batch 130 train Loss 239.2818 test Loss 96.7638 with MSE metric 44188.9190\n",
      "Epoch 42 batch 140 train Loss 239.0636 test Loss 96.6824 with MSE metric 44188.6206\n",
      "Epoch 42 batch 150 train Loss 238.8460 test Loss 96.6011 with MSE metric 44187.4135\n",
      "Epoch 42 batch 160 train Loss 238.6287 test Loss 96.5200 with MSE metric 44187.4582\n",
      "Epoch 42 batch 170 train Loss 238.4116 test Loss 96.4390 with MSE metric 44186.8488\n",
      "Epoch 42 batch 180 train Loss 238.1951 test Loss 96.3582 with MSE metric 44185.8520\n",
      "Epoch 42 batch 190 train Loss 237.9789 test Loss 96.2776 with MSE metric 44185.9766\n",
      "Epoch 42 batch 200 train Loss 237.7631 test Loss 96.1970 with MSE metric 44184.6833\n",
      "Epoch 42 batch 210 train Loss 237.5477 test Loss 96.1166 with MSE metric 44184.1209\n",
      "Epoch 42 batch 220 train Loss 237.3328 test Loss 96.0364 with MSE metric 44183.3255\n",
      "Epoch 42 batch 230 train Loss 237.1182 test Loss 95.9563 with MSE metric 44182.0264\n",
      "Epoch 42 batch 240 train Loss 236.9041 test Loss 95.8763 with MSE metric 44180.0684\n",
      "Time taken for 1 epoch: 33.12707304954529 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 236.6902 test Loss 95.7965 with MSE metric 44178.6335\n",
      "Epoch 43 batch 10 train Loss 236.4769 test Loss 95.7168 with MSE metric 44177.4926\n",
      "Epoch 43 batch 20 train Loss 236.2639 test Loss 95.6373 with MSE metric 44176.4451\n",
      "Epoch 43 batch 30 train Loss 236.0514 test Loss 95.5579 with MSE metric 44175.7181\n",
      "Epoch 43 batch 40 train Loss 235.8392 test Loss 95.4786 with MSE metric 44174.7513\n",
      "Epoch 43 batch 50 train Loss 235.6274 test Loss 95.3995 with MSE metric 44173.3290\n",
      "Epoch 43 batch 60 train Loss 235.4160 test Loss 95.3206 with MSE metric 44171.4713\n",
      "Epoch 43 batch 70 train Loss 235.2050 test Loss 95.2418 with MSE metric 44170.6628\n",
      "Epoch 43 batch 80 train Loss 234.9943 test Loss 95.1631 with MSE metric 44169.3941\n",
      "Epoch 43 batch 90 train Loss 234.7841 test Loss 95.0845 with MSE metric 44169.0801\n",
      "Epoch 43 batch 100 train Loss 234.5742 test Loss 95.0061 with MSE metric 44167.6645\n",
      "Epoch 43 batch 110 train Loss 234.3647 test Loss 94.9278 with MSE metric 44166.2373\n",
      "Epoch 43 batch 120 train Loss 234.1556 test Loss 94.8497 with MSE metric 44165.0383\n",
      "Epoch 43 batch 130 train Loss 233.9468 test Loss 94.7717 with MSE metric 44163.2692\n",
      "Epoch 43 batch 140 train Loss 233.7385 test Loss 94.6938 with MSE metric 44161.9758\n",
      "Epoch 43 batch 150 train Loss 233.5305 test Loss 94.6161 with MSE metric 44160.9732\n",
      "Epoch 43 batch 160 train Loss 233.3229 test Loss 94.5385 with MSE metric 44159.0930\n",
      "Epoch 43 batch 170 train Loss 233.1158 test Loss 94.4610 with MSE metric 44158.6332\n",
      "Epoch 43 batch 180 train Loss 232.9090 test Loss 94.3837 with MSE metric 44157.0270\n",
      "Epoch 43 batch 190 train Loss 232.7026 test Loss 94.3066 with MSE metric 44155.6314\n",
      "Epoch 43 batch 200 train Loss 232.4965 test Loss 94.2295 with MSE metric 44154.2341\n",
      "Epoch 43 batch 210 train Loss 232.2908 test Loss 94.1526 with MSE metric 44154.2762\n",
      "Epoch 43 batch 220 train Loss 232.0855 test Loss 94.0759 with MSE metric 44152.8231\n",
      "Epoch 43 batch 230 train Loss 231.8806 test Loss 93.9992 with MSE metric 44151.3461\n",
      "Epoch 43 batch 240 train Loss 231.6760 test Loss 93.9227 with MSE metric 44149.8299\n",
      "Time taken for 1 epoch: 31.869013786315918 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 231.4718 test Loss 93.8464 with MSE metric 44149.7274\n",
      "Epoch 44 batch 10 train Loss 231.2680 test Loss 93.7701 with MSE metric 44148.9147\n",
      "Epoch 44 batch 20 train Loss 231.0645 test Loss 93.6941 with MSE metric 44147.2564\n",
      "Epoch 44 batch 30 train Loss 230.8614 test Loss 93.6181 with MSE metric 44146.7435\n",
      "Epoch 44 batch 40 train Loss 230.6587 test Loss 93.5423 with MSE metric 44146.6256\n",
      "Epoch 44 batch 50 train Loss 230.4563 test Loss 93.4666 with MSE metric 44145.7459\n",
      "Epoch 44 batch 60 train Loss 230.2545 test Loss 93.3910 with MSE metric 44145.0781\n",
      "Epoch 44 batch 70 train Loss 230.0528 test Loss 93.3156 with MSE metric 44145.2275\n",
      "Epoch 44 batch 80 train Loss 229.8516 test Loss 93.2403 with MSE metric 44142.7437\n",
      "Epoch 44 batch 90 train Loss 229.6507 test Loss 93.1651 with MSE metric 44141.6895\n",
      "Epoch 44 batch 100 train Loss 229.4502 test Loss 93.0900 with MSE metric 44141.1169\n",
      "Epoch 44 batch 110 train Loss 229.2500 test Loss 93.0151 with MSE metric 44141.3896\n",
      "Epoch 44 batch 120 train Loss 229.0501 test Loss 92.9404 with MSE metric 44140.2872\n",
      "Epoch 44 batch 130 train Loss 228.8506 test Loss 92.8657 with MSE metric 44138.6801\n",
      "Epoch 44 batch 140 train Loss 228.6515 test Loss 92.7912 with MSE metric 44138.0042\n",
      "Epoch 44 batch 150 train Loss 228.4528 test Loss 92.7168 with MSE metric 44135.1291\n",
      "Epoch 44 batch 160 train Loss 228.2544 test Loss 92.6426 with MSE metric 44134.1952\n",
      "Epoch 44 batch 170 train Loss 228.0563 test Loss 92.5685 with MSE metric 44132.9806\n",
      "Epoch 44 batch 180 train Loss 227.8585 test Loss 92.4945 with MSE metric 44129.9665\n",
      "Epoch 44 batch 190 train Loss 227.6612 test Loss 92.4206 with MSE metric 44128.8531\n",
      "Epoch 44 batch 200 train Loss 227.4642 test Loss 92.3468 with MSE metric 44127.5864\n",
      "Epoch 44 batch 210 train Loss 227.2675 test Loss 92.2732 with MSE metric 44126.5566\n",
      "Epoch 44 batch 220 train Loss 227.0712 test Loss 92.1997 with MSE metric 44125.5749\n",
      "Epoch 44 batch 230 train Loss 226.8752 test Loss 92.1263 with MSE metric 44124.5601\n",
      "Epoch 44 batch 240 train Loss 226.6797 test Loss 92.0531 with MSE metric 44125.0060\n",
      "Time taken for 1 epoch: 31.517754793167114 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 226.4845 test Loss 91.9800 with MSE metric 44123.4724\n",
      "Epoch 45 batch 10 train Loss 226.2896 test Loss 91.9070 with MSE metric 44122.6808\n",
      "Epoch 45 batch 20 train Loss 226.0952 test Loss 91.8342 with MSE metric 44121.4780\n",
      "Epoch 45 batch 30 train Loss 225.9010 test Loss 91.7615 with MSE metric 44120.8613\n",
      "Epoch 45 batch 40 train Loss 225.7071 test Loss 91.6889 with MSE metric 44119.1131\n",
      "Epoch 45 batch 50 train Loss 225.5136 test Loss 91.6164 with MSE metric 44118.8455\n",
      "Epoch 45 batch 60 train Loss 225.3203 test Loss 91.5441 with MSE metric 44116.8436\n",
      "Epoch 45 batch 70 train Loss 225.1274 test Loss 91.4718 with MSE metric 44114.5110\n",
      "Epoch 45 batch 80 train Loss 224.9349 test Loss 91.3997 with MSE metric 44113.7890\n",
      "Epoch 45 batch 90 train Loss 224.7427 test Loss 91.3277 with MSE metric 44111.3984\n",
      "Epoch 45 batch 100 train Loss 224.5508 test Loss 91.2559 with MSE metric 44109.8675\n",
      "Epoch 45 batch 110 train Loss 224.3593 test Loss 91.1841 with MSE metric 44109.5829\n",
      "Epoch 45 batch 120 train Loss 224.1682 test Loss 91.1125 with MSE metric 44108.1699\n",
      "Epoch 45 batch 130 train Loss 223.9773 test Loss 91.0410 with MSE metric 44107.7133\n",
      "Epoch 45 batch 140 train Loss 223.7868 test Loss 90.9696 with MSE metric 44107.0000\n",
      "Epoch 45 batch 150 train Loss 223.5966 test Loss 90.8984 with MSE metric 44105.2737\n",
      "Epoch 45 batch 160 train Loss 223.4069 test Loss 90.8272 with MSE metric 44104.0978\n",
      "Epoch 45 batch 170 train Loss 223.2174 test Loss 90.7562 with MSE metric 44102.9904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 batch 180 train Loss 223.0282 test Loss 90.6853 with MSE metric 44101.5011\n",
      "Epoch 45 batch 190 train Loss 222.8394 test Loss 90.6146 with MSE metric 44100.6974\n",
      "Epoch 45 batch 200 train Loss 222.6510 test Loss 90.5439 with MSE metric 44099.5101\n",
      "Epoch 45 batch 210 train Loss 222.4628 test Loss 90.4734 with MSE metric 44098.1245\n",
      "Epoch 45 batch 220 train Loss 222.2749 test Loss 90.4029 with MSE metric 44097.0320\n",
      "Epoch 45 batch 230 train Loss 222.0874 test Loss 90.3326 with MSE metric 44096.3366\n",
      "Epoch 45 batch 240 train Loss 221.9002 test Loss 90.2625 with MSE metric 44094.5095\n",
      "Time taken for 1 epoch: 33.040276288986206 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 221.7133 test Loss 90.1924 with MSE metric 44093.0937\n",
      "Epoch 46 batch 10 train Loss 221.5267 test Loss 90.1225 with MSE metric 44091.4200\n",
      "Epoch 46 batch 20 train Loss 221.3404 test Loss 90.0526 with MSE metric 44089.4272\n",
      "Epoch 46 batch 30 train Loss 221.1545 test Loss 89.9829 with MSE metric 44087.6704\n",
      "Epoch 46 batch 40 train Loss 220.9689 test Loss 89.9133 with MSE metric 44085.8873\n",
      "Epoch 46 batch 50 train Loss 220.7836 test Loss 89.8438 with MSE metric 44084.5413\n",
      "Epoch 46 batch 60 train Loss 220.5986 test Loss 89.7745 with MSE metric 44084.2292\n",
      "Epoch 46 batch 70 train Loss 220.4140 test Loss 89.7052 with MSE metric 44083.1551\n",
      "Epoch 46 batch 80 train Loss 220.2296 test Loss 89.6361 with MSE metric 44081.7022\n",
      "Epoch 46 batch 90 train Loss 220.0457 test Loss 89.5670 with MSE metric 44081.0737\n",
      "Epoch 46 batch 100 train Loss 219.8620 test Loss 89.4981 with MSE metric 44079.8102\n",
      "Epoch 46 batch 110 train Loss 219.6786 test Loss 89.4293 with MSE metric 44077.8945\n",
      "Epoch 46 batch 120 train Loss 219.4955 test Loss 89.3606 with MSE metric 44076.7422\n",
      "Epoch 46 batch 130 train Loss 219.3129 test Loss 89.2921 with MSE metric 44075.3843\n",
      "Epoch 46 batch 140 train Loss 219.1304 test Loss 89.2236 with MSE metric 44073.9651\n",
      "Epoch 46 batch 150 train Loss 218.9483 test Loss 89.1553 with MSE metric 44072.9436\n",
      "Epoch 46 batch 160 train Loss 218.7664 test Loss 89.0871 with MSE metric 44071.4618\n",
      "Epoch 46 batch 170 train Loss 218.5850 test Loss 89.0189 with MSE metric 44070.8005\n",
      "Epoch 46 batch 180 train Loss 218.4038 test Loss 88.9509 with MSE metric 44069.9641\n",
      "Epoch 46 batch 190 train Loss 218.2229 test Loss 88.8830 with MSE metric 44068.8582\n",
      "Epoch 46 batch 200 train Loss 218.0424 test Loss 88.8152 with MSE metric 44067.6283\n",
      "Epoch 46 batch 210 train Loss 217.8623 test Loss 88.7476 with MSE metric 44065.9127\n",
      "Epoch 46 batch 220 train Loss 217.6824 test Loss 88.6801 with MSE metric 44064.1582\n",
      "Epoch 46 batch 230 train Loss 217.5027 test Loss 88.6126 with MSE metric 44062.9305\n",
      "Epoch 46 batch 240 train Loss 217.3233 test Loss 88.5453 with MSE metric 44062.9746\n",
      "Time taken for 1 epoch: 31.011949062347412 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 217.1442 test Loss 88.4781 with MSE metric 44060.7639\n",
      "Epoch 47 batch 10 train Loss 216.9654 test Loss 88.4110 with MSE metric 44058.9695\n",
      "Epoch 47 batch 20 train Loss 216.7870 test Loss 88.3440 with MSE metric 44057.8283\n",
      "Epoch 47 batch 30 train Loss 216.6088 test Loss 88.2771 with MSE metric 44056.5806\n",
      "Epoch 47 batch 40 train Loss 216.4310 test Loss 88.2103 with MSE metric 44054.7489\n",
      "Epoch 47 batch 50 train Loss 216.2535 test Loss 88.1437 with MSE metric 44053.6128\n",
      "Epoch 47 batch 60 train Loss 216.0762 test Loss 88.0771 with MSE metric 44051.8764\n",
      "Epoch 47 batch 70 train Loss 215.8992 test Loss 88.0107 with MSE metric 44049.7983\n",
      "Epoch 47 batch 80 train Loss 215.7226 test Loss 87.9443 with MSE metric 44048.1402\n",
      "Epoch 47 batch 90 train Loss 215.5463 test Loss 87.8781 with MSE metric 44046.5615\n",
      "Epoch 47 batch 100 train Loss 215.3702 test Loss 87.8120 with MSE metric 44044.8843\n",
      "Epoch 47 batch 110 train Loss 215.1944 test Loss 87.7460 with MSE metric 44044.2802\n",
      "Epoch 47 batch 120 train Loss 215.0191 test Loss 87.6801 with MSE metric 44042.8478\n",
      "Epoch 47 batch 130 train Loss 214.8440 test Loss 87.6143 with MSE metric 44042.1005\n",
      "Epoch 47 batch 140 train Loss 214.6691 test Loss 87.5486 with MSE metric 44040.8183\n",
      "Epoch 47 batch 150 train Loss 214.4945 test Loss 87.4830 with MSE metric 44039.8121\n",
      "Epoch 47 batch 160 train Loss 214.3202 test Loss 87.4176 with MSE metric 44038.9089\n",
      "Epoch 47 batch 170 train Loss 214.1462 test Loss 87.3522 with MSE metric 44036.8354\n",
      "Epoch 47 batch 180 train Loss 213.9724 test Loss 87.2869 with MSE metric 44035.7625\n",
      "Epoch 47 batch 190 train Loss 213.7990 test Loss 87.2218 with MSE metric 44034.6874\n",
      "Epoch 47 batch 200 train Loss 213.6258 test Loss 87.1567 with MSE metric 44033.2088\n",
      "Epoch 47 batch 210 train Loss 213.4529 test Loss 87.0917 with MSE metric 44031.2126\n",
      "Epoch 47 batch 220 train Loss 213.2804 test Loss 87.0268 with MSE metric 44030.2011\n",
      "Epoch 47 batch 230 train Loss 213.1081 test Loss 86.9621 with MSE metric 44028.6736\n",
      "Epoch 47 batch 240 train Loss 212.9361 test Loss 86.8974 with MSE metric 44026.6710\n",
      "Time taken for 1 epoch: 31.84079623222351 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 212.7644 test Loss 86.8329 with MSE metric 44024.8405\n",
      "Epoch 48 batch 10 train Loss 212.5929 test Loss 86.7684 with MSE metric 44023.5235\n",
      "Epoch 48 batch 20 train Loss 212.4218 test Loss 86.7040 with MSE metric 44022.8148\n",
      "Epoch 48 batch 30 train Loss 212.2510 test Loss 86.6398 with MSE metric 44021.7463\n",
      "Epoch 48 batch 40 train Loss 212.0804 test Loss 86.5757 with MSE metric 44020.3698\n",
      "Epoch 48 batch 50 train Loss 211.9101 test Loss 86.5116 with MSE metric 44018.4045\n",
      "Epoch 48 batch 60 train Loss 211.7401 test Loss 86.4477 with MSE metric 44017.6237\n",
      "Epoch 48 batch 70 train Loss 211.5703 test Loss 86.3838 with MSE metric 44016.4899\n",
      "Epoch 48 batch 80 train Loss 211.4009 test Loss 86.3201 with MSE metric 44015.0179\n",
      "Epoch 48 batch 90 train Loss 211.2317 test Loss 86.2565 with MSE metric 44013.5394\n",
      "Epoch 48 batch 100 train Loss 211.0628 test Loss 86.1929 with MSE metric 44011.9281\n",
      "Epoch 48 batch 110 train Loss 210.8943 test Loss 86.1295 with MSE metric 44009.9233\n",
      "Epoch 48 batch 120 train Loss 210.7260 test Loss 86.0662 with MSE metric 44008.4332\n",
      "Epoch 48 batch 130 train Loss 210.5579 test Loss 86.0029 with MSE metric 44007.3379\n",
      "Epoch 48 batch 140 train Loss 210.3901 test Loss 85.9398 with MSE metric 44005.7455\n",
      "Epoch 48 batch 150 train Loss 210.2226 test Loss 85.8768 with MSE metric 44004.6245\n",
      "Epoch 48 batch 160 train Loss 210.0554 test Loss 85.8139 with MSE metric 44002.9767\n",
      "Epoch 48 batch 170 train Loss 209.8884 test Loss 85.7510 with MSE metric 44001.0329\n",
      "Epoch 48 batch 180 train Loss 209.7217 test Loss 85.6883 with MSE metric 44000.1619\n",
      "Epoch 48 batch 190 train Loss 209.5552 test Loss 85.6257 with MSE metric 43999.0776\n",
      "Epoch 48 batch 200 train Loss 209.3891 test Loss 85.5631 with MSE metric 43996.8495\n",
      "Epoch 48 batch 210 train Loss 209.2232 test Loss 85.5007 with MSE metric 43995.3758\n",
      "Epoch 48 batch 220 train Loss 209.0577 test Loss 85.4384 with MSE metric 43993.8374\n",
      "Epoch 48 batch 230 train Loss 208.8924 test Loss 85.3761 with MSE metric 43992.5598\n",
      "Epoch 48 batch 240 train Loss 208.7274 test Loss 85.3140 with MSE metric 43991.0288\n",
      "Time taken for 1 epoch: 31.3562970161438 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 208.5626 test Loss 85.2519 with MSE metric 43989.2086\n",
      "Epoch 49 batch 10 train Loss 208.3980 test Loss 85.1900 with MSE metric 43987.9699\n",
      "Epoch 49 batch 20 train Loss 208.2337 test Loss 85.1282 with MSE metric 43986.1418\n",
      "Epoch 49 batch 30 train Loss 208.0697 test Loss 85.0664 with MSE metric 43984.4799\n",
      "Epoch 49 batch 40 train Loss 207.9060 test Loss 85.0047 with MSE metric 43982.0884\n",
      "Epoch 49 batch 50 train Loss 207.7425 test Loss 84.9432 with MSE metric 43980.9463\n",
      "Epoch 49 batch 60 train Loss 207.5793 test Loss 84.8817 with MSE metric 43979.2208\n",
      "Epoch 49 batch 70 train Loss 207.4163 test Loss 84.8203 with MSE metric 43977.8558\n",
      "Epoch 49 batch 80 train Loss 207.2537 test Loss 84.7591 with MSE metric 43976.3805\n",
      "Epoch 49 batch 90 train Loss 207.0912 test Loss 84.6979 with MSE metric 43976.0231\n",
      "Epoch 49 batch 100 train Loss 206.9290 test Loss 84.6368 with MSE metric 43974.8148\n",
      "Epoch 49 batch 110 train Loss 206.7672 test Loss 84.5758 with MSE metric 43973.9336\n",
      "Epoch 49 batch 120 train Loss 206.6055 test Loss 84.5149 with MSE metric 43971.6465\n",
      "Epoch 49 batch 130 train Loss 206.4442 test Loss 84.4541 with MSE metric 43969.3327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 batch 140 train Loss 206.2832 test Loss 84.3934 with MSE metric 43967.4418\n",
      "Epoch 49 batch 150 train Loss 206.1224 test Loss 84.3328 with MSE metric 43966.2756\n",
      "Epoch 49 batch 160 train Loss 205.9618 test Loss 84.2723 with MSE metric 43964.9097\n",
      "Epoch 49 batch 170 train Loss 205.8014 test Loss 84.2119 with MSE metric 43963.0417\n",
      "Epoch 49 batch 180 train Loss 205.6413 test Loss 84.1516 with MSE metric 43961.4574\n",
      "Epoch 49 batch 190 train Loss 205.4816 test Loss 84.0914 with MSE metric 43960.0805\n",
      "Epoch 49 batch 200 train Loss 205.3220 test Loss 84.0313 with MSE metric 43957.7946\n",
      "Epoch 49 batch 210 train Loss 205.1627 test Loss 83.9712 with MSE metric 43955.5272\n",
      "Epoch 49 batch 220 train Loss 205.0035 test Loss 83.9113 with MSE metric 43954.2889\n",
      "Epoch 49 batch 230 train Loss 204.8447 test Loss 83.8514 with MSE metric 43952.7308\n",
      "Epoch 49 batch 240 train Loss 204.6862 test Loss 83.7916 with MSE metric 43951.1960\n",
      "Time taken for 1 epoch: 33.75829815864563 secs\n",
      "\n",
      "Epoch 50 batch 0 train Loss 204.5279 test Loss 83.7320 with MSE metric 43949.3619\n",
      "Epoch 50 batch 10 train Loss 204.3699 test Loss 83.6724 with MSE metric 43947.5338\n",
      "Epoch 50 batch 20 train Loss 204.2121 test Loss 83.6129 with MSE metric 43945.6055\n",
      "Epoch 50 batch 30 train Loss 204.0545 test Loss 83.5535 with MSE metric 43944.0121\n",
      "Epoch 50 batch 40 train Loss 203.8972 test Loss 83.4942 with MSE metric 43941.6410\n",
      "Epoch 50 batch 50 train Loss 203.7401 test Loss 83.4349 with MSE metric 43940.1793\n",
      "Epoch 50 batch 60 train Loss 203.5832 test Loss 83.3758 with MSE metric 43938.5313\n",
      "Epoch 50 batch 70 train Loss 203.4266 test Loss 83.3167 with MSE metric 43936.6647\n",
      "Epoch 50 batch 80 train Loss 203.2703 test Loss 83.2577 with MSE metric 43934.5733\n",
      "Epoch 50 batch 90 train Loss 203.1142 test Loss 83.1988 with MSE metric 43932.3335\n",
      "Epoch 50 batch 100 train Loss 202.9584 test Loss 83.1400 with MSE metric 43930.1688\n",
      "Epoch 50 batch 110 train Loss 202.8028 test Loss 83.0813 with MSE metric 43928.2311\n",
      "Epoch 50 batch 120 train Loss 202.6474 test Loss 83.0227 with MSE metric 43926.2042\n",
      "Epoch 50 batch 130 train Loss 202.4923 test Loss 82.9642 with MSE metric 43924.4945\n",
      "Epoch 50 batch 140 train Loss 202.3375 test Loss 82.9057 with MSE metric 43922.3981\n",
      "Epoch 50 batch 150 train Loss 202.1829 test Loss 82.8474 with MSE metric 43920.8013\n",
      "Epoch 50 batch 160 train Loss 202.0285 test Loss 82.7891 with MSE metric 43918.9594\n",
      "Epoch 50 batch 170 train Loss 201.8744 test Loss 82.7309 with MSE metric 43917.5746\n",
      "Epoch 50 batch 180 train Loss 201.7205 test Loss 82.6728 with MSE metric 43916.1582\n",
      "Epoch 50 batch 190 train Loss 201.5668 test Loss 82.6148 with MSE metric 43915.0323\n",
      "Epoch 50 batch 200 train Loss 201.4134 test Loss 82.5569 with MSE metric 43913.3935\n",
      "Epoch 50 batch 210 train Loss 201.2603 test Loss 82.4990 with MSE metric 43912.4971\n",
      "Epoch 50 batch 220 train Loss 201.1073 test Loss 82.4413 with MSE metric 43909.9305\n",
      "Epoch 50 batch 230 train Loss 200.9547 test Loss 82.3836 with MSE metric 43908.3070\n",
      "Epoch 50 batch 240 train Loss 200.8022 test Loss 82.3261 with MSE metric 43906.6740\n",
      "Time taken for 1 epoch: 30.503470182418823 secs\n",
      "\n",
      "Epoch 51 batch 0 train Loss 200.6500 test Loss 82.2686 with MSE metric 43904.8650\n",
      "Epoch 51 batch 10 train Loss 200.4981 test Loss 82.2112 with MSE metric 43903.2019\n",
      "Epoch 51 batch 20 train Loss 200.3463 test Loss 82.1539 with MSE metric 43901.1875\n",
      "Epoch 51 batch 30 train Loss 200.1952 test Loss 82.0967 with MSE metric 43899.3041\n",
      "Epoch 51 batch 40 train Loss 200.0440 test Loss 82.0396 with MSE metric 43897.0319\n",
      "Epoch 51 batch 50 train Loss 199.8929 test Loss 81.9825 with MSE metric 43895.7978\n",
      "Epoch 51 batch 60 train Loss 199.7422 test Loss 81.9256 with MSE metric 43894.3778\n",
      "Epoch 51 batch 70 train Loss 199.5918 test Loss 81.8687 with MSE metric 43892.6570\n",
      "Epoch 51 batch 80 train Loss 199.4414 test Loss 81.8120 with MSE metric 43890.5286\n",
      "Epoch 51 batch 90 train Loss 199.2914 test Loss 81.7553 with MSE metric 43888.8149\n",
      "Epoch 51 batch 100 train Loss 199.1415 test Loss 81.6987 with MSE metric 43887.5705\n",
      "Epoch 51 batch 110 train Loss 198.9919 test Loss 81.6422 with MSE metric 43885.9152\n",
      "Epoch 51 batch 120 train Loss 198.8425 test Loss 81.5858 with MSE metric 43883.9172\n",
      "Epoch 51 batch 130 train Loss 198.6933 test Loss 81.5294 with MSE metric 43881.8234\n",
      "Epoch 51 batch 140 train Loss 198.5443 test Loss 81.4732 with MSE metric 43879.2033\n",
      "Epoch 51 batch 150 train Loss 198.3956 test Loss 81.4170 with MSE metric 43877.9714\n",
      "Epoch 51 batch 160 train Loss 198.2472 test Loss 81.3609 with MSE metric 43875.7586\n",
      "Epoch 51 batch 170 train Loss 198.0989 test Loss 81.3049 with MSE metric 43873.5673\n",
      "Epoch 51 batch 180 train Loss 197.9509 test Loss 81.2489 with MSE metric 43871.8523\n",
      "Epoch 51 batch 190 train Loss 197.8032 test Loss 81.1931 with MSE metric 43869.4629\n",
      "Epoch 51 batch 200 train Loss 197.6557 test Loss 81.1373 with MSE metric 43868.0414\n",
      "Epoch 51 batch 210 train Loss 197.5084 test Loss 81.0817 with MSE metric 43865.8812\n",
      "Epoch 51 batch 220 train Loss 197.3613 test Loss 81.0260 with MSE metric 43863.6250\n",
      "Epoch 51 batch 230 train Loss 197.2143 test Loss 80.9705 with MSE metric 43861.2009\n",
      "Epoch 51 batch 240 train Loss 197.0677 test Loss 80.9151 with MSE metric 43859.8146\n",
      "Time taken for 1 epoch: 28.349923849105835 secs\n",
      "\n",
      "Epoch 52 batch 0 train Loss 196.9212 test Loss 80.8597 with MSE metric 43858.4506\n",
      "Epoch 52 batch 10 train Loss 196.7751 test Loss 80.8044 with MSE metric 43856.2242\n",
      "Epoch 52 batch 20 train Loss 196.6291 test Loss 80.7492 with MSE metric 43854.3221\n",
      "Epoch 52 batch 30 train Loss 196.4833 test Loss 80.6941 with MSE metric 43852.9784\n",
      "Epoch 52 batch 40 train Loss 196.3378 test Loss 80.6391 with MSE metric 43852.0118\n",
      "Epoch 52 batch 50 train Loss 196.1924 test Loss 80.5841 with MSE metric 43849.3785\n",
      "Epoch 52 batch 60 train Loss 196.0474 test Loss 80.5293 with MSE metric 43847.6157\n",
      "Epoch 52 batch 70 train Loss 195.9025 test Loss 80.4745 with MSE metric 43844.7767\n",
      "Epoch 52 batch 80 train Loss 195.7578 test Loss 80.4197 with MSE metric 43842.9796\n",
      "Epoch 52 batch 90 train Loss 195.6133 test Loss 80.3651 with MSE metric 43841.0441\n",
      "Epoch 52 batch 100 train Loss 195.4691 test Loss 80.3105 with MSE metric 43838.8139\n",
      "Epoch 52 batch 110 train Loss 195.3251 test Loss 80.2561 with MSE metric 43836.8640\n",
      "Epoch 52 batch 120 train Loss 195.1813 test Loss 80.2017 with MSE metric 43835.3043\n",
      "Epoch 52 batch 130 train Loss 195.0378 test Loss 80.1473 with MSE metric 43832.9281\n",
      "Epoch 52 batch 140 train Loss 194.8944 test Loss 80.0931 with MSE metric 43830.8829\n",
      "Epoch 52 batch 150 train Loss 194.7513 test Loss 80.0389 with MSE metric 43827.9559\n",
      "Epoch 52 batch 160 train Loss 194.6083 test Loss 79.9848 with MSE metric 43825.3926\n",
      "Epoch 52 batch 170 train Loss 194.4656 test Loss 79.9308 with MSE metric 43822.9552\n",
      "Epoch 52 batch 180 train Loss 194.3231 test Loss 79.8768 with MSE metric 43820.3946\n",
      "Epoch 52 batch 190 train Loss 194.1808 test Loss 79.8230 with MSE metric 43818.4430\n",
      "Epoch 52 batch 200 train Loss 194.0389 test Loss 79.7692 with MSE metric 43816.9433\n",
      "Epoch 52 batch 210 train Loss 193.8970 test Loss 79.7155 with MSE metric 43814.9046\n",
      "Epoch 52 batch 220 train Loss 193.7554 test Loss 79.6619 with MSE metric 43813.0890\n",
      "Epoch 52 batch 230 train Loss 193.6140 test Loss 79.6084 with MSE metric 43810.8684\n",
      "Epoch 52 batch 240 train Loss 193.4728 test Loss 79.5549 with MSE metric 43808.5364\n",
      "Time taken for 1 epoch: 29.446740865707397 secs\n",
      "\n",
      "Epoch 53 batch 0 train Loss 193.3318 test Loss 79.5016 with MSE metric 43806.4766\n",
      "Epoch 53 batch 10 train Loss 193.1910 test Loss 79.4482 with MSE metric 43803.9503\n",
      "Epoch 53 batch 20 train Loss 193.0504 test Loss 79.3950 with MSE metric 43802.6426\n",
      "Epoch 53 batch 30 train Loss 192.9101 test Loss 79.3419 with MSE metric 43800.6102\n",
      "Epoch 53 batch 40 train Loss 192.7699 test Loss 79.2888 with MSE metric 43798.6160\n",
      "Epoch 53 batch 50 train Loss 192.6300 test Loss 79.2358 with MSE metric 43796.6825\n",
      "Epoch 53 batch 60 train Loss 192.4902 test Loss 79.1829 with MSE metric 43795.0944\n",
      "Epoch 53 batch 70 train Loss 192.3507 test Loss 79.1300 with MSE metric 43792.9440\n",
      "Epoch 53 batch 80 train Loss 192.2114 test Loss 79.0772 with MSE metric 43791.0422\n",
      "Epoch 53 batch 90 train Loss 192.0723 test Loss 79.0245 with MSE metric 43788.8212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 batch 100 train Loss 191.9334 test Loss 78.9719 with MSE metric 43786.0568\n",
      "Epoch 53 batch 110 train Loss 191.7946 test Loss 78.9194 with MSE metric 43783.1163\n",
      "Epoch 53 batch 120 train Loss 191.6561 test Loss 78.8669 with MSE metric 43780.4839\n",
      "Epoch 53 batch 130 train Loss 191.5178 test Loss 78.8145 with MSE metric 43778.7929\n",
      "Epoch 53 batch 140 train Loss 191.3798 test Loss 78.7621 with MSE metric 43776.6475\n",
      "Epoch 53 batch 150 train Loss 191.2419 test Loss 78.7098 with MSE metric 43774.1866\n",
      "Epoch 53 batch 160 train Loss 191.1042 test Loss 78.6577 with MSE metric 43770.7261\n",
      "Epoch 53 batch 170 train Loss 190.9667 test Loss 78.6055 with MSE metric 43768.6448\n",
      "Epoch 53 batch 180 train Loss 190.8294 test Loss 78.5535 with MSE metric 43766.5369\n",
      "Epoch 53 batch 190 train Loss 190.6924 test Loss 78.5015 with MSE metric 43763.5039\n",
      "Epoch 53 batch 200 train Loss 190.5555 test Loss 78.4496 with MSE metric 43760.7491\n",
      "Epoch 53 batch 210 train Loss 190.4189 test Loss 78.3978 with MSE metric 43759.1940\n",
      "Epoch 53 batch 220 train Loss 190.2824 test Loss 78.3461 with MSE metric 43756.6820\n",
      "Epoch 53 batch 230 train Loss 190.1462 test Loss 78.2944 with MSE metric 43754.7606\n",
      "Epoch 53 batch 240 train Loss 190.0101 test Loss 78.2428 with MSE metric 43751.9831\n",
      "Time taken for 1 epoch: 28.73857092857361 secs\n",
      "\n",
      "Epoch 54 batch 0 train Loss 189.8743 test Loss 78.1913 with MSE metric 43749.5266\n",
      "Epoch 54 batch 10 train Loss 189.7387 test Loss 78.1399 with MSE metric 43746.3092\n",
      "Epoch 54 batch 20 train Loss 189.6032 test Loss 78.0885 with MSE metric 43743.4048\n",
      "Epoch 54 batch 30 train Loss 189.4680 test Loss 78.0372 with MSE metric 43740.4624\n",
      "Epoch 54 batch 40 train Loss 189.3329 test Loss 77.9860 with MSE metric 43737.4065\n",
      "Epoch 54 batch 50 train Loss 189.1981 test Loss 77.9348 with MSE metric 43735.2705\n",
      "Epoch 54 batch 60 train Loss 189.0634 test Loss 77.8837 with MSE metric 43733.1054\n",
      "Epoch 54 batch 70 train Loss 188.9290 test Loss 77.8327 with MSE metric 43730.5383\n",
      "Epoch 54 batch 80 train Loss 188.7947 test Loss 77.7818 with MSE metric 43728.0665\n",
      "Epoch 54 batch 90 train Loss 188.6606 test Loss 77.7309 with MSE metric 43725.7135\n",
      "Epoch 54 batch 100 train Loss 188.5268 test Loss 77.6801 with MSE metric 43723.0022\n",
      "Epoch 54 batch 110 train Loss 188.3931 test Loss 77.6294 with MSE metric 43720.4951\n",
      "Epoch 54 batch 120 train Loss 188.2596 test Loss 77.5788 with MSE metric 43718.0521\n",
      "Epoch 54 batch 130 train Loss 188.1263 test Loss 77.5282 with MSE metric 43715.6649\n",
      "Epoch 54 batch 140 train Loss 187.9933 test Loss 77.4777 with MSE metric 43713.0730\n",
      "Epoch 54 batch 150 train Loss 187.8604 test Loss 77.4272 with MSE metric 43710.4196\n",
      "Epoch 54 batch 160 train Loss 187.7277 test Loss 77.3769 with MSE metric 43707.2357\n",
      "Epoch 54 batch 170 train Loss 187.5951 test Loss 77.3266 with MSE metric 43704.5649\n",
      "Epoch 54 batch 180 train Loss 187.4628 test Loss 77.2764 with MSE metric 43702.8702\n",
      "Epoch 54 batch 190 train Loss 187.3308 test Loss 77.2262 with MSE metric 43700.7892\n",
      "Epoch 54 batch 200 train Loss 187.1988 test Loss 77.1761 with MSE metric 43698.9194\n",
      "Epoch 54 batch 210 train Loss 187.0671 test Loss 77.1261 with MSE metric 43696.0193\n",
      "Epoch 54 batch 220 train Loss 186.9356 test Loss 77.0762 with MSE metric 43694.1533\n",
      "Epoch 54 batch 230 train Loss 186.8042 test Loss 77.0263 with MSE metric 43691.0829\n",
      "Epoch 54 batch 240 train Loss 186.6730 test Loss 76.9765 with MSE metric 43687.9095\n",
      "Time taken for 1 epoch: 29.11249089241028 secs\n",
      "\n",
      "Epoch 55 batch 0 train Loss 186.5420 test Loss 76.9268 with MSE metric 43685.2027\n",
      "Epoch 55 batch 10 train Loss 186.4112 test Loss 76.8771 with MSE metric 43682.4188\n",
      "Epoch 55 batch 20 train Loss 186.2806 test Loss 76.8275 with MSE metric 43679.7524\n",
      "Epoch 55 batch 30 train Loss 186.1502 test Loss 76.7780 with MSE metric 43676.8364\n",
      "Epoch 55 batch 40 train Loss 186.0200 test Loss 76.7285 with MSE metric 43673.8439\n",
      "Epoch 55 batch 50 train Loss 185.8899 test Loss 76.6791 with MSE metric 43672.0184\n",
      "Epoch 55 batch 60 train Loss 185.7601 test Loss 76.6298 with MSE metric 43668.8093\n",
      "Epoch 55 batch 70 train Loss 185.6304 test Loss 76.5805 with MSE metric 43666.4294\n",
      "Epoch 55 batch 80 train Loss 185.5009 test Loss 76.5313 with MSE metric 43664.0074\n",
      "Epoch 55 batch 90 train Loss 185.3716 test Loss 76.4822 with MSE metric 43661.1757\n",
      "Epoch 55 batch 100 train Loss 185.2425 test Loss 76.4331 with MSE metric 43657.7570\n",
      "Epoch 55 batch 110 train Loss 185.1135 test Loss 76.3842 with MSE metric 43654.7223\n",
      "Epoch 55 batch 120 train Loss 184.9850 test Loss 76.3352 with MSE metric 43651.5493\n",
      "Epoch 55 batch 130 train Loss 184.8565 test Loss 76.2864 with MSE metric 43649.0913\n",
      "Epoch 55 batch 140 train Loss 184.7281 test Loss 76.2377 with MSE metric 43646.1840\n",
      "Epoch 55 batch 150 train Loss 184.5999 test Loss 76.1890 with MSE metric 43643.3045\n",
      "Epoch 55 batch 160 train Loss 184.4719 test Loss 76.1404 with MSE metric 43640.0280\n",
      "Epoch 55 batch 170 train Loss 184.3441 test Loss 76.0918 with MSE metric 43637.6280\n",
      "Epoch 55 batch 180 train Loss 184.2164 test Loss 76.0433 with MSE metric 43634.7114\n",
      "Epoch 55 batch 190 train Loss 184.0890 test Loss 75.9949 with MSE metric 43632.0788\n",
      "Epoch 55 batch 200 train Loss 183.9617 test Loss 75.9465 with MSE metric 43628.8685\n",
      "Epoch 55 batch 210 train Loss 183.8346 test Loss 75.8982 with MSE metric 43625.7771\n",
      "Epoch 55 batch 220 train Loss 183.7077 test Loss 75.8500 with MSE metric 43623.3212\n",
      "Epoch 55 batch 230 train Loss 183.5810 test Loss 75.8018 with MSE metric 43621.0151\n",
      "Epoch 55 batch 240 train Loss 183.4544 test Loss 75.7537 with MSE metric 43618.1138\n",
      "Time taken for 1 epoch: 26.767453908920288 secs\n",
      "\n",
      "Epoch 56 batch 0 train Loss 183.3281 test Loss 75.7057 with MSE metric 43615.2894\n",
      "Epoch 56 batch 10 train Loss 183.2018 test Loss 75.6577 with MSE metric 43611.5622\n",
      "Epoch 56 batch 20 train Loss 183.0758 test Loss 75.6098 with MSE metric 43608.4411\n",
      "Epoch 56 batch 30 train Loss 182.9500 test Loss 75.5619 with MSE metric 43605.1964\n",
      "Epoch 56 batch 40 train Loss 182.8243 test Loss 75.5141 with MSE metric 43602.6448\n",
      "Epoch 56 batch 50 train Loss 182.6988 test Loss 75.4664 with MSE metric 43599.2040\n",
      "Epoch 56 batch 60 train Loss 182.5735 test Loss 75.4187 with MSE metric 43596.6588\n",
      "Epoch 56 batch 70 train Loss 182.4484 test Loss 75.3712 with MSE metric 43594.3468\n",
      "Epoch 56 batch 80 train Loss 182.3235 test Loss 75.3237 with MSE metric 43591.6102\n",
      "Epoch 56 batch 90 train Loss 182.1987 test Loss 75.2762 with MSE metric 43588.8538\n",
      "Epoch 56 batch 100 train Loss 182.0740 test Loss 75.2288 with MSE metric 43585.4673\n",
      "Epoch 56 batch 110 train Loss 181.9496 test Loss 75.1815 with MSE metric 43582.3993\n",
      "Epoch 56 batch 120 train Loss 181.8254 test Loss 75.1342 with MSE metric 43579.1044\n",
      "Epoch 56 batch 130 train Loss 181.7013 test Loss 75.0870 with MSE metric 43575.9364\n",
      "Epoch 56 batch 140 train Loss 181.5774 test Loss 75.0398 with MSE metric 43572.9714\n",
      "Epoch 56 batch 150 train Loss 181.4536 test Loss 74.9928 with MSE metric 43569.8576\n",
      "Epoch 56 batch 160 train Loss 181.3301 test Loss 74.9457 with MSE metric 43567.0044\n",
      "Epoch 56 batch 170 train Loss 181.2067 test Loss 74.8988 with MSE metric 43564.2145\n",
      "Epoch 56 batch 180 train Loss 181.0835 test Loss 74.8519 with MSE metric 43560.2380\n",
      "Epoch 56 batch 190 train Loss 180.9604 test Loss 74.8050 with MSE metric 43557.0544\n",
      "Epoch 56 batch 200 train Loss 180.8376 test Loss 74.7583 with MSE metric 43553.9420\n",
      "Epoch 56 batch 210 train Loss 180.7149 test Loss 74.7116 with MSE metric 43550.8250\n",
      "Epoch 56 batch 220 train Loss 180.5923 test Loss 74.6649 with MSE metric 43547.0558\n",
      "Epoch 56 batch 230 train Loss 180.4700 test Loss 74.6183 with MSE metric 43543.3499\n",
      "Epoch 56 batch 240 train Loss 180.3478 test Loss 74.5718 with MSE metric 43540.3540\n",
      "Time taken for 1 epoch: 31.742398977279663 secs\n",
      "\n",
      "Epoch 57 batch 0 train Loss 180.2258 test Loss 74.5253 with MSE metric 43536.8852\n",
      "Epoch 57 batch 10 train Loss 180.1039 test Loss 74.4789 with MSE metric 43533.0149\n",
      "Epoch 57 batch 20 train Loss 179.9823 test Loss 74.4326 with MSE metric 43530.0612\n",
      "Epoch 57 batch 30 train Loss 179.8607 test Loss 74.3863 with MSE metric 43526.5547\n",
      "Epoch 57 batch 40 train Loss 179.7394 test Loss 74.3401 with MSE metric 43522.9639\n",
      "Epoch 57 batch 50 train Loss 179.6182 test Loss 74.2940 with MSE metric 43519.1685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 batch 60 train Loss 179.4972 test Loss 74.2479 with MSE metric 43516.1597\n",
      "Epoch 57 batch 70 train Loss 179.3764 test Loss 74.2018 with MSE metric 43512.3787\n",
      "Epoch 57 batch 80 train Loss 179.2557 test Loss 74.1558 with MSE metric 43508.0618\n",
      "Epoch 57 batch 90 train Loss 179.1352 test Loss 74.1099 with MSE metric 43504.2698\n",
      "Epoch 57 batch 100 train Loss 179.0149 test Loss 74.0641 with MSE metric 43500.6652\n",
      "Epoch 57 batch 110 train Loss 178.8947 test Loss 74.0183 with MSE metric 43497.0397\n",
      "Epoch 57 batch 120 train Loss 178.7747 test Loss 73.9725 with MSE metric 43493.8067\n",
      "Epoch 57 batch 130 train Loss 178.6549 test Loss 73.9269 with MSE metric 43490.1209\n",
      "Epoch 57 batch 140 train Loss 178.5352 test Loss 73.8812 with MSE metric 43486.7418\n",
      "Epoch 57 batch 150 train Loss 178.4157 test Loss 73.8357 with MSE metric 43482.7810\n",
      "Epoch 57 batch 160 train Loss 178.2964 test Loss 73.7902 with MSE metric 43479.2398\n",
      "Epoch 57 batch 170 train Loss 178.1772 test Loss 73.7448 with MSE metric 43475.4671\n",
      "Epoch 57 batch 180 train Loss 178.0582 test Loss 73.6994 with MSE metric 43472.4251\n",
      "Epoch 57 batch 190 train Loss 177.9394 test Loss 73.6541 with MSE metric 43468.8613\n",
      "Epoch 57 batch 200 train Loss 177.8207 test Loss 73.6089 with MSE metric 43464.9967\n",
      "Epoch 57 batch 210 train Loss 177.7022 test Loss 73.5637 with MSE metric 43461.8264\n",
      "Epoch 57 batch 220 train Loss 177.5838 test Loss 73.5186 with MSE metric 43458.0320\n",
      "Epoch 57 batch 230 train Loss 177.4656 test Loss 73.4736 with MSE metric 43454.9173\n",
      "Epoch 57 batch 240 train Loss 177.3476 test Loss 73.4286 with MSE metric 43451.3658\n",
      "Time taken for 1 epoch: 29.905468940734863 secs\n",
      "\n",
      "Epoch 58 batch 0 train Loss 177.2297 test Loss 73.3836 with MSE metric 43448.2033\n",
      "Epoch 58 batch 10 train Loss 177.1120 test Loss 73.3387 with MSE metric 43444.6326\n",
      "Epoch 58 batch 20 train Loss 176.9945 test Loss 73.2939 with MSE metric 43441.6868\n",
      "Epoch 58 batch 30 train Loss 176.8771 test Loss 73.2492 with MSE metric 43438.0696\n",
      "Epoch 58 batch 40 train Loss 176.7599 test Loss 73.2045 with MSE metric 43434.4981\n",
      "Epoch 58 batch 50 train Loss 176.6429 test Loss 73.1598 with MSE metric 43430.8217\n",
      "Epoch 58 batch 60 train Loss 176.5259 test Loss 73.1152 with MSE metric 43427.1369\n",
      "Epoch 58 batch 70 train Loss 176.4092 test Loss 73.0707 with MSE metric 43423.1432\n",
      "Epoch 58 batch 80 train Loss 176.2926 test Loss 73.0262 with MSE metric 43419.4013\n",
      "Epoch 58 batch 90 train Loss 176.1762 test Loss 72.9818 with MSE metric 43415.6381\n",
      "Epoch 58 batch 100 train Loss 176.0599 test Loss 72.9374 with MSE metric 43411.6572\n",
      "Epoch 58 batch 110 train Loss 175.9438 test Loss 72.8931 with MSE metric 43407.5564\n",
      "Epoch 58 batch 120 train Loss 175.8279 test Loss 72.8489 with MSE metric 43403.4464\n",
      "Epoch 58 batch 130 train Loss 175.7121 test Loss 72.8047 with MSE metric 43399.3165\n",
      "Epoch 58 batch 140 train Loss 175.5964 test Loss 72.7606 with MSE metric 43395.0380\n",
      "Epoch 58 batch 150 train Loss 175.4809 test Loss 72.7165 with MSE metric 43391.5018\n",
      "Epoch 58 batch 160 train Loss 175.3656 test Loss 72.6725 with MSE metric 43387.9000\n",
      "Epoch 58 batch 170 train Loss 175.2505 test Loss 72.6285 with MSE metric 43384.2617\n",
      "Epoch 58 batch 180 train Loss 175.1355 test Loss 72.5847 with MSE metric 43380.1016\n",
      "Epoch 58 batch 190 train Loss 175.0207 test Loss 72.5408 with MSE metric 43376.5708\n",
      "Epoch 58 batch 200 train Loss 174.9060 test Loss 72.4971 with MSE metric 43372.5858\n",
      "Epoch 58 batch 210 train Loss 174.7914 test Loss 72.4534 with MSE metric 43368.7030\n",
      "Epoch 58 batch 220 train Loss 174.6770 test Loss 72.4097 with MSE metric 43364.7422\n",
      "Epoch 58 batch 230 train Loss 174.5628 test Loss 72.3661 with MSE metric 43360.3851\n",
      "Epoch 58 batch 240 train Loss 174.4487 test Loss 72.3225 with MSE metric 43356.1732\n",
      "Time taken for 1 epoch: 28.315624952316284 secs\n",
      "\n",
      "Epoch 59 batch 0 train Loss 174.3347 test Loss 72.2790 with MSE metric 43352.0859\n",
      "Epoch 59 batch 10 train Loss 174.2210 test Loss 72.2355 with MSE metric 43348.5863\n",
      "Epoch 59 batch 20 train Loss 174.1073 test Loss 72.1922 with MSE metric 43344.8289\n",
      "Epoch 59 batch 30 train Loss 173.9939 test Loss 72.1488 with MSE metric 43340.1343\n",
      "Epoch 59 batch 40 train Loss 173.8805 test Loss 72.1055 with MSE metric 43335.9208\n",
      "Epoch 59 batch 50 train Loss 173.7674 test Loss 72.0623 with MSE metric 43331.4245\n",
      "Epoch 59 batch 60 train Loss 173.6543 test Loss 72.0191 with MSE metric 43327.1411\n",
      "Epoch 59 batch 70 train Loss 173.5415 test Loss 71.9760 with MSE metric 43322.9119\n",
      "Epoch 59 batch 80 train Loss 173.4288 test Loss 71.9330 with MSE metric 43318.9039\n",
      "Epoch 59 batch 90 train Loss 173.3162 test Loss 71.8900 with MSE metric 43314.7062\n",
      "Epoch 59 batch 100 train Loss 173.2038 test Loss 71.8470 with MSE metric 43310.7232\n",
      "Epoch 59 batch 110 train Loss 173.0916 test Loss 71.8041 with MSE metric 43306.6079\n",
      "Epoch 59 batch 120 train Loss 172.9794 test Loss 71.7613 with MSE metric 43301.9426\n",
      "Epoch 59 batch 130 train Loss 172.8675 test Loss 71.7185 with MSE metric 43297.9795\n",
      "Epoch 59 batch 140 train Loss 172.7557 test Loss 71.6758 with MSE metric 43293.4728\n",
      "Epoch 59 batch 150 train Loss 172.6440 test Loss 71.6331 with MSE metric 43289.2798\n",
      "Epoch 59 batch 160 train Loss 172.5325 test Loss 71.5905 with MSE metric 43285.0726\n",
      "Epoch 59 batch 170 train Loss 172.4212 test Loss 71.5479 with MSE metric 43280.6683\n",
      "Epoch 59 batch 180 train Loss 172.3099 test Loss 71.5054 with MSE metric 43276.4463\n",
      "Epoch 59 batch 190 train Loss 172.1989 test Loss 71.4630 with MSE metric 43271.5710\n",
      "Epoch 59 batch 200 train Loss 172.0880 test Loss 71.4206 with MSE metric 43267.2065\n",
      "Epoch 59 batch 210 train Loss 171.9772 test Loss 71.3782 with MSE metric 43262.7861\n",
      "Epoch 59 batch 220 train Loss 171.8666 test Loss 71.3359 with MSE metric 43258.6658\n",
      "Epoch 59 batch 230 train Loss 171.7561 test Loss 71.2937 with MSE metric 43253.9281\n",
      "Epoch 59 batch 240 train Loss 171.6458 test Loss 71.2515 with MSE metric 43249.7412\n",
      "Time taken for 1 epoch: 26.753772974014282 secs\n",
      "\n",
      "Epoch 60 batch 0 train Loss 171.5356 test Loss 71.2094 with MSE metric 43244.8727\n",
      "Epoch 60 batch 10 train Loss 171.4255 test Loss 71.1673 with MSE metric 43240.0927\n",
      "Epoch 60 batch 20 train Loss 171.3156 test Loss 71.1253 with MSE metric 43235.8022\n",
      "Epoch 60 batch 30 train Loss 171.2059 test Loss 71.0833 with MSE metric 43231.6479\n",
      "Epoch 60 batch 40 train Loss 171.0963 test Loss 71.0414 with MSE metric 43226.9156\n",
      "Epoch 60 batch 50 train Loss 170.9868 test Loss 70.9996 with MSE metric 43222.1819\n",
      "Epoch 60 batch 60 train Loss 170.8775 test Loss 70.9577 with MSE metric 43217.6738\n",
      "Epoch 60 batch 70 train Loss 170.7683 test Loss 70.9160 with MSE metric 43213.1987\n",
      "Epoch 60 batch 80 train Loss 170.6593 test Loss 70.8743 with MSE metric 43208.1839\n",
      "Epoch 60 batch 90 train Loss 170.5504 test Loss 70.8326 with MSE metric 43203.8348\n",
      "Epoch 60 batch 100 train Loss 170.4417 test Loss 70.7910 with MSE metric 43199.4644\n",
      "Epoch 60 batch 110 train Loss 170.3330 test Loss 70.7495 with MSE metric 43194.8968\n",
      "Epoch 60 batch 120 train Loss 170.2246 test Loss 70.7080 with MSE metric 43189.9520\n",
      "Epoch 60 batch 130 train Loss 170.1163 test Loss 70.6665 with MSE metric 43184.7971\n",
      "Epoch 60 batch 140 train Loss 170.0081 test Loss 70.6252 with MSE metric 43179.9653\n",
      "Epoch 60 batch 150 train Loss 169.9001 test Loss 70.5838 with MSE metric 43174.9743\n",
      "Epoch 60 batch 160 train Loss 169.7922 test Loss 70.5425 with MSE metric 43170.0820\n",
      "Epoch 60 batch 170 train Loss 169.6844 test Loss 70.5013 with MSE metric 43164.9952\n",
      "Epoch 60 batch 180 train Loss 169.5768 test Loss 70.4601 with MSE metric 43160.2009\n",
      "Epoch 60 batch 190 train Loss 169.4694 test Loss 70.4190 with MSE metric 43154.9692\n",
      "Epoch 60 batch 200 train Loss 169.3620 test Loss 70.3779 with MSE metric 43149.8042\n",
      "Epoch 60 batch 210 train Loss 169.2548 test Loss 70.3369 with MSE metric 43144.7235\n",
      "Epoch 60 batch 220 train Loss 169.1478 test Loss 70.2959 with MSE metric 43139.5101\n",
      "Epoch 60 batch 230 train Loss 169.0409 test Loss 70.2549 with MSE metric 43134.4401\n",
      "Epoch 60 batch 240 train Loss 168.9341 test Loss 70.2141 with MSE metric 43129.3470\n",
      "Time taken for 1 epoch: 28.824903964996338 secs\n",
      "\n",
      "Epoch 61 batch 0 train Loss 168.8275 test Loss 70.1732 with MSE metric 43124.0327\n",
      "Epoch 61 batch 10 train Loss 168.7210 test Loss 70.1325 with MSE metric 43119.0453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 batch 20 train Loss 168.6146 test Loss 70.0917 with MSE metric 43113.8015\n",
      "Epoch 61 batch 30 train Loss 168.5084 test Loss 70.0511 with MSE metric 43108.8442\n",
      "Epoch 61 batch 40 train Loss 168.4024 test Loss 70.0104 with MSE metric 43103.4229\n",
      "Epoch 61 batch 50 train Loss 168.2964 test Loss 69.9699 with MSE metric 43098.3316\n",
      "Epoch 61 batch 60 train Loss 168.1906 test Loss 69.9294 with MSE metric 43092.8549\n",
      "Epoch 61 batch 70 train Loss 168.0850 test Loss 69.8889 with MSE metric 43087.4941\n",
      "Epoch 61 batch 80 train Loss 167.9794 test Loss 69.8485 with MSE metric 43082.2711\n",
      "Epoch 61 batch 90 train Loss 167.8740 test Loss 69.8081 with MSE metric 43076.8959\n",
      "Epoch 61 batch 100 train Loss 167.7688 test Loss 69.7678 with MSE metric 43071.2955\n",
      "Epoch 61 batch 110 train Loss 167.6636 test Loss 69.7275 with MSE metric 43065.8990\n",
      "Epoch 61 batch 120 train Loss 167.5587 test Loss 69.6873 with MSE metric 43060.4506\n",
      "Epoch 61 batch 130 train Loss 167.4538 test Loss 69.6471 with MSE metric 43055.0496\n",
      "Epoch 61 batch 140 train Loss 167.3491 test Loss 69.6070 with MSE metric 43049.7267\n",
      "Epoch 61 batch 150 train Loss 167.2445 test Loss 69.5669 with MSE metric 43044.6137\n",
      "Epoch 61 batch 160 train Loss 167.1401 test Loss 69.5269 with MSE metric 43038.9544\n",
      "Epoch 61 batch 170 train Loss 167.0358 test Loss 69.4869 with MSE metric 43033.3742\n",
      "Epoch 61 batch 180 train Loss 166.9316 test Loss 69.4470 with MSE metric 43028.0918\n",
      "Epoch 61 batch 190 train Loss 166.8275 test Loss 69.4071 with MSE metric 43022.6740\n",
      "Epoch 61 batch 200 train Loss 166.7236 test Loss 69.3673 with MSE metric 43016.9091\n",
      "Epoch 61 batch 210 train Loss 166.6199 test Loss 69.3275 with MSE metric 43011.4119\n",
      "Epoch 61 batch 220 train Loss 166.5162 test Loss 69.2878 with MSE metric 43005.9976\n",
      "Epoch 61 batch 230 train Loss 166.4127 test Loss 69.2481 with MSE metric 43000.5982\n",
      "Epoch 61 batch 240 train Loss 166.3093 test Loss 69.2085 with MSE metric 42995.1483\n",
      "Time taken for 1 epoch: 28.834238290786743 secs\n",
      "\n",
      "Epoch 62 batch 0 train Loss 166.2061 test Loss 69.1689 with MSE metric 42989.7156\n",
      "Epoch 62 batch 10 train Loss 166.1030 test Loss 69.1294 with MSE metric 42984.0361\n",
      "Epoch 62 batch 20 train Loss 166.0000 test Loss 69.0899 with MSE metric 42978.4185\n",
      "Epoch 62 batch 30 train Loss 165.8972 test Loss 69.0505 with MSE metric 42972.7200\n",
      "Epoch 62 batch 40 train Loss 165.7945 test Loss 69.0111 with MSE metric 42967.1405\n",
      "Epoch 62 batch 50 train Loss 165.6919 test Loss 68.9717 with MSE metric 42961.6433\n",
      "Epoch 62 batch 60 train Loss 165.5894 test Loss 68.9324 with MSE metric 42955.9225\n",
      "Epoch 62 batch 70 train Loss 165.4871 test Loss 68.8932 with MSE metric 42949.9089\n",
      "Epoch 62 batch 80 train Loss 165.3849 test Loss 68.8540 with MSE metric 42944.0122\n",
      "Epoch 62 batch 90 train Loss 165.2828 test Loss 68.8148 with MSE metric 42938.6614\n",
      "Epoch 62 batch 100 train Loss 165.1809 test Loss 68.7757 with MSE metric 42932.7897\n",
      "Epoch 62 batch 110 train Loss 165.0791 test Loss 68.7367 with MSE metric 42926.6772\n",
      "Epoch 62 batch 120 train Loss 164.9774 test Loss 68.6976 with MSE metric 42921.0952\n",
      "Epoch 62 batch 130 train Loss 164.8759 test Loss 68.6587 with MSE metric 42915.4066\n",
      "Epoch 62 batch 140 train Loss 164.7744 test Loss 68.6198 with MSE metric 42909.7451\n",
      "Epoch 62 batch 150 train Loss 164.6732 test Loss 68.5809 with MSE metric 42903.6422\n",
      "Epoch 62 batch 160 train Loss 164.5720 test Loss 68.5421 with MSE metric 42897.2911\n",
      "Epoch 62 batch 170 train Loss 164.4710 test Loss 68.5033 with MSE metric 42891.7323\n",
      "Epoch 62 batch 180 train Loss 164.3700 test Loss 68.4646 with MSE metric 42885.2592\n",
      "Epoch 62 batch 190 train Loss 164.2693 test Loss 68.4259 with MSE metric 42879.2374\n",
      "Epoch 62 batch 200 train Loss 164.1686 test Loss 68.3873 with MSE metric 42873.1244\n",
      "Epoch 62 batch 210 train Loss 164.0681 test Loss 68.3487 with MSE metric 42866.5745\n",
      "Epoch 62 batch 220 train Loss 163.9677 test Loss 68.3101 with MSE metric 42860.1648\n",
      "Epoch 62 batch 230 train Loss 163.8674 test Loss 68.2716 with MSE metric 42853.6989\n",
      "Epoch 62 batch 240 train Loss 163.7673 test Loss 68.2332 with MSE metric 42847.3848\n",
      "Time taken for 1 epoch: 28.10753607749939 secs\n",
      "\n",
      "Epoch 63 batch 0 train Loss 163.6672 test Loss 68.1948 with MSE metric 42840.9608\n",
      "Epoch 63 batch 10 train Loss 163.5673 test Loss 68.1564 with MSE metric 42834.5325\n",
      "Epoch 63 batch 20 train Loss 163.4676 test Loss 68.1181 with MSE metric 42828.0451\n",
      "Epoch 63 batch 30 train Loss 163.3679 test Loss 68.0798 with MSE metric 42821.6197\n",
      "Epoch 63 batch 40 train Loss 163.2684 test Loss 68.0416 with MSE metric 42815.6070\n",
      "Epoch 63 batch 50 train Loss 163.1690 test Loss 68.0034 with MSE metric 42809.5196\n",
      "Epoch 63 batch 60 train Loss 163.0698 test Loss 67.9653 with MSE metric 42803.4461\n",
      "Epoch 63 batch 70 train Loss 162.9706 test Loss 67.9272 with MSE metric 42797.0054\n",
      "Epoch 63 batch 80 train Loss 162.8716 test Loss 67.8892 with MSE metric 42790.3928\n",
      "Epoch 63 batch 90 train Loss 162.7727 test Loss 67.8512 with MSE metric 42783.8533\n",
      "Epoch 63 batch 100 train Loss 162.6740 test Loss 67.8133 with MSE metric 42777.3968\n",
      "Epoch 63 batch 110 train Loss 162.5753 test Loss 67.7754 with MSE metric 42770.7631\n",
      "Epoch 63 batch 120 train Loss 162.4768 test Loss 67.7375 with MSE metric 42764.2400\n",
      "Epoch 63 batch 130 train Loss 162.3784 test Loss 67.6997 with MSE metric 42757.6464\n",
      "Epoch 63 batch 140 train Loss 162.2801 test Loss 67.6619 with MSE metric 42750.9514\n",
      "Epoch 63 batch 150 train Loss 162.1819 test Loss 67.6242 with MSE metric 42744.1505\n",
      "Epoch 63 batch 160 train Loss 162.0839 test Loss 67.5865 with MSE metric 42737.6017\n",
      "Epoch 63 batch 170 train Loss 161.9860 test Loss 67.5489 with MSE metric 42730.8763\n",
      "Epoch 63 batch 180 train Loss 161.8882 test Loss 67.5113 with MSE metric 42724.1452\n",
      "Epoch 63 batch 190 train Loss 161.7905 test Loss 67.4737 with MSE metric 42717.5548\n",
      "Epoch 63 batch 200 train Loss 161.6930 test Loss 67.4362 with MSE metric 42710.5049\n",
      "Epoch 63 batch 210 train Loss 161.5956 test Loss 67.3988 with MSE metric 42703.4320\n",
      "Epoch 63 batch 220 train Loss 161.4982 test Loss 67.3613 with MSE metric 42696.4244\n",
      "Epoch 63 batch 230 train Loss 161.4011 test Loss 67.3240 with MSE metric 42689.8492\n",
      "Epoch 63 batch 240 train Loss 161.3040 test Loss 67.2866 with MSE metric 42683.0080\n",
      "Time taken for 1 epoch: 27.69277310371399 secs\n",
      "\n",
      "Epoch 64 batch 0 train Loss 161.2071 test Loss 67.2494 with MSE metric 42676.1277\n",
      "Epoch 64 batch 10 train Loss 161.1103 test Loss 67.2121 with MSE metric 42669.2647\n",
      "Epoch 64 batch 20 train Loss 161.0136 test Loss 67.1750 with MSE metric 42662.1516\n",
      "Epoch 64 batch 30 train Loss 160.9170 test Loss 67.1378 with MSE metric 42654.8948\n",
      "Epoch 64 batch 40 train Loss 160.8205 test Loss 67.1007 with MSE metric 42647.5283\n",
      "Epoch 64 batch 50 train Loss 160.7241 test Loss 67.0636 with MSE metric 42640.3071\n",
      "Epoch 64 batch 60 train Loss 160.6279 test Loss 67.0266 with MSE metric 42633.2090\n",
      "Epoch 64 batch 70 train Loss 160.5318 test Loss 66.9896 with MSE metric 42626.0112\n",
      "Epoch 64 batch 80 train Loss 160.4358 test Loss 66.9527 with MSE metric 42619.0219\n",
      "Epoch 64 batch 90 train Loss 160.3400 test Loss 66.9158 with MSE metric 42612.0332\n",
      "Epoch 64 batch 100 train Loss 160.2442 test Loss 66.8790 with MSE metric 42605.0939\n",
      "Epoch 64 batch 110 train Loss 160.1486 test Loss 66.8422 with MSE metric 42597.7137\n",
      "Epoch 64 batch 120 train Loss 160.0531 test Loss 66.8055 with MSE metric 42590.5262\n",
      "Epoch 64 batch 130 train Loss 159.9577 test Loss 66.7687 with MSE metric 42583.0483\n",
      "Epoch 64 batch 140 train Loss 159.8624 test Loss 66.7321 with MSE metric 42575.6664\n",
      "Epoch 64 batch 150 train Loss 159.7672 test Loss 66.6954 with MSE metric 42568.4315\n",
      "Epoch 64 batch 160 train Loss 159.6722 test Loss 66.6589 with MSE metric 42560.9661\n",
      "Epoch 64 batch 170 train Loss 159.5772 test Loss 66.6223 with MSE metric 42553.5855\n",
      "Epoch 64 batch 180 train Loss 159.4824 test Loss 66.5858 with MSE metric 42546.2355\n",
      "Epoch 64 batch 190 train Loss 159.3877 test Loss 66.5494 with MSE metric 42538.5223\n",
      "Epoch 64 batch 200 train Loss 159.2932 test Loss 66.5130 with MSE metric 42531.0735\n",
      "Epoch 64 batch 210 train Loss 159.1987 test Loss 66.4766 with MSE metric 42523.5069\n",
      "Epoch 64 batch 220 train Loss 159.1043 test Loss 66.4403 with MSE metric 42515.9933\n",
      "Epoch 64 batch 230 train Loss 159.0101 test Loss 66.4040 with MSE metric 42508.4692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 batch 240 train Loss 158.9160 test Loss 66.3678 with MSE metric 42500.7959\n",
      "Time taken for 1 epoch: 33.85796880722046 secs\n",
      "\n",
      "Epoch 65 batch 0 train Loss 158.8220 test Loss 66.3316 with MSE metric 42493.1513\n",
      "Epoch 65 batch 10 train Loss 158.7281 test Loss 66.2954 with MSE metric 42485.4820\n",
      "Epoch 65 batch 20 train Loss 158.6344 test Loss 66.2593 with MSE metric 42477.7437\n",
      "Epoch 65 batch 30 train Loss 158.5407 test Loss 66.2233 with MSE metric 42469.8829\n",
      "Epoch 65 batch 40 train Loss 158.4471 test Loss 66.1873 with MSE metric 42462.1801\n",
      "Epoch 65 batch 50 train Loss 158.3537 test Loss 66.1513 with MSE metric 42454.3806\n",
      "Epoch 65 batch 60 train Loss 158.2604 test Loss 66.1154 with MSE metric 42446.5080\n",
      "Epoch 65 batch 70 train Loss 158.1672 test Loss 66.0795 with MSE metric 42438.6307\n",
      "Epoch 65 batch 80 train Loss 158.0741 test Loss 66.0436 with MSE metric 42430.9167\n",
      "Epoch 65 batch 90 train Loss 157.9811 test Loss 66.0078 with MSE metric 42422.9520\n",
      "Epoch 65 batch 100 train Loss 157.8882 test Loss 65.9720 with MSE metric 42414.8458\n",
      "Epoch 65 batch 110 train Loss 157.7955 test Loss 65.9363 with MSE metric 42406.8340\n",
      "Epoch 65 batch 120 train Loss 157.7028 test Loss 65.9006 with MSE metric 42398.8270\n",
      "Epoch 65 batch 130 train Loss 157.6103 test Loss 65.8650 with MSE metric 42390.8602\n",
      "Epoch 65 batch 140 train Loss 157.5179 test Loss 65.8294 with MSE metric 42382.7277\n",
      "Epoch 65 batch 150 train Loss 157.4256 test Loss 65.7938 with MSE metric 42374.4895\n",
      "Epoch 65 batch 160 train Loss 157.3334 test Loss 65.7583 with MSE metric 42366.3895\n",
      "Epoch 65 batch 170 train Loss 157.2413 test Loss 65.7228 with MSE metric 42358.2427\n",
      "Epoch 65 batch 180 train Loss 157.1493 test Loss 65.6874 with MSE metric 42350.0857\n",
      "Epoch 65 batch 190 train Loss 157.0574 test Loss 65.6520 with MSE metric 42341.9305\n",
      "Epoch 65 batch 200 train Loss 156.9657 test Loss 65.6166 with MSE metric 42333.6246\n",
      "Epoch 65 batch 210 train Loss 156.8740 test Loss 65.5813 with MSE metric 42325.3072\n",
      "Epoch 65 batch 220 train Loss 156.7825 test Loss 65.5460 with MSE metric 42317.0713\n",
      "Epoch 65 batch 230 train Loss 156.6911 test Loss 65.5108 with MSE metric 42308.8391\n",
      "Epoch 65 batch 240 train Loss 156.5998 test Loss 65.4756 with MSE metric 42300.5058\n",
      "Time taken for 1 epoch: 28.668142080307007 secs\n",
      "\n",
      "Epoch 66 batch 0 train Loss 156.5086 test Loss 65.4404 with MSE metric 42292.1275\n",
      "Epoch 66 batch 10 train Loss 156.4175 test Loss 65.4053 with MSE metric 42283.6773\n",
      "Epoch 66 batch 20 train Loss 156.3265 test Loss 65.3702 with MSE metric 42275.2465\n",
      "Epoch 66 batch 30 train Loss 156.2356 test Loss 65.3352 with MSE metric 42266.7583\n",
      "Epoch 66 batch 40 train Loss 156.1449 test Loss 65.3002 with MSE metric 42258.2951\n",
      "Epoch 66 batch 50 train Loss 156.0542 test Loss 65.2652 with MSE metric 42249.6271\n",
      "Epoch 66 batch 60 train Loss 155.9636 test Loss 65.2303 with MSE metric 42241.0425\n",
      "Epoch 66 batch 70 train Loss 155.8732 test Loss 65.1954 with MSE metric 42232.4509\n",
      "Epoch 66 batch 80 train Loss 155.7828 test Loss 65.1606 with MSE metric 42223.7382\n",
      "Epoch 66 batch 90 train Loss 155.6926 test Loss 65.1258 with MSE metric 42215.0234\n",
      "Epoch 66 batch 100 train Loss 155.6025 test Loss 65.0910 with MSE metric 42206.4154\n",
      "Epoch 66 batch 110 train Loss 155.5125 test Loss 65.0563 with MSE metric 42197.6414\n",
      "Epoch 66 batch 120 train Loss 155.4226 test Loss 65.0216 with MSE metric 42188.9902\n",
      "Epoch 66 batch 130 train Loss 155.3328 test Loss 64.9870 with MSE metric 42180.1554\n",
      "Epoch 66 batch 140 train Loss 155.2431 test Loss 64.9524 with MSE metric 42171.4424\n",
      "Epoch 66 batch 150 train Loss 155.1535 test Loss 64.9178 with MSE metric 42162.6111\n",
      "Epoch 66 batch 160 train Loss 155.0640 test Loss 64.8833 with MSE metric 42153.7066\n",
      "Epoch 66 batch 170 train Loss 154.9747 test Loss 64.8488 with MSE metric 42144.7720\n",
      "Epoch 66 batch 180 train Loss 154.8854 test Loss 64.8143 with MSE metric 42135.8302\n",
      "Epoch 66 batch 190 train Loss 154.7962 test Loss 64.7799 with MSE metric 42126.8900\n",
      "Epoch 66 batch 200 train Loss 154.7072 test Loss 64.7456 with MSE metric 42117.9413\n",
      "Epoch 66 batch 210 train Loss 154.6182 test Loss 64.7112 with MSE metric 42108.9688\n",
      "Epoch 66 batch 220 train Loss 154.5294 test Loss 64.6770 with MSE metric 42099.9596\n",
      "Epoch 66 batch 230 train Loss 154.4407 test Loss 64.6427 with MSE metric 42090.8101\n",
      "Epoch 66 batch 240 train Loss 154.3520 test Loss 64.6085 with MSE metric 42081.7809\n",
      "Time taken for 1 epoch: 27.26377010345459 secs\n",
      "\n",
      "Epoch 67 batch 0 train Loss 154.2635 test Loss 64.5743 with MSE metric 42072.6101\n",
      "Epoch 67 batch 10 train Loss 154.1751 test Loss 64.5402 with MSE metric 42063.5422\n",
      "Epoch 67 batch 20 train Loss 154.0868 test Loss 64.5061 with MSE metric 42054.4073\n",
      "Epoch 67 batch 30 train Loss 153.9986 test Loss 64.4720 with MSE metric 42045.1876\n",
      "Epoch 67 batch 40 train Loss 153.9104 test Loss 64.4380 with MSE metric 42035.9566\n",
      "Epoch 67 batch 50 train Loss 153.8224 test Loss 64.4040 with MSE metric 42026.7144\n",
      "Epoch 67 batch 60 train Loss 153.7345 test Loss 64.3701 with MSE metric 42017.4195\n",
      "Epoch 67 batch 70 train Loss 153.6467 test Loss 64.3362 with MSE metric 42008.0999\n",
      "Epoch 67 batch 80 train Loss 153.5590 test Loss 64.3023 with MSE metric 41998.7790\n",
      "Epoch 67 batch 90 train Loss 153.4715 test Loss 64.2685 with MSE metric 41989.4606\n",
      "Epoch 67 batch 100 train Loss 153.3840 test Loss 64.2347 with MSE metric 41980.1195\n",
      "Epoch 67 batch 110 train Loss 153.2966 test Loss 64.2009 with MSE metric 41970.7079\n",
      "Epoch 67 batch 120 train Loss 153.2093 test Loss 64.1672 with MSE metric 41961.2605\n",
      "Epoch 67 batch 130 train Loss 153.1221 test Loss 64.1335 with MSE metric 41951.8243\n",
      "Epoch 67 batch 140 train Loss 153.0350 test Loss 64.0999 with MSE metric 41942.3736\n",
      "Epoch 67 batch 150 train Loss 152.9481 test Loss 64.0663 with MSE metric 41932.9023\n",
      "Epoch 67 batch 160 train Loss 152.8612 test Loss 64.0327 with MSE metric 41923.3700\n",
      "Epoch 67 batch 170 train Loss 152.7744 test Loss 63.9992 with MSE metric 41913.7935\n",
      "Epoch 67 batch 180 train Loss 152.6877 test Loss 63.9657 with MSE metric 41904.2384\n",
      "Epoch 67 batch 190 train Loss 152.6012 test Loss 63.9322 with MSE metric 41894.6322\n",
      "Epoch 67 batch 200 train Loss 152.5147 test Loss 63.8988 with MSE metric 41885.0183\n",
      "Epoch 67 batch 210 train Loss 152.4283 test Loss 63.8654 with MSE metric 41875.3895\n",
      "Epoch 67 batch 220 train Loss 152.3421 test Loss 63.8321 with MSE metric 41865.7247\n",
      "Epoch 67 batch 230 train Loss 152.2559 test Loss 63.7988 with MSE metric 41855.9598\n",
      "Epoch 67 batch 240 train Loss 152.1698 test Loss 63.7655 with MSE metric 41846.2157\n",
      "Time taken for 1 epoch: 29.05475616455078 secs\n",
      "\n",
      "Epoch 68 batch 0 train Loss 152.0839 test Loss 63.7323 with MSE metric 41836.4717\n",
      "Epoch 68 batch 10 train Loss 151.9980 test Loss 63.6991 with MSE metric 41826.6757\n",
      "Epoch 68 batch 20 train Loss 151.9122 test Loss 63.6659 with MSE metric 41816.8493\n",
      "Epoch 68 batch 30 train Loss 151.8266 test Loss 63.6328 with MSE metric 41806.9834\n",
      "Epoch 68 batch 40 train Loss 151.7410 test Loss 63.5997 with MSE metric 41797.0685\n",
      "Epoch 68 batch 50 train Loss 151.6555 test Loss 63.5667 with MSE metric 41787.2046\n",
      "Epoch 68 batch 60 train Loss 151.5702 test Loss 63.5337 with MSE metric 41777.3605\n",
      "Epoch 68 batch 70 train Loss 151.4849 test Loss 63.5007 with MSE metric 41767.4110\n",
      "Epoch 68 batch 80 train Loss 151.3997 test Loss 63.4678 with MSE metric 41757.4665\n",
      "Epoch 68 batch 90 train Loss 151.3146 test Loss 63.4349 with MSE metric 41747.5424\n",
      "Epoch 68 batch 100 train Loss 151.2297 test Loss 63.4020 with MSE metric 41737.5320\n",
      "Epoch 68 batch 110 train Loss 151.1448 test Loss 63.3692 with MSE metric 41727.5032\n",
      "Epoch 68 batch 120 train Loss 151.0600 test Loss 63.3364 with MSE metric 41717.5139\n",
      "Epoch 68 batch 130 train Loss 150.9753 test Loss 63.3036 with MSE metric 41707.4381\n",
      "Epoch 68 batch 140 train Loss 150.8908 test Loss 63.2709 with MSE metric 41697.3176\n",
      "Epoch 68 batch 150 train Loss 150.8063 test Loss 63.2382 with MSE metric 41687.1536\n",
      "Epoch 68 batch 160 train Loss 150.7219 test Loss 63.2056 with MSE metric 41676.9816\n",
      "Epoch 68 batch 170 train Loss 150.6376 test Loss 63.1729 with MSE metric 41666.8268\n",
      "Epoch 68 batch 180 train Loss 150.5534 test Loss 63.1404 with MSE metric 41656.6228\n",
      "Epoch 68 batch 190 train Loss 150.4693 test Loss 63.1078 with MSE metric 41646.3712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 batch 200 train Loss 150.3853 test Loss 63.0753 with MSE metric 41636.1150\n",
      "Epoch 68 batch 210 train Loss 150.3014 test Loss 63.0428 with MSE metric 41625.8660\n",
      "Epoch 68 batch 220 train Loss 150.2176 test Loss 63.0104 with MSE metric 41615.6253\n",
      "Epoch 68 batch 230 train Loss 150.1339 test Loss 62.9780 with MSE metric 41605.3965\n",
      "Epoch 68 batch 240 train Loss 150.0503 test Loss 62.9456 with MSE metric 41595.0589\n",
      "Time taken for 1 epoch: 30.368643045425415 secs\n",
      "\n",
      "Epoch 69 batch 0 train Loss 149.9668 test Loss 62.9133 with MSE metric 41584.6989\n",
      "Epoch 69 batch 10 train Loss 149.8834 test Loss 62.8810 with MSE metric 41574.3208\n",
      "Epoch 69 batch 20 train Loss 149.8000 test Loss 62.8488 with MSE metric 41563.9285\n",
      "Epoch 69 batch 30 train Loss 149.7168 test Loss 62.8166 with MSE metric 41553.4348\n",
      "Epoch 69 batch 40 train Loss 149.6337 test Loss 62.7844 with MSE metric 41543.0765\n",
      "Epoch 69 batch 50 train Loss 149.5506 test Loss 62.7522 with MSE metric 41532.6208\n",
      "Epoch 69 batch 60 train Loss 149.4677 test Loss 62.7201 with MSE metric 41522.2116\n",
      "Epoch 69 batch 70 train Loss 149.3849 test Loss 62.6881 with MSE metric 41511.7367\n",
      "Epoch 69 batch 80 train Loss 149.3021 test Loss 62.6560 with MSE metric 41501.3991\n",
      "Epoch 69 batch 90 train Loss 149.2195 test Loss 62.6240 with MSE metric 41490.8022\n",
      "Epoch 69 batch 100 train Loss 149.1369 test Loss 62.5920 with MSE metric 41480.3564\n",
      "Epoch 69 batch 110 train Loss 149.0544 test Loss 62.5601 with MSE metric 41469.7668\n",
      "Epoch 69 batch 120 train Loss 148.9720 test Loss 62.5282 with MSE metric 41459.1455\n",
      "Epoch 69 batch 130 train Loss 148.8898 test Loss 62.4963 with MSE metric 41448.5424\n",
      "Epoch 69 batch 140 train Loss 148.8076 test Loss 62.4645 with MSE metric 41438.0274\n",
      "Epoch 69 batch 150 train Loss 148.7255 test Loss 62.4327 with MSE metric 41427.4789\n",
      "Epoch 69 batch 160 train Loss 148.6435 test Loss 62.4009 with MSE metric 41416.6827\n",
      "Epoch 69 batch 170 train Loss 148.5616 test Loss 62.3692 with MSE metric 41406.2326\n",
      "Epoch 69 batch 180 train Loss 148.4798 test Loss 62.3375 with MSE metric 41395.5808\n",
      "Epoch 69 batch 190 train Loss 148.3980 test Loss 62.3058 with MSE metric 41384.9308\n",
      "Epoch 69 batch 200 train Loss 148.3164 test Loss 62.2742 with MSE metric 41374.2180\n",
      "Epoch 69 batch 210 train Loss 148.2349 test Loss 62.2426 with MSE metric 41363.3264\n",
      "Epoch 69 batch 220 train Loss 148.1534 test Loss 62.2110 with MSE metric 41352.5513\n",
      "Epoch 69 batch 230 train Loss 148.0721 test Loss 62.1795 with MSE metric 41341.7810\n",
      "Epoch 69 batch 240 train Loss 147.9908 test Loss 62.1480 with MSE metric 41330.9080\n",
      "Time taken for 1 epoch: 30.47159695625305 secs\n",
      "\n",
      "Epoch 70 batch 0 train Loss 147.9097 test Loss 62.1165 with MSE metric 41320.3583\n",
      "Epoch 70 batch 10 train Loss 147.8286 test Loss 62.0851 with MSE metric 41309.5206\n",
      "Epoch 70 batch 20 train Loss 147.7476 test Loss 62.0537 with MSE metric 41298.7273\n",
      "Epoch 70 batch 30 train Loss 147.6667 test Loss 62.0224 with MSE metric 41287.8478\n",
      "Epoch 70 batch 40 train Loss 147.5859 test Loss 61.9911 with MSE metric 41277.0249\n",
      "Epoch 70 batch 50 train Loss 147.5052 test Loss 61.9598 with MSE metric 41266.2027\n",
      "Epoch 70 batch 60 train Loss 147.4246 test Loss 61.9285 with MSE metric 41255.3228\n",
      "Epoch 70 batch 70 train Loss 147.3441 test Loss 61.8973 with MSE metric 41244.3834\n",
      "Epoch 70 batch 80 train Loss 147.2636 test Loss 61.8661 with MSE metric 41233.5566\n",
      "Epoch 70 batch 90 train Loss 147.1833 test Loss 61.8350 with MSE metric 41222.6930\n",
      "Epoch 70 batch 100 train Loss 147.1030 test Loss 61.8039 with MSE metric 41211.8326\n",
      "Epoch 70 batch 110 train Loss 147.0229 test Loss 61.7728 with MSE metric 41200.7581\n",
      "Epoch 70 batch 120 train Loss 146.9428 test Loss 61.7417 with MSE metric 41189.6718\n",
      "Epoch 70 batch 130 train Loss 146.8628 test Loss 61.7107 with MSE metric 41178.7499\n",
      "Epoch 70 batch 140 train Loss 146.7829 test Loss 61.6797 with MSE metric 41167.4896\n",
      "Epoch 70 batch 150 train Loss 146.7031 test Loss 61.6488 with MSE metric 41156.4000\n",
      "Epoch 70 batch 160 train Loss 146.6234 test Loss 61.6179 with MSE metric 41145.3502\n",
      "Epoch 70 batch 170 train Loss 146.5438 test Loss 61.5870 with MSE metric 41134.5399\n",
      "Epoch 70 batch 180 train Loss 146.4642 test Loss 61.5561 with MSE metric 41123.5992\n",
      "Epoch 70 batch 190 train Loss 146.3848 test Loss 61.5253 with MSE metric 41112.6043\n",
      "Epoch 70 batch 200 train Loss 146.3054 test Loss 61.4946 with MSE metric 41101.6976\n",
      "Epoch 70 batch 210 train Loss 146.2262 test Loss 61.4638 with MSE metric 41090.5710\n",
      "Epoch 70 batch 220 train Loss 146.1470 test Loss 61.4331 with MSE metric 41079.5631\n",
      "Epoch 70 batch 230 train Loss 146.0679 test Loss 61.4024 with MSE metric 41068.4162\n",
      "Epoch 70 batch 240 train Loss 145.9889 test Loss 61.3717 with MSE metric 41057.3240\n",
      "Time taken for 1 epoch: 27.16862177848816 secs\n",
      "\n",
      "Epoch 71 batch 0 train Loss 145.9100 test Loss 61.3411 with MSE metric 41045.9344\n",
      "Epoch 71 batch 10 train Loss 145.8312 test Loss 61.3105 with MSE metric 41034.8285\n",
      "Epoch 71 batch 20 train Loss 145.7524 test Loss 61.2799 with MSE metric 41023.6497\n",
      "Epoch 71 batch 30 train Loss 145.6738 test Loss 61.2494 with MSE metric 41012.4526\n",
      "Epoch 71 batch 40 train Loss 145.5952 test Loss 61.2189 with MSE metric 41001.6148\n",
      "Epoch 71 batch 50 train Loss 145.5167 test Loss 61.1885 with MSE metric 40990.4045\n",
      "Epoch 71 batch 60 train Loss 145.4384 test Loss 61.1580 with MSE metric 40979.0357\n",
      "Epoch 71 batch 70 train Loss 145.3601 test Loss 61.1276 with MSE metric 40967.6509\n",
      "Epoch 71 batch 80 train Loss 145.2818 test Loss 61.0973 with MSE metric 40956.6075\n",
      "Epoch 71 batch 90 train Loss 145.2037 test Loss 61.0670 with MSE metric 40945.4023\n",
      "Epoch 71 batch 100 train Loss 145.1257 test Loss 61.0367 with MSE metric 40934.0568\n",
      "Epoch 71 batch 110 train Loss 145.0477 test Loss 61.0064 with MSE metric 40922.7122\n",
      "Epoch 71 batch 120 train Loss 144.9699 test Loss 60.9762 with MSE metric 40911.3208\n",
      "Epoch 71 batch 130 train Loss 144.8921 test Loss 60.9459 with MSE metric 40900.2494\n",
      "Epoch 71 batch 140 train Loss 144.8144 test Loss 60.9158 with MSE metric 40889.1019\n",
      "Epoch 71 batch 150 train Loss 144.7368 test Loss 60.8856 with MSE metric 40877.8842\n",
      "Epoch 71 batch 160 train Loss 144.6592 test Loss 60.8555 with MSE metric 40866.5019\n",
      "Epoch 71 batch 170 train Loss 144.5818 test Loss 60.8254 with MSE metric 40855.1920\n",
      "Epoch 71 batch 180 train Loss 144.5045 test Loss 60.7954 with MSE metric 40843.9633\n",
      "Epoch 71 batch 190 train Loss 144.4272 test Loss 60.7654 with MSE metric 40832.3917\n",
      "Epoch 71 batch 200 train Loss 144.3500 test Loss 60.7354 with MSE metric 40821.0116\n",
      "Epoch 71 batch 210 train Loss 144.2729 test Loss 60.7054 with MSE metric 40809.4363\n",
      "Epoch 71 batch 220 train Loss 144.1959 test Loss 60.6755 with MSE metric 40797.7273\n",
      "Epoch 71 batch 230 train Loss 144.1190 test Loss 60.6456 with MSE metric 40785.9982\n",
      "Epoch 71 batch 240 train Loss 144.0421 test Loss 60.6157 with MSE metric 40774.4469\n",
      "Time taken for 1 epoch: 32.16075277328491 secs\n",
      "\n",
      "Epoch 72 batch 0 train Loss 143.9654 test Loss 60.5859 with MSE metric 40762.5107\n",
      "Epoch 72 batch 10 train Loss 143.8887 test Loss 60.5561 with MSE metric 40750.7718\n",
      "Epoch 72 batch 20 train Loss 143.8121 test Loss 60.5263 with MSE metric 40739.1344\n",
      "Epoch 72 batch 30 train Loss 143.7356 test Loss 60.4966 with MSE metric 40727.5488\n",
      "Epoch 72 batch 40 train Loss 143.6592 test Loss 60.4669 with MSE metric 40715.9193\n",
      "Epoch 72 batch 50 train Loss 143.5828 test Loss 60.4372 with MSE metric 40704.1958\n",
      "Epoch 72 batch 60 train Loss 143.5066 test Loss 60.4076 with MSE metric 40692.4861\n",
      "Epoch 72 batch 70 train Loss 143.4304 test Loss 60.3780 with MSE metric 40680.7769\n",
      "Epoch 72 batch 80 train Loss 143.3543 test Loss 60.3484 with MSE metric 40668.8122\n",
      "Epoch 72 batch 90 train Loss 143.2783 test Loss 60.3188 with MSE metric 40657.0702\n",
      "Epoch 72 batch 100 train Loss 143.2024 test Loss 60.2893 with MSE metric 40645.1719\n",
      "Epoch 72 batch 110 train Loss 143.1265 test Loss 60.2598 with MSE metric 40633.0413\n",
      "Epoch 72 batch 120 train Loss 143.0508 test Loss 60.2304 with MSE metric 40621.0644\n",
      "Epoch 72 batch 130 train Loss 142.9751 test Loss 60.2009 with MSE metric 40608.9495\n",
      "Epoch 72 batch 140 train Loss 142.8995 test Loss 60.1715 with MSE metric 40596.8507\n",
      "Epoch 72 batch 150 train Loss 142.8240 test Loss 60.1422 with MSE metric 40584.8480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 batch 160 train Loss 142.7486 test Loss 60.1128 with MSE metric 40572.6756\n",
      "Epoch 72 batch 170 train Loss 142.6732 test Loss 60.0835 with MSE metric 40560.4983\n",
      "Epoch 72 batch 180 train Loss 142.5979 test Loss 60.0542 with MSE metric 40548.1898\n",
      "Epoch 72 batch 190 train Loss 142.5227 test Loss 60.0250 with MSE metric 40535.7614\n",
      "Epoch 72 batch 200 train Loss 142.4476 test Loss 59.9958 with MSE metric 40523.3154\n",
      "Epoch 72 batch 210 train Loss 142.3726 test Loss 59.9666 with MSE metric 40510.8252\n",
      "Epoch 72 batch 220 train Loss 142.2977 test Loss 59.9374 with MSE metric 40498.3458\n",
      "Epoch 72 batch 230 train Loss 142.2228 test Loss 59.9083 with MSE metric 40485.6728\n",
      "Epoch 72 batch 240 train Loss 142.1480 test Loss 59.8792 with MSE metric 40473.2145\n",
      "Time taken for 1 epoch: 30.630433082580566 secs\n",
      "\n",
      "Epoch 73 batch 0 train Loss 142.0733 test Loss 59.8502 with MSE metric 40460.6855\n",
      "Epoch 73 batch 10 train Loss 141.9987 test Loss 59.8211 with MSE metric 40447.9178\n",
      "Epoch 73 batch 20 train Loss 141.9241 test Loss 59.7921 with MSE metric 40435.1300\n",
      "Epoch 73 batch 30 train Loss 141.8497 test Loss 59.7632 with MSE metric 40422.4521\n",
      "Epoch 73 batch 40 train Loss 141.7753 test Loss 59.7342 with MSE metric 40409.6150\n",
      "Epoch 73 batch 50 train Loss 141.7010 test Loss 59.7053 with MSE metric 40396.6692\n",
      "Epoch 73 batch 60 train Loss 141.6268 test Loss 59.6764 with MSE metric 40383.7299\n",
      "Epoch 73 batch 70 train Loss 141.5526 test Loss 59.6475 with MSE metric 40370.5949\n",
      "Epoch 73 batch 80 train Loss 141.4785 test Loss 59.6187 with MSE metric 40357.6134\n",
      "Epoch 73 batch 90 train Loss 141.4045 test Loss 59.5899 with MSE metric 40344.5376\n",
      "Epoch 73 batch 100 train Loss 141.3306 test Loss 59.5611 with MSE metric 40331.5678\n",
      "Epoch 73 batch 110 train Loss 141.2568 test Loss 59.5323 with MSE metric 40318.5728\n",
      "Epoch 73 batch 120 train Loss 141.1830 test Loss 59.5036 with MSE metric 40305.3252\n",
      "Epoch 73 batch 130 train Loss 141.1094 test Loss 59.4749 with MSE metric 40292.1207\n",
      "Epoch 73 batch 140 train Loss 141.0358 test Loss 59.4463 with MSE metric 40278.9925\n",
      "Epoch 73 batch 150 train Loss 140.9622 test Loss 59.4176 with MSE metric 40265.5833\n",
      "Epoch 73 batch 160 train Loss 140.8888 test Loss 59.3890 with MSE metric 40252.0902\n",
      "Epoch 73 batch 170 train Loss 140.8154 test Loss 59.3604 with MSE metric 40238.6254\n",
      "Epoch 73 batch 180 train Loss 140.7421 test Loss 59.3319 with MSE metric 40224.9918\n",
      "Epoch 73 batch 190 train Loss 140.6689 test Loss 59.3033 with MSE metric 40211.4678\n",
      "Epoch 73 batch 200 train Loss 140.5958 test Loss 59.2748 with MSE metric 40197.8195\n",
      "Epoch 73 batch 210 train Loss 140.5227 test Loss 59.2463 with MSE metric 40184.0814\n",
      "Epoch 73 batch 220 train Loss 140.4497 test Loss 59.2179 with MSE metric 40170.3875\n",
      "Epoch 73 batch 230 train Loss 140.3768 test Loss 59.1894 with MSE metric 40156.4917\n",
      "Epoch 73 batch 240 train Loss 140.3040 test Loss 59.1610 with MSE metric 40142.7588\n",
      "Time taken for 1 epoch: 30.322659015655518 secs\n",
      "\n",
      "Epoch 74 batch 0 train Loss 140.2312 test Loss 59.1326 with MSE metric 40128.9351\n",
      "Epoch 74 batch 10 train Loss 140.1585 test Loss 59.1043 with MSE metric 40114.9478\n",
      "Epoch 74 batch 20 train Loss 140.0860 test Loss 59.0760 with MSE metric 40100.8185\n",
      "Epoch 74 batch 30 train Loss 140.0134 test Loss 59.0477 with MSE metric 40086.9668\n",
      "Epoch 74 batch 40 train Loss 139.9410 test Loss 59.0194 with MSE metric 40072.8706\n",
      "Epoch 74 batch 50 train Loss 139.8686 test Loss 58.9911 with MSE metric 40058.3927\n",
      "Epoch 74 batch 60 train Loss 139.7963 test Loss 58.9629 with MSE metric 40044.1670\n",
      "Epoch 74 batch 70 train Loss 139.7241 test Loss 58.9347 with MSE metric 40029.9479\n",
      "Epoch 74 batch 80 train Loss 139.6520 test Loss 58.9065 with MSE metric 40015.7301\n",
      "Epoch 74 batch 90 train Loss 139.5799 test Loss 58.8784 with MSE metric 40001.2639\n",
      "Epoch 74 batch 100 train Loss 139.5079 test Loss 58.8503 with MSE metric 39986.7239\n",
      "Epoch 74 batch 110 train Loss 139.4360 test Loss 58.8222 with MSE metric 39972.3729\n",
      "Epoch 74 batch 120 train Loss 139.3641 test Loss 58.7941 with MSE metric 39957.7098\n",
      "Epoch 74 batch 130 train Loss 139.2923 test Loss 58.7661 with MSE metric 39943.3174\n",
      "Epoch 74 batch 140 train Loss 139.2207 test Loss 58.7380 with MSE metric 39928.8487\n",
      "Epoch 74 batch 150 train Loss 139.1490 test Loss 58.7101 with MSE metric 39914.5044\n",
      "Epoch 74 batch 160 train Loss 139.0775 test Loss 58.6821 with MSE metric 39900.1642\n",
      "Epoch 74 batch 170 train Loss 139.0060 test Loss 58.6541 with MSE metric 39885.2876\n",
      "Epoch 74 batch 180 train Loss 138.9346 test Loss 58.6262 with MSE metric 39870.5266\n",
      "Epoch 74 batch 190 train Loss 138.8633 test Loss 58.5983 with MSE metric 39855.5212\n",
      "Epoch 74 batch 200 train Loss 138.7920 test Loss 58.5705 with MSE metric 39840.7704\n",
      "Epoch 74 batch 210 train Loss 138.7209 test Loss 58.5426 with MSE metric 39825.8147\n",
      "Epoch 74 batch 220 train Loss 138.6498 test Loss 58.5148 with MSE metric 39811.0064\n",
      "Epoch 74 batch 230 train Loss 138.5787 test Loss 58.4870 with MSE metric 39796.1255\n",
      "Epoch 74 batch 240 train Loss 138.5078 test Loss 58.4593 with MSE metric 39781.4510\n",
      "Time taken for 1 epoch: 28.702443838119507 secs\n",
      "\n",
      "Epoch 75 batch 0 train Loss 138.4369 test Loss 58.4315 with MSE metric 39766.9989\n",
      "Epoch 75 batch 10 train Loss 138.3661 test Loss 58.4038 with MSE metric 39752.1950\n",
      "Epoch 75 batch 20 train Loss 138.2954 test Loss 58.3761 with MSE metric 39737.3770\n",
      "Epoch 75 batch 30 train Loss 138.2247 test Loss 58.3485 with MSE metric 39722.3972\n",
      "Epoch 75 batch 40 train Loss 138.1541 test Loss 58.3208 with MSE metric 39707.3188\n",
      "Epoch 75 batch 50 train Loss 138.0836 test Loss 58.2932 with MSE metric 39692.3229\n",
      "Epoch 75 batch 60 train Loss 138.0132 test Loss 58.2656 with MSE metric 39677.4046\n",
      "Epoch 75 batch 70 train Loss 137.9428 test Loss 58.2381 with MSE metric 39662.5257\n",
      "Epoch 75 batch 80 train Loss 137.8725 test Loss 58.2105 with MSE metric 39647.3674\n",
      "Epoch 75 batch 90 train Loss 137.8023 test Loss 58.1830 with MSE metric 39632.1705\n",
      "Epoch 75 batch 100 train Loss 137.7321 test Loss 58.1555 with MSE metric 39617.4476\n",
      "Epoch 75 batch 110 train Loss 137.6621 test Loss 58.1281 with MSE metric 39602.4476\n",
      "Epoch 75 batch 120 train Loss 137.5921 test Loss 58.1007 with MSE metric 39587.6013\n",
      "Epoch 75 batch 130 train Loss 137.5221 test Loss 58.0733 with MSE metric 39572.5775\n",
      "Epoch 75 batch 140 train Loss 137.4523 test Loss 58.0459 with MSE metric 39557.7535\n",
      "Epoch 75 batch 150 train Loss 137.3825 test Loss 58.0185 with MSE metric 39542.6547\n",
      "Epoch 75 batch 160 train Loss 137.3128 test Loss 57.9913 with MSE metric 39527.4826\n",
      "Epoch 75 batch 170 train Loss 137.2432 test Loss 57.9639 with MSE metric 39512.4983\n",
      "Epoch 75 batch 180 train Loss 137.1736 test Loss 57.9367 with MSE metric 39497.4066\n",
      "Epoch 75 batch 190 train Loss 137.1041 test Loss 57.9094 with MSE metric 39482.2844\n",
      "Epoch 75 batch 200 train Loss 137.0347 test Loss 57.8822 with MSE metric 39467.1012\n",
      "Epoch 75 batch 210 train Loss 136.9653 test Loss 57.8550 with MSE metric 39451.7784\n",
      "Epoch 75 batch 220 train Loss 136.8961 test Loss 57.8278 with MSE metric 39436.4256\n",
      "Epoch 75 batch 230 train Loss 136.8269 test Loss 57.8007 with MSE metric 39421.3102\n",
      "Epoch 75 batch 240 train Loss 136.7577 test Loss 57.7736 with MSE metric 39406.3997\n",
      "Time taken for 1 epoch: 30.58616614341736 secs\n",
      "\n",
      "Epoch 76 batch 0 train Loss 136.6887 test Loss 57.7465 with MSE metric 39391.6844\n",
      "Epoch 76 batch 10 train Loss 136.6197 test Loss 57.7195 with MSE metric 39376.5777\n",
      "Epoch 76 batch 20 train Loss 136.5508 test Loss 57.6924 with MSE metric 39361.5618\n",
      "Epoch 76 batch 30 train Loss 136.4819 test Loss 57.6654 with MSE metric 39346.3575\n",
      "Epoch 76 batch 40 train Loss 136.4132 test Loss 57.6384 with MSE metric 39330.7564\n",
      "Epoch 76 batch 50 train Loss 136.3444 test Loss 57.6115 with MSE metric 39315.4568\n",
      "Epoch 76 batch 60 train Loss 136.2758 test Loss 57.5846 with MSE metric 39300.2733\n",
      "Epoch 76 batch 70 train Loss 136.2073 test Loss 57.5577 with MSE metric 39284.9475\n",
      "Epoch 76 batch 80 train Loss 136.1388 test Loss 57.5308 with MSE metric 39269.9365\n",
      "Epoch 76 batch 90 train Loss 136.0704 test Loss 57.5039 with MSE metric 39255.0298\n",
      "Epoch 76 batch 100 train Loss 136.0020 test Loss 57.4771 with MSE metric 39240.0044\n",
      "Epoch 76 batch 110 train Loss 135.9337 test Loss 57.4503 with MSE metric 39225.0446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 batch 120 train Loss 135.8655 test Loss 57.4235 with MSE metric 39210.0506\n",
      "Epoch 76 batch 130 train Loss 135.7974 test Loss 57.3968 with MSE metric 39194.9978\n",
      "Epoch 76 batch 140 train Loss 135.7293 test Loss 57.3700 with MSE metric 39179.8237\n",
      "Epoch 76 batch 150 train Loss 135.6613 test Loss 57.3434 with MSE metric 39164.6366\n",
      "Epoch 76 batch 160 train Loss 135.5934 test Loss 57.3167 with MSE metric 39149.2184\n",
      "Epoch 76 batch 170 train Loss 135.5255 test Loss 57.2901 with MSE metric 39134.1760\n",
      "Epoch 76 batch 180 train Loss 135.4578 test Loss 57.2635 with MSE metric 39119.2946\n",
      "Epoch 76 batch 190 train Loss 135.3901 test Loss 57.2368 with MSE metric 39104.2474\n",
      "Epoch 76 batch 200 train Loss 135.3224 test Loss 57.2103 with MSE metric 39089.0369\n",
      "Epoch 76 batch 210 train Loss 135.2548 test Loss 57.1837 with MSE metric 39074.3843\n",
      "Epoch 76 batch 220 train Loss 135.1873 test Loss 57.1572 with MSE metric 39059.4687\n",
      "Epoch 76 batch 230 train Loss 135.1199 test Loss 57.1307 with MSE metric 39044.6261\n",
      "Epoch 76 batch 240 train Loss 135.0525 test Loss 57.1043 with MSE metric 39029.3246\n",
      "Time taken for 1 epoch: 28.293371200561523 secs\n",
      "\n",
      "Epoch 77 batch 0 train Loss 134.9852 test Loss 57.0778 with MSE metric 39014.0633\n",
      "Epoch 77 batch 10 train Loss 134.9180 test Loss 57.0514 with MSE metric 38998.9089\n",
      "Epoch 77 batch 20 train Loss 134.8508 test Loss 57.0250 with MSE metric 38983.8725\n",
      "Epoch 77 batch 30 train Loss 134.7837 test Loss 56.9987 with MSE metric 38968.8196\n",
      "Epoch 77 batch 40 train Loss 134.7167 test Loss 56.9723 with MSE metric 38954.1289\n",
      "Epoch 77 batch 50 train Loss 134.6497 test Loss 56.9460 with MSE metric 38939.0289\n",
      "Epoch 77 batch 60 train Loss 134.5829 test Loss 56.9197 with MSE metric 38923.7256\n",
      "Epoch 77 batch 70 train Loss 134.5161 test Loss 56.8935 with MSE metric 38908.7992\n",
      "Epoch 77 batch 80 train Loss 134.4493 test Loss 56.8673 with MSE metric 38893.8241\n",
      "Epoch 77 batch 90 train Loss 134.3826 test Loss 56.8411 with MSE metric 38878.5880\n",
      "Epoch 77 batch 100 train Loss 134.3160 test Loss 56.8149 with MSE metric 38863.6772\n",
      "Epoch 77 batch 110 train Loss 134.2495 test Loss 56.7887 with MSE metric 38848.7453\n",
      "Epoch 77 batch 120 train Loss 134.1830 test Loss 56.7626 with MSE metric 38833.8226\n",
      "Epoch 77 batch 130 train Loss 134.1166 test Loss 56.7365 with MSE metric 38818.8083\n",
      "Epoch 77 batch 140 train Loss 134.0503 test Loss 56.7105 with MSE metric 38803.9108\n",
      "Epoch 77 batch 150 train Loss 133.9840 test Loss 56.6844 with MSE metric 38789.1250\n",
      "Epoch 77 batch 160 train Loss 133.9178 test Loss 56.6584 with MSE metric 38774.3016\n",
      "Epoch 77 batch 170 train Loss 133.8517 test Loss 56.6324 with MSE metric 38759.9262\n",
      "Epoch 77 batch 180 train Loss 133.7856 test Loss 56.6064 with MSE metric 38744.8547\n",
      "Epoch 77 batch 190 train Loss 133.7196 test Loss 56.5805 with MSE metric 38729.6934\n",
      "Epoch 77 batch 200 train Loss 133.6537 test Loss 56.5546 with MSE metric 38714.8364\n",
      "Epoch 77 batch 210 train Loss 133.5878 test Loss 56.5287 with MSE metric 38699.7127\n",
      "Epoch 77 batch 220 train Loss 133.5221 test Loss 56.5028 with MSE metric 38684.6278\n",
      "Epoch 77 batch 230 train Loss 133.4563 test Loss 56.4770 with MSE metric 38670.0957\n",
      "Epoch 77 batch 240 train Loss 133.3907 test Loss 56.4512 with MSE metric 38655.0636\n",
      "Time taken for 1 epoch: 28.490095853805542 secs\n",
      "\n",
      "Epoch 78 batch 0 train Loss 133.3251 test Loss 56.4254 with MSE metric 38640.0711\n",
      "Epoch 78 batch 10 train Loss 133.2595 test Loss 56.3995 with MSE metric 38625.1871\n",
      "Epoch 78 batch 20 train Loss 133.1941 test Loss 56.3738 with MSE metric 38610.4693\n",
      "Epoch 78 batch 30 train Loss 133.1287 test Loss 56.3481 with MSE metric 38595.6391\n",
      "Epoch 78 batch 40 train Loss 133.0634 test Loss 56.3224 with MSE metric 38581.1735\n",
      "Epoch 78 batch 50 train Loss 132.9981 test Loss 56.2967 with MSE metric 38566.2812\n",
      "Epoch 78 batch 60 train Loss 132.9329 test Loss 56.2711 with MSE metric 38551.3894\n",
      "Epoch 78 batch 70 train Loss 132.8678 test Loss 56.2454 with MSE metric 38536.4769\n",
      "Epoch 78 batch 80 train Loss 132.8027 test Loss 56.2198 with MSE metric 38521.3645\n",
      "Epoch 78 batch 90 train Loss 132.7377 test Loss 56.1942 with MSE metric 38506.9022\n",
      "Epoch 78 batch 100 train Loss 132.6728 test Loss 56.1687 with MSE metric 38491.9813\n",
      "Epoch 78 batch 110 train Loss 132.6079 test Loss 56.1432 with MSE metric 38476.9965\n",
      "Epoch 78 batch 120 train Loss 132.5431 test Loss 56.1177 with MSE metric 38462.4756\n",
      "Epoch 78 batch 130 train Loss 132.4784 test Loss 56.0922 with MSE metric 38447.4838\n",
      "Epoch 78 batch 140 train Loss 132.4137 test Loss 56.0668 with MSE metric 38432.4455\n",
      "Epoch 78 batch 150 train Loss 132.3491 test Loss 56.0413 with MSE metric 38417.8679\n",
      "Epoch 78 batch 160 train Loss 132.2846 test Loss 56.0159 with MSE metric 38403.3239\n",
      "Epoch 78 batch 170 train Loss 132.2201 test Loss 55.9905 with MSE metric 38388.5080\n",
      "Epoch 78 batch 180 train Loss 132.1557 test Loss 55.9652 with MSE metric 38373.6801\n",
      "Epoch 78 batch 190 train Loss 132.0913 test Loss 55.9399 with MSE metric 38358.9644\n",
      "Epoch 78 batch 200 train Loss 132.0270 test Loss 55.9146 with MSE metric 38344.4728\n",
      "Epoch 78 batch 210 train Loss 131.9628 test Loss 55.8893 with MSE metric 38329.4662\n",
      "Epoch 78 batch 220 train Loss 131.8987 test Loss 55.8641 with MSE metric 38315.1627\n",
      "Epoch 78 batch 230 train Loss 131.8346 test Loss 55.8388 with MSE metric 38300.3013\n",
      "Epoch 78 batch 240 train Loss 131.7706 test Loss 55.8136 with MSE metric 38285.5020\n",
      "Time taken for 1 epoch: 26.25069499015808 secs\n",
      "\n",
      "Epoch 79 batch 0 train Loss 131.7066 test Loss 55.7884 with MSE metric 38270.9527\n",
      "Epoch 79 batch 10 train Loss 131.6427 test Loss 55.7632 with MSE metric 38256.2198\n",
      "Epoch 79 batch 20 train Loss 131.5789 test Loss 55.7381 with MSE metric 38241.6214\n",
      "Epoch 79 batch 30 train Loss 131.5151 test Loss 55.7130 with MSE metric 38227.0034\n",
      "Epoch 79 batch 40 train Loss 131.4515 test Loss 55.6879 with MSE metric 38212.1441\n",
      "Epoch 79 batch 50 train Loss 131.3878 test Loss 55.6629 with MSE metric 38197.4192\n",
      "Epoch 79 batch 60 train Loss 131.3243 test Loss 55.6378 with MSE metric 38182.6350\n",
      "Epoch 79 batch 70 train Loss 131.2608 test Loss 55.6128 with MSE metric 38168.1558\n",
      "Epoch 79 batch 80 train Loss 131.1973 test Loss 55.5878 with MSE metric 38153.9717\n",
      "Epoch 79 batch 90 train Loss 131.1339 test Loss 55.5628 with MSE metric 38139.5295\n",
      "Epoch 79 batch 100 train Loss 131.0706 test Loss 55.5379 with MSE metric 38125.0231\n",
      "Epoch 79 batch 110 train Loss 131.0074 test Loss 55.5130 with MSE metric 38110.3974\n",
      "Epoch 79 batch 120 train Loss 130.9442 test Loss 55.4881 with MSE metric 38096.2828\n",
      "Epoch 79 batch 130 train Loss 130.8811 test Loss 55.4633 with MSE metric 38081.8592\n",
      "Epoch 79 batch 140 train Loss 130.8180 test Loss 55.4384 with MSE metric 38067.2644\n",
      "Epoch 79 batch 150 train Loss 130.7550 test Loss 55.4136 with MSE metric 38052.9136\n",
      "Epoch 79 batch 160 train Loss 130.6921 test Loss 55.3888 with MSE metric 38038.2927\n",
      "Epoch 79 batch 170 train Loss 130.6292 test Loss 55.3640 with MSE metric 38023.8726\n",
      "Epoch 79 batch 180 train Loss 130.5664 test Loss 55.3393 with MSE metric 38009.6347\n",
      "Epoch 79 batch 190 train Loss 130.5037 test Loss 55.3146 with MSE metric 37995.4391\n",
      "Epoch 79 batch 200 train Loss 130.4410 test Loss 55.2899 with MSE metric 37981.0020\n",
      "Epoch 79 batch 210 train Loss 130.3784 test Loss 55.2652 with MSE metric 37966.5081\n",
      "Epoch 79 batch 220 train Loss 130.3158 test Loss 55.2406 with MSE metric 37952.0925\n",
      "Epoch 79 batch 230 train Loss 130.2533 test Loss 55.2159 with MSE metric 37937.7177\n",
      "Epoch 79 batch 240 train Loss 130.1909 test Loss 55.1913 with MSE metric 37923.3867\n",
      "Time taken for 1 epoch: 28.670654296875 secs\n",
      "\n",
      "Epoch 80 batch 0 train Loss 130.1285 test Loss 55.1667 with MSE metric 37909.2052\n",
      "Epoch 80 batch 10 train Loss 130.0662 test Loss 55.1422 with MSE metric 37895.2866\n",
      "Epoch 80 batch 20 train Loss 130.0040 test Loss 55.1177 with MSE metric 37881.1669\n",
      "Epoch 80 batch 30 train Loss 129.9418 test Loss 55.0931 with MSE metric 37866.9995\n",
      "Epoch 80 batch 40 train Loss 129.8797 test Loss 55.0686 with MSE metric 37852.7332\n",
      "Epoch 80 batch 50 train Loss 129.8176 test Loss 55.0441 with MSE metric 37838.4936\n",
      "Epoch 80 batch 60 train Loss 129.7556 test Loss 55.0197 with MSE metric 37824.1689\n",
      "Epoch 80 batch 70 train Loss 129.6937 test Loss 54.9953 with MSE metric 37809.7514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 batch 80 train Loss 129.6318 test Loss 54.9709 with MSE metric 37795.2310\n",
      "Epoch 80 batch 90 train Loss 129.5700 test Loss 54.9465 with MSE metric 37780.9588\n",
      "Epoch 80 batch 100 train Loss 129.5082 test Loss 54.9222 with MSE metric 37766.7114\n",
      "Epoch 80 batch 110 train Loss 129.4465 test Loss 54.8979 with MSE metric 37752.4802\n",
      "Epoch 80 batch 120 train Loss 129.3849 test Loss 54.8736 with MSE metric 37738.4823\n",
      "Epoch 80 batch 130 train Loss 129.3233 test Loss 54.8493 with MSE metric 37724.2958\n",
      "Epoch 80 batch 140 train Loss 129.2618 test Loss 54.8250 with MSE metric 37710.2895\n",
      "Epoch 80 batch 150 train Loss 129.2004 test Loss 54.8008 with MSE metric 37696.2608\n",
      "Epoch 80 batch 160 train Loss 129.1389 test Loss 54.7765 with MSE metric 37681.8415\n",
      "Epoch 80 batch 170 train Loss 129.0776 test Loss 54.7523 with MSE metric 37667.8520\n",
      "Epoch 80 batch 180 train Loss 129.0164 test Loss 54.7281 with MSE metric 37653.4626\n",
      "Epoch 80 batch 190 train Loss 128.9551 test Loss 54.7040 with MSE metric 37639.1241\n",
      "Epoch 80 batch 200 train Loss 128.8940 test Loss 54.6799 with MSE metric 37624.8232\n",
      "Epoch 80 batch 210 train Loss 128.8329 test Loss 54.6558 with MSE metric 37610.6792\n",
      "Epoch 80 batch 220 train Loss 128.7719 test Loss 54.6317 with MSE metric 37596.4860\n",
      "Epoch 80 batch 230 train Loss 128.7109 test Loss 54.6076 with MSE metric 37582.2471\n",
      "Epoch 80 batch 240 train Loss 128.6501 test Loss 54.5836 with MSE metric 37568.2681\n",
      "Time taken for 1 epoch: 30.585593223571777 secs\n",
      "\n",
      "Epoch 81 batch 0 train Loss 128.5892 test Loss 54.5596 with MSE metric 37554.1452\n",
      "Epoch 81 batch 10 train Loss 128.5284 test Loss 54.5356 with MSE metric 37540.0550\n",
      "Epoch 81 batch 20 train Loss 128.4677 test Loss 54.5117 with MSE metric 37525.9173\n",
      "Epoch 81 batch 30 train Loss 128.4071 test Loss 54.4877 with MSE metric 37512.1257\n",
      "Epoch 81 batch 40 train Loss 128.3465 test Loss 54.4638 with MSE metric 37497.9043\n",
      "Epoch 81 batch 50 train Loss 128.2859 test Loss 54.4399 with MSE metric 37483.8179\n",
      "Epoch 81 batch 60 train Loss 128.2254 test Loss 54.4160 with MSE metric 37470.1824\n",
      "Epoch 81 batch 70 train Loss 128.1650 test Loss 54.3922 with MSE metric 37456.2096\n",
      "Epoch 81 batch 80 train Loss 128.1047 test Loss 54.3684 with MSE metric 37441.9643\n",
      "Epoch 81 batch 90 train Loss 128.0443 test Loss 54.3446 with MSE metric 37427.9230\n",
      "Epoch 81 batch 100 train Loss 127.9841 test Loss 54.3208 with MSE metric 37413.9359\n",
      "Epoch 81 batch 110 train Loss 127.9239 test Loss 54.2970 with MSE metric 37400.2938\n",
      "Epoch 81 batch 120 train Loss 127.8638 test Loss 54.2733 with MSE metric 37386.3343\n",
      "Epoch 81 batch 130 train Loss 127.8037 test Loss 54.2496 with MSE metric 37372.6961\n",
      "Epoch 81 batch 140 train Loss 127.7437 test Loss 54.2259 with MSE metric 37358.7445\n",
      "Epoch 81 batch 150 train Loss 127.6837 test Loss 54.2022 with MSE metric 37344.7822\n",
      "Epoch 81 batch 160 train Loss 127.6238 test Loss 54.1785 with MSE metric 37331.2113\n",
      "Epoch 81 batch 170 train Loss 127.5640 test Loss 54.1549 with MSE metric 37317.5284\n",
      "Epoch 81 batch 180 train Loss 127.5042 test Loss 54.1313 with MSE metric 37303.6246\n",
      "Epoch 81 batch 190 train Loss 127.4445 test Loss 54.1077 with MSE metric 37289.6973\n",
      "Epoch 81 batch 200 train Loss 127.3848 test Loss 54.0841 with MSE metric 37275.7505\n",
      "Epoch 81 batch 210 train Loss 127.3252 test Loss 54.0606 with MSE metric 37262.0182\n",
      "Epoch 81 batch 220 train Loss 127.2657 test Loss 54.0371 with MSE metric 37248.1477\n",
      "Epoch 81 batch 230 train Loss 127.2062 test Loss 54.0136 with MSE metric 37233.9302\n",
      "Epoch 81 batch 240 train Loss 127.1467 test Loss 53.9901 with MSE metric 37220.1932\n",
      "Time taken for 1 epoch: 29.99742603302002 secs\n",
      "\n",
      "Epoch 82 batch 0 train Loss 127.0874 test Loss 53.9666 with MSE metric 37206.2590\n",
      "Epoch 82 batch 10 train Loss 127.0281 test Loss 53.9432 with MSE metric 37192.4407\n",
      "Epoch 82 batch 20 train Loss 126.9688 test Loss 53.9198 with MSE metric 37178.6591\n",
      "Epoch 82 batch 30 train Loss 126.9096 test Loss 53.8964 with MSE metric 37165.0671\n",
      "Epoch 82 batch 40 train Loss 126.8505 test Loss 53.8730 with MSE metric 37151.4464\n",
      "Epoch 82 batch 50 train Loss 126.7914 test Loss 53.8498 with MSE metric 37137.8901\n",
      "Epoch 82 batch 60 train Loss 126.7324 test Loss 53.8264 with MSE metric 37124.0000\n",
      "Epoch 82 batch 70 train Loss 126.6734 test Loss 53.8031 with MSE metric 37110.3543\n",
      "Epoch 82 batch 80 train Loss 126.6145 test Loss 53.7798 with MSE metric 37096.5516\n",
      "Epoch 82 batch 90 train Loss 126.5556 test Loss 53.7566 with MSE metric 37082.6849\n",
      "Epoch 82 batch 100 train Loss 126.4968 test Loss 53.7334 with MSE metric 37069.2253\n",
      "Epoch 82 batch 110 train Loss 126.4381 test Loss 53.7101 with MSE metric 37055.8611\n",
      "Epoch 82 batch 120 train Loss 126.3794 test Loss 53.6870 with MSE metric 37042.5181\n",
      "Epoch 82 batch 130 train Loss 126.3208 test Loss 53.6638 with MSE metric 37028.7034\n",
      "Epoch 82 batch 140 train Loss 126.2622 test Loss 53.6406 with MSE metric 37015.2007\n",
      "Epoch 82 batch 150 train Loss 126.2037 test Loss 53.6175 with MSE metric 37001.6121\n",
      "Epoch 82 batch 160 train Loss 126.1452 test Loss 53.5944 with MSE metric 36987.8973\n",
      "Epoch 82 batch 170 train Loss 126.0868 test Loss 53.5713 with MSE metric 36973.8281\n",
      "Epoch 82 batch 180 train Loss 126.0285 test Loss 53.5482 with MSE metric 36960.3531\n",
      "Epoch 82 batch 190 train Loss 125.9702 test Loss 53.5252 with MSE metric 36946.6795\n",
      "Epoch 82 batch 200 train Loss 125.9119 test Loss 53.5021 with MSE metric 36932.9769\n",
      "Epoch 82 batch 210 train Loss 125.8537 test Loss 53.4792 with MSE metric 36919.2049\n",
      "Epoch 82 batch 220 train Loss 125.7956 test Loss 53.4561 with MSE metric 36905.7147\n",
      "Epoch 82 batch 230 train Loss 125.7375 test Loss 53.4332 with MSE metric 36892.1182\n",
      "Epoch 82 batch 240 train Loss 125.6795 test Loss 53.4103 with MSE metric 36878.5675\n",
      "Time taken for 1 epoch: 26.556370973587036 secs\n",
      "\n",
      "Epoch 83 batch 0 train Loss 125.6216 test Loss 53.3873 with MSE metric 36865.3364\n",
      "Epoch 83 batch 10 train Loss 125.5637 test Loss 53.3644 with MSE metric 36851.6711\n",
      "Epoch 83 batch 20 train Loss 125.5058 test Loss 53.3416 with MSE metric 36838.3566\n",
      "Epoch 83 batch 30 train Loss 125.4480 test Loss 53.3187 with MSE metric 36824.6944\n",
      "Epoch 83 batch 40 train Loss 125.3903 test Loss 53.2959 with MSE metric 36811.2905\n",
      "Epoch 83 batch 50 train Loss 125.3326 test Loss 53.2731 with MSE metric 36797.8951\n",
      "Epoch 83 batch 60 train Loss 125.2750 test Loss 53.2503 with MSE metric 36784.4124\n",
      "Epoch 83 batch 70 train Loss 125.2174 test Loss 53.2275 with MSE metric 36770.9667\n",
      "Epoch 83 batch 80 train Loss 125.1599 test Loss 53.2048 with MSE metric 36757.4183\n",
      "Epoch 83 batch 90 train Loss 125.1025 test Loss 53.1821 with MSE metric 36744.0589\n",
      "Epoch 83 batch 100 train Loss 125.0450 test Loss 53.1593 with MSE metric 36730.5977\n",
      "Epoch 83 batch 110 train Loss 124.9877 test Loss 53.1366 with MSE metric 36717.0665\n",
      "Epoch 83 batch 120 train Loss 124.9304 test Loss 53.1140 with MSE metric 36703.7982\n",
      "Epoch 83 batch 130 train Loss 124.8732 test Loss 53.0913 with MSE metric 36690.6089\n",
      "Epoch 83 batch 140 train Loss 124.8160 test Loss 53.0687 with MSE metric 36677.1981\n",
      "Epoch 83 batch 150 train Loss 124.7589 test Loss 53.0461 with MSE metric 36663.7738\n",
      "Epoch 83 batch 160 train Loss 124.7018 test Loss 53.0235 with MSE metric 36650.1199\n",
      "Epoch 83 batch 170 train Loss 124.6448 test Loss 53.0010 with MSE metric 36636.7852\n",
      "Epoch 83 batch 180 train Loss 124.5878 test Loss 52.9784 with MSE metric 36623.4023\n",
      "Epoch 83 batch 190 train Loss 124.5309 test Loss 52.9559 with MSE metric 36609.9750\n",
      "Epoch 83 batch 200 train Loss 124.4740 test Loss 52.9334 with MSE metric 36596.7163\n",
      "Epoch 83 batch 210 train Loss 124.4173 test Loss 52.9110 with MSE metric 36583.3592\n",
      "Epoch 83 batch 220 train Loss 124.3605 test Loss 52.8885 with MSE metric 36570.2660\n",
      "Epoch 83 batch 230 train Loss 124.3038 test Loss 52.8661 with MSE metric 36557.3109\n",
      "Epoch 83 batch 240 train Loss 124.2472 test Loss 52.8436 with MSE metric 36544.0717\n",
      "Time taken for 1 epoch: 27.831175327301025 secs\n",
      "\n",
      "Epoch 84 batch 0 train Loss 124.1906 test Loss 52.8212 with MSE metric 36530.7556\n",
      "Epoch 84 batch 10 train Loss 124.1341 test Loss 52.7989 with MSE metric 36517.5496\n",
      "Epoch 84 batch 20 train Loss 124.0776 test Loss 52.7765 with MSE metric 36504.4115\n",
      "Epoch 84 batch 30 train Loss 124.0212 test Loss 52.7542 with MSE metric 36491.2317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 batch 40 train Loss 123.9648 test Loss 52.7319 with MSE metric 36478.2039\n",
      "Epoch 84 batch 50 train Loss 123.9085 test Loss 52.7096 with MSE metric 36464.8137\n",
      "Epoch 84 batch 60 train Loss 123.8522 test Loss 52.6873 with MSE metric 36451.4320\n",
      "Epoch 84 batch 70 train Loss 123.7960 test Loss 52.6650 with MSE metric 36438.3814\n",
      "Epoch 84 batch 80 train Loss 123.7398 test Loss 52.6428 with MSE metric 36425.4428\n",
      "Epoch 84 batch 90 train Loss 123.6837 test Loss 52.6206 with MSE metric 36412.3501\n",
      "Epoch 84 batch 100 train Loss 123.6277 test Loss 52.5984 with MSE metric 36399.2930\n",
      "Epoch 84 batch 110 train Loss 123.5717 test Loss 52.5762 with MSE metric 36386.0748\n",
      "Epoch 84 batch 120 train Loss 123.5157 test Loss 52.5540 with MSE metric 36372.9495\n",
      "Epoch 84 batch 130 train Loss 123.4598 test Loss 52.5319 with MSE metric 36360.0178\n",
      "Epoch 84 batch 140 train Loss 123.4040 test Loss 52.5098 with MSE metric 36346.8503\n",
      "Epoch 84 batch 150 train Loss 123.3482 test Loss 52.4877 with MSE metric 36333.8904\n",
      "Epoch 84 batch 160 train Loss 123.2924 test Loss 52.4656 with MSE metric 36320.8083\n",
      "Epoch 84 batch 170 train Loss 123.2367 test Loss 52.4436 with MSE metric 36307.6770\n",
      "Epoch 84 batch 180 train Loss 123.1811 test Loss 52.4215 with MSE metric 36294.8356\n",
      "Epoch 84 batch 190 train Loss 123.1255 test Loss 52.3995 with MSE metric 36281.8847\n",
      "Epoch 84 batch 200 train Loss 123.0700 test Loss 52.3776 with MSE metric 36268.7868\n",
      "Epoch 84 batch 210 train Loss 123.0145 test Loss 52.3556 with MSE metric 36255.8645\n",
      "Epoch 84 batch 220 train Loss 122.9591 test Loss 52.3337 with MSE metric 36243.3213\n",
      "Epoch 84 batch 230 train Loss 122.9037 test Loss 52.3117 with MSE metric 36230.2997\n",
      "Epoch 84 batch 240 train Loss 122.8484 test Loss 52.2898 with MSE metric 36217.0667\n",
      "Time taken for 1 epoch: 28.473644018173218 secs\n",
      "\n",
      "Epoch 85 batch 0 train Loss 122.7932 test Loss 52.2679 with MSE metric 36204.5103\n",
      "Epoch 85 batch 10 train Loss 122.7379 test Loss 52.2461 with MSE metric 36191.3732\n",
      "Epoch 85 batch 20 train Loss 122.6828 test Loss 52.2243 with MSE metric 36178.8371\n",
      "Epoch 85 batch 30 train Loss 122.6277 test Loss 52.2024 with MSE metric 36165.5853\n",
      "Epoch 85 batch 40 train Loss 122.5726 test Loss 52.1806 with MSE metric 36152.6394\n",
      "Epoch 85 batch 50 train Loss 122.5176 test Loss 52.1588 with MSE metric 36139.9085\n",
      "Epoch 85 batch 60 train Loss 122.4627 test Loss 52.1370 with MSE metric 36127.0976\n",
      "Epoch 85 batch 70 train Loss 122.4077 test Loss 52.1153 with MSE metric 36114.1450\n",
      "Epoch 85 batch 80 train Loss 122.3529 test Loss 52.0935 with MSE metric 36101.3696\n",
      "Epoch 85 batch 90 train Loss 122.2981 test Loss 52.0718 with MSE metric 36088.3648\n",
      "Epoch 85 batch 100 train Loss 122.2434 test Loss 52.0502 with MSE metric 36075.7586\n",
      "Epoch 85 batch 110 train Loss 122.1886 test Loss 52.0285 with MSE metric 36062.9581\n",
      "Epoch 85 batch 120 train Loss 122.1340 test Loss 52.0068 with MSE metric 36050.0883\n",
      "Epoch 85 batch 130 train Loss 122.0794 test Loss 51.9852 with MSE metric 36037.5171\n",
      "Epoch 85 batch 140 train Loss 122.0249 test Loss 51.9636 with MSE metric 36024.7875\n",
      "Epoch 85 batch 150 train Loss 121.9704 test Loss 51.9420 with MSE metric 36011.9313\n",
      "Epoch 85 batch 160 train Loss 121.9159 test Loss 51.9205 with MSE metric 35999.3886\n",
      "Epoch 85 batch 170 train Loss 121.8615 test Loss 51.8989 with MSE metric 35986.6099\n",
      "Epoch 85 batch 180 train Loss 121.8072 test Loss 51.8774 with MSE metric 35974.0050\n",
      "Epoch 85 batch 190 train Loss 121.7529 test Loss 51.8559 with MSE metric 35961.0748\n",
      "Epoch 85 batch 200 train Loss 121.6987 test Loss 51.8344 with MSE metric 35948.4587\n",
      "Epoch 85 batch 210 train Loss 121.6445 test Loss 51.8129 with MSE metric 35935.7789\n",
      "Epoch 85 batch 220 train Loss 121.5903 test Loss 51.7914 with MSE metric 35923.1786\n",
      "Epoch 85 batch 230 train Loss 121.5362 test Loss 51.7700 with MSE metric 35910.2983\n",
      "Epoch 85 batch 240 train Loss 121.4822 test Loss 51.7486 with MSE metric 35897.6131\n",
      "Time taken for 1 epoch: 28.711445093154907 secs\n",
      "\n",
      "Epoch 86 batch 0 train Loss 121.4282 test Loss 51.7272 with MSE metric 35884.7798\n",
      "Epoch 86 batch 10 train Loss 121.3742 test Loss 51.7058 with MSE metric 35872.0719\n",
      "Epoch 86 batch 20 train Loss 121.3203 test Loss 51.6844 with MSE metric 35859.3630\n",
      "Epoch 86 batch 30 train Loss 121.2665 test Loss 51.6631 with MSE metric 35846.6328\n",
      "Epoch 86 batch 40 train Loss 121.2127 test Loss 51.6418 with MSE metric 35834.1344\n",
      "Epoch 86 batch 50 train Loss 121.1590 test Loss 51.6205 with MSE metric 35821.6434\n",
      "Epoch 86 batch 60 train Loss 121.1053 test Loss 51.5992 with MSE metric 35808.7649\n",
      "Epoch 86 batch 70 train Loss 121.0516 test Loss 51.5779 with MSE metric 35796.2711\n",
      "Epoch 86 batch 80 train Loss 120.9980 test Loss 51.5566 with MSE metric 35783.6608\n",
      "Epoch 86 batch 90 train Loss 120.9445 test Loss 51.5354 with MSE metric 35771.0033\n",
      "Epoch 86 batch 100 train Loss 120.8910 test Loss 51.5142 with MSE metric 35758.3220\n",
      "Epoch 86 batch 110 train Loss 120.8376 test Loss 51.4930 with MSE metric 35745.7917\n",
      "Epoch 86 batch 120 train Loss 120.7842 test Loss 51.4718 with MSE metric 35733.3462\n",
      "Epoch 86 batch 130 train Loss 120.7308 test Loss 51.4507 with MSE metric 35720.7724\n",
      "Epoch 86 batch 140 train Loss 120.6775 test Loss 51.4296 with MSE metric 35708.1471\n",
      "Epoch 86 batch 150 train Loss 120.6243 test Loss 51.4085 with MSE metric 35695.8876\n",
      "Epoch 86 batch 160 train Loss 120.5711 test Loss 51.3874 with MSE metric 35683.2825\n",
      "Epoch 86 batch 170 train Loss 120.5180 test Loss 51.3663 with MSE metric 35670.7274\n",
      "Epoch 86 batch 180 train Loss 120.4649 test Loss 51.3452 with MSE metric 35658.4502\n",
      "Epoch 86 batch 190 train Loss 120.4118 test Loss 51.3242 with MSE metric 35645.9689\n",
      "Epoch 86 batch 200 train Loss 120.3588 test Loss 51.3032 with MSE metric 35633.4245\n",
      "Epoch 86 batch 210 train Loss 120.3059 test Loss 51.2822 with MSE metric 35620.9262\n",
      "Epoch 86 batch 220 train Loss 120.2530 test Loss 51.2612 with MSE metric 35608.2648\n",
      "Epoch 86 batch 230 train Loss 120.2001 test Loss 51.2402 with MSE metric 35596.1672\n",
      "Epoch 86 batch 240 train Loss 120.1473 test Loss 51.2193 with MSE metric 35583.7518\n",
      "Time taken for 1 epoch: 26.9031240940094 secs\n",
      "\n",
      "Epoch 87 batch 0 train Loss 120.0945 test Loss 51.1983 with MSE metric 35571.5946\n",
      "Epoch 87 batch 10 train Loss 120.0418 test Loss 51.1774 with MSE metric 35559.0657\n",
      "Epoch 87 batch 20 train Loss 119.9892 test Loss 51.1565 with MSE metric 35546.4849\n",
      "Epoch 87 batch 30 train Loss 119.9366 test Loss 51.1357 with MSE metric 35534.1668\n",
      "Epoch 87 batch 40 train Loss 119.8840 test Loss 51.1148 with MSE metric 35521.8065\n",
      "Epoch 87 batch 50 train Loss 119.8315 test Loss 51.0939 with MSE metric 35509.4280\n",
      "Epoch 87 batch 60 train Loss 119.7790 test Loss 51.0731 with MSE metric 35496.9324\n",
      "Epoch 87 batch 70 train Loss 119.7266 test Loss 51.0524 with MSE metric 35484.8619\n",
      "Epoch 87 batch 80 train Loss 119.6742 test Loss 51.0316 with MSE metric 35472.3143\n",
      "Epoch 87 batch 90 train Loss 119.6219 test Loss 51.0108 with MSE metric 35460.1244\n",
      "Epoch 87 batch 100 train Loss 119.5696 test Loss 50.9901 with MSE metric 35447.9666\n",
      "Epoch 87 batch 110 train Loss 119.5174 test Loss 50.9694 with MSE metric 35435.7025\n",
      "Epoch 87 batch 120 train Loss 119.4652 test Loss 50.9487 with MSE metric 35423.6667\n",
      "Epoch 87 batch 130 train Loss 119.4131 test Loss 50.9280 with MSE metric 35411.4989\n",
      "Epoch 87 batch 140 train Loss 119.3610 test Loss 50.9073 with MSE metric 35399.3227\n",
      "Epoch 87 batch 150 train Loss 119.3090 test Loss 50.8867 with MSE metric 35387.0408\n",
      "Epoch 87 batch 160 train Loss 119.2570 test Loss 50.8660 with MSE metric 35374.5054\n",
      "Epoch 87 batch 170 train Loss 119.2050 test Loss 50.8454 with MSE metric 35362.2514\n",
      "Epoch 87 batch 180 train Loss 119.1531 test Loss 50.8248 with MSE metric 35350.0486\n",
      "Epoch 87 batch 190 train Loss 119.1013 test Loss 50.8042 with MSE metric 35337.8693\n",
      "Epoch 87 batch 200 train Loss 119.0495 test Loss 50.7837 with MSE metric 35325.5976\n",
      "Epoch 87 batch 210 train Loss 118.9977 test Loss 50.7631 with MSE metric 35313.0729\n",
      "Epoch 87 batch 220 train Loss 118.9460 test Loss 50.7426 with MSE metric 35300.8269\n",
      "Epoch 87 batch 230 train Loss 118.8943 test Loss 50.7221 with MSE metric 35288.6504\n",
      "Epoch 87 batch 240 train Loss 118.8427 test Loss 50.7016 with MSE metric 35276.5655\n",
      "Time taken for 1 epoch: 29.772953987121582 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 batch 0 train Loss 118.7912 test Loss 50.6811 with MSE metric 35264.1606\n",
      "Epoch 88 batch 10 train Loss 118.7396 test Loss 50.6607 with MSE metric 35251.7845\n",
      "Epoch 88 batch 20 train Loss 118.6882 test Loss 50.6403 with MSE metric 35239.4349\n",
      "Epoch 88 batch 30 train Loss 118.6367 test Loss 50.6198 with MSE metric 35227.2359\n",
      "Epoch 88 batch 40 train Loss 118.5853 test Loss 50.5994 with MSE metric 35214.9937\n",
      "Epoch 88 batch 50 train Loss 118.5340 test Loss 50.5791 with MSE metric 35202.9079\n",
      "Epoch 88 batch 60 train Loss 118.4827 test Loss 50.5587 with MSE metric 35190.8104\n",
      "Epoch 88 batch 70 train Loss 118.4315 test Loss 50.5383 with MSE metric 35178.8856\n",
      "Epoch 88 batch 80 train Loss 118.3803 test Loss 50.5180 with MSE metric 35166.7100\n",
      "Epoch 88 batch 90 train Loss 118.3291 test Loss 50.4977 with MSE metric 35154.8275\n",
      "Epoch 88 batch 100 train Loss 118.2780 test Loss 50.4774 with MSE metric 35142.9022\n",
      "Epoch 88 batch 110 train Loss 118.2270 test Loss 50.4571 with MSE metric 35130.8062\n",
      "Epoch 88 batch 120 train Loss 118.1760 test Loss 50.4369 with MSE metric 35118.8437\n",
      "Epoch 88 batch 130 train Loss 118.1250 test Loss 50.4167 with MSE metric 35106.8928\n",
      "Epoch 88 batch 140 train Loss 118.0741 test Loss 50.3964 with MSE metric 35094.7706\n",
      "Epoch 88 batch 150 train Loss 118.0232 test Loss 50.3763 with MSE metric 35082.9530\n",
      "Epoch 88 batch 160 train Loss 117.9724 test Loss 50.3561 with MSE metric 35071.1108\n",
      "Epoch 88 batch 170 train Loss 117.9216 test Loss 50.3359 with MSE metric 35059.2271\n",
      "Epoch 88 batch 180 train Loss 117.8709 test Loss 50.3158 with MSE metric 35047.3415\n",
      "Epoch 88 batch 190 train Loss 117.8202 test Loss 50.2956 with MSE metric 35035.3211\n",
      "Epoch 88 batch 200 train Loss 117.7695 test Loss 50.2755 with MSE metric 35023.4286\n",
      "Epoch 88 batch 210 train Loss 117.7189 test Loss 50.2554 with MSE metric 35011.3295\n",
      "Epoch 88 batch 220 train Loss 117.6684 test Loss 50.2353 with MSE metric 34999.4796\n",
      "Epoch 88 batch 230 train Loss 117.6179 test Loss 50.2153 with MSE metric 34987.7064\n",
      "Epoch 88 batch 240 train Loss 117.5674 test Loss 50.1953 with MSE metric 34975.4399\n",
      "Time taken for 1 epoch: 28.711122035980225 secs\n",
      "\n",
      "Epoch 89 batch 0 train Loss 117.5170 test Loss 50.1752 with MSE metric 34963.8214\n",
      "Epoch 89 batch 10 train Loss 117.4666 test Loss 50.1552 with MSE metric 34951.8742\n",
      "Epoch 89 batch 20 train Loss 117.4163 test Loss 50.1353 with MSE metric 34939.8266\n",
      "Epoch 89 batch 30 train Loss 117.3660 test Loss 50.1153 with MSE metric 34927.9616\n",
      "Epoch 89 batch 40 train Loss 117.3158 test Loss 50.0953 with MSE metric 34916.1217\n",
      "Epoch 89 batch 50 train Loss 117.2656 test Loss 50.0754 with MSE metric 34904.2652\n",
      "Epoch 89 batch 60 train Loss 117.2154 test Loss 50.0554 with MSE metric 34892.3672\n",
      "Epoch 89 batch 70 train Loss 117.1653 test Loss 50.0355 with MSE metric 34880.6198\n",
      "Epoch 89 batch 80 train Loss 117.1152 test Loss 50.0156 with MSE metric 34868.6848\n",
      "Epoch 89 batch 90 train Loss 117.0652 test Loss 49.9958 with MSE metric 34857.0178\n",
      "Epoch 89 batch 100 train Loss 117.0153 test Loss 49.9759 with MSE metric 34845.2691\n",
      "Epoch 89 batch 110 train Loss 116.9653 test Loss 49.9561 with MSE metric 34833.3267\n",
      "Epoch 89 batch 120 train Loss 116.9155 test Loss 49.9363 with MSE metric 34821.5429\n",
      "Epoch 89 batch 130 train Loss 116.8656 test Loss 49.9165 with MSE metric 34810.0474\n",
      "Epoch 89 batch 140 train Loss 116.8158 test Loss 49.8967 with MSE metric 34798.1686\n",
      "Epoch 89 batch 150 train Loss 116.7661 test Loss 49.8769 with MSE metric 34786.7227\n",
      "Epoch 89 batch 160 train Loss 116.7164 test Loss 49.8572 with MSE metric 34775.0784\n",
      "Epoch 89 batch 170 train Loss 116.6667 test Loss 49.8375 with MSE metric 34763.0049\n",
      "Epoch 89 batch 180 train Loss 116.6171 test Loss 49.8177 with MSE metric 34751.5088\n",
      "Epoch 89 batch 190 train Loss 116.5675 test Loss 49.7980 with MSE metric 34739.8179\n",
      "Epoch 89 batch 200 train Loss 116.5180 test Loss 49.7783 with MSE metric 34728.2458\n",
      "Epoch 89 batch 210 train Loss 116.4685 test Loss 49.7586 with MSE metric 34716.5814\n",
      "Epoch 89 batch 220 train Loss 116.4191 test Loss 49.7390 with MSE metric 34705.0800\n",
      "Epoch 89 batch 230 train Loss 116.3697 test Loss 49.7194 with MSE metric 34693.3330\n",
      "Epoch 89 batch 240 train Loss 116.3203 test Loss 49.6998 with MSE metric 34681.5996\n",
      "Time taken for 1 epoch: 28.93132185935974 secs\n",
      "\n",
      "Epoch 90 batch 0 train Loss 116.2710 test Loss 49.6802 with MSE metric 34669.6057\n",
      "Epoch 90 batch 10 train Loss 116.2218 test Loss 49.6606 with MSE metric 34657.9557\n",
      "Epoch 90 batch 20 train Loss 116.1726 test Loss 49.6410 with MSE metric 34646.5622\n",
      "Epoch 90 batch 30 train Loss 116.1234 test Loss 49.6215 with MSE metric 34635.0885\n",
      "Epoch 90 batch 40 train Loss 116.0743 test Loss 49.6020 with MSE metric 34623.3107\n",
      "Epoch 90 batch 50 train Loss 116.0252 test Loss 49.5824 with MSE metric 34611.6140\n",
      "Epoch 90 batch 60 train Loss 115.9761 test Loss 49.5629 with MSE metric 34600.2021\n",
      "Epoch 90 batch 70 train Loss 115.9271 test Loss 49.5435 with MSE metric 34588.7032\n",
      "Epoch 90 batch 80 train Loss 115.8782 test Loss 49.5240 with MSE metric 34577.2263\n",
      "Epoch 90 batch 90 train Loss 115.8293 test Loss 49.5045 with MSE metric 34565.6928\n",
      "Epoch 90 batch 100 train Loss 115.7804 test Loss 49.4851 with MSE metric 34553.9790\n",
      "Epoch 90 batch 110 train Loss 115.7316 test Loss 49.4657 with MSE metric 34542.3549\n",
      "Epoch 90 batch 120 train Loss 115.6828 test Loss 49.4463 with MSE metric 34530.8727\n",
      "Epoch 90 batch 130 train Loss 115.6340 test Loss 49.4269 with MSE metric 34519.1732\n",
      "Epoch 90 batch 140 train Loss 115.5853 test Loss 49.4076 with MSE metric 34507.6098\n",
      "Epoch 90 batch 150 train Loss 115.5367 test Loss 49.3882 with MSE metric 34496.0160\n",
      "Epoch 90 batch 160 train Loss 115.4881 test Loss 49.3689 with MSE metric 34484.4661\n",
      "Epoch 90 batch 170 train Loss 115.4395 test Loss 49.3496 with MSE metric 34472.8018\n",
      "Epoch 90 batch 180 train Loss 115.3910 test Loss 49.3303 with MSE metric 34461.4711\n",
      "Epoch 90 batch 190 train Loss 115.3425 test Loss 49.3110 with MSE metric 34450.2122\n",
      "Epoch 90 batch 200 train Loss 115.2940 test Loss 49.2917 with MSE metric 34438.9307\n",
      "Epoch 90 batch 210 train Loss 115.2456 test Loss 49.2725 with MSE metric 34427.2819\n",
      "Epoch 90 batch 220 train Loss 115.1973 test Loss 49.2533 with MSE metric 34415.8056\n",
      "Epoch 90 batch 230 train Loss 115.1490 test Loss 49.2340 with MSE metric 34403.9961\n",
      "Epoch 90 batch 240 train Loss 115.1007 test Loss 49.2149 with MSE metric 34392.8294\n",
      "Time taken for 1 epoch: 28.234251976013184 secs\n",
      "\n",
      "Epoch 91 batch 0 train Loss 115.0525 test Loss 49.1957 with MSE metric 34381.5051\n",
      "Epoch 91 batch 10 train Loss 115.0043 test Loss 49.1765 with MSE metric 34369.7702\n",
      "Epoch 91 batch 20 train Loss 114.9561 test Loss 49.1574 with MSE metric 34358.6047\n",
      "Epoch 91 batch 30 train Loss 114.9080 test Loss 49.1382 with MSE metric 34347.2081\n",
      "Epoch 91 batch 40 train Loss 114.8600 test Loss 49.1191 with MSE metric 34335.6948\n",
      "Epoch 91 batch 50 train Loss 114.8119 test Loss 49.1000 with MSE metric 34324.0664\n",
      "Epoch 91 batch 60 train Loss 114.7640 test Loss 49.0810 with MSE metric 34312.5360\n",
      "Epoch 91 batch 70 train Loss 114.7160 test Loss 49.0619 with MSE metric 34301.2800\n",
      "Epoch 91 batch 80 train Loss 114.6681 test Loss 49.0428 with MSE metric 34290.0468\n",
      "Epoch 91 batch 90 train Loss 114.6203 test Loss 49.0238 with MSE metric 34278.7930\n",
      "Epoch 91 batch 100 train Loss 114.5725 test Loss 49.0048 with MSE metric 34267.7794\n",
      "Epoch 91 batch 110 train Loss 114.5247 test Loss 48.9858 with MSE metric 34256.3674\n",
      "Epoch 91 batch 120 train Loss 114.4770 test Loss 48.9668 with MSE metric 34245.2848\n",
      "Epoch 91 batch 130 train Loss 114.4293 test Loss 48.9478 with MSE metric 34234.0731\n",
      "Epoch 91 batch 140 train Loss 114.3817 test Loss 48.9289 with MSE metric 34222.7388\n",
      "Epoch 91 batch 150 train Loss 114.3341 test Loss 48.9100 with MSE metric 34211.2392\n",
      "Epoch 91 batch 160 train Loss 114.2865 test Loss 48.8910 with MSE metric 34199.9172\n",
      "Epoch 91 batch 170 train Loss 114.2390 test Loss 48.8721 with MSE metric 34188.9039\n",
      "Epoch 91 batch 180 train Loss 114.1915 test Loss 48.8533 with MSE metric 34177.5470\n",
      "Epoch 91 batch 190 train Loss 114.1441 test Loss 48.8344 with MSE metric 34166.3138\n",
      "Epoch 91 batch 200 train Loss 114.0967 test Loss 48.8156 with MSE metric 34155.0892\n",
      "Epoch 91 batch 210 train Loss 114.0494 test Loss 48.7967 with MSE metric 34144.2325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 batch 220 train Loss 114.0021 test Loss 48.7779 with MSE metric 34133.0409\n",
      "Epoch 91 batch 230 train Loss 113.9548 test Loss 48.7591 with MSE metric 34121.9004\n",
      "Epoch 91 batch 240 train Loss 113.9075 test Loss 48.7403 with MSE metric 34110.8586\n",
      "Time taken for 1 epoch: 27.0658917427063 secs\n",
      "\n",
      "Epoch 92 batch 0 train Loss 113.8604 test Loss 48.7215 with MSE metric 34099.5668\n",
      "Epoch 92 batch 10 train Loss 113.8132 test Loss 48.7028 with MSE metric 34088.2863\n",
      "Epoch 92 batch 20 train Loss 113.7661 test Loss 48.6840 with MSE metric 34077.0376\n",
      "Epoch 92 batch 30 train Loss 113.7190 test Loss 48.6653 with MSE metric 34065.7620\n",
      "Epoch 92 batch 40 train Loss 113.6720 test Loss 48.6466 with MSE metric 34054.4325\n",
      "Epoch 92 batch 50 train Loss 113.6250 test Loss 48.6279 with MSE metric 34043.4206\n",
      "Epoch 92 batch 60 train Loss 113.5781 test Loss 48.6092 with MSE metric 34032.3460\n",
      "Epoch 92 batch 70 train Loss 113.5312 test Loss 48.5906 with MSE metric 34021.3367\n",
      "Epoch 92 batch 80 train Loss 113.4843 test Loss 48.5719 with MSE metric 34010.0142\n",
      "Epoch 92 batch 90 train Loss 113.4375 test Loss 48.5533 with MSE metric 33998.8778\n",
      "Epoch 92 batch 100 train Loss 113.3907 test Loss 48.5347 with MSE metric 33987.7504\n",
      "Epoch 92 batch 110 train Loss 113.3440 test Loss 48.5161 with MSE metric 33976.6596\n",
      "Epoch 92 batch 120 train Loss 113.2973 test Loss 48.4975 with MSE metric 33965.5938\n",
      "Epoch 92 batch 130 train Loss 113.2506 test Loss 48.4789 with MSE metric 33954.6676\n",
      "Epoch 92 batch 140 train Loss 113.2040 test Loss 48.4604 with MSE metric 33943.7377\n",
      "Epoch 92 batch 150 train Loss 113.1574 test Loss 48.4419 with MSE metric 33932.5317\n",
      "Epoch 92 batch 160 train Loss 113.1109 test Loss 48.4233 with MSE metric 33921.5122\n",
      "Epoch 92 batch 170 train Loss 113.0644 test Loss 48.4049 with MSE metric 33910.3084\n",
      "Epoch 92 batch 180 train Loss 113.0179 test Loss 48.3864 with MSE metric 33899.2825\n",
      "Epoch 92 batch 190 train Loss 112.9715 test Loss 48.3679 with MSE metric 33888.1899\n",
      "Epoch 92 batch 200 train Loss 112.9251 test Loss 48.3495 with MSE metric 33877.2388\n",
      "Epoch 92 batch 210 train Loss 112.8788 test Loss 48.3310 with MSE metric 33866.1667\n",
      "Epoch 92 batch 220 train Loss 112.8325 test Loss 48.3126 with MSE metric 33855.2464\n",
      "Epoch 92 batch 230 train Loss 112.7862 test Loss 48.2942 with MSE metric 33844.2547\n",
      "Epoch 92 batch 240 train Loss 112.7400 test Loss 48.2758 with MSE metric 33833.3165\n",
      "Time taken for 1 epoch: 30.692069053649902 secs\n",
      "\n",
      "Epoch 93 batch 0 train Loss 112.6938 test Loss 48.2574 with MSE metric 33822.4563\n",
      "Epoch 93 batch 10 train Loss 112.6477 test Loss 48.2390 with MSE metric 33811.5394\n",
      "Epoch 93 batch 20 train Loss 112.6015 test Loss 48.2207 with MSE metric 33800.6440\n",
      "Epoch 93 batch 30 train Loss 112.5555 test Loss 48.2023 with MSE metric 33789.6729\n",
      "Epoch 93 batch 40 train Loss 112.5094 test Loss 48.1840 with MSE metric 33778.6100\n",
      "Epoch 93 batch 50 train Loss 112.4635 test Loss 48.1657 with MSE metric 33767.5643\n",
      "Epoch 93 batch 60 train Loss 112.4175 test Loss 48.1474 with MSE metric 33756.5172\n",
      "Epoch 93 batch 70 train Loss 112.3716 test Loss 48.1292 with MSE metric 33745.6577\n",
      "Epoch 93 batch 80 train Loss 112.3257 test Loss 48.1109 with MSE metric 33734.6148\n",
      "Epoch 93 batch 90 train Loss 112.2799 test Loss 48.0926 with MSE metric 33723.9084\n",
      "Epoch 93 batch 100 train Loss 112.2341 test Loss 48.0744 with MSE metric 33712.7708\n",
      "Epoch 93 batch 110 train Loss 112.1884 test Loss 48.0562 with MSE metric 33701.9712\n",
      "Epoch 93 batch 120 train Loss 112.1427 test Loss 48.0380 with MSE metric 33691.2884\n",
      "Epoch 93 batch 130 train Loss 112.0970 test Loss 48.0198 with MSE metric 33680.2992\n",
      "Epoch 93 batch 140 train Loss 112.0514 test Loss 48.0016 with MSE metric 33669.4677\n",
      "Epoch 93 batch 150 train Loss 112.0058 test Loss 47.9835 with MSE metric 33658.7497\n",
      "Epoch 93 batch 160 train Loss 111.9602 test Loss 47.9654 with MSE metric 33647.8791\n",
      "Epoch 93 batch 170 train Loss 111.9147 test Loss 47.9472 with MSE metric 33636.9411\n",
      "Epoch 93 batch 180 train Loss 111.8692 test Loss 47.9291 with MSE metric 33626.1840\n",
      "Epoch 93 batch 190 train Loss 111.8238 test Loss 47.9110 with MSE metric 33615.5724\n",
      "Epoch 93 batch 200 train Loss 111.7784 test Loss 47.8929 with MSE metric 33604.7079\n",
      "Epoch 93 batch 210 train Loss 111.7330 test Loss 47.8749 with MSE metric 33593.9999\n",
      "Epoch 93 batch 220 train Loss 111.6877 test Loss 47.8569 with MSE metric 33583.0324\n",
      "Epoch 93 batch 230 train Loss 111.6424 test Loss 47.8388 with MSE metric 33572.1539\n",
      "Epoch 93 batch 240 train Loss 111.5972 test Loss 47.8208 with MSE metric 33561.2642\n",
      "Time taken for 1 epoch: 28.66495394706726 secs\n",
      "\n",
      "Epoch 94 batch 0 train Loss 111.5520 test Loss 47.8028 with MSE metric 33550.3150\n",
      "Epoch 94 batch 10 train Loss 111.5068 test Loss 47.7848 with MSE metric 33539.1493\n",
      "Epoch 94 batch 20 train Loss 111.4617 test Loss 47.7669 with MSE metric 33528.2171\n",
      "Epoch 94 batch 30 train Loss 111.4166 test Loss 47.7489 with MSE metric 33517.4185\n",
      "Epoch 94 batch 40 train Loss 111.3716 test Loss 47.7309 with MSE metric 33506.5622\n",
      "Epoch 94 batch 50 train Loss 111.3265 test Loss 47.7130 with MSE metric 33495.8761\n",
      "Epoch 94 batch 60 train Loss 111.2816 test Loss 47.6951 with MSE metric 33485.0852\n",
      "Epoch 94 batch 70 train Loss 111.2366 test Loss 47.6772 with MSE metric 33474.0776\n",
      "Epoch 94 batch 80 train Loss 111.1917 test Loss 47.6593 with MSE metric 33463.1532\n",
      "Epoch 94 batch 90 train Loss 111.1469 test Loss 47.6415 with MSE metric 33452.3178\n",
      "Epoch 94 batch 100 train Loss 111.1020 test Loss 47.6236 with MSE metric 33441.7232\n",
      "Epoch 94 batch 110 train Loss 111.0573 test Loss 47.6058 with MSE metric 33430.7771\n",
      "Epoch 94 batch 120 train Loss 111.0125 test Loss 47.5880 with MSE metric 33419.9746\n",
      "Epoch 94 batch 130 train Loss 110.9678 test Loss 47.5702 with MSE metric 33409.5346\n",
      "Epoch 94 batch 140 train Loss 110.9231 test Loss 47.5524 with MSE metric 33398.8362\n",
      "Epoch 94 batch 150 train Loss 110.8785 test Loss 47.5347 with MSE metric 33388.1409\n",
      "Epoch 94 batch 160 train Loss 110.8339 test Loss 47.5169 with MSE metric 33377.4850\n",
      "Epoch 94 batch 170 train Loss 110.7893 test Loss 47.4991 with MSE metric 33366.8575\n",
      "Epoch 94 batch 180 train Loss 110.7448 test Loss 47.4814 with MSE metric 33356.1893\n",
      "Epoch 94 batch 190 train Loss 110.7003 test Loss 47.4637 with MSE metric 33345.5533\n",
      "Epoch 94 batch 200 train Loss 110.6559 test Loss 47.4460 with MSE metric 33334.7904\n",
      "Epoch 94 batch 210 train Loss 110.6115 test Loss 47.4283 with MSE metric 33323.9235\n",
      "Epoch 94 batch 220 train Loss 110.5671 test Loss 47.4106 with MSE metric 33313.1246\n",
      "Epoch 94 batch 230 train Loss 110.5228 test Loss 47.3930 with MSE metric 33302.5617\n",
      "Epoch 94 batch 240 train Loss 110.4785 test Loss 47.3753 with MSE metric 33292.0231\n",
      "Time taken for 1 epoch: 26.929031133651733 secs\n",
      "\n",
      "Epoch 95 batch 0 train Loss 110.4342 test Loss 47.3577 with MSE metric 33281.3839\n",
      "Epoch 95 batch 10 train Loss 110.3900 test Loss 47.3401 with MSE metric 33270.9367\n",
      "Epoch 95 batch 20 train Loss 110.3458 test Loss 47.3225 with MSE metric 33260.2302\n",
      "Epoch 95 batch 30 train Loss 110.3017 test Loss 47.3049 with MSE metric 33249.5377\n",
      "Epoch 95 batch 40 train Loss 110.2575 test Loss 47.2874 with MSE metric 33238.9940\n",
      "Epoch 95 batch 50 train Loss 110.2135 test Loss 47.2698 with MSE metric 33228.4847\n",
      "Epoch 95 batch 60 train Loss 110.1695 test Loss 47.2523 with MSE metric 33218.1127\n",
      "Epoch 95 batch 70 train Loss 110.1254 test Loss 47.2347 with MSE metric 33207.8191\n",
      "Epoch 95 batch 80 train Loss 110.0815 test Loss 47.2172 with MSE metric 33197.5007\n",
      "Epoch 95 batch 90 train Loss 110.0375 test Loss 47.1997 with MSE metric 33187.0032\n",
      "Epoch 95 batch 100 train Loss 109.9936 test Loss 47.1822 with MSE metric 33176.7044\n",
      "Epoch 95 batch 110 train Loss 109.9498 test Loss 47.1647 with MSE metric 33166.3680\n",
      "Epoch 95 batch 120 train Loss 109.9060 test Loss 47.1473 with MSE metric 33155.7173\n",
      "Epoch 95 batch 130 train Loss 109.8622 test Loss 47.1298 with MSE metric 33145.1189\n",
      "Epoch 95 batch 140 train Loss 109.8184 test Loss 47.1124 with MSE metric 33134.7119\n",
      "Epoch 95 batch 150 train Loss 109.7747 test Loss 47.0950 with MSE metric 33124.1403\n",
      "Epoch 95 batch 160 train Loss 109.7311 test Loss 47.0776 with MSE metric 33113.7431\n",
      "Epoch 95 batch 170 train Loss 109.6874 test Loss 47.0602 with MSE metric 33103.4048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 batch 180 train Loss 109.6438 test Loss 47.0428 with MSE metric 33092.9508\n",
      "Epoch 95 batch 190 train Loss 109.6003 test Loss 47.0255 with MSE metric 33082.4504\n",
      "Epoch 95 batch 200 train Loss 109.5567 test Loss 47.0081 with MSE metric 33072.0017\n",
      "Epoch 95 batch 210 train Loss 109.5132 test Loss 46.9908 with MSE metric 33061.5160\n",
      "Epoch 95 batch 220 train Loss 109.4698 test Loss 46.9735 with MSE metric 33051.1071\n",
      "Epoch 95 batch 230 train Loss 109.4264 test Loss 46.9562 with MSE metric 33040.6384\n",
      "Epoch 95 batch 240 train Loss 109.3830 test Loss 46.9389 with MSE metric 33030.0505\n",
      "Time taken for 1 epoch: 29.40114188194275 secs\n",
      "\n",
      "Epoch 96 batch 0 train Loss 109.3396 test Loss 46.9216 with MSE metric 33019.6527\n",
      "Epoch 96 batch 10 train Loss 109.2963 test Loss 46.9044 with MSE metric 33009.4183\n",
      "Epoch 96 batch 20 train Loss 109.2531 test Loss 46.8872 with MSE metric 32999.1946\n",
      "Epoch 96 batch 30 train Loss 109.2098 test Loss 46.8699 with MSE metric 32988.7520\n",
      "Epoch 96 batch 40 train Loss 109.1666 test Loss 46.8527 with MSE metric 32978.4528\n",
      "Epoch 96 batch 50 train Loss 109.1235 test Loss 46.8355 with MSE metric 32968.3716\n",
      "Epoch 96 batch 60 train Loss 109.0803 test Loss 46.8183 with MSE metric 32958.1123\n",
      "Epoch 96 batch 70 train Loss 109.0373 test Loss 46.8011 with MSE metric 32947.7239\n",
      "Epoch 96 batch 80 train Loss 108.9942 test Loss 46.7840 with MSE metric 32937.4174\n",
      "Epoch 96 batch 90 train Loss 108.9512 test Loss 46.7668 with MSE metric 32927.0755\n",
      "Epoch 96 batch 100 train Loss 108.9082 test Loss 46.7497 with MSE metric 32916.7348\n",
      "Epoch 96 batch 110 train Loss 108.8653 test Loss 46.7326 with MSE metric 32906.4651\n",
      "Epoch 96 batch 120 train Loss 108.8224 test Loss 46.7155 with MSE metric 32896.0444\n",
      "Epoch 96 batch 130 train Loss 108.7795 test Loss 46.6984 with MSE metric 32885.8542\n",
      "Epoch 96 batch 140 train Loss 108.7366 test Loss 46.6813 with MSE metric 32875.5537\n",
      "Epoch 96 batch 150 train Loss 108.6938 test Loss 46.6643 with MSE metric 32865.3991\n",
      "Epoch 96 batch 160 train Loss 108.6510 test Loss 46.6472 with MSE metric 32855.1004\n",
      "Epoch 96 batch 170 train Loss 108.6083 test Loss 46.6302 with MSE metric 32844.9478\n",
      "Epoch 96 batch 180 train Loss 108.5656 test Loss 46.6132 with MSE metric 32834.7217\n",
      "Epoch 96 batch 190 train Loss 108.5229 test Loss 46.5961 with MSE metric 32824.4309\n",
      "Epoch 96 batch 200 train Loss 108.4803 test Loss 46.5791 with MSE metric 32814.3736\n",
      "Epoch 96 batch 210 train Loss 108.4377 test Loss 46.5622 with MSE metric 32804.1907\n",
      "Epoch 96 batch 220 train Loss 108.3951 test Loss 46.5452 with MSE metric 32794.1016\n",
      "Epoch 96 batch 230 train Loss 108.3526 test Loss 46.5283 with MSE metric 32783.9188\n",
      "Epoch 96 batch 240 train Loss 108.3101 test Loss 46.5113 with MSE metric 32773.7319\n",
      "Time taken for 1 epoch: 31.52545714378357 secs\n",
      "\n",
      "Epoch 97 batch 0 train Loss 108.2676 test Loss 46.4944 with MSE metric 32763.4565\n",
      "Epoch 97 batch 10 train Loss 108.2252 test Loss 46.4775 with MSE metric 32753.0587\n",
      "Epoch 97 batch 20 train Loss 108.1828 test Loss 46.4605 with MSE metric 32743.0179\n",
      "Epoch 97 batch 30 train Loss 108.1405 test Loss 46.4437 with MSE metric 32732.7522\n",
      "Epoch 97 batch 40 train Loss 108.0981 test Loss 46.4268 with MSE metric 32722.5412\n",
      "Epoch 97 batch 50 train Loss 108.0559 test Loss 46.4099 with MSE metric 32712.2303\n",
      "Epoch 97 batch 60 train Loss 108.0136 test Loss 46.3931 with MSE metric 32702.2039\n",
      "Epoch 97 batch 70 train Loss 107.9714 test Loss 46.3762 with MSE metric 32692.1065\n",
      "Epoch 97 batch 80 train Loss 107.9292 test Loss 46.3594 with MSE metric 32681.8297\n",
      "Epoch 97 batch 90 train Loss 107.8871 test Loss 46.3426 with MSE metric 32671.6810\n",
      "Epoch 97 batch 100 train Loss 107.8450 test Loss 46.3258 with MSE metric 32661.7245\n",
      "Epoch 97 batch 110 train Loss 107.8029 test Loss 46.3091 with MSE metric 32651.5502\n",
      "Epoch 97 batch 120 train Loss 107.7608 test Loss 46.2923 with MSE metric 32641.4826\n",
      "Epoch 97 batch 130 train Loss 107.7188 test Loss 46.2756 with MSE metric 32631.3543\n",
      "Epoch 97 batch 140 train Loss 107.6769 test Loss 46.2588 with MSE metric 32621.4619\n",
      "Epoch 97 batch 150 train Loss 107.6349 test Loss 46.2421 with MSE metric 32611.3201\n",
      "Epoch 97 batch 160 train Loss 107.5930 test Loss 46.2254 with MSE metric 32601.3200\n",
      "Epoch 97 batch 170 train Loss 107.5511 test Loss 46.2087 with MSE metric 32591.1513\n",
      "Epoch 97 batch 180 train Loss 107.5093 test Loss 46.1921 with MSE metric 32581.1389\n",
      "Epoch 97 batch 190 train Loss 107.4675 test Loss 46.1754 with MSE metric 32571.0197\n",
      "Epoch 97 batch 200 train Loss 107.4257 test Loss 46.1587 with MSE metric 32561.0391\n",
      "Epoch 97 batch 210 train Loss 107.3840 test Loss 46.1421 with MSE metric 32551.0568\n",
      "Epoch 97 batch 220 train Loss 107.3423 test Loss 46.1255 with MSE metric 32540.7121\n",
      "Epoch 97 batch 230 train Loss 107.3007 test Loss 46.1089 with MSE metric 32530.5736\n",
      "Epoch 97 batch 240 train Loss 107.2590 test Loss 46.0923 with MSE metric 32520.4879\n",
      "Time taken for 1 epoch: 28.25147008895874 secs\n",
      "\n",
      "Epoch 98 batch 0 train Loss 107.2174 test Loss 46.0757 with MSE metric 32510.5219\n",
      "Epoch 98 batch 10 train Loss 107.1758 test Loss 46.0591 with MSE metric 32500.6082\n",
      "Epoch 98 batch 20 train Loss 107.1343 test Loss 46.0426 with MSE metric 32490.5739\n",
      "Epoch 98 batch 30 train Loss 107.0928 test Loss 46.0260 with MSE metric 32480.5344\n",
      "Epoch 98 batch 40 train Loss 107.0514 test Loss 46.0095 with MSE metric 32470.3987\n",
      "Epoch 98 batch 50 train Loss 107.0099 test Loss 45.9930 with MSE metric 32460.4937\n",
      "Epoch 98 batch 60 train Loss 106.9685 test Loss 45.9764 with MSE metric 32450.6341\n",
      "Epoch 98 batch 70 train Loss 106.9272 test Loss 45.9600 with MSE metric 32440.7214\n",
      "Epoch 98 batch 80 train Loss 106.8858 test Loss 45.9435 with MSE metric 32430.6171\n",
      "Epoch 98 batch 90 train Loss 106.8446 test Loss 45.9270 with MSE metric 32420.7362\n",
      "Epoch 98 batch 100 train Loss 106.8033 test Loss 45.9106 with MSE metric 32410.8796\n",
      "Epoch 98 batch 110 train Loss 106.7621 test Loss 45.8941 with MSE metric 32400.9207\n",
      "Epoch 98 batch 120 train Loss 106.7209 test Loss 45.8777 with MSE metric 32390.8387\n",
      "Epoch 98 batch 130 train Loss 106.6797 test Loss 45.8613 with MSE metric 32380.8270\n",
      "Epoch 98 batch 140 train Loss 106.6386 test Loss 45.8449 with MSE metric 32370.7229\n",
      "Epoch 98 batch 150 train Loss 106.5975 test Loss 45.8285 with MSE metric 32360.7894\n",
      "Epoch 98 batch 160 train Loss 106.5565 test Loss 45.8121 with MSE metric 32350.8785\n",
      "Epoch 98 batch 170 train Loss 106.5154 test Loss 45.7958 with MSE metric 32340.9212\n",
      "Epoch 98 batch 180 train Loss 106.4744 test Loss 45.7794 with MSE metric 32331.0136\n",
      "Epoch 98 batch 190 train Loss 106.4335 test Loss 45.7631 with MSE metric 32321.3603\n",
      "Epoch 98 batch 200 train Loss 106.3925 test Loss 45.7467 with MSE metric 32311.3489\n",
      "Epoch 98 batch 210 train Loss 106.3516 test Loss 45.7304 with MSE metric 32301.5118\n",
      "Epoch 98 batch 220 train Loss 106.3108 test Loss 45.7141 with MSE metric 32291.7192\n",
      "Epoch 98 batch 230 train Loss 106.2700 test Loss 45.6978 with MSE metric 32282.0460\n",
      "Epoch 98 batch 240 train Loss 106.2292 test Loss 45.6816 with MSE metric 32272.0803\n",
      "Time taken for 1 epoch: 26.038506031036377 secs\n",
      "\n",
      "Epoch 99 batch 0 train Loss 106.1884 test Loss 45.6653 with MSE metric 32262.4008\n",
      "Epoch 99 batch 10 train Loss 106.1477 test Loss 45.6491 with MSE metric 32252.5571\n",
      "Epoch 99 batch 20 train Loss 106.1070 test Loss 45.6328 with MSE metric 32242.7423\n",
      "Epoch 99 batch 30 train Loss 106.0663 test Loss 45.6166 with MSE metric 32232.8288\n",
      "Epoch 99 batch 40 train Loss 106.0257 test Loss 45.6004 with MSE metric 32223.0498\n",
      "Epoch 99 batch 50 train Loss 105.9851 test Loss 45.5843 with MSE metric 32213.2962\n",
      "Epoch 99 batch 60 train Loss 105.9445 test Loss 45.5681 with MSE metric 32203.6243\n",
      "Epoch 99 batch 70 train Loss 105.9040 test Loss 45.5519 with MSE metric 32193.8232\n",
      "Epoch 99 batch 80 train Loss 105.8635 test Loss 45.5358 with MSE metric 32184.1179\n",
      "Epoch 99 batch 90 train Loss 105.8230 test Loss 45.5197 with MSE metric 32174.3057\n",
      "Epoch 99 batch 100 train Loss 105.7826 test Loss 45.5035 with MSE metric 32164.6323\n",
      "Epoch 99 batch 110 train Loss 105.7422 test Loss 45.4875 with MSE metric 32155.0032\n",
      "Epoch 99 batch 120 train Loss 105.7018 test Loss 45.4714 with MSE metric 32145.2894\n",
      "Epoch 99 batch 130 train Loss 105.6615 test Loss 45.4553 with MSE metric 32135.5962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 batch 140 train Loss 105.6212 test Loss 45.4392 with MSE metric 32125.8314\n",
      "Epoch 99 batch 150 train Loss 105.5809 test Loss 45.4231 with MSE metric 32116.0185\n",
      "Epoch 99 batch 160 train Loss 105.5407 test Loss 45.4071 with MSE metric 32106.2467\n",
      "Epoch 99 batch 170 train Loss 105.5004 test Loss 45.3911 with MSE metric 32096.5018\n",
      "Epoch 99 batch 180 train Loss 105.4603 test Loss 45.3751 with MSE metric 32086.8557\n",
      "Epoch 99 batch 190 train Loss 105.4201 test Loss 45.3591 with MSE metric 32077.2450\n",
      "Epoch 99 batch 200 train Loss 105.3800 test Loss 45.3431 with MSE metric 32067.6552\n",
      "Epoch 99 batch 210 train Loss 105.3400 test Loss 45.3271 with MSE metric 32058.0011\n",
      "Epoch 99 batch 220 train Loss 105.2999 test Loss 45.3111 with MSE metric 32048.1450\n",
      "Epoch 99 batch 230 train Loss 105.2599 test Loss 45.2952 with MSE metric 32038.5602\n",
      "Epoch 99 batch 240 train Loss 105.2199 test Loss 45.2792 with MSE metric 32028.8144\n",
      "Time taken for 1 epoch: 27.713969230651855 secs\n",
      "\n",
      "Epoch 100 batch 0 train Loss 105.1800 test Loss 45.2633 with MSE metric 32019.0124\n",
      "Epoch 100 batch 10 train Loss 105.1400 test Loss 45.2474 with MSE metric 32009.3105\n",
      "Epoch 100 batch 20 train Loss 105.1002 test Loss 45.2315 with MSE metric 31999.7504\n",
      "Epoch 100 batch 30 train Loss 105.0603 test Loss 45.2156 with MSE metric 31990.2761\n",
      "Epoch 100 batch 40 train Loss 105.0205 test Loss 45.1997 with MSE metric 31980.8934\n",
      "Epoch 100 batch 50 train Loss 104.9807 test Loss 45.1838 with MSE metric 31971.1068\n",
      "Epoch 100 batch 60 train Loss 104.9409 test Loss 45.1680 with MSE metric 31961.6266\n",
      "Epoch 100 batch 70 train Loss 104.9012 test Loss 45.1521 with MSE metric 31952.1451\n",
      "Epoch 100 batch 80 train Loss 104.8615 test Loss 45.1363 with MSE metric 31942.5863\n",
      "Epoch 100 batch 90 train Loss 104.8218 test Loss 45.1205 with MSE metric 31933.0147\n",
      "Epoch 100 batch 100 train Loss 104.7822 test Loss 45.1047 with MSE metric 31923.4367\n",
      "Epoch 100 batch 110 train Loss 104.7426 test Loss 45.0888 with MSE metric 31913.7974\n",
      "Epoch 100 batch 120 train Loss 104.7030 test Loss 45.0731 with MSE metric 31904.1441\n",
      "Epoch 100 batch 130 train Loss 104.6635 test Loss 45.0573 with MSE metric 31894.5162\n",
      "Epoch 100 batch 140 train Loss 104.6240 test Loss 45.0415 with MSE metric 31884.9041\n",
      "Epoch 100 batch 150 train Loss 104.5845 test Loss 45.0258 with MSE metric 31875.4649\n",
      "Epoch 100 batch 160 train Loss 104.5451 test Loss 45.0101 with MSE metric 31865.8413\n",
      "Epoch 100 batch 170 train Loss 104.5056 test Loss 44.9943 with MSE metric 31856.3958\n",
      "Epoch 100 batch 180 train Loss 104.4663 test Loss 44.9786 with MSE metric 31846.6413\n",
      "Epoch 100 batch 190 train Loss 104.4269 test Loss 44.9629 with MSE metric 31836.9411\n",
      "Epoch 100 batch 200 train Loss 104.3876 test Loss 44.9472 with MSE metric 31827.2294\n",
      "Epoch 100 batch 210 train Loss 104.3483 test Loss 44.9316 with MSE metric 31817.8270\n",
      "Epoch 100 batch 220 train Loss 104.3090 test Loss 44.9159 with MSE metric 31808.2092\n",
      "Epoch 100 batch 230 train Loss 104.2698 test Loss 44.9003 with MSE metric 31798.7636\n",
      "Epoch 100 batch 240 train Loss 104.2306 test Loss 44.8846 with MSE metric 31789.2764\n",
      "Time taken for 1 epoch: 28.345388889312744 secs\n",
      "\n",
      "Epoch 101 batch 0 train Loss 104.1915 test Loss 44.8690 with MSE metric 31779.8286\n",
      "Epoch 101 batch 10 train Loss 104.1523 test Loss 44.8534 with MSE metric 31770.3639\n",
      "Epoch 101 batch 20 train Loss 104.1132 test Loss 44.8378 with MSE metric 31760.8435\n",
      "Epoch 101 batch 30 train Loss 104.0741 test Loss 44.8222 with MSE metric 31751.4755\n",
      "Epoch 101 batch 40 train Loss 104.0351 test Loss 44.8067 with MSE metric 31742.0831\n",
      "Epoch 101 batch 50 train Loss 103.9961 test Loss 44.7911 with MSE metric 31732.8216\n",
      "Epoch 101 batch 60 train Loss 103.9571 test Loss 44.7756 with MSE metric 31723.4606\n",
      "Epoch 101 batch 70 train Loss 103.9182 test Loss 44.7600 with MSE metric 31713.9932\n",
      "Epoch 101 batch 80 train Loss 103.8793 test Loss 44.7445 with MSE metric 31704.8389\n",
      "Epoch 101 batch 90 train Loss 103.8404 test Loss 44.7290 with MSE metric 31695.4479\n",
      "Epoch 101 batch 100 train Loss 103.8015 test Loss 44.7135 with MSE metric 31686.1608\n",
      "Epoch 101 batch 110 train Loss 103.7627 test Loss 44.6980 with MSE metric 31676.7818\n",
      "Epoch 101 batch 120 train Loss 103.7239 test Loss 44.6826 with MSE metric 31667.4839\n",
      "Epoch 101 batch 130 train Loss 103.6851 test Loss 44.6671 with MSE metric 31658.0496\n",
      "Epoch 101 batch 140 train Loss 103.6464 test Loss 44.6516 with MSE metric 31648.7621\n",
      "Epoch 101 batch 150 train Loss 103.6077 test Loss 44.6362 with MSE metric 31639.4699\n",
      "Epoch 101 batch 160 train Loss 103.5690 test Loss 44.6208 with MSE metric 31630.1875\n",
      "Epoch 101 batch 170 train Loss 103.5303 test Loss 44.6054 with MSE metric 31620.8964\n",
      "Epoch 101 batch 180 train Loss 103.4917 test Loss 44.5900 with MSE metric 31611.6224\n",
      "Epoch 101 batch 190 train Loss 103.4532 test Loss 44.5746 with MSE metric 31602.3237\n",
      "Epoch 101 batch 200 train Loss 103.4146 test Loss 44.5592 with MSE metric 31593.0273\n",
      "Epoch 101 batch 210 train Loss 103.3761 test Loss 44.5438 with MSE metric 31583.6892\n",
      "Epoch 101 batch 220 train Loss 103.3376 test Loss 44.5284 with MSE metric 31574.4725\n",
      "Epoch 101 batch 230 train Loss 103.2991 test Loss 44.5130 with MSE metric 31565.0084\n",
      "Epoch 101 batch 240 train Loss 103.2607 test Loss 44.4977 with MSE metric 31555.6931\n",
      "Time taken for 1 epoch: 28.568345069885254 secs\n",
      "\n",
      "Epoch 102 batch 0 train Loss 103.2223 test Loss 44.4824 with MSE metric 31546.3805\n",
      "Epoch 102 batch 10 train Loss 103.1839 test Loss 44.4671 with MSE metric 31537.1168\n",
      "Epoch 102 batch 20 train Loss 103.1455 test Loss 44.4518 with MSE metric 31527.7975\n",
      "Epoch 102 batch 30 train Loss 103.1072 test Loss 44.4365 with MSE metric 31518.7101\n",
      "Epoch 102 batch 40 train Loss 103.0690 test Loss 44.4213 with MSE metric 31509.4590\n",
      "Epoch 102 batch 50 train Loss 103.0307 test Loss 44.4061 with MSE metric 31500.3524\n",
      "Epoch 102 batch 60 train Loss 102.9925 test Loss 44.3908 with MSE metric 31491.1629\n",
      "Epoch 102 batch 70 train Loss 102.9543 test Loss 44.3756 with MSE metric 31482.0019\n",
      "Epoch 102 batch 80 train Loss 102.9161 test Loss 44.3603 with MSE metric 31472.8927\n",
      "Epoch 102 batch 90 train Loss 102.8780 test Loss 44.3451 with MSE metric 31463.6955\n",
      "Epoch 102 batch 100 train Loss 102.8399 test Loss 44.3299 with MSE metric 31454.4179\n",
      "Epoch 102 batch 110 train Loss 102.8018 test Loss 44.3147 with MSE metric 31445.1865\n",
      "Epoch 102 batch 120 train Loss 102.7638 test Loss 44.2996 with MSE metric 31436.0038\n",
      "Epoch 102 batch 130 train Loss 102.7258 test Loss 44.2844 with MSE metric 31426.8498\n",
      "Epoch 102 batch 140 train Loss 102.6878 test Loss 44.2692 with MSE metric 31417.7261\n",
      "Epoch 102 batch 150 train Loss 102.6498 test Loss 44.2541 with MSE metric 31408.5776\n",
      "Epoch 102 batch 160 train Loss 102.6119 test Loss 44.2390 with MSE metric 31399.2372\n",
      "Epoch 102 batch 170 train Loss 102.5740 test Loss 44.2239 with MSE metric 31390.2470\n",
      "Epoch 102 batch 180 train Loss 102.5362 test Loss 44.2088 with MSE metric 31381.2524\n",
      "Epoch 102 batch 190 train Loss 102.4983 test Loss 44.1937 with MSE metric 31372.1437\n",
      "Epoch 102 batch 200 train Loss 102.4605 test Loss 44.1786 with MSE metric 31363.2301\n",
      "Epoch 102 batch 210 train Loss 102.4228 test Loss 44.1635 with MSE metric 31354.1353\n",
      "Epoch 102 batch 220 train Loss 102.3850 test Loss 44.1484 with MSE metric 31345.1429\n",
      "Epoch 102 batch 230 train Loss 102.3473 test Loss 44.1334 with MSE metric 31336.0870\n",
      "Epoch 102 batch 240 train Loss 102.3096 test Loss 44.1183 with MSE metric 31327.0829\n",
      "Time taken for 1 epoch: 26.883676052093506 secs\n",
      "\n",
      "Epoch 103 batch 0 train Loss 102.2719 test Loss 44.1033 with MSE metric 31317.8870\n",
      "Epoch 103 batch 10 train Loss 102.2343 test Loss 44.0883 with MSE metric 31308.8518\n",
      "Epoch 103 batch 20 train Loss 102.1967 test Loss 44.0733 with MSE metric 31299.8989\n",
      "Epoch 103 batch 30 train Loss 102.1591 test Loss 44.0583 with MSE metric 31290.6824\n",
      "Epoch 103 batch 40 train Loss 102.1216 test Loss 44.0434 with MSE metric 31281.7444\n",
      "Epoch 103 batch 50 train Loss 102.0841 test Loss 44.0284 with MSE metric 31272.7202\n",
      "Epoch 103 batch 60 train Loss 102.0466 test Loss 44.0134 with MSE metric 31263.6752\n",
      "Epoch 103 batch 70 train Loss 102.0092 test Loss 43.9985 with MSE metric 31254.6553\n",
      "Epoch 103 batch 80 train Loss 101.9717 test Loss 43.9836 with MSE metric 31245.6754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103 batch 90 train Loss 101.9343 test Loss 43.9686 with MSE metric 31236.6261\n",
      "Epoch 103 batch 100 train Loss 101.8970 test Loss 43.9537 with MSE metric 31227.6646\n",
      "Epoch 103 batch 110 train Loss 101.8596 test Loss 43.9388 with MSE metric 31218.5012\n",
      "Epoch 103 batch 120 train Loss 101.8223 test Loss 43.9239 with MSE metric 31209.4002\n",
      "Epoch 103 batch 130 train Loss 101.7850 test Loss 43.9091 with MSE metric 31200.4025\n",
      "Epoch 103 batch 140 train Loss 101.7478 test Loss 43.8942 with MSE metric 31191.6758\n",
      "Epoch 103 batch 150 train Loss 101.7106 test Loss 43.8793 with MSE metric 31182.8135\n",
      "Epoch 103 batch 160 train Loss 101.6734 test Loss 43.8645 with MSE metric 31173.9150\n",
      "Epoch 103 batch 170 train Loss 101.6362 test Loss 43.8497 with MSE metric 31164.8827\n",
      "Epoch 103 batch 180 train Loss 101.5991 test Loss 43.8349 with MSE metric 31155.7066\n",
      "Epoch 103 batch 190 train Loss 101.5619 test Loss 43.8201 with MSE metric 31146.6242\n",
      "Epoch 103 batch 200 train Loss 101.5249 test Loss 43.8052 with MSE metric 31137.6494\n",
      "Epoch 103 batch 210 train Loss 101.4878 test Loss 43.7905 with MSE metric 31128.4839\n",
      "Epoch 103 batch 220 train Loss 101.4508 test Loss 43.7757 with MSE metric 31119.6643\n",
      "Epoch 103 batch 230 train Loss 101.4138 test Loss 43.7609 with MSE metric 31110.8313\n",
      "Epoch 103 batch 240 train Loss 101.3768 test Loss 43.7462 with MSE metric 31102.0709\n",
      "Time taken for 1 epoch: 28.16719889640808 secs\n",
      "\n",
      "Epoch 104 batch 0 train Loss 101.3399 test Loss 43.7314 with MSE metric 31093.2774\n",
      "Epoch 104 batch 10 train Loss 101.3030 test Loss 43.7167 with MSE metric 31084.2834\n",
      "Epoch 104 batch 20 train Loss 101.2661 test Loss 43.7020 with MSE metric 31075.2284\n",
      "Epoch 104 batch 30 train Loss 101.2292 test Loss 43.6873 with MSE metric 31066.1193\n",
      "Epoch 104 batch 40 train Loss 101.1924 test Loss 43.6726 with MSE metric 31057.2219\n",
      "Epoch 104 batch 50 train Loss 101.1556 test Loss 43.6579 with MSE metric 31048.2675\n",
      "Epoch 104 batch 60 train Loss 101.1188 test Loss 43.6432 with MSE metric 31039.4539\n",
      "Epoch 104 batch 70 train Loss 101.0821 test Loss 43.6285 with MSE metric 31030.7132\n",
      "Epoch 104 batch 80 train Loss 101.0454 test Loss 43.6139 with MSE metric 31021.8059\n",
      "Epoch 104 batch 90 train Loss 101.0087 test Loss 43.5992 with MSE metric 31012.9501\n",
      "Epoch 104 batch 100 train Loss 100.9720 test Loss 43.5846 with MSE metric 31004.0649\n",
      "Epoch 104 batch 110 train Loss 100.9354 test Loss 43.5700 with MSE metric 30995.2820\n",
      "Epoch 104 batch 120 train Loss 100.8988 test Loss 43.5554 with MSE metric 30986.4021\n",
      "Epoch 104 batch 130 train Loss 100.8622 test Loss 43.5408 with MSE metric 30977.5887\n",
      "Epoch 104 batch 140 train Loss 100.8257 test Loss 43.5262 with MSE metric 30968.5798\n",
      "Epoch 104 batch 150 train Loss 100.7892 test Loss 43.5117 with MSE metric 30959.7253\n",
      "Epoch 104 batch 160 train Loss 100.7527 test Loss 43.4971 with MSE metric 30950.8943\n",
      "Epoch 104 batch 170 train Loss 100.7162 test Loss 43.4825 with MSE metric 30941.9972\n",
      "Epoch 104 batch 180 train Loss 100.6798 test Loss 43.4680 with MSE metric 30933.4659\n",
      "Epoch 104 batch 190 train Loss 100.6434 test Loss 43.4535 with MSE metric 30924.5675\n",
      "Epoch 104 batch 200 train Loss 100.6070 test Loss 43.4390 with MSE metric 30915.5761\n",
      "Epoch 104 batch 210 train Loss 100.5707 test Loss 43.4244 with MSE metric 30906.6666\n",
      "Epoch 104 batch 220 train Loss 100.5343 test Loss 43.4099 with MSE metric 30897.8219\n",
      "Epoch 104 batch 230 train Loss 100.4980 test Loss 43.3955 with MSE metric 30889.0290\n",
      "Epoch 104 batch 240 train Loss 100.4618 test Loss 43.3810 with MSE metric 30880.3710\n",
      "Time taken for 1 epoch: 31.459469079971313 secs\n",
      "\n",
      "Epoch 105 batch 0 train Loss 100.4256 test Loss 43.3665 with MSE metric 30871.5848\n",
      "Epoch 105 batch 10 train Loss 100.3893 test Loss 43.3521 with MSE metric 30862.9798\n",
      "Epoch 105 batch 20 train Loss 100.3532 test Loss 43.3376 with MSE metric 30854.2197\n",
      "Epoch 105 batch 30 train Loss 100.3170 test Loss 43.3232 with MSE metric 30845.4978\n",
      "Epoch 105 batch 40 train Loss 100.2809 test Loss 43.3088 with MSE metric 30836.7468\n",
      "Epoch 105 batch 50 train Loss 100.2448 test Loss 43.2944 with MSE metric 30828.1666\n",
      "Epoch 105 batch 60 train Loss 100.2087 test Loss 43.2800 with MSE metric 30819.3921\n",
      "Epoch 105 batch 70 train Loss 100.1727 test Loss 43.2656 with MSE metric 30810.6060\n",
      "Epoch 105 batch 80 train Loss 100.1366 test Loss 43.2513 with MSE metric 30801.8555\n",
      "Epoch 105 batch 90 train Loss 100.1007 test Loss 43.2369 with MSE metric 30793.1158\n",
      "Epoch 105 batch 100 train Loss 100.0647 test Loss 43.2225 with MSE metric 30784.3836\n",
      "Epoch 105 batch 110 train Loss 100.0288 test Loss 43.2082 with MSE metric 30775.7465\n",
      "Epoch 105 batch 120 train Loss 99.9928 test Loss 43.1939 with MSE metric 30767.1545\n",
      "Epoch 105 batch 130 train Loss 99.9570 test Loss 43.1795 with MSE metric 30758.5421\n",
      "Epoch 105 batch 140 train Loss 99.9211 test Loss 43.1652 with MSE metric 30749.9958\n",
      "Epoch 105 batch 150 train Loss 99.8853 test Loss 43.1509 with MSE metric 30741.3122\n",
      "Epoch 105 batch 160 train Loss 99.8495 test Loss 43.1366 with MSE metric 30732.6425\n",
      "Epoch 105 batch 170 train Loss 99.8137 test Loss 43.1223 with MSE metric 30724.2721\n",
      "Epoch 105 batch 180 train Loss 99.7780 test Loss 43.1081 with MSE metric 30715.7169\n",
      "Epoch 105 batch 190 train Loss 99.7423 test Loss 43.0938 with MSE metric 30707.1053\n",
      "Epoch 105 batch 200 train Loss 99.7066 test Loss 43.0796 with MSE metric 30698.6924\n",
      "Epoch 105 batch 210 train Loss 99.6709 test Loss 43.0653 with MSE metric 30690.0142\n",
      "Epoch 105 batch 220 train Loss 99.6353 test Loss 43.0511 with MSE metric 30681.3904\n",
      "Epoch 105 batch 230 train Loss 99.5997 test Loss 43.0369 with MSE metric 30673.0027\n",
      "Epoch 105 batch 240 train Loss 99.5641 test Loss 43.0227 with MSE metric 30664.2181\n",
      "Time taken for 1 epoch: 31.871758937835693 secs\n",
      "\n",
      "Epoch 106 batch 0 train Loss 99.5285 test Loss 43.0086 with MSE metric 30655.7611\n",
      "Epoch 106 batch 10 train Loss 99.4930 test Loss 42.9944 with MSE metric 30647.2913\n",
      "Epoch 106 batch 20 train Loss 99.4575 test Loss 42.9802 with MSE metric 30638.6837\n",
      "Epoch 106 batch 30 train Loss 99.4220 test Loss 42.9660 with MSE metric 30630.0998\n",
      "Epoch 106 batch 40 train Loss 99.3866 test Loss 42.9519 with MSE metric 30621.5408\n",
      "Epoch 106 batch 50 train Loss 99.3511 test Loss 42.9378 with MSE metric 30612.9393\n",
      "Epoch 106 batch 60 train Loss 99.3157 test Loss 42.9236 with MSE metric 30604.4384\n",
      "Epoch 106 batch 70 train Loss 99.2804 test Loss 42.9095 with MSE metric 30595.9048\n",
      "Epoch 106 batch 80 train Loss 99.2450 test Loss 42.8954 with MSE metric 30587.2775\n",
      "Epoch 106 batch 90 train Loss 99.2097 test Loss 42.8813 with MSE metric 30578.6598\n",
      "Epoch 106 batch 100 train Loss 99.1744 test Loss 42.8672 with MSE metric 30570.2668\n",
      "Epoch 106 batch 110 train Loss 99.1392 test Loss 42.8531 with MSE metric 30561.5728\n",
      "Epoch 106 batch 120 train Loss 99.1039 test Loss 42.8391 with MSE metric 30553.1799\n",
      "Epoch 106 batch 130 train Loss 99.0687 test Loss 42.8250 with MSE metric 30544.8469\n",
      "Epoch 106 batch 140 train Loss 99.0335 test Loss 42.8110 with MSE metric 30536.3526\n",
      "Epoch 106 batch 150 train Loss 98.9984 test Loss 42.7969 with MSE metric 30527.8888\n",
      "Epoch 106 batch 160 train Loss 98.9632 test Loss 42.7829 with MSE metric 30519.6177\n",
      "Epoch 106 batch 170 train Loss 98.9281 test Loss 42.7689 with MSE metric 30511.1145\n",
      "Epoch 106 batch 180 train Loss 98.8931 test Loss 42.7549 with MSE metric 30502.6761\n",
      "Epoch 106 batch 190 train Loss 98.8580 test Loss 42.7409 with MSE metric 30494.2303\n",
      "Epoch 106 batch 200 train Loss 98.8230 test Loss 42.7269 with MSE metric 30485.6727\n",
      "Epoch 106 batch 210 train Loss 98.7880 test Loss 42.7129 with MSE metric 30477.1718\n",
      "Epoch 106 batch 220 train Loss 98.7530 test Loss 42.6990 with MSE metric 30468.6497\n",
      "Epoch 106 batch 230 train Loss 98.7181 test Loss 42.6850 with MSE metric 30460.1800\n",
      "Epoch 106 batch 240 train Loss 98.6831 test Loss 42.6711 with MSE metric 30451.7517\n",
      "Time taken for 1 epoch: 25.96045684814453 secs\n",
      "\n",
      "Epoch 107 batch 0 train Loss 98.6482 test Loss 42.6572 with MSE metric 30443.2720\n",
      "Epoch 107 batch 10 train Loss 98.6134 test Loss 42.6432 with MSE metric 30434.6876\n",
      "Epoch 107 batch 20 train Loss 98.5785 test Loss 42.6293 with MSE metric 30426.2550\n",
      "Epoch 107 batch 30 train Loss 98.5437 test Loss 42.6154 with MSE metric 30417.6232\n",
      "Epoch 107 batch 40 train Loss 98.5089 test Loss 42.6015 with MSE metric 30409.2313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 batch 50 train Loss 98.4741 test Loss 42.5876 with MSE metric 30400.7129\n",
      "Epoch 107 batch 60 train Loss 98.4394 test Loss 42.5738 with MSE metric 30392.4174\n",
      "Epoch 107 batch 70 train Loss 98.4047 test Loss 42.5599 with MSE metric 30383.9689\n",
      "Epoch 107 batch 80 train Loss 98.3700 test Loss 42.5461 with MSE metric 30375.6375\n",
      "Epoch 107 batch 90 train Loss 98.3353 test Loss 42.5322 with MSE metric 30367.4440\n",
      "Epoch 107 batch 100 train Loss 98.3007 test Loss 42.5185 with MSE metric 30358.9690\n",
      "Epoch 107 batch 110 train Loss 98.2661 test Loss 42.5046 with MSE metric 30350.3764\n",
      "Epoch 107 batch 120 train Loss 98.2315 test Loss 42.4908 with MSE metric 30341.9188\n",
      "Epoch 107 batch 130 train Loss 98.1969 test Loss 42.4770 with MSE metric 30333.7240\n",
      "Epoch 107 batch 140 train Loss 98.1624 test Loss 42.4632 with MSE metric 30325.4446\n",
      "Epoch 107 batch 150 train Loss 98.1279 test Loss 42.4495 with MSE metric 30317.0796\n",
      "Epoch 107 batch 160 train Loss 98.0934 test Loss 42.4357 with MSE metric 30308.8086\n",
      "Epoch 107 batch 170 train Loss 98.0590 test Loss 42.4220 with MSE metric 30300.4248\n",
      "Epoch 107 batch 180 train Loss 98.0246 test Loss 42.4083 with MSE metric 30292.1254\n",
      "Epoch 107 batch 190 train Loss 97.9902 test Loss 42.3945 with MSE metric 30283.8325\n",
      "Epoch 107 batch 200 train Loss 97.9558 test Loss 42.3808 with MSE metric 30275.4033\n",
      "Epoch 107 batch 210 train Loss 97.9214 test Loss 42.3671 with MSE metric 30267.2297\n",
      "Epoch 107 batch 220 train Loss 97.8871 test Loss 42.3534 with MSE metric 30259.1369\n",
      "Epoch 107 batch 230 train Loss 97.8528 test Loss 42.3397 with MSE metric 30250.9327\n",
      "Epoch 107 batch 240 train Loss 97.8185 test Loss 42.3260 with MSE metric 30242.6013\n",
      "Time taken for 1 epoch: 29.344777822494507 secs\n",
      "\n",
      "Epoch 108 batch 0 train Loss 97.7843 test Loss 42.3124 with MSE metric 30234.2650\n",
      "Epoch 108 batch 10 train Loss 97.7500 test Loss 42.2987 with MSE metric 30225.9878\n",
      "Epoch 108 batch 20 train Loss 97.7159 test Loss 42.2851 with MSE metric 30217.7733\n",
      "Epoch 108 batch 30 train Loss 97.6817 test Loss 42.2714 with MSE metric 30209.6196\n",
      "Epoch 108 batch 40 train Loss 97.6475 test Loss 42.2578 with MSE metric 30201.3845\n",
      "Epoch 108 batch 50 train Loss 97.6134 test Loss 42.2442 with MSE metric 30193.2195\n",
      "Epoch 108 batch 60 train Loss 97.5793 test Loss 42.2306 with MSE metric 30184.7769\n",
      "Epoch 108 batch 70 train Loss 97.5452 test Loss 42.2170 with MSE metric 30176.7613\n",
      "Epoch 108 batch 80 train Loss 97.5112 test Loss 42.2034 with MSE metric 30168.6435\n",
      "Epoch 108 batch 90 train Loss 97.4772 test Loss 42.1898 with MSE metric 30160.4986\n",
      "Epoch 108 batch 100 train Loss 97.4432 test Loss 42.1763 with MSE metric 30152.3472\n",
      "Epoch 108 batch 110 train Loss 97.4092 test Loss 42.1627 with MSE metric 30144.1904\n",
      "Epoch 108 batch 120 train Loss 97.3752 test Loss 42.1492 with MSE metric 30135.9719\n",
      "Epoch 108 batch 130 train Loss 97.3413 test Loss 42.1356 with MSE metric 30127.7147\n",
      "Epoch 108 batch 140 train Loss 97.3074 test Loss 42.1221 with MSE metric 30119.6628\n",
      "Epoch 108 batch 150 train Loss 97.2735 test Loss 42.1086 with MSE metric 30111.4765\n",
      "Epoch 108 batch 160 train Loss 97.2397 test Loss 42.0951 with MSE metric 30103.3353\n",
      "Epoch 108 batch 170 train Loss 97.2059 test Loss 42.0816 with MSE metric 30095.0450\n",
      "Epoch 108 batch 180 train Loss 97.1721 test Loss 42.0681 with MSE metric 30086.8442\n",
      "Epoch 108 batch 190 train Loss 97.1383 test Loss 42.0546 with MSE metric 30078.7788\n",
      "Epoch 108 batch 200 train Loss 97.1045 test Loss 42.0411 with MSE metric 30070.7100\n",
      "Epoch 108 batch 210 train Loss 97.0708 test Loss 42.0277 with MSE metric 30062.4680\n",
      "Epoch 108 batch 220 train Loss 97.0371 test Loss 42.0142 with MSE metric 30054.3321\n",
      "Epoch 108 batch 230 train Loss 97.0034 test Loss 42.0008 with MSE metric 30046.0623\n",
      "Epoch 108 batch 240 train Loss 96.9698 test Loss 41.9874 with MSE metric 30037.9212\n",
      "Time taken for 1 epoch: 28.47303080558777 secs\n",
      "\n",
      "Epoch 109 batch 0 train Loss 96.9362 test Loss 41.9739 with MSE metric 30029.7258\n",
      "Epoch 109 batch 10 train Loss 96.9026 test Loss 41.9605 with MSE metric 30021.5687\n",
      "Epoch 109 batch 20 train Loss 96.8690 test Loss 41.9472 with MSE metric 30013.5248\n",
      "Epoch 109 batch 30 train Loss 96.8354 test Loss 41.9338 with MSE metric 30005.3815\n",
      "Epoch 109 batch 40 train Loss 96.8019 test Loss 41.9204 with MSE metric 29997.2887\n",
      "Epoch 109 batch 50 train Loss 96.7684 test Loss 41.9070 with MSE metric 29989.0086\n",
      "Epoch 109 batch 60 train Loss 96.7349 test Loss 41.8937 with MSE metric 29980.9757\n",
      "Epoch 109 batch 70 train Loss 96.7015 test Loss 41.8803 with MSE metric 29972.9636\n",
      "Epoch 109 batch 80 train Loss 96.6680 test Loss 41.8670 with MSE metric 29964.7830\n",
      "Epoch 109 batch 90 train Loss 96.6346 test Loss 41.8537 with MSE metric 29956.7362\n",
      "Epoch 109 batch 100 train Loss 96.6012 test Loss 41.8403 with MSE metric 29948.4809\n",
      "Epoch 109 batch 110 train Loss 96.5679 test Loss 41.8270 with MSE metric 29940.4427\n",
      "Epoch 109 batch 120 train Loss 96.5346 test Loss 41.8137 with MSE metric 29932.5083\n",
      "Epoch 109 batch 130 train Loss 96.5013 test Loss 41.8005 with MSE metric 29924.4516\n",
      "Epoch 109 batch 140 train Loss 96.4680 test Loss 41.7872 with MSE metric 29916.3992\n",
      "Epoch 109 batch 150 train Loss 96.4347 test Loss 41.7739 with MSE metric 29908.5752\n",
      "Epoch 109 batch 160 train Loss 96.4015 test Loss 41.7606 with MSE metric 29900.5185\n",
      "Epoch 109 batch 170 train Loss 96.3683 test Loss 41.7474 with MSE metric 29892.3733\n",
      "Epoch 109 batch 180 train Loss 96.3351 test Loss 41.7341 with MSE metric 29884.4898\n",
      "Epoch 109 batch 190 train Loss 96.3019 test Loss 41.7209 with MSE metric 29876.4783\n",
      "Epoch 109 batch 200 train Loss 96.2688 test Loss 41.7076 with MSE metric 29868.4907\n",
      "Epoch 109 batch 210 train Loss 96.2357 test Loss 41.6944 with MSE metric 29860.5009\n",
      "Epoch 109 batch 220 train Loss 96.2026 test Loss 41.6812 with MSE metric 29852.3500\n",
      "Epoch 109 batch 230 train Loss 96.1695 test Loss 41.6680 with MSE metric 29844.2969\n",
      "Epoch 109 batch 240 train Loss 96.1365 test Loss 41.6548 with MSE metric 29836.4909\n",
      "Time taken for 1 epoch: 28.596256017684937 secs\n",
      "\n",
      "Epoch 110 batch 0 train Loss 96.1035 test Loss 41.6416 with MSE metric 29828.4297\n",
      "Epoch 110 batch 10 train Loss 96.0705 test Loss 41.6284 with MSE metric 29820.5561\n",
      "Epoch 110 batch 20 train Loss 96.0375 test Loss 41.6153 with MSE metric 29812.6112\n",
      "Epoch 110 batch 30 train Loss 96.0046 test Loss 41.6021 with MSE metric 29804.5617\n",
      "Epoch 110 batch 40 train Loss 95.9716 test Loss 41.5890 with MSE metric 29796.6041\n",
      "Epoch 110 batch 50 train Loss 95.9388 test Loss 41.5759 with MSE metric 29788.7088\n",
      "Epoch 110 batch 60 train Loss 95.9059 test Loss 41.5628 with MSE metric 29780.7663\n",
      "Epoch 110 batch 70 train Loss 95.8730 test Loss 41.5496 with MSE metric 29772.7973\n",
      "Epoch 110 batch 80 train Loss 95.8402 test Loss 41.5365 with MSE metric 29764.7601\n",
      "Epoch 110 batch 90 train Loss 95.8074 test Loss 41.5234 with MSE metric 29756.8775\n",
      "Epoch 110 batch 100 train Loss 95.7746 test Loss 41.5103 with MSE metric 29748.9354\n",
      "Epoch 110 batch 110 train Loss 95.7419 test Loss 41.4972 with MSE metric 29740.9911\n",
      "Epoch 110 batch 120 train Loss 95.7092 test Loss 41.4842 with MSE metric 29733.1425\n",
      "Epoch 110 batch 130 train Loss 95.6765 test Loss 41.4711 with MSE metric 29725.2007\n",
      "Epoch 110 batch 140 train Loss 95.6438 test Loss 41.4581 with MSE metric 29717.4110\n",
      "Epoch 110 batch 150 train Loss 95.6111 test Loss 41.4451 with MSE metric 29709.3724\n",
      "Epoch 110 batch 160 train Loss 95.5785 test Loss 41.4320 with MSE metric 29701.3236\n",
      "Epoch 110 batch 170 train Loss 95.5458 test Loss 41.4190 with MSE metric 29693.3444\n",
      "Epoch 110 batch 180 train Loss 95.5133 test Loss 41.4060 with MSE metric 29685.3438\n",
      "Epoch 110 batch 190 train Loss 95.4807 test Loss 41.3930 with MSE metric 29677.4680\n",
      "Epoch 110 batch 200 train Loss 95.4481 test Loss 41.3801 with MSE metric 29669.5388\n",
      "Epoch 110 batch 210 train Loss 95.4156 test Loss 41.3671 with MSE metric 29661.3802\n",
      "Epoch 110 batch 220 train Loss 95.3831 test Loss 41.3541 with MSE metric 29653.6001\n",
      "Epoch 110 batch 230 train Loss 95.3506 test Loss 41.3412 with MSE metric 29645.7745\n",
      "Epoch 110 batch 240 train Loss 95.3182 test Loss 41.3282 with MSE metric 29637.8652\n",
      "Time taken for 1 epoch: 28.423455953598022 secs\n",
      "\n",
      "Epoch 111 batch 0 train Loss 95.2858 test Loss 41.3153 with MSE metric 29630.0407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111 batch 10 train Loss 95.2533 test Loss 41.3024 with MSE metric 29622.2102\n",
      "Epoch 111 batch 20 train Loss 95.2210 test Loss 41.2894 with MSE metric 29614.4415\n",
      "Epoch 111 batch 30 train Loss 95.1886 test Loss 41.2765 with MSE metric 29606.6714\n",
      "Epoch 111 batch 40 train Loss 95.1563 test Loss 41.2636 with MSE metric 29598.9304\n",
      "Epoch 111 batch 50 train Loss 95.1240 test Loss 41.2508 with MSE metric 29591.1642\n",
      "Epoch 111 batch 60 train Loss 95.0917 test Loss 41.2379 with MSE metric 29583.3331\n",
      "Epoch 111 batch 70 train Loss 95.0594 test Loss 41.2250 with MSE metric 29575.4821\n",
      "Epoch 111 batch 80 train Loss 95.0272 test Loss 41.2121 with MSE metric 29567.6766\n",
      "Epoch 111 batch 90 train Loss 94.9949 test Loss 41.1992 with MSE metric 29559.7282\n",
      "Epoch 111 batch 100 train Loss 94.9627 test Loss 41.1864 with MSE metric 29551.9263\n",
      "Epoch 111 batch 110 train Loss 94.9306 test Loss 41.1736 with MSE metric 29543.9535\n",
      "Epoch 111 batch 120 train Loss 94.8984 test Loss 41.1608 with MSE metric 29536.1938\n",
      "Epoch 111 batch 130 train Loss 94.8663 test Loss 41.1480 with MSE metric 29528.5472\n",
      "Epoch 111 batch 140 train Loss 94.8342 test Loss 41.1351 with MSE metric 29520.8191\n",
      "Epoch 111 batch 150 train Loss 94.8021 test Loss 41.1223 with MSE metric 29513.1049\n",
      "Epoch 111 batch 160 train Loss 94.7701 test Loss 41.1095 with MSE metric 29505.3819\n",
      "Epoch 111 batch 170 train Loss 94.7380 test Loss 41.0968 with MSE metric 29497.6674\n",
      "Epoch 111 batch 180 train Loss 94.7060 test Loss 41.0840 with MSE metric 29489.8868\n",
      "Epoch 111 batch 190 train Loss 94.6740 test Loss 41.0713 with MSE metric 29482.0090\n",
      "Epoch 111 batch 200 train Loss 94.6421 test Loss 41.0585 with MSE metric 29474.4096\n",
      "Epoch 111 batch 210 train Loss 94.6101 test Loss 41.0458 with MSE metric 29466.7602\n",
      "Epoch 111 batch 220 train Loss 94.5782 test Loss 41.0330 with MSE metric 29458.8888\n",
      "Epoch 111 batch 230 train Loss 94.5463 test Loss 41.0203 with MSE metric 29451.0124\n",
      "Epoch 111 batch 240 train Loss 94.5145 test Loss 41.0076 with MSE metric 29443.3682\n",
      "Time taken for 1 epoch: 30.190441846847534 secs\n",
      "\n",
      "Epoch 112 batch 0 train Loss 94.4826 test Loss 40.9948 with MSE metric 29435.7371\n",
      "Epoch 112 batch 10 train Loss 94.4508 test Loss 40.9821 with MSE metric 29428.0271\n",
      "Epoch 112 batch 20 train Loss 94.4190 test Loss 40.9694 with MSE metric 29420.2958\n",
      "Epoch 112 batch 30 train Loss 94.3872 test Loss 40.9567 with MSE metric 29412.7139\n",
      "Epoch 112 batch 40 train Loss 94.3555 test Loss 40.9440 with MSE metric 29405.0170\n",
      "Epoch 112 batch 50 train Loss 94.3237 test Loss 40.9314 with MSE metric 29397.1995\n",
      "Epoch 112 batch 60 train Loss 94.2920 test Loss 40.9187 with MSE metric 29389.6868\n",
      "Epoch 112 batch 70 train Loss 94.2603 test Loss 40.9060 with MSE metric 29382.0077\n",
      "Epoch 112 batch 80 train Loss 94.2286 test Loss 40.8934 with MSE metric 29374.4096\n",
      "Epoch 112 batch 90 train Loss 94.1970 test Loss 40.8808 with MSE metric 29366.8892\n",
      "Epoch 112 batch 100 train Loss 94.1654 test Loss 40.8682 with MSE metric 29359.2003\n",
      "Epoch 112 batch 110 train Loss 94.1338 test Loss 40.8555 with MSE metric 29351.4512\n",
      "Epoch 112 batch 120 train Loss 94.1022 test Loss 40.8429 with MSE metric 29343.8705\n",
      "Epoch 112 batch 130 train Loss 94.0706 test Loss 40.8303 with MSE metric 29336.1849\n",
      "Epoch 112 batch 140 train Loss 94.0391 test Loss 40.8177 with MSE metric 29328.6019\n",
      "Epoch 112 batch 150 train Loss 94.0076 test Loss 40.8052 with MSE metric 29320.9299\n",
      "Epoch 112 batch 160 train Loss 93.9761 test Loss 40.7926 with MSE metric 29313.2429\n",
      "Epoch 112 batch 170 train Loss 93.9446 test Loss 40.7801 with MSE metric 29305.6541\n",
      "Epoch 112 batch 180 train Loss 93.9132 test Loss 40.7676 with MSE metric 29298.1742\n",
      "Epoch 112 batch 190 train Loss 93.8818 test Loss 40.7550 with MSE metric 29290.5524\n",
      "Epoch 112 batch 200 train Loss 93.8503 test Loss 40.7425 with MSE metric 29282.7907\n",
      "Epoch 112 batch 210 train Loss 93.8190 test Loss 40.7300 with MSE metric 29275.1790\n",
      "Epoch 112 batch 220 train Loss 93.7876 test Loss 40.7174 with MSE metric 29267.5365\n",
      "Epoch 112 batch 230 train Loss 93.7563 test Loss 40.7049 with MSE metric 29259.9529\n",
      "Epoch 112 batch 240 train Loss 93.7250 test Loss 40.6924 with MSE metric 29252.4106\n",
      "Time taken for 1 epoch: 29.34632182121277 secs\n",
      "\n",
      "Epoch 113 batch 0 train Loss 93.6937 test Loss 40.6799 with MSE metric 29244.9397\n",
      "Epoch 113 batch 10 train Loss 93.6624 test Loss 40.6674 with MSE metric 29237.3839\n",
      "Epoch 113 batch 20 train Loss 93.6312 test Loss 40.6550 with MSE metric 29229.7010\n",
      "Epoch 113 batch 30 train Loss 93.5999 test Loss 40.6425 with MSE metric 29222.1353\n",
      "Epoch 113 batch 40 train Loss 93.5687 test Loss 40.6300 with MSE metric 29214.4680\n",
      "Epoch 113 batch 50 train Loss 93.5376 test Loss 40.6176 with MSE metric 29206.8698\n",
      "Epoch 113 batch 60 train Loss 93.5064 test Loss 40.6051 with MSE metric 29199.3276\n",
      "Epoch 113 batch 70 train Loss 93.4753 test Loss 40.5927 with MSE metric 29191.8893\n",
      "Epoch 113 batch 80 train Loss 93.4442 test Loss 40.5802 with MSE metric 29184.2635\n",
      "Epoch 113 batch 90 train Loss 93.4131 test Loss 40.5678 with MSE metric 29176.7537\n",
      "Epoch 113 batch 100 train Loss 93.3820 test Loss 40.5554 with MSE metric 29169.2444\n",
      "Epoch 113 batch 110 train Loss 93.3510 test Loss 40.5430 with MSE metric 29161.8714\n",
      "Epoch 113 batch 120 train Loss 93.3199 test Loss 40.5306 with MSE metric 29154.4804\n",
      "Epoch 113 batch 130 train Loss 93.2889 test Loss 40.5183 with MSE metric 29147.0185\n",
      "Epoch 113 batch 140 train Loss 93.2580 test Loss 40.5059 with MSE metric 29139.4912\n",
      "Epoch 113 batch 150 train Loss 93.2270 test Loss 40.4936 with MSE metric 29132.0288\n",
      "Epoch 113 batch 160 train Loss 93.1961 test Loss 40.4813 with MSE metric 29124.4980\n",
      "Epoch 113 batch 170 train Loss 93.1651 test Loss 40.4689 with MSE metric 29116.9954\n",
      "Epoch 113 batch 180 train Loss 93.1343 test Loss 40.4566 with MSE metric 29109.4900\n",
      "Epoch 113 batch 190 train Loss 93.1034 test Loss 40.4442 with MSE metric 29102.1488\n",
      "Epoch 113 batch 200 train Loss 93.0725 test Loss 40.4319 with MSE metric 29094.7156\n",
      "Epoch 113 batch 210 train Loss 93.0417 test Loss 40.4196 with MSE metric 29087.3633\n",
      "Epoch 113 batch 220 train Loss 93.0109 test Loss 40.4073 with MSE metric 29079.9026\n",
      "Epoch 113 batch 230 train Loss 92.9801 test Loss 40.3951 with MSE metric 29072.4350\n",
      "Epoch 113 batch 240 train Loss 92.9493 test Loss 40.3828 with MSE metric 29064.9552\n",
      "Time taken for 1 epoch: 28.517497062683105 secs\n",
      "\n",
      "Epoch 114 batch 0 train Loss 92.9186 test Loss 40.3705 with MSE metric 29057.4565\n",
      "Epoch 114 batch 10 train Loss 92.8879 test Loss 40.3583 with MSE metric 29050.0564\n",
      "Epoch 114 batch 20 train Loss 92.8572 test Loss 40.3460 with MSE metric 29042.6607\n",
      "Epoch 114 batch 30 train Loss 92.8265 test Loss 40.3338 with MSE metric 29035.2392\n",
      "Epoch 114 batch 40 train Loss 92.7958 test Loss 40.3216 with MSE metric 29027.9092\n",
      "Epoch 114 batch 50 train Loss 92.7652 test Loss 40.3094 with MSE metric 29020.5248\n",
      "Epoch 114 batch 60 train Loss 92.7346 test Loss 40.2971 with MSE metric 29013.0846\n",
      "Epoch 114 batch 70 train Loss 92.7040 test Loss 40.2849 with MSE metric 29005.7478\n",
      "Epoch 114 batch 80 train Loss 92.6734 test Loss 40.2727 with MSE metric 28998.3655\n",
      "Epoch 114 batch 90 train Loss 92.6428 test Loss 40.2605 with MSE metric 28991.0286\n",
      "Epoch 114 batch 100 train Loss 92.6123 test Loss 40.2483 with MSE metric 28983.5584\n",
      "Epoch 114 batch 110 train Loss 92.5818 test Loss 40.2361 with MSE metric 28976.0954\n",
      "Epoch 114 batch 120 train Loss 92.5513 test Loss 40.2239 with MSE metric 28968.7238\n",
      "Epoch 114 batch 130 train Loss 92.5209 test Loss 40.2118 with MSE metric 28961.4738\n",
      "Epoch 114 batch 140 train Loss 92.4904 test Loss 40.1996 with MSE metric 28954.1891\n",
      "Epoch 114 batch 150 train Loss 92.4600 test Loss 40.1875 with MSE metric 28946.9186\n",
      "Epoch 114 batch 160 train Loss 92.4296 test Loss 40.1753 with MSE metric 28939.6455\n",
      "Epoch 114 batch 170 train Loss 92.3992 test Loss 40.1632 with MSE metric 28932.2553\n",
      "Epoch 114 batch 180 train Loss 92.3689 test Loss 40.1511 with MSE metric 28924.8088\n",
      "Epoch 114 batch 190 train Loss 92.3385 test Loss 40.1390 with MSE metric 28917.4748\n",
      "Epoch 114 batch 200 train Loss 92.3082 test Loss 40.1269 with MSE metric 28910.1020\n",
      "Epoch 114 batch 210 train Loss 92.2779 test Loss 40.1148 with MSE metric 28902.7845\n",
      "Epoch 114 batch 220 train Loss 92.2476 test Loss 40.1028 with MSE metric 28895.2584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114 batch 230 train Loss 92.2174 test Loss 40.0907 with MSE metric 28887.9765\n",
      "Epoch 114 batch 240 train Loss 92.1872 test Loss 40.0786 with MSE metric 28880.7357\n",
      "Time taken for 1 epoch: 27.59711194038391 secs\n",
      "\n",
      "Epoch 115 batch 0 train Loss 92.1569 test Loss 40.0666 with MSE metric 28873.2700\n",
      "Epoch 115 batch 10 train Loss 92.1268 test Loss 40.0545 with MSE metric 28866.0110\n",
      "Epoch 115 batch 20 train Loss 92.0966 test Loss 40.0425 with MSE metric 28858.7105\n",
      "Epoch 115 batch 30 train Loss 92.0665 test Loss 40.0305 with MSE metric 28851.2848\n",
      "Epoch 115 batch 40 train Loss 92.0363 test Loss 40.0184 with MSE metric 28844.0118\n",
      "Epoch 115 batch 50 train Loss 92.0062 test Loss 40.0064 with MSE metric 28837.0103\n",
      "Epoch 115 batch 60 train Loss 91.9761 test Loss 39.9944 with MSE metric 28829.7175\n",
      "Epoch 115 batch 70 train Loss 91.9461 test Loss 39.9824 with MSE metric 28822.4194\n",
      "Epoch 115 batch 80 train Loss 91.9160 test Loss 39.9704 with MSE metric 28815.0127\n",
      "Epoch 115 batch 90 train Loss 91.8860 test Loss 39.9584 with MSE metric 28807.6541\n",
      "Epoch 115 batch 100 train Loss 91.8560 test Loss 39.9464 with MSE metric 28800.3691\n",
      "Epoch 115 batch 110 train Loss 91.8260 test Loss 39.9345 with MSE metric 28793.1162\n",
      "Epoch 115 batch 120 train Loss 91.7960 test Loss 39.9225 with MSE metric 28785.8626\n",
      "Epoch 115 batch 130 train Loss 91.7661 test Loss 39.9106 with MSE metric 28778.6108\n",
      "Epoch 115 batch 140 train Loss 91.7362 test Loss 39.8986 with MSE metric 28771.4004\n",
      "Epoch 115 batch 150 train Loss 91.7063 test Loss 39.8867 with MSE metric 28764.1281\n",
      "Epoch 115 batch 160 train Loss 91.6764 test Loss 39.8748 with MSE metric 28756.8287\n",
      "Epoch 115 batch 170 train Loss 91.6465 test Loss 39.8629 with MSE metric 28749.4918\n",
      "Epoch 115 batch 180 train Loss 91.6167 test Loss 39.8510 with MSE metric 28742.2827\n",
      "Epoch 115 batch 190 train Loss 91.5869 test Loss 39.8391 with MSE metric 28735.2714\n",
      "Epoch 115 batch 200 train Loss 91.5571 test Loss 39.8272 with MSE metric 28728.0593\n",
      "Epoch 115 batch 210 train Loss 91.5273 test Loss 39.8153 with MSE metric 28720.9207\n",
      "Epoch 115 batch 220 train Loss 91.4976 test Loss 39.8035 with MSE metric 28713.7001\n",
      "Epoch 115 batch 230 train Loss 91.4679 test Loss 39.7916 with MSE metric 28706.5123\n",
      "Epoch 115 batch 240 train Loss 91.4382 test Loss 39.7797 with MSE metric 28699.4528\n",
      "Time taken for 1 epoch: 29.247021913528442 secs\n",
      "\n",
      "Epoch 116 batch 0 train Loss 91.4085 test Loss 39.7679 with MSE metric 28692.3445\n",
      "Epoch 116 batch 10 train Loss 91.3788 test Loss 39.7560 with MSE metric 28685.2238\n",
      "Epoch 116 batch 20 train Loss 91.3492 test Loss 39.7442 with MSE metric 28678.0286\n",
      "Epoch 116 batch 30 train Loss 91.3195 test Loss 39.7324 with MSE metric 28670.7004\n",
      "Epoch 116 batch 40 train Loss 91.2899 test Loss 39.7205 with MSE metric 28663.4653\n",
      "Epoch 116 batch 50 train Loss 91.2603 test Loss 39.7087 with MSE metric 28656.3715\n",
      "Epoch 116 batch 60 train Loss 91.2308 test Loss 39.6969 with MSE metric 28649.2540\n",
      "Epoch 116 batch 70 train Loss 91.2012 test Loss 39.6851 with MSE metric 28641.9643\n",
      "Epoch 116 batch 80 train Loss 91.1717 test Loss 39.6734 with MSE metric 28634.7407\n",
      "Epoch 116 batch 90 train Loss 91.1422 test Loss 39.6616 with MSE metric 28627.6159\n",
      "Epoch 116 batch 100 train Loss 91.1127 test Loss 39.6498 with MSE metric 28620.5132\n",
      "Epoch 116 batch 110 train Loss 91.0832 test Loss 39.6381 with MSE metric 28613.2744\n",
      "Epoch 116 batch 120 train Loss 91.0538 test Loss 39.6263 with MSE metric 28606.1035\n",
      "Epoch 116 batch 130 train Loss 91.0244 test Loss 39.6146 with MSE metric 28598.9904\n",
      "Epoch 116 batch 140 train Loss 90.9949 test Loss 39.6029 with MSE metric 28591.8041\n",
      "Epoch 116 batch 150 train Loss 90.9656 test Loss 39.5912 with MSE metric 28584.5588\n",
      "Epoch 116 batch 160 train Loss 90.9362 test Loss 39.5794 with MSE metric 28577.4865\n",
      "Epoch 116 batch 170 train Loss 90.9068 test Loss 39.5677 with MSE metric 28570.1996\n",
      "Epoch 116 batch 180 train Loss 90.8775 test Loss 39.5560 with MSE metric 28563.1269\n",
      "Epoch 116 batch 190 train Loss 90.8482 test Loss 39.5443 with MSE metric 28556.1203\n",
      "Epoch 116 batch 200 train Loss 90.8189 test Loss 39.5326 with MSE metric 28548.9282\n",
      "Epoch 116 batch 210 train Loss 90.7896 test Loss 39.5209 with MSE metric 28541.7862\n",
      "Epoch 116 batch 220 train Loss 90.7604 test Loss 39.5092 with MSE metric 28534.7224\n",
      "Epoch 116 batch 230 train Loss 90.7311 test Loss 39.4975 with MSE metric 28527.6566\n",
      "Epoch 116 batch 240 train Loss 90.7019 test Loss 39.4859 with MSE metric 28520.5403\n",
      "Time taken for 1 epoch: 28.821264266967773 secs\n",
      "\n",
      "Epoch 117 batch 0 train Loss 90.6728 test Loss 39.4742 with MSE metric 28513.5551\n",
      "Epoch 117 batch 10 train Loss 90.6436 test Loss 39.4626 with MSE metric 28506.5231\n",
      "Epoch 117 batch 20 train Loss 90.6144 test Loss 39.4510 with MSE metric 28499.4034\n",
      "Epoch 117 batch 30 train Loss 90.5853 test Loss 39.4393 with MSE metric 28492.4368\n",
      "Epoch 117 batch 40 train Loss 90.5562 test Loss 39.4277 with MSE metric 28485.4032\n",
      "Epoch 117 batch 50 train Loss 90.5271 test Loss 39.4161 with MSE metric 28478.4185\n",
      "Epoch 117 batch 60 train Loss 90.4981 test Loss 39.4045 with MSE metric 28471.4640\n",
      "Epoch 117 batch 70 train Loss 90.4690 test Loss 39.3929 with MSE metric 28464.3711\n",
      "Epoch 117 batch 80 train Loss 90.4400 test Loss 39.3813 with MSE metric 28457.3163\n",
      "Epoch 117 batch 90 train Loss 90.4110 test Loss 39.3697 with MSE metric 28450.3845\n",
      "Epoch 117 batch 100 train Loss 90.3820 test Loss 39.3582 with MSE metric 28443.3251\n",
      "Epoch 117 batch 110 train Loss 90.3531 test Loss 39.3466 with MSE metric 28436.3912\n",
      "Epoch 117 batch 120 train Loss 90.3241 test Loss 39.3350 with MSE metric 28429.3741\n",
      "Epoch 117 batch 130 train Loss 90.2952 test Loss 39.3235 with MSE metric 28422.3895\n",
      "Epoch 117 batch 140 train Loss 90.2663 test Loss 39.3120 with MSE metric 28415.4140\n",
      "Epoch 117 batch 150 train Loss 90.2374 test Loss 39.3004 with MSE metric 28408.3756\n",
      "Epoch 117 batch 160 train Loss 90.2085 test Loss 39.2889 with MSE metric 28401.3629\n",
      "Epoch 117 batch 170 train Loss 90.1797 test Loss 39.2774 with MSE metric 28394.3503\n",
      "Epoch 117 batch 180 train Loss 90.1509 test Loss 39.2659 with MSE metric 28387.3761\n",
      "Epoch 117 batch 190 train Loss 90.1221 test Loss 39.2544 with MSE metric 28380.4425\n",
      "Epoch 117 batch 200 train Loss 90.0933 test Loss 39.2429 with MSE metric 28373.4222\n",
      "Epoch 117 batch 210 train Loss 90.0645 test Loss 39.2315 with MSE metric 28366.5303\n",
      "Epoch 117 batch 220 train Loss 90.0357 test Loss 39.2200 with MSE metric 28359.6120\n",
      "Epoch 117 batch 230 train Loss 90.0070 test Loss 39.2085 with MSE metric 28352.6424\n",
      "Epoch 117 batch 240 train Loss 89.9783 test Loss 39.1971 with MSE metric 28345.6760\n",
      "Time taken for 1 epoch: 28.30176877975464 secs\n",
      "\n",
      "Epoch 118 batch 0 train Loss 89.9496 test Loss 39.1857 with MSE metric 28338.7099\n",
      "Epoch 118 batch 10 train Loss 89.9209 test Loss 39.1742 with MSE metric 28331.8909\n",
      "Epoch 118 batch 20 train Loss 89.8923 test Loss 39.1628 with MSE metric 28325.0276\n",
      "Epoch 118 batch 30 train Loss 89.8637 test Loss 39.1513 with MSE metric 28318.0073\n",
      "Epoch 118 batch 40 train Loss 89.8350 test Loss 39.1399 with MSE metric 28310.9781\n",
      "Epoch 118 batch 50 train Loss 89.8064 test Loss 39.1285 with MSE metric 28304.0391\n",
      "Epoch 118 batch 60 train Loss 89.7779 test Loss 39.1172 with MSE metric 28297.1605\n",
      "Epoch 118 batch 70 train Loss 89.7493 test Loss 39.1057 with MSE metric 28290.2099\n",
      "Epoch 118 batch 80 train Loss 89.7208 test Loss 39.0943 with MSE metric 28283.4118\n",
      "Epoch 118 batch 90 train Loss 89.6922 test Loss 39.0830 with MSE metric 28276.5662\n",
      "Epoch 118 batch 100 train Loss 89.6637 test Loss 39.0716 with MSE metric 28269.6741\n",
      "Epoch 118 batch 110 train Loss 89.6353 test Loss 39.0603 with MSE metric 28262.7265\n",
      "Epoch 118 batch 120 train Loss 89.6068 test Loss 39.0489 with MSE metric 28255.8280\n",
      "Epoch 118 batch 130 train Loss 89.5784 test Loss 39.0375 with MSE metric 28249.0942\n",
      "Epoch 118 batch 140 train Loss 89.5499 test Loss 39.0262 with MSE metric 28242.2680\n",
      "Epoch 118 batch 150 train Loss 89.5215 test Loss 39.0149 with MSE metric 28235.4069\n",
      "Epoch 118 batch 160 train Loss 89.4932 test Loss 39.0035 with MSE metric 28228.5561\n",
      "Epoch 118 batch 170 train Loss 89.4648 test Loss 38.9922 with MSE metric 28221.6302\n",
      "Epoch 118 batch 180 train Loss 89.4364 test Loss 38.9809 with MSE metric 28214.6177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 batch 190 train Loss 89.4081 test Loss 38.9696 with MSE metric 28207.7923\n",
      "Epoch 118 batch 200 train Loss 89.3798 test Loss 38.9583 with MSE metric 28201.0245\n",
      "Epoch 118 batch 210 train Loss 89.3515 test Loss 38.9470 with MSE metric 28194.1737\n",
      "Epoch 118 batch 220 train Loss 89.3233 test Loss 38.9357 with MSE metric 28187.4148\n",
      "Epoch 118 batch 230 train Loss 89.2950 test Loss 38.9244 with MSE metric 28180.6652\n",
      "Epoch 118 batch 240 train Loss 89.2668 test Loss 38.9132 with MSE metric 28173.8964\n",
      "Time taken for 1 epoch: 27.982534170150757 secs\n",
      "\n",
      "Epoch 119 batch 0 train Loss 89.2386 test Loss 38.9020 with MSE metric 28166.9767\n",
      "Epoch 119 batch 10 train Loss 89.2104 test Loss 38.8907 with MSE metric 28160.1548\n",
      "Epoch 119 batch 20 train Loss 89.1822 test Loss 38.8794 with MSE metric 28153.4326\n",
      "Epoch 119 batch 30 train Loss 89.1541 test Loss 38.8682 with MSE metric 28146.6927\n",
      "Epoch 119 batch 40 train Loss 89.1259 test Loss 38.8569 with MSE metric 28139.8330\n",
      "Epoch 119 batch 50 train Loss 89.0978 test Loss 38.8457 with MSE metric 28133.0390\n",
      "Epoch 119 batch 60 train Loss 89.0697 test Loss 38.8345 with MSE metric 28126.2342\n",
      "Epoch 119 batch 70 train Loss 89.0416 test Loss 38.8233 with MSE metric 28119.2662\n",
      "Epoch 119 batch 80 train Loss 89.0136 test Loss 38.8121 with MSE metric 28112.5257\n",
      "Epoch 119 batch 90 train Loss 88.9855 test Loss 38.8009 with MSE metric 28105.6702\n",
      "Epoch 119 batch 100 train Loss 88.9575 test Loss 38.7897 with MSE metric 28098.8224\n",
      "Epoch 119 batch 110 train Loss 88.9295 test Loss 38.7785 with MSE metric 28092.1314\n",
      "Epoch 119 batch 120 train Loss 88.9015 test Loss 38.7673 with MSE metric 28085.3796\n",
      "Epoch 119 batch 130 train Loss 88.8735 test Loss 38.7562 with MSE metric 28078.7338\n",
      "Epoch 119 batch 140 train Loss 88.8456 test Loss 38.7450 with MSE metric 28071.8057\n",
      "Epoch 119 batch 150 train Loss 88.8177 test Loss 38.7338 with MSE metric 28065.0626\n",
      "Epoch 119 batch 160 train Loss 88.7897 test Loss 38.7227 with MSE metric 28058.2589\n",
      "Epoch 119 batch 170 train Loss 88.7618 test Loss 38.7116 with MSE metric 28051.3246\n",
      "Epoch 119 batch 180 train Loss 88.7340 test Loss 38.7004 with MSE metric 28044.4558\n",
      "Epoch 119 batch 190 train Loss 88.7061 test Loss 38.6894 with MSE metric 28037.6732\n",
      "Epoch 119 batch 200 train Loss 88.6783 test Loss 38.6782 with MSE metric 28030.9819\n",
      "Epoch 119 batch 210 train Loss 88.6505 test Loss 38.6671 with MSE metric 28024.2063\n",
      "Epoch 119 batch 220 train Loss 88.6227 test Loss 38.6561 with MSE metric 28017.6189\n",
      "Epoch 119 batch 230 train Loss 88.5949 test Loss 38.6450 with MSE metric 28010.8480\n",
      "Epoch 119 batch 240 train Loss 88.5671 test Loss 38.6339 with MSE metric 28004.1227\n",
      "Time taken for 1 epoch: 32.7302680015564 secs\n",
      "\n",
      "Epoch 120 batch 0 train Loss 88.5394 test Loss 38.6228 with MSE metric 27997.3128\n",
      "Epoch 120 batch 10 train Loss 88.5116 test Loss 38.6117 with MSE metric 27990.6013\n",
      "Epoch 120 batch 20 train Loss 88.4839 test Loss 38.6007 with MSE metric 27983.8182\n",
      "Epoch 120 batch 30 train Loss 88.4562 test Loss 38.5896 with MSE metric 27977.1350\n",
      "Epoch 120 batch 40 train Loss 88.4286 test Loss 38.5786 with MSE metric 27970.4705\n",
      "Epoch 120 batch 50 train Loss 88.4009 test Loss 38.5675 with MSE metric 27963.9780\n",
      "Epoch 120 batch 60 train Loss 88.3733 test Loss 38.5565 with MSE metric 27957.3578\n",
      "Epoch 120 batch 70 train Loss 88.3457 test Loss 38.5455 with MSE metric 27950.6762\n",
      "Epoch 120 batch 80 train Loss 88.3181 test Loss 38.5344 with MSE metric 27944.0085\n",
      "Epoch 120 batch 90 train Loss 88.2905 test Loss 38.5235 with MSE metric 27937.2402\n",
      "Epoch 120 batch 100 train Loss 88.2629 test Loss 38.5124 with MSE metric 27930.4934\n",
      "Epoch 120 batch 110 train Loss 88.2354 test Loss 38.5014 with MSE metric 27923.8874\n",
      "Epoch 120 batch 120 train Loss 88.2079 test Loss 38.4904 with MSE metric 27917.2720\n",
      "Epoch 120 batch 130 train Loss 88.1803 test Loss 38.4795 with MSE metric 27910.5272\n",
      "Epoch 120 batch 140 train Loss 88.1529 test Loss 38.4685 with MSE metric 27903.8882\n",
      "Epoch 120 batch 150 train Loss 88.1254 test Loss 38.4575 with MSE metric 27897.2627\n",
      "Epoch 120 batch 160 train Loss 88.0979 test Loss 38.4465 with MSE metric 27890.5324\n",
      "Epoch 120 batch 170 train Loss 88.0705 test Loss 38.4356 with MSE metric 27883.8028\n",
      "Epoch 120 batch 180 train Loss 88.0431 test Loss 38.4246 with MSE metric 27877.2307\n",
      "Epoch 120 batch 190 train Loss 88.0157 test Loss 38.4137 with MSE metric 27870.5859\n",
      "Epoch 120 batch 200 train Loss 87.9883 test Loss 38.4028 with MSE metric 27863.9115\n",
      "Epoch 120 batch 210 train Loss 87.9609 test Loss 38.3918 with MSE metric 27857.1899\n",
      "Epoch 120 batch 220 train Loss 87.9336 test Loss 38.3809 with MSE metric 27850.3131\n",
      "Epoch 120 batch 230 train Loss 87.9063 test Loss 38.3700 with MSE metric 27843.6875\n",
      "Epoch 120 batch 240 train Loss 87.8790 test Loss 38.3591 with MSE metric 27837.0643\n",
      "Time taken for 1 epoch: 29.06910514831543 secs\n",
      "\n",
      "Epoch 121 batch 0 train Loss 87.8517 test Loss 38.3482 with MSE metric 27830.4224\n",
      "Epoch 121 batch 10 train Loss 87.8244 test Loss 38.3373 with MSE metric 27823.7639\n",
      "Epoch 121 batch 20 train Loss 87.7972 test Loss 38.3264 with MSE metric 27817.2004\n",
      "Epoch 121 batch 30 train Loss 87.7699 test Loss 38.3155 with MSE metric 27810.5177\n",
      "Epoch 121 batch 40 train Loss 87.7427 test Loss 38.3047 with MSE metric 27803.8710\n",
      "Epoch 121 batch 50 train Loss 87.7155 test Loss 38.2938 with MSE metric 27797.3189\n",
      "Epoch 121 batch 60 train Loss 87.6883 test Loss 38.2830 with MSE metric 27790.5252\n",
      "Epoch 121 batch 70 train Loss 87.6612 test Loss 38.2721 with MSE metric 27783.8572\n",
      "Epoch 121 batch 80 train Loss 87.6340 test Loss 38.2613 with MSE metric 27777.2674\n",
      "Epoch 121 batch 90 train Loss 87.6069 test Loss 38.2504 with MSE metric 27770.7018\n",
      "Epoch 121 batch 100 train Loss 87.5798 test Loss 38.2396 with MSE metric 27764.2665\n",
      "Epoch 121 batch 110 train Loss 87.5527 test Loss 38.2288 with MSE metric 27757.7245\n",
      "Epoch 121 batch 120 train Loss 87.5256 test Loss 38.2180 with MSE metric 27751.2420\n",
      "Epoch 121 batch 130 train Loss 87.4986 test Loss 38.2072 with MSE metric 27744.7205\n",
      "Epoch 121 batch 140 train Loss 87.4715 test Loss 38.1964 with MSE metric 27738.3189\n",
      "Epoch 121 batch 150 train Loss 87.4445 test Loss 38.1857 with MSE metric 27731.6442\n",
      "Epoch 121 batch 160 train Loss 87.4175 test Loss 38.1749 with MSE metric 27725.0851\n",
      "Epoch 121 batch 170 train Loss 87.3905 test Loss 38.1641 with MSE metric 27718.5124\n",
      "Epoch 121 batch 180 train Loss 87.3635 test Loss 38.1533 with MSE metric 27711.9084\n",
      "Epoch 121 batch 190 train Loss 87.3366 test Loss 38.1425 with MSE metric 27705.3642\n",
      "Epoch 121 batch 200 train Loss 87.3097 test Loss 38.1318 with MSE metric 27698.8860\n",
      "Epoch 121 batch 210 train Loss 87.2828 test Loss 38.1211 with MSE metric 27692.4285\n",
      "Epoch 121 batch 220 train Loss 87.2559 test Loss 38.1104 with MSE metric 27685.9611\n",
      "Epoch 121 batch 230 train Loss 87.2290 test Loss 38.0997 with MSE metric 27679.4116\n",
      "Epoch 121 batch 240 train Loss 87.2021 test Loss 38.0890 with MSE metric 27672.8679\n",
      "Time taken for 1 epoch: 26.83838415145874 secs\n",
      "\n",
      "Epoch 122 batch 0 train Loss 87.1753 test Loss 38.0782 with MSE metric 27666.3800\n",
      "Epoch 122 batch 10 train Loss 87.1485 test Loss 38.0675 with MSE metric 27659.9753\n",
      "Epoch 122 batch 20 train Loss 87.1217 test Loss 38.0568 with MSE metric 27653.4693\n",
      "Epoch 122 batch 30 train Loss 87.0949 test Loss 38.0461 with MSE metric 27647.0003\n",
      "Epoch 122 batch 40 train Loss 87.0681 test Loss 38.0354 with MSE metric 27640.4297\n",
      "Epoch 122 batch 50 train Loss 87.0413 test Loss 38.0248 with MSE metric 27633.9600\n",
      "Epoch 122 batch 60 train Loss 87.0146 test Loss 38.0141 with MSE metric 27627.4672\n",
      "Epoch 122 batch 70 train Loss 86.9879 test Loss 38.0034 with MSE metric 27621.1547\n",
      "Epoch 122 batch 80 train Loss 86.9612 test Loss 37.9927 with MSE metric 27614.7017\n",
      "Epoch 122 batch 90 train Loss 86.9345 test Loss 37.9821 with MSE metric 27608.2342\n",
      "Epoch 122 batch 100 train Loss 86.9078 test Loss 37.9715 with MSE metric 27601.7627\n",
      "Epoch 122 batch 110 train Loss 86.8812 test Loss 37.9608 with MSE metric 27595.2569\n",
      "Epoch 122 batch 120 train Loss 86.8546 test Loss 37.9502 with MSE metric 27588.6459\n",
      "Epoch 122 batch 130 train Loss 86.8279 test Loss 37.9396 with MSE metric 27582.2933\n",
      "Epoch 122 batch 140 train Loss 86.8013 test Loss 37.9290 with MSE metric 27575.7631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122 batch 150 train Loss 86.7748 test Loss 37.9184 with MSE metric 27569.4596\n",
      "Epoch 122 batch 160 train Loss 86.7482 test Loss 37.9078 with MSE metric 27562.9950\n",
      "Epoch 122 batch 170 train Loss 86.7217 test Loss 37.8972 with MSE metric 27556.6100\n",
      "Epoch 122 batch 180 train Loss 86.6951 test Loss 37.8866 with MSE metric 27550.1523\n",
      "Epoch 122 batch 190 train Loss 86.6686 test Loss 37.8760 with MSE metric 27543.5999\n",
      "Epoch 122 batch 200 train Loss 86.6421 test Loss 37.8654 with MSE metric 27537.1904\n",
      "Epoch 122 batch 210 train Loss 86.6157 test Loss 37.8549 with MSE metric 27530.6569\n",
      "Epoch 122 batch 220 train Loss 86.5892 test Loss 37.8443 with MSE metric 27524.2387\n",
      "Epoch 122 batch 230 train Loss 86.5628 test Loss 37.8338 with MSE metric 27517.8297\n",
      "Epoch 122 batch 240 train Loss 86.5363 test Loss 37.8233 with MSE metric 27511.3952\n",
      "Time taken for 1 epoch: 33.81719183921814 secs\n",
      "\n",
      "Epoch 123 batch 0 train Loss 86.5099 test Loss 37.8127 with MSE metric 27504.9214\n",
      "Epoch 123 batch 10 train Loss 86.4835 test Loss 37.8021 with MSE metric 27498.5116\n",
      "Epoch 123 batch 20 train Loss 86.4571 test Loss 37.7916 with MSE metric 27492.1599\n",
      "Epoch 123 batch 30 train Loss 86.4308 test Loss 37.7811 with MSE metric 27485.9324\n",
      "Epoch 123 batch 40 train Loss 86.4044 test Loss 37.7706 with MSE metric 27479.6119\n",
      "Epoch 123 batch 50 train Loss 86.3781 test Loss 37.7600 with MSE metric 27473.2467\n",
      "Epoch 123 batch 60 train Loss 86.3518 test Loss 37.7496 with MSE metric 27467.0229\n",
      "Epoch 123 batch 70 train Loss 86.3255 test Loss 37.7391 with MSE metric 27460.7154\n",
      "Epoch 123 batch 80 train Loss 86.2992 test Loss 37.7286 with MSE metric 27454.3033\n",
      "Epoch 123 batch 90 train Loss 86.2730 test Loss 37.7181 with MSE metric 27448.0261\n",
      "Epoch 123 batch 100 train Loss 86.2468 test Loss 37.7076 with MSE metric 27441.6109\n",
      "Epoch 123 batch 110 train Loss 86.2205 test Loss 37.6971 with MSE metric 27435.3220\n",
      "Epoch 123 batch 120 train Loss 86.1943 test Loss 37.6867 with MSE metric 27428.9009\n",
      "Epoch 123 batch 130 train Loss 86.1682 test Loss 37.6762 with MSE metric 27422.4431\n",
      "Epoch 123 batch 140 train Loss 86.1420 test Loss 37.6658 with MSE metric 27416.1451\n",
      "Epoch 123 batch 150 train Loss 86.1158 test Loss 37.6554 with MSE metric 27409.7870\n",
      "Epoch 123 batch 160 train Loss 86.0897 test Loss 37.6449 with MSE metric 27403.3085\n",
      "Epoch 123 batch 170 train Loss 86.0636 test Loss 37.6345 with MSE metric 27396.9652\n",
      "Epoch 123 batch 180 train Loss 86.0375 test Loss 37.6241 with MSE metric 27390.7183\n",
      "Epoch 123 batch 190 train Loss 86.0114 test Loss 37.6136 with MSE metric 27384.4542\n",
      "Epoch 123 batch 200 train Loss 85.9853 test Loss 37.6032 with MSE metric 27378.1336\n",
      "Epoch 123 batch 210 train Loss 85.9593 test Loss 37.5928 with MSE metric 27371.8320\n",
      "Epoch 123 batch 220 train Loss 85.9332 test Loss 37.5824 with MSE metric 27365.5585\n",
      "Epoch 123 batch 230 train Loss 85.9072 test Loss 37.5720 with MSE metric 27359.1640\n",
      "Epoch 123 batch 240 train Loss 85.8812 test Loss 37.5616 with MSE metric 27352.9978\n",
      "Time taken for 1 epoch: 30.51098394393921 secs\n",
      "\n",
      "Epoch 124 batch 0 train Loss 85.8552 test Loss 37.5513 with MSE metric 27346.7509\n",
      "Epoch 124 batch 10 train Loss 85.8293 test Loss 37.5409 with MSE metric 27340.4922\n",
      "Epoch 124 batch 20 train Loss 85.8033 test Loss 37.5305 with MSE metric 27334.1686\n",
      "Epoch 124 batch 30 train Loss 85.7774 test Loss 37.5202 with MSE metric 27327.9085\n",
      "Epoch 124 batch 40 train Loss 85.7515 test Loss 37.5099 with MSE metric 27321.6714\n",
      "Epoch 124 batch 50 train Loss 85.7256 test Loss 37.4995 with MSE metric 27315.5306\n",
      "Epoch 124 batch 60 train Loss 85.6997 test Loss 37.4892 with MSE metric 27309.3164\n",
      "Epoch 124 batch 70 train Loss 85.6738 test Loss 37.4789 with MSE metric 27303.1001\n",
      "Epoch 124 batch 80 train Loss 85.6480 test Loss 37.4686 with MSE metric 27296.8612\n",
      "Epoch 124 batch 90 train Loss 85.6221 test Loss 37.4582 with MSE metric 27290.7512\n",
      "Epoch 124 batch 100 train Loss 85.5963 test Loss 37.4479 with MSE metric 27284.4220\n",
      "Epoch 124 batch 110 train Loss 85.5705 test Loss 37.4376 with MSE metric 27278.0084\n",
      "Epoch 124 batch 120 train Loss 85.5447 test Loss 37.4274 with MSE metric 27271.7846\n",
      "Epoch 124 batch 130 train Loss 85.5190 test Loss 37.4171 with MSE metric 27265.4916\n",
      "Epoch 124 batch 140 train Loss 85.4932 test Loss 37.4068 with MSE metric 27259.1177\n",
      "Epoch 124 batch 150 train Loss 85.4675 test Loss 37.3965 with MSE metric 27252.9791\n",
      "Epoch 124 batch 160 train Loss 85.4418 test Loss 37.3863 with MSE metric 27246.7132\n",
      "Epoch 124 batch 170 train Loss 85.4161 test Loss 37.3760 with MSE metric 27240.5486\n",
      "Epoch 124 batch 180 train Loss 85.3904 test Loss 37.3658 with MSE metric 27234.2337\n",
      "Epoch 124 batch 190 train Loss 85.3647 test Loss 37.3555 with MSE metric 27228.0154\n",
      "Epoch 124 batch 200 train Loss 85.3390 test Loss 37.3453 with MSE metric 27221.7778\n",
      "Epoch 124 batch 210 train Loss 85.3134 test Loss 37.3351 with MSE metric 27215.6472\n",
      "Epoch 124 batch 220 train Loss 85.2878 test Loss 37.3249 with MSE metric 27209.4067\n",
      "Epoch 124 batch 230 train Loss 85.2622 test Loss 37.3146 with MSE metric 27203.2040\n",
      "Epoch 124 batch 240 train Loss 85.2366 test Loss 37.3044 with MSE metric 27196.9495\n",
      "Time taken for 1 epoch: 30.494865894317627 secs\n",
      "\n",
      "Epoch 125 batch 0 train Loss 85.2110 test Loss 37.2942 with MSE metric 27190.7142\n",
      "Epoch 125 batch 10 train Loss 85.1855 test Loss 37.2840 with MSE metric 27184.4516\n",
      "Epoch 125 batch 20 train Loss 85.1599 test Loss 37.2738 with MSE metric 27178.2740\n",
      "Epoch 125 batch 30 train Loss 85.1344 test Loss 37.2636 with MSE metric 27172.0833\n",
      "Epoch 125 batch 40 train Loss 85.1089 test Loss 37.2534 with MSE metric 27165.9299\n",
      "Epoch 125 batch 50 train Loss 85.0834 test Loss 37.2432 with MSE metric 27159.6745\n",
      "Epoch 125 batch 60 train Loss 85.0579 test Loss 37.2331 with MSE metric 27153.6605\n",
      "Epoch 125 batch 70 train Loss 85.0325 test Loss 37.2229 with MSE metric 27147.4313\n",
      "Epoch 125 batch 80 train Loss 85.0070 test Loss 37.2128 with MSE metric 27141.3107\n",
      "Epoch 125 batch 90 train Loss 84.9816 test Loss 37.2026 with MSE metric 27135.0900\n",
      "Epoch 125 batch 100 train Loss 84.9562 test Loss 37.1924 with MSE metric 27128.9566\n",
      "Epoch 125 batch 110 train Loss 84.9308 test Loss 37.1822 with MSE metric 27122.7185\n",
      "Epoch 125 batch 120 train Loss 84.9055 test Loss 37.1722 with MSE metric 27116.7605\n",
      "Epoch 125 batch 130 train Loss 84.8801 test Loss 37.1620 with MSE metric 27110.6002\n",
      "Epoch 125 batch 140 train Loss 84.8548 test Loss 37.1519 with MSE metric 27104.3782\n",
      "Epoch 125 batch 150 train Loss 84.8294 test Loss 37.1418 with MSE metric 27098.2119\n",
      "Epoch 125 batch 160 train Loss 84.8041 test Loss 37.1317 with MSE metric 27092.1064\n",
      "Epoch 125 batch 170 train Loss 84.7789 test Loss 37.1216 with MSE metric 27085.9850\n",
      "Epoch 125 batch 180 train Loss 84.7536 test Loss 37.1115 with MSE metric 27079.7897\n",
      "Epoch 125 batch 190 train Loss 84.7283 test Loss 37.1014 with MSE metric 27073.6783\n",
      "Epoch 125 batch 200 train Loss 84.7030 test Loss 37.0913 with MSE metric 27067.5753\n",
      "Epoch 125 batch 210 train Loss 84.6778 test Loss 37.0812 with MSE metric 27061.6004\n",
      "Epoch 125 batch 220 train Loss 84.6526 test Loss 37.0712 with MSE metric 27055.5197\n",
      "Epoch 125 batch 230 train Loss 84.6274 test Loss 37.0611 with MSE metric 27049.3387\n",
      "Epoch 125 batch 240 train Loss 84.6022 test Loss 37.0511 with MSE metric 27043.1734\n",
      "Time taken for 1 epoch: 28.12926197052002 secs\n",
      "\n",
      "Epoch 126 batch 0 train Loss 84.5771 test Loss 37.0410 with MSE metric 27037.1751\n",
      "Epoch 126 batch 10 train Loss 84.5519 test Loss 37.0310 with MSE metric 27031.1160\n",
      "Epoch 126 batch 20 train Loss 84.5268 test Loss 37.0210 with MSE metric 27024.8790\n",
      "Epoch 126 batch 30 train Loss 84.5017 test Loss 37.0109 with MSE metric 27018.6619\n",
      "Epoch 126 batch 40 train Loss 84.4766 test Loss 37.0009 with MSE metric 27012.6778\n",
      "Epoch 126 batch 50 train Loss 84.4515 test Loss 36.9908 with MSE metric 27006.6533\n",
      "Epoch 126 batch 60 train Loss 84.4264 test Loss 36.9809 with MSE metric 27000.5615\n",
      "Epoch 126 batch 70 train Loss 84.4014 test Loss 36.9709 with MSE metric 26994.5509\n",
      "Epoch 126 batch 80 train Loss 84.3763 test Loss 36.9609 with MSE metric 26988.4827\n",
      "Epoch 126 batch 90 train Loss 84.3513 test Loss 36.9509 with MSE metric 26982.4069\n",
      "Epoch 126 batch 100 train Loss 84.3263 test Loss 36.9409 with MSE metric 26976.2711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126 batch 110 train Loss 84.3013 test Loss 36.9309 with MSE metric 26970.1358\n",
      "Epoch 126 batch 120 train Loss 84.2763 test Loss 36.9210 with MSE metric 26964.0724\n",
      "Epoch 126 batch 130 train Loss 84.2514 test Loss 36.9110 with MSE metric 26958.0586\n",
      "Epoch 126 batch 140 train Loss 84.2264 test Loss 36.9010 with MSE metric 26951.8988\n",
      "Epoch 126 batch 150 train Loss 84.2015 test Loss 36.8911 with MSE metric 26945.8564\n",
      "Epoch 126 batch 160 train Loss 84.1766 test Loss 36.8811 with MSE metric 26939.7884\n",
      "Epoch 126 batch 170 train Loss 84.1517 test Loss 36.8712 with MSE metric 26933.7258\n",
      "Epoch 126 batch 180 train Loss 84.1268 test Loss 36.8612 with MSE metric 26927.7728\n",
      "Epoch 126 batch 190 train Loss 84.1020 test Loss 36.8513 with MSE metric 26921.9377\n",
      "Epoch 126 batch 200 train Loss 84.0771 test Loss 36.8414 with MSE metric 26915.8886\n",
      "Epoch 126 batch 210 train Loss 84.0523 test Loss 36.8315 with MSE metric 26909.7919\n",
      "Epoch 126 batch 220 train Loss 84.0275 test Loss 36.8215 with MSE metric 26903.7120\n",
      "Epoch 126 batch 230 train Loss 84.0026 test Loss 36.8116 with MSE metric 26897.6983\n",
      "Epoch 126 batch 240 train Loss 83.9779 test Loss 36.8017 with MSE metric 26891.7886\n",
      "Time taken for 1 epoch: 29.55920100212097 secs\n",
      "\n",
      "Epoch 127 batch 0 train Loss 83.9531 test Loss 36.7919 with MSE metric 26885.6613\n",
      "Epoch 127 batch 10 train Loss 83.9283 test Loss 36.7820 with MSE metric 26879.6801\n",
      "Epoch 127 batch 20 train Loss 83.9036 test Loss 36.7721 with MSE metric 26873.6210\n",
      "Epoch 127 batch 30 train Loss 83.8789 test Loss 36.7622 with MSE metric 26867.6975\n",
      "Epoch 127 batch 40 train Loss 83.8541 test Loss 36.7524 with MSE metric 26861.7520\n",
      "Epoch 127 batch 50 train Loss 83.8294 test Loss 36.7425 with MSE metric 26855.7261\n",
      "Epoch 127 batch 60 train Loss 83.8048 test Loss 36.7327 with MSE metric 26849.8082\n",
      "Epoch 127 batch 70 train Loss 83.7801 test Loss 36.7229 with MSE metric 26843.8673\n",
      "Epoch 127 batch 80 train Loss 83.7554 test Loss 36.7130 with MSE metric 26837.9205\n",
      "Epoch 127 batch 90 train Loss 83.7308 test Loss 36.7032 with MSE metric 26831.9925\n",
      "Epoch 127 batch 100 train Loss 83.7062 test Loss 36.6933 with MSE metric 26825.9943\n",
      "Epoch 127 batch 110 train Loss 83.6816 test Loss 36.6835 with MSE metric 26820.1429\n",
      "Epoch 127 batch 120 train Loss 83.6570 test Loss 36.6737 with MSE metric 26814.2273\n",
      "Epoch 127 batch 130 train Loss 83.6324 test Loss 36.6639 with MSE metric 26808.1279\n",
      "Epoch 127 batch 140 train Loss 83.6079 test Loss 36.6541 with MSE metric 26802.1095\n",
      "Epoch 127 batch 150 train Loss 83.5834 test Loss 36.6443 with MSE metric 26796.2188\n",
      "Epoch 127 batch 160 train Loss 83.5588 test Loss 36.6345 with MSE metric 26790.1812\n",
      "Epoch 127 batch 170 train Loss 83.5343 test Loss 36.6247 with MSE metric 26784.3419\n",
      "Epoch 127 batch 180 train Loss 83.5098 test Loss 36.6149 with MSE metric 26778.3465\n",
      "Epoch 127 batch 190 train Loss 83.4854 test Loss 36.6052 with MSE metric 26772.4959\n",
      "Epoch 127 batch 200 train Loss 83.4609 test Loss 36.5954 with MSE metric 26766.5360\n",
      "Epoch 127 batch 210 train Loss 83.4364 test Loss 36.5857 with MSE metric 26760.6225\n",
      "Epoch 127 batch 220 train Loss 83.4120 test Loss 36.5760 with MSE metric 26754.6448\n",
      "Epoch 127 batch 230 train Loss 83.3876 test Loss 36.5662 with MSE metric 26748.7613\n",
      "Epoch 127 batch 240 train Loss 83.3632 test Loss 36.5565 with MSE metric 26742.7108\n",
      "Time taken for 1 epoch: 29.727536916732788 secs\n",
      "\n",
      "Epoch 128 batch 0 train Loss 83.3388 test Loss 36.5468 with MSE metric 26736.7194\n",
      "Epoch 128 batch 10 train Loss 83.3144 test Loss 36.5370 with MSE metric 26730.8581\n",
      "Epoch 128 batch 20 train Loss 83.2901 test Loss 36.5273 with MSE metric 26724.8585\n",
      "Epoch 128 batch 30 train Loss 83.2657 test Loss 36.5176 with MSE metric 26718.9024\n",
      "Epoch 128 batch 40 train Loss 83.2414 test Loss 36.5079 with MSE metric 26712.9938\n",
      "Epoch 128 batch 50 train Loss 83.2171 test Loss 36.4982 with MSE metric 26707.1607\n",
      "Epoch 128 batch 60 train Loss 83.1928 test Loss 36.4885 with MSE metric 26701.2023\n",
      "Epoch 128 batch 70 train Loss 83.1685 test Loss 36.4788 with MSE metric 26695.3076\n",
      "Epoch 128 batch 80 train Loss 83.1443 test Loss 36.4691 with MSE metric 26689.4015\n",
      "Epoch 128 batch 90 train Loss 83.1200 test Loss 36.4594 with MSE metric 26683.6319\n",
      "Epoch 128 batch 100 train Loss 83.0958 test Loss 36.4498 with MSE metric 26677.8129\n",
      "Epoch 128 batch 110 train Loss 83.0716 test Loss 36.4402 with MSE metric 26671.8754\n",
      "Epoch 128 batch 120 train Loss 83.0474 test Loss 36.4305 with MSE metric 26666.0435\n",
      "Epoch 128 batch 130 train Loss 83.0232 test Loss 36.4208 with MSE metric 26660.2089\n",
      "Epoch 128 batch 140 train Loss 82.9990 test Loss 36.4112 with MSE metric 26654.2568\n",
      "Epoch 128 batch 150 train Loss 82.9749 test Loss 36.4015 with MSE metric 26648.3747\n",
      "Epoch 128 batch 160 train Loss 82.9507 test Loss 36.3919 with MSE metric 26642.4568\n",
      "Epoch 128 batch 170 train Loss 82.9266 test Loss 36.3823 with MSE metric 26636.5043\n",
      "Epoch 128 batch 180 train Loss 82.9025 test Loss 36.3727 with MSE metric 26630.6097\n",
      "Epoch 128 batch 190 train Loss 82.8784 test Loss 36.3631 with MSE metric 26624.6757\n",
      "Epoch 128 batch 200 train Loss 82.8543 test Loss 36.3535 with MSE metric 26618.9483\n",
      "Epoch 128 batch 210 train Loss 82.8302 test Loss 36.3439 with MSE metric 26613.1944\n",
      "Epoch 128 batch 220 train Loss 82.8062 test Loss 36.3343 with MSE metric 26607.4919\n",
      "Epoch 128 batch 230 train Loss 82.7821 test Loss 36.3247 with MSE metric 26601.7500\n",
      "Epoch 128 batch 240 train Loss 82.7581 test Loss 36.3150 with MSE metric 26595.8950\n",
      "Time taken for 1 epoch: 27.416393756866455 secs\n",
      "\n",
      "Epoch 129 batch 0 train Loss 82.7341 test Loss 36.3055 with MSE metric 26590.1809\n",
      "Epoch 129 batch 10 train Loss 82.7101 test Loss 36.2959 with MSE metric 26584.3414\n",
      "Epoch 129 batch 20 train Loss 82.6861 test Loss 36.2863 with MSE metric 26578.4841\n",
      "Epoch 129 batch 30 train Loss 82.6622 test Loss 36.2768 with MSE metric 26572.7379\n",
      "Epoch 129 batch 40 train Loss 82.6382 test Loss 36.2672 with MSE metric 26566.9669\n",
      "Epoch 129 batch 50 train Loss 82.6143 test Loss 36.2577 with MSE metric 26561.2338\n",
      "Epoch 129 batch 60 train Loss 82.5904 test Loss 36.2482 with MSE metric 26555.4710\n",
      "Epoch 129 batch 70 train Loss 82.5665 test Loss 36.2386 with MSE metric 26549.7215\n",
      "Epoch 129 batch 80 train Loss 82.5426 test Loss 36.2291 with MSE metric 26543.9373\n",
      "Epoch 129 batch 90 train Loss 82.5187 test Loss 36.2196 with MSE metric 26538.2157\n",
      "Epoch 129 batch 100 train Loss 82.4948 test Loss 36.2101 with MSE metric 26532.4013\n",
      "Epoch 129 batch 110 train Loss 82.4710 test Loss 36.2006 with MSE metric 26526.5278\n",
      "Epoch 129 batch 120 train Loss 82.4471 test Loss 36.1911 with MSE metric 26520.7117\n",
      "Epoch 129 batch 130 train Loss 82.4233 test Loss 36.1816 with MSE metric 26514.9449\n",
      "Epoch 129 batch 140 train Loss 82.3995 test Loss 36.1720 with MSE metric 26509.2267\n",
      "Epoch 129 batch 150 train Loss 82.3757 test Loss 36.1626 with MSE metric 26503.3724\n",
      "Epoch 129 batch 160 train Loss 82.3520 test Loss 36.1531 with MSE metric 26497.5164\n",
      "Epoch 129 batch 170 train Loss 82.3282 test Loss 36.1436 with MSE metric 26491.8391\n",
      "Epoch 129 batch 180 train Loss 82.3045 test Loss 36.1342 with MSE metric 26486.2294\n",
      "Epoch 129 batch 190 train Loss 82.2807 test Loss 36.1247 with MSE metric 26480.5619\n",
      "Epoch 129 batch 200 train Loss 82.2570 test Loss 36.1152 with MSE metric 26474.7351\n",
      "Epoch 129 batch 210 train Loss 82.2333 test Loss 36.1058 with MSE metric 26469.0635\n",
      "Epoch 129 batch 220 train Loss 82.2097 test Loss 36.0963 with MSE metric 26463.4009\n",
      "Epoch 129 batch 230 train Loss 82.1860 test Loss 36.0869 with MSE metric 26457.6274\n",
      "Epoch 129 batch 240 train Loss 82.1623 test Loss 36.0774 with MSE metric 26451.8163\n",
      "Time taken for 1 epoch: 29.210986137390137 secs\n",
      "\n",
      "Epoch 130 batch 0 train Loss 82.1387 test Loss 36.0680 with MSE metric 26445.9242\n",
      "Epoch 130 batch 10 train Loss 82.1150 test Loss 36.0586 with MSE metric 26440.1916\n",
      "Epoch 130 batch 20 train Loss 82.0914 test Loss 36.0491 with MSE metric 26434.4334\n",
      "Epoch 130 batch 30 train Loss 82.0678 test Loss 36.0397 with MSE metric 26428.6613\n",
      "Epoch 130 batch 40 train Loss 82.0442 test Loss 36.0303 with MSE metric 26422.9740\n",
      "Epoch 130 batch 50 train Loss 82.0207 test Loss 36.0209 with MSE metric 26417.3172\n",
      "Epoch 130 batch 60 train Loss 81.9971 test Loss 36.0115 with MSE metric 26411.5834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130 batch 70 train Loss 81.9736 test Loss 36.0021 with MSE metric 26405.7328\n",
      "Epoch 130 batch 80 train Loss 81.9501 test Loss 35.9927 with MSE metric 26400.0933\n",
      "Epoch 130 batch 90 train Loss 81.9265 test Loss 35.9833 with MSE metric 26394.4222\n",
      "Epoch 130 batch 100 train Loss 81.9031 test Loss 35.9740 with MSE metric 26388.7200\n",
      "Epoch 130 batch 110 train Loss 81.8796 test Loss 35.9646 with MSE metric 26382.9804\n",
      "Epoch 130 batch 120 train Loss 81.8561 test Loss 35.9552 with MSE metric 26377.1933\n",
      "Epoch 130 batch 130 train Loss 81.8327 test Loss 35.9459 with MSE metric 26371.5630\n",
      "Epoch 130 batch 140 train Loss 81.8092 test Loss 35.9365 with MSE metric 26365.9300\n",
      "Epoch 130 batch 150 train Loss 81.7858 test Loss 35.9272 with MSE metric 26360.2619\n",
      "Epoch 130 batch 160 train Loss 81.7624 test Loss 35.9178 with MSE metric 26354.5930\n",
      "Epoch 130 batch 170 train Loss 81.7390 test Loss 35.9085 with MSE metric 26348.9874\n",
      "Epoch 130 batch 180 train Loss 81.7156 test Loss 35.8991 with MSE metric 26343.3804\n",
      "Epoch 130 batch 190 train Loss 81.6923 test Loss 35.8898 with MSE metric 26337.7696\n",
      "Epoch 130 batch 200 train Loss 81.6689 test Loss 35.8805 with MSE metric 26332.2108\n",
      "Epoch 130 batch 210 train Loss 81.6456 test Loss 35.8712 with MSE metric 26326.4695\n",
      "Epoch 130 batch 220 train Loss 81.6222 test Loss 35.8619 with MSE metric 26320.8374\n",
      "Epoch 130 batch 230 train Loss 81.5989 test Loss 35.8525 with MSE metric 26315.1168\n",
      "Epoch 130 batch 240 train Loss 81.5756 test Loss 35.8432 with MSE metric 26309.4191\n",
      "Time taken for 1 epoch: 28.365396976470947 secs\n",
      "\n",
      "Epoch 131 batch 0 train Loss 81.5524 test Loss 35.8339 with MSE metric 26303.7383\n",
      "Epoch 131 batch 10 train Loss 81.5291 test Loss 35.8247 with MSE metric 26297.9286\n",
      "Epoch 131 batch 20 train Loss 81.5058 test Loss 35.8154 with MSE metric 26292.2433\n",
      "Epoch 131 batch 30 train Loss 81.4826 test Loss 35.8061 with MSE metric 26286.6154\n",
      "Epoch 131 batch 40 train Loss 81.4594 test Loss 35.7968 with MSE metric 26280.9925\n",
      "Epoch 131 batch 50 train Loss 81.4362 test Loss 35.7876 with MSE metric 26275.4610\n",
      "Epoch 131 batch 60 train Loss 81.4130 test Loss 35.7783 with MSE metric 26269.8449\n",
      "Epoch 131 batch 70 train Loss 81.3898 test Loss 35.7691 with MSE metric 26264.1250\n",
      "Epoch 131 batch 80 train Loss 81.3666 test Loss 35.7599 with MSE metric 26258.4683\n",
      "Epoch 131 batch 90 train Loss 81.3434 test Loss 35.7507 with MSE metric 26252.8406\n",
      "Epoch 131 batch 100 train Loss 81.3203 test Loss 35.7414 with MSE metric 26247.4197\n",
      "Epoch 131 batch 110 train Loss 81.2972 test Loss 35.7322 with MSE metric 26241.9298\n",
      "Epoch 131 batch 120 train Loss 81.2741 test Loss 35.7231 with MSE metric 26236.2002\n",
      "Epoch 131 batch 130 train Loss 81.2510 test Loss 35.7138 with MSE metric 26230.7969\n",
      "Epoch 131 batch 140 train Loss 81.2279 test Loss 35.7046 with MSE metric 26225.2591\n",
      "Epoch 131 batch 150 train Loss 81.2049 test Loss 35.6954 with MSE metric 26219.7055\n",
      "Epoch 131 batch 160 train Loss 81.1818 test Loss 35.6862 with MSE metric 26214.1165\n",
      "Epoch 131 batch 170 train Loss 81.1588 test Loss 35.6770 with MSE metric 26208.6038\n",
      "Epoch 131 batch 180 train Loss 81.1357 test Loss 35.6679 with MSE metric 26202.9957\n",
      "Epoch 131 batch 190 train Loss 81.1127 test Loss 35.6587 with MSE metric 26197.3215\n",
      "Epoch 131 batch 200 train Loss 81.0897 test Loss 35.6495 with MSE metric 26191.7867\n",
      "Epoch 131 batch 210 train Loss 81.0667 test Loss 35.6403 with MSE metric 26186.3160\n",
      "Epoch 131 batch 220 train Loss 81.0437 test Loss 35.6312 with MSE metric 26180.6810\n",
      "Epoch 131 batch 230 train Loss 81.0208 test Loss 35.6220 with MSE metric 26175.1084\n",
      "Epoch 131 batch 240 train Loss 80.9978 test Loss 35.6129 with MSE metric 26169.6626\n",
      "Time taken for 1 epoch: 27.152642250061035 secs\n",
      "\n",
      "Epoch 132 batch 0 train Loss 80.9749 test Loss 35.6037 with MSE metric 26164.1478\n",
      "Epoch 132 batch 10 train Loss 80.9520 test Loss 35.5946 with MSE metric 26158.6005\n",
      "Epoch 132 batch 20 train Loss 80.9291 test Loss 35.5854 with MSE metric 26152.9396\n",
      "Epoch 132 batch 30 train Loss 80.9062 test Loss 35.5763 with MSE metric 26147.4439\n",
      "Epoch 132 batch 40 train Loss 80.8833 test Loss 35.5672 with MSE metric 26141.8979\n",
      "Epoch 132 batch 50 train Loss 80.8604 test Loss 35.5580 with MSE metric 26136.3325\n",
      "Epoch 132 batch 60 train Loss 80.8376 test Loss 35.5489 with MSE metric 26130.9449\n",
      "Epoch 132 batch 70 train Loss 80.8148 test Loss 35.5398 with MSE metric 26125.4929\n",
      "Epoch 132 batch 80 train Loss 80.7920 test Loss 35.5307 with MSE metric 26119.9682\n",
      "Epoch 132 batch 90 train Loss 80.7692 test Loss 35.5216 with MSE metric 26114.4288\n",
      "Epoch 132 batch 100 train Loss 80.7464 test Loss 35.5125 with MSE metric 26108.9423\n",
      "Epoch 132 batch 110 train Loss 80.7236 test Loss 35.5034 with MSE metric 26103.4995\n",
      "Epoch 132 batch 120 train Loss 80.7008 test Loss 35.4943 with MSE metric 26098.1114\n",
      "Epoch 132 batch 130 train Loss 80.6781 test Loss 35.4852 with MSE metric 26092.5760\n",
      "Epoch 132 batch 140 train Loss 80.6553 test Loss 35.4762 with MSE metric 26087.0949\n",
      "Epoch 132 batch 150 train Loss 80.6326 test Loss 35.4671 with MSE metric 26081.6027\n",
      "Epoch 132 batch 160 train Loss 80.6099 test Loss 35.4581 with MSE metric 26076.1427\n",
      "Epoch 132 batch 170 train Loss 80.5872 test Loss 35.4490 with MSE metric 26070.6614\n",
      "Epoch 132 batch 180 train Loss 80.5645 test Loss 35.4400 with MSE metric 26065.1979\n",
      "Epoch 132 batch 190 train Loss 80.5419 test Loss 35.4309 with MSE metric 26059.6720\n",
      "Epoch 132 batch 200 train Loss 80.5192 test Loss 35.4219 with MSE metric 26054.2368\n",
      "Epoch 132 batch 210 train Loss 80.4966 test Loss 35.4128 with MSE metric 26048.7588\n",
      "Epoch 132 batch 220 train Loss 80.4739 test Loss 35.4038 with MSE metric 26043.2798\n",
      "Epoch 132 batch 230 train Loss 80.4513 test Loss 35.3947 with MSE metric 26037.7704\n",
      "Epoch 132 batch 240 train Loss 80.4287 test Loss 35.3857 with MSE metric 26032.3767\n",
      "Time taken for 1 epoch: 27.889068126678467 secs\n",
      "\n",
      "Epoch 133 batch 0 train Loss 80.4061 test Loss 35.3766 with MSE metric 26026.8404\n",
      "Epoch 133 batch 10 train Loss 80.3836 test Loss 35.3676 with MSE metric 26021.3534\n",
      "Epoch 133 batch 20 train Loss 80.3610 test Loss 35.3586 with MSE metric 26015.9103\n",
      "Epoch 133 batch 30 train Loss 80.3385 test Loss 35.3496 with MSE metric 26010.3642\n",
      "Epoch 133 batch 40 train Loss 80.3160 test Loss 35.3406 with MSE metric 26004.8527\n",
      "Epoch 133 batch 50 train Loss 80.2934 test Loss 35.3316 with MSE metric 25999.4118\n",
      "Epoch 133 batch 60 train Loss 80.2709 test Loss 35.3226 with MSE metric 25993.8886\n",
      "Epoch 133 batch 70 train Loss 80.2484 test Loss 35.3137 with MSE metric 25988.4431\n",
      "Epoch 133 batch 80 train Loss 80.2260 test Loss 35.3047 with MSE metric 25982.9222\n",
      "Epoch 133 batch 90 train Loss 80.2035 test Loss 35.2957 with MSE metric 25977.4931\n",
      "Epoch 133 batch 100 train Loss 80.1810 test Loss 35.2868 with MSE metric 25972.0440\n",
      "Epoch 133 batch 110 train Loss 80.1586 test Loss 35.2778 with MSE metric 25966.5614\n",
      "Epoch 133 batch 120 train Loss 80.1362 test Loss 35.2689 with MSE metric 25961.0795\n",
      "Epoch 133 batch 130 train Loss 80.1138 test Loss 35.2600 with MSE metric 25955.6418\n",
      "Epoch 133 batch 140 train Loss 80.0914 test Loss 35.2510 with MSE metric 25950.1630\n",
      "Epoch 133 batch 150 train Loss 80.0690 test Loss 35.2421 with MSE metric 25944.6273\n",
      "Epoch 133 batch 160 train Loss 80.0466 test Loss 35.2332 with MSE metric 25939.1263\n",
      "Epoch 133 batch 170 train Loss 80.0242 test Loss 35.2243 with MSE metric 25933.6685\n",
      "Epoch 133 batch 180 train Loss 80.0019 test Loss 35.2154 with MSE metric 25928.3219\n",
      "Epoch 133 batch 190 train Loss 79.9796 test Loss 35.2064 with MSE metric 25922.8807\n",
      "Epoch 133 batch 200 train Loss 79.9573 test Loss 35.1975 with MSE metric 25917.3624\n",
      "Epoch 133 batch 210 train Loss 79.9350 test Loss 35.1886 with MSE metric 25911.9651\n",
      "Epoch 133 batch 220 train Loss 79.9127 test Loss 35.1798 with MSE metric 25906.5078\n",
      "Epoch 133 batch 230 train Loss 79.8904 test Loss 35.1709 with MSE metric 25901.1469\n",
      "Epoch 133 batch 240 train Loss 79.8681 test Loss 35.1620 with MSE metric 25895.7015\n",
      "Time taken for 1 epoch: 28.49120306968689 secs\n",
      "\n",
      "Epoch 134 batch 0 train Loss 79.8459 test Loss 35.1531 with MSE metric 25890.3742\n",
      "Epoch 134 batch 10 train Loss 79.8236 test Loss 35.1443 with MSE metric 25884.9459\n",
      "Epoch 134 batch 20 train Loss 79.8014 test Loss 35.1354 with MSE metric 25879.4756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134 batch 30 train Loss 79.7792 test Loss 35.1266 with MSE metric 25874.0669\n",
      "Epoch 134 batch 40 train Loss 79.7570 test Loss 35.1177 with MSE metric 25868.7295\n",
      "Epoch 134 batch 50 train Loss 79.7348 test Loss 35.1089 with MSE metric 25863.4031\n",
      "Epoch 134 batch 60 train Loss 79.7127 test Loss 35.1000 with MSE metric 25858.1110\n",
      "Epoch 134 batch 70 train Loss 79.6905 test Loss 35.0911 with MSE metric 25852.8749\n",
      "Epoch 134 batch 80 train Loss 79.6684 test Loss 35.0823 with MSE metric 25847.4875\n",
      "Epoch 134 batch 90 train Loss 79.6463 test Loss 35.0735 with MSE metric 25842.0956\n",
      "Epoch 134 batch 100 train Loss 79.6241 test Loss 35.0646 with MSE metric 25836.7356\n",
      "Epoch 134 batch 110 train Loss 79.6020 test Loss 35.0558 with MSE metric 25831.4325\n",
      "Epoch 134 batch 120 train Loss 79.5799 test Loss 35.0470 with MSE metric 25825.9455\n",
      "Epoch 134 batch 130 train Loss 79.5579 test Loss 35.0381 with MSE metric 25820.6783\n",
      "Epoch 134 batch 140 train Loss 79.5358 test Loss 35.0293 with MSE metric 25815.2680\n",
      "Epoch 134 batch 150 train Loss 79.5137 test Loss 35.0205 with MSE metric 25809.9402\n",
      "Epoch 134 batch 160 train Loss 79.4917 test Loss 35.0117 with MSE metric 25804.4938\n",
      "Epoch 134 batch 170 train Loss 79.4697 test Loss 35.0029 with MSE metric 25799.0932\n",
      "Epoch 134 batch 180 train Loss 79.4476 test Loss 34.9941 with MSE metric 25793.8312\n",
      "Epoch 134 batch 190 train Loss 79.4256 test Loss 34.9853 with MSE metric 25788.5446\n",
      "Epoch 134 batch 200 train Loss 79.4036 test Loss 34.9765 with MSE metric 25783.2729\n",
      "Epoch 134 batch 210 train Loss 79.3817 test Loss 34.9678 with MSE metric 25777.9982\n",
      "Epoch 134 batch 220 train Loss 79.3597 test Loss 34.9590 with MSE metric 25772.5800\n",
      "Epoch 134 batch 230 train Loss 79.3378 test Loss 34.9503 with MSE metric 25767.2663\n",
      "Epoch 134 batch 240 train Loss 79.3158 test Loss 34.9415 with MSE metric 25761.9377\n",
      "Time taken for 1 epoch: 29.51839804649353 secs\n",
      "\n",
      "Epoch 135 batch 0 train Loss 79.2939 test Loss 34.9327 with MSE metric 25756.5683\n",
      "Epoch 135 batch 10 train Loss 79.2720 test Loss 34.9240 with MSE metric 25751.1836\n",
      "Epoch 135 batch 20 train Loss 79.2501 test Loss 34.9152 with MSE metric 25745.9078\n",
      "Epoch 135 batch 30 train Loss 79.2282 test Loss 34.9065 with MSE metric 25740.5466\n",
      "Epoch 135 batch 40 train Loss 79.2063 test Loss 34.8978 with MSE metric 25735.2527\n",
      "Epoch 135 batch 50 train Loss 79.1845 test Loss 34.8891 with MSE metric 25729.9019\n",
      "Epoch 135 batch 60 train Loss 79.1626 test Loss 34.8803 with MSE metric 25724.4452\n",
      "Epoch 135 batch 70 train Loss 79.1408 test Loss 34.8716 with MSE metric 25719.1359\n",
      "Epoch 135 batch 80 train Loss 79.1190 test Loss 34.8629 with MSE metric 25713.7171\n",
      "Epoch 135 batch 90 train Loss 79.0972 test Loss 34.8542 with MSE metric 25708.4072\n",
      "Epoch 135 batch 100 train Loss 79.0754 test Loss 34.8455 with MSE metric 25703.0425\n",
      "Epoch 135 batch 110 train Loss 79.0536 test Loss 34.8368 with MSE metric 25697.8293\n",
      "Epoch 135 batch 120 train Loss 79.0318 test Loss 34.8281 with MSE metric 25692.4908\n",
      "Epoch 135 batch 130 train Loss 79.0101 test Loss 34.8194 with MSE metric 25687.3220\n",
      "Epoch 135 batch 140 train Loss 78.9883 test Loss 34.8108 with MSE metric 25682.0777\n",
      "Epoch 135 batch 150 train Loss 78.9666 test Loss 34.8021 with MSE metric 25676.7198\n",
      "Epoch 135 batch 160 train Loss 78.9449 test Loss 34.7934 with MSE metric 25671.5355\n",
      "Epoch 135 batch 170 train Loss 78.9232 test Loss 34.7848 with MSE metric 25666.2140\n",
      "Epoch 135 batch 180 train Loss 78.9015 test Loss 34.7762 with MSE metric 25660.8932\n",
      "Epoch 135 batch 190 train Loss 78.8798 test Loss 34.7675 with MSE metric 25655.5498\n",
      "Epoch 135 batch 200 train Loss 78.8581 test Loss 34.7589 with MSE metric 25650.3608\n",
      "Epoch 135 batch 210 train Loss 78.8365 test Loss 34.7502 with MSE metric 25645.0433\n",
      "Epoch 135 batch 220 train Loss 78.8148 test Loss 34.7416 with MSE metric 25639.8004\n",
      "Epoch 135 batch 230 train Loss 78.7932 test Loss 34.7330 with MSE metric 25634.5509\n",
      "Epoch 135 batch 240 train Loss 78.7716 test Loss 34.7244 with MSE metric 25629.3188\n",
      "Time taken for 1 epoch: 26.238627195358276 secs\n",
      "\n",
      "Epoch 136 batch 0 train Loss 78.7500 test Loss 34.7158 with MSE metric 25624.0854\n",
      "Epoch 136 batch 10 train Loss 78.7284 test Loss 34.7072 with MSE metric 25618.9734\n",
      "Epoch 136 batch 20 train Loss 78.7068 test Loss 34.6985 with MSE metric 25613.6844\n",
      "Epoch 136 batch 30 train Loss 78.6853 test Loss 34.6899 with MSE metric 25608.5133\n",
      "Epoch 136 batch 40 train Loss 78.6637 test Loss 34.6813 with MSE metric 25603.1819\n",
      "Epoch 136 batch 50 train Loss 78.6422 test Loss 34.6727 with MSE metric 25597.9809\n",
      "Epoch 136 batch 60 train Loss 78.6207 test Loss 34.6642 with MSE metric 25592.7383\n",
      "Epoch 136 batch 70 train Loss 78.5992 test Loss 34.6556 with MSE metric 25587.5366\n",
      "Epoch 136 batch 80 train Loss 78.5777 test Loss 34.6470 with MSE metric 25582.3309\n",
      "Epoch 136 batch 90 train Loss 78.5562 test Loss 34.6384 with MSE metric 25577.0367\n",
      "Epoch 136 batch 100 train Loss 78.5347 test Loss 34.6299 with MSE metric 25571.7479\n",
      "Epoch 136 batch 110 train Loss 78.5132 test Loss 34.6213 with MSE metric 25566.5123\n",
      "Epoch 136 batch 120 train Loss 78.4918 test Loss 34.6128 with MSE metric 25561.2719\n",
      "Epoch 136 batch 130 train Loss 78.4703 test Loss 34.6042 with MSE metric 25555.9186\n",
      "Epoch 136 batch 140 train Loss 78.4489 test Loss 34.5957 with MSE metric 25550.6869\n",
      "Epoch 136 batch 150 train Loss 78.4275 test Loss 34.5871 with MSE metric 25545.4888\n",
      "Epoch 136 batch 160 train Loss 78.4061 test Loss 34.5786 with MSE metric 25540.2776\n",
      "Epoch 136 batch 170 train Loss 78.3847 test Loss 34.5700 with MSE metric 25534.9778\n",
      "Epoch 136 batch 180 train Loss 78.3633 test Loss 34.5615 with MSE metric 25529.7759\n",
      "Epoch 136 batch 190 train Loss 78.3420 test Loss 34.5529 with MSE metric 25524.4785\n",
      "Epoch 136 batch 200 train Loss 78.3206 test Loss 34.5444 with MSE metric 25519.2131\n",
      "Epoch 136 batch 210 train Loss 78.2993 test Loss 34.5359 with MSE metric 25514.0537\n",
      "Epoch 136 batch 220 train Loss 78.2780 test Loss 34.5274 with MSE metric 25508.8872\n",
      "Epoch 136 batch 230 train Loss 78.2567 test Loss 34.5189 with MSE metric 25503.7922\n",
      "Epoch 136 batch 240 train Loss 78.2354 test Loss 34.5104 with MSE metric 25498.4827\n",
      "Time taken for 1 epoch: 24.546266794204712 secs\n",
      "\n",
      "Epoch 137 batch 0 train Loss 78.2141 test Loss 34.5019 with MSE metric 25493.2470\n",
      "Epoch 137 batch 10 train Loss 78.1928 test Loss 34.4934 with MSE metric 25487.9786\n",
      "Epoch 137 batch 20 train Loss 78.1715 test Loss 34.4849 with MSE metric 25482.8685\n",
      "Epoch 137 batch 30 train Loss 78.1503 test Loss 34.4764 with MSE metric 25477.9236\n",
      "Epoch 137 batch 40 train Loss 78.1291 test Loss 34.4679 with MSE metric 25472.7291\n",
      "Epoch 137 batch 50 train Loss 78.1078 test Loss 34.4595 with MSE metric 25467.6436\n",
      "Epoch 137 batch 60 train Loss 78.0866 test Loss 34.4510 with MSE metric 25462.5255\n",
      "Epoch 137 batch 70 train Loss 78.0654 test Loss 34.4426 with MSE metric 25457.4902\n",
      "Epoch 137 batch 80 train Loss 78.0442 test Loss 34.4341 with MSE metric 25452.4199\n",
      "Epoch 137 batch 90 train Loss 78.0231 test Loss 34.4256 with MSE metric 25447.1934\n",
      "Epoch 137 batch 100 train Loss 78.0019 test Loss 34.4172 with MSE metric 25442.0606\n",
      "Epoch 137 batch 110 train Loss 77.9808 test Loss 34.4087 with MSE metric 25436.8540\n",
      "Epoch 137 batch 120 train Loss 77.9596 test Loss 34.4003 with MSE metric 25431.7993\n",
      "Epoch 137 batch 130 train Loss 77.9385 test Loss 34.3918 with MSE metric 25426.7237\n",
      "Epoch 137 batch 140 train Loss 77.9174 test Loss 34.3834 with MSE metric 25421.6228\n",
      "Epoch 137 batch 150 train Loss 77.8963 test Loss 34.3750 with MSE metric 25416.4535\n",
      "Epoch 137 batch 160 train Loss 77.8752 test Loss 34.3666 with MSE metric 25411.4338\n",
      "Epoch 137 batch 170 train Loss 77.8541 test Loss 34.3582 with MSE metric 25406.4515\n",
      "Epoch 137 batch 180 train Loss 77.8331 test Loss 34.3498 with MSE metric 25401.3126\n",
      "Epoch 137 batch 190 train Loss 77.8120 test Loss 34.3413 with MSE metric 25396.2245\n",
      "Epoch 137 batch 200 train Loss 77.7910 test Loss 34.3329 with MSE metric 25391.0720\n",
      "Epoch 137 batch 210 train Loss 77.7699 test Loss 34.3245 with MSE metric 25386.0710\n",
      "Epoch 137 batch 220 train Loss 77.7489 test Loss 34.3162 with MSE metric 25381.0012\n",
      "Epoch 137 batch 230 train Loss 77.7279 test Loss 34.3078 with MSE metric 25375.9243\n",
      "Epoch 137 batch 240 train Loss 77.7069 test Loss 34.2994 with MSE metric 25370.7721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 24.199883699417114 secs\n",
      "\n",
      "Epoch 138 batch 0 train Loss 77.6859 test Loss 34.2910 with MSE metric 25365.6653\n",
      "Epoch 138 batch 10 train Loss 77.6650 test Loss 34.2826 with MSE metric 25360.6146\n",
      "Epoch 138 batch 20 train Loss 77.6440 test Loss 34.2743 with MSE metric 25355.5820\n",
      "Epoch 138 batch 30 train Loss 77.6231 test Loss 34.2659 with MSE metric 25350.5373\n",
      "Epoch 138 batch 40 train Loss 77.6021 test Loss 34.2576 with MSE metric 25345.5554\n",
      "Epoch 138 batch 50 train Loss 77.5812 test Loss 34.2492 with MSE metric 25340.4687\n",
      "Epoch 138 batch 60 train Loss 77.5603 test Loss 34.2409 with MSE metric 25335.3297\n",
      "Epoch 138 batch 70 train Loss 77.5394 test Loss 34.2325 with MSE metric 25330.2843\n",
      "Epoch 138 batch 80 train Loss 77.5185 test Loss 34.2242 with MSE metric 25325.0618\n",
      "Epoch 138 batch 90 train Loss 77.4977 test Loss 34.2159 with MSE metric 25319.9729\n",
      "Epoch 138 batch 100 train Loss 77.4768 test Loss 34.2075 with MSE metric 25315.0424\n",
      "Epoch 138 batch 110 train Loss 77.4560 test Loss 34.1992 with MSE metric 25309.8970\n",
      "Epoch 138 batch 120 train Loss 77.4351 test Loss 34.1909 with MSE metric 25304.7812\n",
      "Epoch 138 batch 130 train Loss 77.4143 test Loss 34.1826 with MSE metric 25299.7961\n",
      "Epoch 138 batch 140 train Loss 77.3935 test Loss 34.1743 with MSE metric 25294.7609\n",
      "Epoch 138 batch 150 train Loss 77.3727 test Loss 34.1660 with MSE metric 25289.6968\n",
      "Epoch 138 batch 160 train Loss 77.3519 test Loss 34.1577 with MSE metric 25284.6369\n",
      "Epoch 138 batch 170 train Loss 77.3311 test Loss 34.1494 with MSE metric 25279.4685\n",
      "Epoch 138 batch 180 train Loss 77.3104 test Loss 34.1411 with MSE metric 25274.4225\n",
      "Epoch 138 batch 190 train Loss 77.2896 test Loss 34.1329 with MSE metric 25269.3138\n",
      "Epoch 138 batch 200 train Loss 77.2689 test Loss 34.1246 with MSE metric 25264.2625\n",
      "Epoch 138 batch 210 train Loss 77.2481 test Loss 34.1163 with MSE metric 25259.1399\n",
      "Epoch 138 batch 220 train Loss 77.2274 test Loss 34.1081 with MSE metric 25254.1421\n",
      "Epoch 138 batch 230 train Loss 77.2067 test Loss 34.0998 with MSE metric 25249.0201\n",
      "Epoch 138 batch 240 train Loss 77.1860 test Loss 34.0915 with MSE metric 25244.0583\n",
      "Time taken for 1 epoch: 24.382557153701782 secs\n",
      "\n",
      "Epoch 139 batch 0 train Loss 77.1654 test Loss 34.0833 with MSE metric 25239.1073\n",
      "Epoch 139 batch 10 train Loss 77.1447 test Loss 34.0750 with MSE metric 25234.0348\n",
      "Epoch 139 batch 20 train Loss 77.1240 test Loss 34.0668 with MSE metric 25228.9755\n",
      "Epoch 139 batch 30 train Loss 77.1034 test Loss 34.0585 with MSE metric 25223.9187\n",
      "Epoch 139 batch 40 train Loss 77.0828 test Loss 34.0503 with MSE metric 25218.9499\n",
      "Epoch 139 batch 50 train Loss 77.0621 test Loss 34.0420 with MSE metric 25213.9705\n",
      "Epoch 139 batch 60 train Loss 77.0415 test Loss 34.0338 with MSE metric 25208.9418\n",
      "Epoch 139 batch 70 train Loss 77.0209 test Loss 34.0256 with MSE metric 25203.8476\n",
      "Epoch 139 batch 80 train Loss 77.0003 test Loss 34.0174 with MSE metric 25198.9197\n",
      "Epoch 139 batch 90 train Loss 76.9798 test Loss 34.0092 with MSE metric 25193.8136\n",
      "Epoch 139 batch 100 train Loss 76.9592 test Loss 34.0009 with MSE metric 25188.9640\n",
      "Epoch 139 batch 110 train Loss 76.9387 test Loss 33.9927 with MSE metric 25183.9933\n",
      "Epoch 139 batch 120 train Loss 76.9181 test Loss 33.9846 with MSE metric 25179.0312\n",
      "Epoch 139 batch 130 train Loss 76.8976 test Loss 33.9764 with MSE metric 25174.1490\n",
      "Epoch 139 batch 140 train Loss 76.8771 test Loss 33.9682 with MSE metric 25169.2013\n",
      "Epoch 139 batch 150 train Loss 76.8566 test Loss 33.9600 with MSE metric 25164.2892\n",
      "Epoch 139 batch 160 train Loss 76.8361 test Loss 33.9518 with MSE metric 25159.2533\n",
      "Epoch 139 batch 170 train Loss 76.8156 test Loss 33.9437 with MSE metric 25154.3264\n",
      "Epoch 139 batch 180 train Loss 76.7952 test Loss 33.9355 with MSE metric 25149.3283\n",
      "Epoch 139 batch 190 train Loss 76.7747 test Loss 33.9274 with MSE metric 25144.4057\n",
      "Epoch 139 batch 200 train Loss 76.7543 test Loss 33.9192 with MSE metric 25139.4995\n",
      "Epoch 139 batch 210 train Loss 76.7338 test Loss 33.9110 with MSE metric 25134.6504\n",
      "Epoch 139 batch 220 train Loss 76.7134 test Loss 33.9029 with MSE metric 25129.7450\n",
      "Epoch 139 batch 230 train Loss 76.6930 test Loss 33.8948 with MSE metric 25124.7819\n",
      "Epoch 139 batch 240 train Loss 76.6726 test Loss 33.8866 with MSE metric 25119.8320\n",
      "Time taken for 1 epoch: 24.744321584701538 secs\n",
      "\n",
      "Epoch 140 batch 0 train Loss 76.6523 test Loss 33.8785 with MSE metric 25115.0006\n",
      "Epoch 140 batch 10 train Loss 76.6319 test Loss 33.8703 with MSE metric 25110.0148\n",
      "Epoch 140 batch 20 train Loss 76.6115 test Loss 33.8622 with MSE metric 25105.0572\n",
      "Epoch 140 batch 30 train Loss 76.5912 test Loss 33.8541 with MSE metric 25100.1742\n",
      "Epoch 140 batch 40 train Loss 76.5708 test Loss 33.8459 with MSE metric 25095.3593\n",
      "Epoch 140 batch 50 train Loss 76.5505 test Loss 33.8379 with MSE metric 25090.5025\n",
      "Epoch 140 batch 60 train Loss 76.5302 test Loss 33.8297 with MSE metric 25085.5205\n",
      "Epoch 140 batch 70 train Loss 76.5099 test Loss 33.8216 with MSE metric 25080.5925\n",
      "Epoch 140 batch 80 train Loss 76.4896 test Loss 33.8135 with MSE metric 25075.6943\n",
      "Epoch 140 batch 90 train Loss 76.4693 test Loss 33.8054 with MSE metric 25070.7258\n",
      "Epoch 140 batch 100 train Loss 76.4491 test Loss 33.7973 with MSE metric 25065.7358\n",
      "Epoch 140 batch 110 train Loss 76.4288 test Loss 33.7892 with MSE metric 25060.8257\n",
      "Epoch 140 batch 120 train Loss 76.4086 test Loss 33.7812 with MSE metric 25055.8546\n",
      "Epoch 140 batch 130 train Loss 76.3883 test Loss 33.7731 with MSE metric 25051.0178\n",
      "Epoch 140 batch 140 train Loss 76.3681 test Loss 33.7650 with MSE metric 25046.0439\n",
      "Epoch 140 batch 150 train Loss 76.3479 test Loss 33.7570 with MSE metric 25041.0953\n",
      "Epoch 140 batch 160 train Loss 76.3277 test Loss 33.7489 with MSE metric 25036.1320\n",
      "Epoch 140 batch 170 train Loss 76.3075 test Loss 33.7409 with MSE metric 25031.2386\n",
      "Epoch 140 batch 180 train Loss 76.2873 test Loss 33.7329 with MSE metric 25026.3347\n",
      "Epoch 140 batch 190 train Loss 76.2672 test Loss 33.7248 with MSE metric 25021.4443\n",
      "Epoch 140 batch 200 train Loss 76.2470 test Loss 33.7168 with MSE metric 25016.5456\n",
      "Epoch 140 batch 210 train Loss 76.2269 test Loss 33.7087 with MSE metric 25011.7586\n",
      "Epoch 140 batch 220 train Loss 76.2067 test Loss 33.7007 with MSE metric 25006.8961\n",
      "Epoch 140 batch 230 train Loss 76.1866 test Loss 33.6927 with MSE metric 25002.0203\n",
      "Epoch 140 batch 240 train Loss 76.1665 test Loss 33.6847 with MSE metric 24997.0547\n",
      "Time taken for 1 epoch: 24.941713094711304 secs\n",
      "\n",
      "Epoch 141 batch 0 train Loss 76.1464 test Loss 33.6766 with MSE metric 24992.1384\n",
      "Epoch 141 batch 10 train Loss 76.1263 test Loss 33.6686 with MSE metric 24987.2803\n",
      "Epoch 141 batch 20 train Loss 76.1063 test Loss 33.6606 with MSE metric 24982.3483\n",
      "Epoch 141 batch 30 train Loss 76.0862 test Loss 33.6526 with MSE metric 24977.3754\n",
      "Epoch 141 batch 40 train Loss 76.0661 test Loss 33.6446 with MSE metric 24972.5207\n",
      "Epoch 141 batch 50 train Loss 76.0461 test Loss 33.6366 with MSE metric 24967.6977\n",
      "Epoch 141 batch 60 train Loss 76.0261 test Loss 33.6286 with MSE metric 24962.8635\n",
      "Epoch 141 batch 70 train Loss 76.0061 test Loss 33.6207 with MSE metric 24958.0472\n",
      "Epoch 141 batch 80 train Loss 75.9861 test Loss 33.6127 with MSE metric 24953.1461\n",
      "Epoch 141 batch 90 train Loss 75.9661 test Loss 33.6047 with MSE metric 24948.3204\n",
      "Epoch 141 batch 100 train Loss 75.9461 test Loss 33.5967 with MSE metric 24943.5262\n",
      "Epoch 141 batch 110 train Loss 75.9261 test Loss 33.5888 with MSE metric 24938.7476\n",
      "Epoch 141 batch 120 train Loss 75.9062 test Loss 33.5808 with MSE metric 24933.8040\n",
      "Epoch 141 batch 130 train Loss 75.8862 test Loss 33.5728 with MSE metric 24929.0002\n",
      "Epoch 141 batch 140 train Loss 75.8663 test Loss 33.5649 with MSE metric 24924.0589\n",
      "Epoch 141 batch 150 train Loss 75.8464 test Loss 33.5569 with MSE metric 24919.2771\n",
      "Epoch 141 batch 160 train Loss 75.8264 test Loss 33.5490 with MSE metric 24914.4375\n",
      "Epoch 141 batch 170 train Loss 75.8065 test Loss 33.5411 with MSE metric 24909.6171\n",
      "Epoch 141 batch 180 train Loss 75.7866 test Loss 33.5332 with MSE metric 24904.7000\n",
      "Epoch 141 batch 190 train Loss 75.7668 test Loss 33.5253 with MSE metric 24899.7762\n",
      "Epoch 141 batch 200 train Loss 75.7469 test Loss 33.5174 with MSE metric 24895.0162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141 batch 210 train Loss 75.7270 test Loss 33.5094 with MSE metric 24890.1360\n",
      "Epoch 141 batch 220 train Loss 75.7072 test Loss 33.5015 with MSE metric 24885.2757\n",
      "Epoch 141 batch 230 train Loss 75.6873 test Loss 33.4936 with MSE metric 24880.5000\n",
      "Epoch 141 batch 240 train Loss 75.6675 test Loss 33.4857 with MSE metric 24875.6948\n",
      "Time taken for 1 epoch: 24.328299045562744 secs\n",
      "\n",
      "Epoch 142 batch 0 train Loss 75.6477 test Loss 33.4778 with MSE metric 24870.9259\n",
      "Epoch 142 batch 10 train Loss 75.6279 test Loss 33.4699 with MSE metric 24866.1084\n",
      "Epoch 142 batch 20 train Loss 75.6081 test Loss 33.4620 with MSE metric 24861.2389\n",
      "Epoch 142 batch 30 train Loss 75.5883 test Loss 33.4541 with MSE metric 24856.4235\n",
      "Epoch 142 batch 40 train Loss 75.5686 test Loss 33.4462 with MSE metric 24851.5631\n",
      "Epoch 142 batch 50 train Loss 75.5488 test Loss 33.4383 with MSE metric 24846.6711\n",
      "Epoch 142 batch 60 train Loss 75.5291 test Loss 33.4305 with MSE metric 24841.7122\n",
      "Epoch 142 batch 70 train Loss 75.5093 test Loss 33.4226 with MSE metric 24836.9098\n",
      "Epoch 142 batch 80 train Loss 75.4896 test Loss 33.4147 with MSE metric 24832.1127\n",
      "Epoch 142 batch 90 train Loss 75.4699 test Loss 33.4068 with MSE metric 24827.3156\n",
      "Epoch 142 batch 100 train Loss 75.4502 test Loss 33.3990 with MSE metric 24822.5312\n",
      "Epoch 142 batch 110 train Loss 75.4305 test Loss 33.3911 with MSE metric 24817.6872\n",
      "Epoch 142 batch 120 train Loss 75.4108 test Loss 33.3833 with MSE metric 24812.9875\n",
      "Epoch 142 batch 130 train Loss 75.3912 test Loss 33.3754 with MSE metric 24808.1854\n",
      "Epoch 142 batch 140 train Loss 75.3715 test Loss 33.3676 with MSE metric 24803.4666\n",
      "Epoch 142 batch 150 train Loss 75.3518 test Loss 33.3597 with MSE metric 24798.6202\n",
      "Epoch 142 batch 160 train Loss 75.3322 test Loss 33.3519 with MSE metric 24793.8104\n",
      "Epoch 142 batch 170 train Loss 75.3126 test Loss 33.3441 with MSE metric 24788.9152\n",
      "Epoch 142 batch 180 train Loss 75.2929 test Loss 33.3363 with MSE metric 24784.1168\n",
      "Epoch 142 batch 190 train Loss 75.2733 test Loss 33.3284 with MSE metric 24779.4641\n",
      "Epoch 142 batch 200 train Loss 75.2537 test Loss 33.3206 with MSE metric 24774.5651\n",
      "Epoch 142 batch 210 train Loss 75.2342 test Loss 33.3128 with MSE metric 24769.8441\n",
      "Epoch 142 batch 220 train Loss 75.2146 test Loss 33.3050 with MSE metric 24765.0739\n",
      "Epoch 142 batch 230 train Loss 75.1950 test Loss 33.2972 with MSE metric 24760.2630\n",
      "Epoch 142 batch 240 train Loss 75.1755 test Loss 33.2894 with MSE metric 24755.4939\n",
      "Time taken for 1 epoch: 24.614437103271484 secs\n",
      "\n",
      "Epoch 143 batch 0 train Loss 75.1559 test Loss 33.2816 with MSE metric 24750.7828\n",
      "Epoch 143 batch 10 train Loss 75.1364 test Loss 33.2739 with MSE metric 24746.0625\n",
      "Epoch 143 batch 20 train Loss 75.1169 test Loss 33.2661 with MSE metric 24741.2171\n",
      "Epoch 143 batch 30 train Loss 75.0974 test Loss 33.2583 with MSE metric 24736.6090\n",
      "Epoch 143 batch 40 train Loss 75.0779 test Loss 33.2506 with MSE metric 24731.7699\n",
      "Epoch 143 batch 50 train Loss 75.0584 test Loss 33.2428 with MSE metric 24727.0859\n",
      "Epoch 143 batch 60 train Loss 75.0389 test Loss 33.2350 with MSE metric 24722.3928\n",
      "Epoch 143 batch 70 train Loss 75.0195 test Loss 33.2273 with MSE metric 24717.6807\n",
      "Epoch 143 batch 80 train Loss 75.0000 test Loss 33.2195 with MSE metric 24712.9881\n",
      "Epoch 143 batch 90 train Loss 74.9806 test Loss 33.2118 with MSE metric 24708.3242\n",
      "Epoch 143 batch 100 train Loss 74.9612 test Loss 33.2040 with MSE metric 24703.5790\n",
      "Epoch 143 batch 110 train Loss 74.9418 test Loss 33.1962 with MSE metric 24698.8976\n",
      "Epoch 143 batch 120 train Loss 74.9224 test Loss 33.1885 with MSE metric 24694.1563\n",
      "Epoch 143 batch 130 train Loss 74.9030 test Loss 33.1807 with MSE metric 24689.3837\n",
      "Epoch 143 batch 140 train Loss 74.8836 test Loss 33.1730 with MSE metric 24684.6942\n",
      "Epoch 143 batch 150 train Loss 74.8642 test Loss 33.1653 with MSE metric 24679.9405\n",
      "Epoch 143 batch 160 train Loss 74.8448 test Loss 33.1576 with MSE metric 24675.2475\n",
      "Epoch 143 batch 170 train Loss 74.8255 test Loss 33.1498 with MSE metric 24670.5180\n",
      "Epoch 143 batch 180 train Loss 74.8061 test Loss 33.1421 with MSE metric 24665.9049\n",
      "Epoch 143 batch 190 train Loss 74.7868 test Loss 33.1344 with MSE metric 24661.2626\n",
      "Epoch 143 batch 200 train Loss 74.7675 test Loss 33.1267 with MSE metric 24656.5923\n",
      "Epoch 143 batch 210 train Loss 74.7482 test Loss 33.1190 with MSE metric 24651.9648\n",
      "Epoch 143 batch 220 train Loss 74.7289 test Loss 33.1113 with MSE metric 24647.3107\n",
      "Epoch 143 batch 230 train Loss 74.7096 test Loss 33.1036 with MSE metric 24642.6905\n",
      "Epoch 143 batch 240 train Loss 74.6903 test Loss 33.0960 with MSE metric 24638.0299\n",
      "Time taken for 1 epoch: 24.851551055908203 secs\n",
      "\n",
      "Epoch 144 batch 0 train Loss 74.6710 test Loss 33.0883 with MSE metric 24633.2971\n",
      "Epoch 144 batch 10 train Loss 74.6518 test Loss 33.0806 with MSE metric 24628.5153\n",
      "Epoch 144 batch 20 train Loss 74.6325 test Loss 33.0729 with MSE metric 24623.8382\n",
      "Epoch 144 batch 30 train Loss 74.6133 test Loss 33.0652 with MSE metric 24619.1579\n",
      "Epoch 144 batch 40 train Loss 74.5941 test Loss 33.0576 with MSE metric 24614.5511\n",
      "Epoch 144 batch 50 train Loss 74.5748 test Loss 33.0499 with MSE metric 24609.8961\n",
      "Epoch 144 batch 60 train Loss 74.5556 test Loss 33.0422 with MSE metric 24605.2320\n",
      "Epoch 144 batch 70 train Loss 74.5364 test Loss 33.0345 with MSE metric 24600.5932\n",
      "Epoch 144 batch 80 train Loss 74.5173 test Loss 33.0269 with MSE metric 24595.8647\n",
      "Epoch 144 batch 90 train Loss 74.4981 test Loss 33.0192 with MSE metric 24591.2547\n",
      "Epoch 144 batch 100 train Loss 74.4789 test Loss 33.0116 with MSE metric 24586.5094\n",
      "Epoch 144 batch 110 train Loss 74.4598 test Loss 33.0039 with MSE metric 24581.9042\n",
      "Epoch 144 batch 120 train Loss 74.4406 test Loss 32.9963 with MSE metric 24577.3066\n",
      "Epoch 144 batch 130 train Loss 74.4215 test Loss 32.9887 with MSE metric 24572.7496\n",
      "Epoch 144 batch 140 train Loss 74.4024 test Loss 32.9810 with MSE metric 24568.0981\n",
      "Epoch 144 batch 150 train Loss 74.3833 test Loss 32.9734 with MSE metric 24563.4553\n",
      "Epoch 144 batch 160 train Loss 74.3642 test Loss 32.9658 with MSE metric 24558.8436\n",
      "Epoch 144 batch 170 train Loss 74.3451 test Loss 32.9581 with MSE metric 24554.2629\n",
      "Epoch 144 batch 180 train Loss 74.3260 test Loss 32.9505 with MSE metric 24549.7346\n",
      "Epoch 144 batch 190 train Loss 74.3070 test Loss 32.9429 with MSE metric 24545.0222\n",
      "Epoch 144 batch 200 train Loss 74.2879 test Loss 32.9353 with MSE metric 24540.3255\n",
      "Epoch 144 batch 210 train Loss 74.2688 test Loss 32.9277 with MSE metric 24535.6776\n",
      "Epoch 144 batch 220 train Loss 74.2498 test Loss 32.9201 with MSE metric 24531.0055\n",
      "Epoch 144 batch 230 train Loss 74.2308 test Loss 32.9125 with MSE metric 24526.4242\n",
      "Epoch 144 batch 240 train Loss 74.2118 test Loss 32.9049 with MSE metric 24521.7929\n",
      "Time taken for 1 epoch: 24.435301780700684 secs\n",
      "\n",
      "Epoch 145 batch 0 train Loss 74.1928 test Loss 32.8973 with MSE metric 24517.1606\n",
      "Epoch 145 batch 10 train Loss 74.1738 test Loss 32.8897 with MSE metric 24512.5281\n",
      "Epoch 145 batch 20 train Loss 74.1548 test Loss 32.8822 with MSE metric 24508.0468\n",
      "Epoch 145 batch 30 train Loss 74.1358 test Loss 32.8746 with MSE metric 24503.3761\n",
      "Epoch 145 batch 40 train Loss 74.1169 test Loss 32.8671 with MSE metric 24498.6697\n",
      "Epoch 145 batch 50 train Loss 74.0979 test Loss 32.8595 with MSE metric 24494.1465\n",
      "Epoch 145 batch 60 train Loss 74.0790 test Loss 32.8519 with MSE metric 24489.5235\n",
      "Epoch 145 batch 70 train Loss 74.0600 test Loss 32.8444 with MSE metric 24484.9326\n",
      "Epoch 145 batch 80 train Loss 74.0411 test Loss 32.8368 with MSE metric 24480.3586\n",
      "Epoch 145 batch 90 train Loss 74.0222 test Loss 32.8293 with MSE metric 24475.7789\n",
      "Epoch 145 batch 100 train Loss 74.0033 test Loss 32.8217 with MSE metric 24471.1388\n",
      "Epoch 145 batch 110 train Loss 73.9844 test Loss 32.8142 with MSE metric 24466.5109\n",
      "Epoch 145 batch 120 train Loss 73.9655 test Loss 32.8066 with MSE metric 24461.9098\n",
      "Epoch 145 batch 130 train Loss 73.9467 test Loss 32.7991 with MSE metric 24457.3699\n",
      "Epoch 145 batch 140 train Loss 73.9278 test Loss 32.7916 with MSE metric 24452.7451\n",
      "Epoch 145 batch 150 train Loss 73.9090 test Loss 32.7840 with MSE metric 24448.1197\n",
      "Epoch 145 batch 160 train Loss 73.8901 test Loss 32.7765 with MSE metric 24443.4680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145 batch 170 train Loss 73.8713 test Loss 32.7690 with MSE metric 24438.9423\n",
      "Epoch 145 batch 180 train Loss 73.8525 test Loss 32.7615 with MSE metric 24434.2931\n",
      "Epoch 145 batch 190 train Loss 73.8337 test Loss 32.7540 with MSE metric 24429.6422\n",
      "Epoch 145 batch 200 train Loss 73.8149 test Loss 32.7465 with MSE metric 24425.0028\n",
      "Epoch 145 batch 210 train Loss 73.7961 test Loss 32.7390 with MSE metric 24420.3445\n",
      "Epoch 145 batch 220 train Loss 73.7773 test Loss 32.7315 with MSE metric 24415.8053\n",
      "Epoch 145 batch 230 train Loss 73.7586 test Loss 32.7240 with MSE metric 24411.1150\n",
      "Epoch 145 batch 240 train Loss 73.7398 test Loss 32.7165 with MSE metric 24406.4486\n",
      "Time taken for 1 epoch: 25.1923770904541 secs\n",
      "\n",
      "Epoch 146 batch 0 train Loss 73.7210 test Loss 32.7091 with MSE metric 24401.9152\n",
      "Epoch 146 batch 10 train Loss 73.7023 test Loss 32.7016 with MSE metric 24397.3176\n",
      "Epoch 146 batch 20 train Loss 73.6836 test Loss 32.6941 with MSE metric 24392.7946\n",
      "Epoch 146 batch 30 train Loss 73.6649 test Loss 32.6866 with MSE metric 24388.2306\n",
      "Epoch 146 batch 40 train Loss 73.6462 test Loss 32.6792 with MSE metric 24383.6439\n",
      "Epoch 146 batch 50 train Loss 73.6275 test Loss 32.6717 with MSE metric 24379.0964\n",
      "Epoch 146 batch 60 train Loss 73.6088 test Loss 32.6643 with MSE metric 24374.5674\n",
      "Epoch 146 batch 70 train Loss 73.5901 test Loss 32.6568 with MSE metric 24370.0557\n",
      "Epoch 146 batch 80 train Loss 73.5715 test Loss 32.6494 with MSE metric 24365.4675\n",
      "Epoch 146 batch 90 train Loss 73.5528 test Loss 32.6419 with MSE metric 24360.9883\n",
      "Epoch 146 batch 100 train Loss 73.5342 test Loss 32.6345 with MSE metric 24356.4451\n",
      "Epoch 146 batch 110 train Loss 73.5155 test Loss 32.6270 with MSE metric 24351.9073\n",
      "Epoch 146 batch 120 train Loss 73.4969 test Loss 32.6196 with MSE metric 24347.3890\n",
      "Epoch 146 batch 130 train Loss 73.4783 test Loss 32.6122 with MSE metric 24342.8736\n",
      "Epoch 146 batch 140 train Loss 73.4597 test Loss 32.6048 with MSE metric 24338.3268\n",
      "Epoch 146 batch 150 train Loss 73.4411 test Loss 32.5973 with MSE metric 24333.7697\n",
      "Epoch 146 batch 160 train Loss 73.4225 test Loss 32.5899 with MSE metric 24329.3372\n",
      "Epoch 146 batch 170 train Loss 73.4040 test Loss 32.5825 with MSE metric 24324.8150\n",
      "Epoch 146 batch 180 train Loss 73.3854 test Loss 32.5751 with MSE metric 24320.2410\n",
      "Epoch 146 batch 190 train Loss 73.3668 test Loss 32.5677 with MSE metric 24315.7784\n",
      "Epoch 146 batch 200 train Loss 73.3483 test Loss 32.5603 with MSE metric 24311.3586\n",
      "Epoch 146 batch 210 train Loss 73.3298 test Loss 32.5529 with MSE metric 24306.8194\n",
      "Epoch 146 batch 220 train Loss 73.3113 test Loss 32.5455 with MSE metric 24302.3434\n",
      "Epoch 146 batch 230 train Loss 73.2928 test Loss 32.5382 with MSE metric 24297.7746\n",
      "Epoch 146 batch 240 train Loss 73.2742 test Loss 32.5308 with MSE metric 24293.1468\n",
      "Time taken for 1 epoch: 24.46507716178894 secs\n",
      "\n",
      "Epoch 147 batch 0 train Loss 73.2558 test Loss 32.5234 with MSE metric 24288.6187\n",
      "Epoch 147 batch 10 train Loss 73.2373 test Loss 32.5160 with MSE metric 24284.0871\n",
      "Epoch 147 batch 20 train Loss 73.2188 test Loss 32.5086 with MSE metric 24279.5823\n",
      "Epoch 147 batch 30 train Loss 73.2003 test Loss 32.5013 with MSE metric 24275.1341\n",
      "Epoch 147 batch 40 train Loss 73.1819 test Loss 32.4939 with MSE metric 24270.6063\n",
      "Epoch 147 batch 50 train Loss 73.1635 test Loss 32.4866 with MSE metric 24266.1834\n",
      "Epoch 147 batch 60 train Loss 73.1450 test Loss 32.4792 with MSE metric 24261.7338\n",
      "Epoch 147 batch 70 train Loss 73.1266 test Loss 32.4718 with MSE metric 24257.1653\n",
      "Epoch 147 batch 80 train Loss 73.1082 test Loss 32.4645 with MSE metric 24252.6542\n",
      "Epoch 147 batch 90 train Loss 73.0898 test Loss 32.4572 with MSE metric 24248.1578\n",
      "Epoch 147 batch 100 train Loss 73.0714 test Loss 32.4498 with MSE metric 24243.6169\n",
      "Epoch 147 batch 110 train Loss 73.0530 test Loss 32.4425 with MSE metric 24239.2124\n",
      "Epoch 147 batch 120 train Loss 73.0347 test Loss 32.4352 with MSE metric 24234.7328\n",
      "Epoch 147 batch 130 train Loss 73.0163 test Loss 32.4278 with MSE metric 24230.3336\n",
      "Epoch 147 batch 140 train Loss 72.9980 test Loss 32.4205 with MSE metric 24225.8573\n",
      "Epoch 147 batch 150 train Loss 72.9796 test Loss 32.4132 with MSE metric 24221.4188\n",
      "Epoch 147 batch 160 train Loss 72.9613 test Loss 32.4059 with MSE metric 24216.8563\n",
      "Epoch 147 batch 170 train Loss 72.9430 test Loss 32.3986 with MSE metric 24212.3497\n",
      "Epoch 147 batch 180 train Loss 72.9246 test Loss 32.3913 with MSE metric 24207.8512\n",
      "Epoch 147 batch 190 train Loss 72.9063 test Loss 32.3840 with MSE metric 24203.3723\n",
      "Epoch 147 batch 200 train Loss 72.8881 test Loss 32.3767 with MSE metric 24198.9420\n",
      "Epoch 147 batch 210 train Loss 72.8698 test Loss 32.3694 with MSE metric 24194.6204\n",
      "Epoch 147 batch 220 train Loss 72.8515 test Loss 32.3621 with MSE metric 24190.1667\n",
      "Epoch 147 batch 230 train Loss 72.8333 test Loss 32.3548 with MSE metric 24185.7695\n",
      "Epoch 147 batch 240 train Loss 72.8150 test Loss 32.3476 with MSE metric 24181.3151\n",
      "Time taken for 1 epoch: 24.521133184432983 secs\n",
      "\n",
      "Epoch 148 batch 0 train Loss 72.7968 test Loss 32.3403 with MSE metric 24176.9412\n",
      "Epoch 148 batch 10 train Loss 72.7785 test Loss 32.3330 with MSE metric 24172.5221\n",
      "Epoch 148 batch 20 train Loss 72.7603 test Loss 32.3257 with MSE metric 24168.0282\n",
      "Epoch 148 batch 30 train Loss 72.7421 test Loss 32.3185 with MSE metric 24163.5484\n",
      "Epoch 148 batch 40 train Loss 72.7239 test Loss 32.3112 with MSE metric 24159.0869\n",
      "Epoch 148 batch 50 train Loss 72.7057 test Loss 32.3040 with MSE metric 24154.6348\n",
      "Epoch 148 batch 60 train Loss 72.6875 test Loss 32.2967 with MSE metric 24150.1849\n",
      "Epoch 148 batch 70 train Loss 72.6693 test Loss 32.2895 with MSE metric 24145.7961\n",
      "Epoch 148 batch 80 train Loss 72.6512 test Loss 32.2822 with MSE metric 24141.3919\n",
      "Epoch 148 batch 90 train Loss 72.6330 test Loss 32.2750 with MSE metric 24136.9897\n",
      "Epoch 148 batch 100 train Loss 72.6149 test Loss 32.2677 with MSE metric 24132.4984\n",
      "Epoch 148 batch 110 train Loss 72.5968 test Loss 32.2605 with MSE metric 24128.1130\n",
      "Epoch 148 batch 120 train Loss 72.5786 test Loss 32.2532 with MSE metric 24123.7590\n",
      "Epoch 148 batch 130 train Loss 72.5605 test Loss 32.2460 with MSE metric 24119.3895\n",
      "Epoch 148 batch 140 train Loss 72.5424 test Loss 32.2388 with MSE metric 24114.9104\n",
      "Epoch 148 batch 150 train Loss 72.5243 test Loss 32.2315 with MSE metric 24110.5227\n",
      "Epoch 148 batch 160 train Loss 72.5062 test Loss 32.2243 with MSE metric 24106.1620\n",
      "Epoch 148 batch 170 train Loss 72.4882 test Loss 32.2171 with MSE metric 24101.8303\n",
      "Epoch 148 batch 180 train Loss 72.4701 test Loss 32.2099 with MSE metric 24097.4192\n",
      "Epoch 148 batch 190 train Loss 72.4520 test Loss 32.2027 with MSE metric 24093.0339\n",
      "Epoch 148 batch 200 train Loss 72.4340 test Loss 32.1955 with MSE metric 24088.6437\n",
      "Epoch 148 batch 210 train Loss 72.4160 test Loss 32.1883 with MSE metric 24084.2281\n",
      "Epoch 148 batch 220 train Loss 72.3979 test Loss 32.1811 with MSE metric 24079.8194\n",
      "Epoch 148 batch 230 train Loss 72.3799 test Loss 32.1739 with MSE metric 24075.4338\n",
      "Epoch 148 batch 240 train Loss 72.3619 test Loss 32.1667 with MSE metric 24071.0930\n",
      "Time taken for 1 epoch: 24.51261615753174 secs\n",
      "\n",
      "Epoch 149 batch 0 train Loss 72.3439 test Loss 32.1595 with MSE metric 24066.7483\n",
      "Epoch 149 batch 10 train Loss 72.3259 test Loss 32.1524 with MSE metric 24062.2946\n",
      "Epoch 149 batch 20 train Loss 72.3080 test Loss 32.1452 with MSE metric 24057.8834\n",
      "Epoch 149 batch 30 train Loss 72.2900 test Loss 32.1380 with MSE metric 24053.5661\n",
      "Epoch 149 batch 40 train Loss 72.2720 test Loss 32.1308 with MSE metric 24049.1617\n",
      "Epoch 149 batch 50 train Loss 72.2541 test Loss 32.1237 with MSE metric 24044.9322\n",
      "Epoch 149 batch 60 train Loss 72.2362 test Loss 32.1165 with MSE metric 24040.6833\n",
      "Epoch 149 batch 70 train Loss 72.2182 test Loss 32.1093 with MSE metric 24036.2966\n",
      "Epoch 149 batch 80 train Loss 72.2003 test Loss 32.1022 with MSE metric 24032.0201\n",
      "Epoch 149 batch 90 train Loss 72.1824 test Loss 32.0950 with MSE metric 24027.5800\n",
      "Epoch 149 batch 100 train Loss 72.1645 test Loss 32.0879 with MSE metric 24023.1513\n",
      "Epoch 149 batch 110 train Loss 72.1466 test Loss 32.0807 with MSE metric 24018.7478\n",
      "Epoch 149 batch 120 train Loss 72.1287 test Loss 32.0736 with MSE metric 24014.4846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 batch 130 train Loss 72.1109 test Loss 32.0665 with MSE metric 24010.0225\n",
      "Epoch 149 batch 140 train Loss 72.0930 test Loss 32.0593 with MSE metric 24005.7174\n",
      "Epoch 149 batch 150 train Loss 72.0751 test Loss 32.0522 with MSE metric 24001.4127\n",
      "Epoch 149 batch 160 train Loss 72.0573 test Loss 32.0451 with MSE metric 23997.1488\n",
      "Epoch 149 batch 170 train Loss 72.0395 test Loss 32.0380 with MSE metric 23992.7672\n",
      "Epoch 149 batch 180 train Loss 72.0216 test Loss 32.0309 with MSE metric 23988.4053\n",
      "Epoch 149 batch 190 train Loss 72.0038 test Loss 32.0237 with MSE metric 23983.9895\n",
      "Epoch 149 batch 200 train Loss 71.9860 test Loss 32.0166 with MSE metric 23979.6844\n",
      "Epoch 149 batch 210 train Loss 71.9682 test Loss 32.0095 with MSE metric 23975.2811\n",
      "Epoch 149 batch 220 train Loss 71.9504 test Loss 32.0024 with MSE metric 23970.9111\n",
      "Epoch 149 batch 230 train Loss 71.9327 test Loss 31.9953 with MSE metric 23966.5716\n",
      "Epoch 149 batch 240 train Loss 71.9149 test Loss 31.9882 with MSE metric 23962.2356\n",
      "Time taken for 1 epoch: 24.88361096382141 secs\n",
      "\n",
      "Epoch 150 batch 0 train Loss 71.8972 test Loss 31.9811 with MSE metric 23957.8454\n",
      "Epoch 150 batch 10 train Loss 71.8794 test Loss 31.9740 with MSE metric 23953.5077\n",
      "Epoch 150 batch 20 train Loss 71.8617 test Loss 31.9669 with MSE metric 23949.2376\n",
      "Epoch 150 batch 30 train Loss 71.8439 test Loss 31.9599 with MSE metric 23944.9398\n",
      "Epoch 150 batch 40 train Loss 71.8262 test Loss 31.9528 with MSE metric 23940.6475\n",
      "Epoch 150 batch 50 train Loss 71.8085 test Loss 31.9457 with MSE metric 23936.3849\n",
      "Epoch 150 batch 60 train Loss 71.7908 test Loss 31.9387 with MSE metric 23932.0888\n",
      "Epoch 150 batch 70 train Loss 71.7731 test Loss 31.9316 with MSE metric 23927.7308\n",
      "Epoch 150 batch 80 train Loss 71.7554 test Loss 31.9245 with MSE metric 23923.2826\n",
      "Epoch 150 batch 90 train Loss 71.7378 test Loss 31.9175 with MSE metric 23918.9956\n",
      "Epoch 150 batch 100 train Loss 71.7201 test Loss 31.9104 with MSE metric 23914.6957\n",
      "Epoch 150 batch 110 train Loss 71.7024 test Loss 31.9034 with MSE metric 23910.3982\n",
      "Epoch 150 batch 120 train Loss 71.6848 test Loss 31.8963 with MSE metric 23906.0635\n",
      "Epoch 150 batch 130 train Loss 71.6672 test Loss 31.8893 with MSE metric 23901.8992\n",
      "Epoch 150 batch 140 train Loss 71.6495 test Loss 31.8823 with MSE metric 23897.5987\n",
      "Epoch 150 batch 150 train Loss 71.6319 test Loss 31.8752 with MSE metric 23893.3064\n",
      "Epoch 150 batch 160 train Loss 71.6143 test Loss 31.8682 with MSE metric 23889.0842\n",
      "Epoch 150 batch 170 train Loss 71.5967 test Loss 31.8612 with MSE metric 23884.8348\n",
      "Epoch 150 batch 180 train Loss 71.5791 test Loss 31.8541 with MSE metric 23880.5278\n",
      "Epoch 150 batch 190 train Loss 71.5616 test Loss 31.8471 with MSE metric 23876.3434\n",
      "Epoch 150 batch 200 train Loss 71.5440 test Loss 31.8401 with MSE metric 23872.0110\n",
      "Epoch 150 batch 210 train Loss 71.5264 test Loss 31.8332 with MSE metric 23867.6374\n",
      "Epoch 150 batch 220 train Loss 71.5089 test Loss 31.8262 with MSE metric 23863.2666\n",
      "Epoch 150 batch 230 train Loss 71.4913 test Loss 31.8192 with MSE metric 23858.9259\n",
      "Epoch 150 batch 240 train Loss 71.4738 test Loss 31.8122 with MSE metric 23854.6966\n",
      "Time taken for 1 epoch: 24.510573148727417 secs\n",
      "\n",
      "Epoch 151 batch 0 train Loss 71.4563 test Loss 31.8052 with MSE metric 23850.4981\n",
      "Epoch 151 batch 10 train Loss 71.4388 test Loss 31.7982 with MSE metric 23846.1688\n",
      "Epoch 151 batch 20 train Loss 71.4213 test Loss 31.7912 with MSE metric 23841.9706\n",
      "Epoch 151 batch 30 train Loss 71.4038 test Loss 31.7842 with MSE metric 23837.7635\n",
      "Epoch 151 batch 40 train Loss 71.3863 test Loss 31.7773 with MSE metric 23833.4457\n",
      "Epoch 151 batch 50 train Loss 71.3688 test Loss 31.7703 with MSE metric 23829.2667\n",
      "Epoch 151 batch 60 train Loss 71.3513 test Loss 31.7633 with MSE metric 23825.0091\n",
      "Epoch 151 batch 70 train Loss 71.3339 test Loss 31.7563 with MSE metric 23820.6712\n",
      "Epoch 151 batch 80 train Loss 71.3164 test Loss 31.7494 with MSE metric 23816.4611\n",
      "Epoch 151 batch 90 train Loss 71.2990 test Loss 31.7424 with MSE metric 23812.2253\n",
      "Epoch 151 batch 100 train Loss 71.2816 test Loss 31.7355 with MSE metric 23807.9600\n",
      "Epoch 151 batch 110 train Loss 71.2642 test Loss 31.7286 with MSE metric 23803.7124\n",
      "Epoch 151 batch 120 train Loss 71.2467 test Loss 31.7216 with MSE metric 23799.4589\n",
      "Epoch 151 batch 130 train Loss 71.2293 test Loss 31.7146 with MSE metric 23795.2379\n",
      "Epoch 151 batch 140 train Loss 71.2120 test Loss 31.7077 with MSE metric 23791.0707\n",
      "Epoch 151 batch 150 train Loss 71.1946 test Loss 31.7008 with MSE metric 23786.8914\n",
      "Epoch 151 batch 160 train Loss 71.1772 test Loss 31.6939 with MSE metric 23782.6713\n",
      "Epoch 151 batch 170 train Loss 71.1598 test Loss 31.6870 with MSE metric 23778.4192\n",
      "Epoch 151 batch 180 train Loss 71.1425 test Loss 31.6800 with MSE metric 23774.2058\n",
      "Epoch 151 batch 190 train Loss 71.1251 test Loss 31.6731 with MSE metric 23770.0134\n",
      "Epoch 151 batch 200 train Loss 71.1078 test Loss 31.6662 with MSE metric 23765.6755\n",
      "Epoch 151 batch 210 train Loss 71.0905 test Loss 31.6592 with MSE metric 23761.4291\n",
      "Epoch 151 batch 220 train Loss 71.0731 test Loss 31.6523 with MSE metric 23757.2099\n",
      "Epoch 151 batch 230 train Loss 71.0558 test Loss 31.6454 with MSE metric 23752.9364\n",
      "Epoch 151 batch 240 train Loss 71.0385 test Loss 31.6385 with MSE metric 23748.6596\n",
      "Time taken for 1 epoch: 25.26799488067627 secs\n",
      "\n",
      "Epoch 152 batch 0 train Loss 71.0212 test Loss 31.6316 with MSE metric 23744.4322\n",
      "Epoch 152 batch 10 train Loss 71.0039 test Loss 31.6247 with MSE metric 23740.2910\n",
      "Epoch 152 batch 20 train Loss 70.9867 test Loss 31.6178 with MSE metric 23736.0876\n",
      "Epoch 152 batch 30 train Loss 70.9694 test Loss 31.6109 with MSE metric 23731.8283\n",
      "Epoch 152 batch 40 train Loss 70.9521 test Loss 31.6040 with MSE metric 23727.6479\n",
      "Epoch 152 batch 50 train Loss 70.9349 test Loss 31.5972 with MSE metric 23723.4343\n",
      "Epoch 152 batch 60 train Loss 70.9177 test Loss 31.5903 with MSE metric 23719.2960\n",
      "Epoch 152 batch 70 train Loss 70.9004 test Loss 31.5834 with MSE metric 23714.9947\n",
      "Epoch 152 batch 80 train Loss 70.8832 test Loss 31.5765 with MSE metric 23710.7954\n",
      "Epoch 152 batch 90 train Loss 70.8660 test Loss 31.5697 with MSE metric 23706.6291\n",
      "Epoch 152 batch 100 train Loss 70.8488 test Loss 31.5628 with MSE metric 23702.4631\n",
      "Epoch 152 batch 110 train Loss 70.8316 test Loss 31.5559 with MSE metric 23698.3055\n",
      "Epoch 152 batch 120 train Loss 70.8144 test Loss 31.5491 with MSE metric 23694.2586\n",
      "Epoch 152 batch 130 train Loss 70.7973 test Loss 31.5422 with MSE metric 23690.1506\n",
      "Epoch 152 batch 140 train Loss 70.7801 test Loss 31.5354 with MSE metric 23685.9550\n",
      "Epoch 152 batch 150 train Loss 70.7630 test Loss 31.5285 with MSE metric 23681.8122\n",
      "Epoch 152 batch 160 train Loss 70.7458 test Loss 31.5217 with MSE metric 23677.5576\n",
      "Epoch 152 batch 170 train Loss 70.7287 test Loss 31.5148 with MSE metric 23673.3128\n",
      "Epoch 152 batch 180 train Loss 70.7115 test Loss 31.5080 with MSE metric 23669.1430\n",
      "Epoch 152 batch 190 train Loss 70.6944 test Loss 31.5012 with MSE metric 23664.9548\n",
      "Epoch 152 batch 200 train Loss 70.6773 test Loss 31.4943 with MSE metric 23660.7120\n",
      "Epoch 152 batch 210 train Loss 70.6602 test Loss 31.4875 with MSE metric 23656.5428\n",
      "Epoch 152 batch 220 train Loss 70.6431 test Loss 31.4807 with MSE metric 23652.3630\n",
      "Epoch 152 batch 230 train Loss 70.6260 test Loss 31.4739 with MSE metric 23648.2436\n",
      "Epoch 152 batch 240 train Loss 70.6089 test Loss 31.4671 with MSE metric 23644.0598\n",
      "Time taken for 1 epoch: 26.558639764785767 secs\n",
      "\n",
      "Epoch 153 batch 0 train Loss 70.5918 test Loss 31.4603 with MSE metric 23639.8993\n",
      "Epoch 153 batch 10 train Loss 70.5748 test Loss 31.4535 with MSE metric 23635.8123\n",
      "Epoch 153 batch 20 train Loss 70.5577 test Loss 31.4467 with MSE metric 23631.6723\n",
      "Epoch 153 batch 30 train Loss 70.5407 test Loss 31.4399 with MSE metric 23627.5327\n",
      "Epoch 153 batch 40 train Loss 70.5237 test Loss 31.4332 with MSE metric 23623.4801\n",
      "Epoch 153 batch 50 train Loss 70.5067 test Loss 31.4264 with MSE metric 23619.3833\n",
      "Epoch 153 batch 60 train Loss 70.4896 test Loss 31.4196 with MSE metric 23615.2702\n",
      "Epoch 153 batch 70 train Loss 70.4726 test Loss 31.4128 with MSE metric 23611.0498\n",
      "Epoch 153 batch 80 train Loss 70.4556 test Loss 31.4060 with MSE metric 23606.8276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153 batch 90 train Loss 70.4386 test Loss 31.3992 with MSE metric 23602.6221\n",
      "Epoch 153 batch 100 train Loss 70.4217 test Loss 31.3924 with MSE metric 23598.4642\n",
      "Epoch 153 batch 110 train Loss 70.4047 test Loss 31.3857 with MSE metric 23594.3075\n",
      "Epoch 153 batch 120 train Loss 70.3877 test Loss 31.3789 with MSE metric 23590.1726\n",
      "Epoch 153 batch 130 train Loss 70.3708 test Loss 31.3721 with MSE metric 23585.9709\n",
      "Epoch 153 batch 140 train Loss 70.3538 test Loss 31.3654 with MSE metric 23581.7662\n",
      "Epoch 153 batch 150 train Loss 70.3369 test Loss 31.3586 with MSE metric 23577.6375\n",
      "Epoch 153 batch 160 train Loss 70.3200 test Loss 31.3519 with MSE metric 23573.4799\n",
      "Epoch 153 batch 170 train Loss 70.3030 test Loss 31.3451 with MSE metric 23569.3567\n",
      "Epoch 153 batch 180 train Loss 70.2861 test Loss 31.3384 with MSE metric 23565.2846\n",
      "Epoch 153 batch 190 train Loss 70.2692 test Loss 31.3316 with MSE metric 23561.1597\n",
      "Epoch 153 batch 200 train Loss 70.2524 test Loss 31.3248 with MSE metric 23557.0870\n",
      "Epoch 153 batch 210 train Loss 70.2355 test Loss 31.3181 with MSE metric 23552.9660\n",
      "Epoch 153 batch 220 train Loss 70.2186 test Loss 31.3114 with MSE metric 23548.9255\n",
      "Epoch 153 batch 230 train Loss 70.2017 test Loss 31.3046 with MSE metric 23544.7227\n",
      "Epoch 153 batch 240 train Loss 70.1849 test Loss 31.2979 with MSE metric 23540.4954\n",
      "Time taken for 1 epoch: 25.758726119995117 secs\n",
      "\n",
      "Epoch 154 batch 0 train Loss 70.1680 test Loss 31.2912 with MSE metric 23536.3588\n",
      "Epoch 154 batch 10 train Loss 70.1512 test Loss 31.2845 with MSE metric 23532.2651\n",
      "Epoch 154 batch 20 train Loss 70.1344 test Loss 31.2777 with MSE metric 23528.3438\n",
      "Epoch 154 batch 30 train Loss 70.1175 test Loss 31.2710 with MSE metric 23524.3436\n",
      "Epoch 154 batch 40 train Loss 70.1007 test Loss 31.2643 with MSE metric 23520.2471\n",
      "Epoch 154 batch 50 train Loss 70.0839 test Loss 31.2576 with MSE metric 23516.1775\n",
      "Epoch 154 batch 60 train Loss 70.0671 test Loss 31.2509 with MSE metric 23512.1009\n",
      "Epoch 154 batch 70 train Loss 70.0503 test Loss 31.2442 with MSE metric 23508.0603\n",
      "Epoch 154 batch 80 train Loss 70.0336 test Loss 31.2375 with MSE metric 23503.9804\n",
      "Epoch 154 batch 90 train Loss 70.0168 test Loss 31.2308 with MSE metric 23499.8184\n",
      "Epoch 154 batch 100 train Loss 70.0000 test Loss 31.2241 with MSE metric 23495.7206\n",
      "Epoch 154 batch 110 train Loss 69.9833 test Loss 31.2174 with MSE metric 23491.5779\n",
      "Epoch 154 batch 120 train Loss 69.9665 test Loss 31.2107 with MSE metric 23487.4978\n",
      "Epoch 154 batch 130 train Loss 69.9498 test Loss 31.2040 with MSE metric 23483.4461\n",
      "Epoch 154 batch 140 train Loss 69.9331 test Loss 31.1974 with MSE metric 23479.3386\n",
      "Epoch 154 batch 150 train Loss 69.9164 test Loss 31.1907 with MSE metric 23475.2554\n",
      "Epoch 154 batch 160 train Loss 69.8997 test Loss 31.1840 with MSE metric 23471.2030\n",
      "Epoch 154 batch 170 train Loss 69.8830 test Loss 31.1773 with MSE metric 23467.1039\n",
      "Epoch 154 batch 180 train Loss 69.8663 test Loss 31.1707 with MSE metric 23463.0616\n",
      "Epoch 154 batch 190 train Loss 69.8496 test Loss 31.1640 with MSE metric 23458.8855\n",
      "Epoch 154 batch 200 train Loss 69.8329 test Loss 31.1574 with MSE metric 23454.7727\n",
      "Epoch 154 batch 210 train Loss 69.8162 test Loss 31.1507 with MSE metric 23450.7310\n",
      "Epoch 154 batch 220 train Loss 69.7996 test Loss 31.1441 with MSE metric 23446.6938\n",
      "Epoch 154 batch 230 train Loss 69.7829 test Loss 31.1374 with MSE metric 23442.6895\n",
      "Epoch 154 batch 240 train Loss 69.7663 test Loss 31.1308 with MSE metric 23438.5917\n",
      "Time taken for 1 epoch: 23.247355937957764 secs\n",
      "\n",
      "Epoch 155 batch 0 train Loss 69.7497 test Loss 31.1241 with MSE metric 23434.5385\n",
      "Epoch 155 batch 10 train Loss 69.7330 test Loss 31.1175 with MSE metric 23430.5627\n",
      "Epoch 155 batch 20 train Loss 69.7164 test Loss 31.1108 with MSE metric 23426.5623\n",
      "Epoch 155 batch 30 train Loss 69.6998 test Loss 31.1042 with MSE metric 23422.5576\n",
      "Epoch 155 batch 40 train Loss 69.6832 test Loss 31.0976 with MSE metric 23418.5405\n",
      "Epoch 155 batch 50 train Loss 69.6666 test Loss 31.0910 with MSE metric 23414.4595\n",
      "Epoch 155 batch 60 train Loss 69.6501 test Loss 31.0843 with MSE metric 23410.4023\n",
      "Epoch 155 batch 70 train Loss 69.6335 test Loss 31.0777 with MSE metric 23406.4554\n",
      "Epoch 155 batch 80 train Loss 69.6169 test Loss 31.0711 with MSE metric 23402.4108\n",
      "Epoch 155 batch 90 train Loss 69.6004 test Loss 31.0645 with MSE metric 23398.2949\n",
      "Epoch 155 batch 100 train Loss 69.5838 test Loss 31.0579 with MSE metric 23394.3027\n",
      "Epoch 155 batch 110 train Loss 69.5673 test Loss 31.0513 with MSE metric 23390.2559\n",
      "Epoch 155 batch 120 train Loss 69.5508 test Loss 31.0447 with MSE metric 23386.1963\n",
      "Epoch 155 batch 130 train Loss 69.5342 test Loss 31.0381 with MSE metric 23382.1483\n",
      "Epoch 155 batch 140 train Loss 69.5177 test Loss 31.0315 with MSE metric 23378.0751\n",
      "Epoch 155 batch 150 train Loss 69.5012 test Loss 31.0249 with MSE metric 23374.0544\n",
      "Epoch 155 batch 160 train Loss 69.4847 test Loss 31.0183 with MSE metric 23370.0344\n",
      "Epoch 155 batch 170 train Loss 69.4682 test Loss 31.0118 with MSE metric 23366.0072\n",
      "Epoch 155 batch 180 train Loss 69.4518 test Loss 31.0051 with MSE metric 23361.9073\n",
      "Epoch 155 batch 190 train Loss 69.4353 test Loss 30.9986 with MSE metric 23357.9025\n",
      "Epoch 155 batch 200 train Loss 69.4188 test Loss 30.9920 with MSE metric 23353.8800\n",
      "Epoch 155 batch 210 train Loss 69.4024 test Loss 30.9854 with MSE metric 23349.8326\n",
      "Epoch 155 batch 220 train Loss 69.3860 test Loss 30.9789 with MSE metric 23345.8606\n",
      "Epoch 155 batch 230 train Loss 69.3695 test Loss 30.9723 with MSE metric 23341.9136\n",
      "Epoch 155 batch 240 train Loss 69.3531 test Loss 30.9658 with MSE metric 23337.7879\n",
      "Time taken for 1 epoch: 23.58967113494873 secs\n",
      "\n",
      "Epoch 156 batch 0 train Loss 69.3367 test Loss 30.9592 with MSE metric 23333.6823\n",
      "Epoch 156 batch 10 train Loss 69.3203 test Loss 30.9526 with MSE metric 23329.7062\n",
      "Epoch 156 batch 20 train Loss 69.3039 test Loss 30.9461 with MSE metric 23325.7658\n",
      "Epoch 156 batch 30 train Loss 69.2875 test Loss 30.9396 with MSE metric 23321.7427\n",
      "Epoch 156 batch 40 train Loss 69.2711 test Loss 30.9330 with MSE metric 23317.6587\n",
      "Epoch 156 batch 50 train Loss 69.2547 test Loss 30.9265 with MSE metric 23313.7016\n",
      "Epoch 156 batch 60 train Loss 69.2383 test Loss 30.9200 with MSE metric 23309.6486\n",
      "Epoch 156 batch 70 train Loss 69.2220 test Loss 30.9135 with MSE metric 23305.6339\n",
      "Epoch 156 batch 80 train Loss 69.2056 test Loss 30.9070 with MSE metric 23301.7139\n",
      "Epoch 156 batch 90 train Loss 69.1893 test Loss 30.9005 with MSE metric 23297.6806\n",
      "Epoch 156 batch 100 train Loss 69.1729 test Loss 30.8939 with MSE metric 23293.7234\n",
      "Epoch 156 batch 110 train Loss 69.1566 test Loss 30.8874 with MSE metric 23289.6703\n",
      "Epoch 156 batch 120 train Loss 69.1403 test Loss 30.8809 with MSE metric 23285.7480\n",
      "Epoch 156 batch 130 train Loss 69.1240 test Loss 30.8744 with MSE metric 23281.6878\n",
      "Epoch 156 batch 140 train Loss 69.1077 test Loss 30.8679 with MSE metric 23277.8343\n",
      "Epoch 156 batch 150 train Loss 69.0914 test Loss 30.8614 with MSE metric 23273.8160\n",
      "Epoch 156 batch 160 train Loss 69.0751 test Loss 30.8549 with MSE metric 23269.7716\n",
      "Epoch 156 batch 170 train Loss 69.0588 test Loss 30.8484 with MSE metric 23265.9556\n",
      "Epoch 156 batch 180 train Loss 69.0426 test Loss 30.8419 with MSE metric 23261.9487\n",
      "Epoch 156 batch 190 train Loss 69.0263 test Loss 30.8354 with MSE metric 23257.9535\n",
      "Epoch 156 batch 200 train Loss 69.0100 test Loss 30.8289 with MSE metric 23253.9616\n",
      "Epoch 156 batch 210 train Loss 68.9938 test Loss 30.8224 with MSE metric 23250.0048\n",
      "Epoch 156 batch 220 train Loss 68.9776 test Loss 30.8159 with MSE metric 23246.0728\n",
      "Epoch 156 batch 230 train Loss 68.9613 test Loss 30.8094 with MSE metric 23242.0416\n",
      "Epoch 156 batch 240 train Loss 68.9451 test Loss 30.8029 with MSE metric 23238.0719\n",
      "Time taken for 1 epoch: 23.96631669998169 secs\n",
      "\n",
      "Epoch 157 batch 0 train Loss 68.9289 test Loss 30.7965 with MSE metric 23234.1206\n",
      "Epoch 157 batch 10 train Loss 68.9127 test Loss 30.7900 with MSE metric 23230.1241\n",
      "Epoch 157 batch 20 train Loss 68.8965 test Loss 30.7835 with MSE metric 23226.1485\n",
      "Epoch 157 batch 30 train Loss 68.8803 test Loss 30.7770 with MSE metric 23222.2101\n",
      "Epoch 157 batch 40 train Loss 68.8641 test Loss 30.7706 with MSE metric 23218.3414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157 batch 50 train Loss 68.8480 test Loss 30.7641 with MSE metric 23214.3497\n",
      "Epoch 157 batch 60 train Loss 68.8318 test Loss 30.7577 with MSE metric 23210.5449\n",
      "Epoch 157 batch 70 train Loss 68.8157 test Loss 30.7512 with MSE metric 23206.6009\n",
      "Epoch 157 batch 80 train Loss 68.7995 test Loss 30.7448 with MSE metric 23202.7014\n",
      "Epoch 157 batch 90 train Loss 68.7834 test Loss 30.7383 with MSE metric 23198.7679\n",
      "Epoch 157 batch 100 train Loss 68.7673 test Loss 30.7319 with MSE metric 23194.8515\n",
      "Epoch 157 batch 110 train Loss 68.7511 test Loss 30.7254 with MSE metric 23190.9681\n",
      "Epoch 157 batch 120 train Loss 68.7350 test Loss 30.7190 with MSE metric 23187.0221\n",
      "Epoch 157 batch 130 train Loss 68.7189 test Loss 30.7125 with MSE metric 23183.1634\n",
      "Epoch 157 batch 140 train Loss 68.7028 test Loss 30.7061 with MSE metric 23179.2916\n",
      "Epoch 157 batch 150 train Loss 68.6868 test Loss 30.6997 with MSE metric 23175.4284\n",
      "Epoch 157 batch 160 train Loss 68.6707 test Loss 30.6932 with MSE metric 23171.5667\n",
      "Epoch 157 batch 170 train Loss 68.6546 test Loss 30.6868 with MSE metric 23167.6905\n",
      "Epoch 157 batch 180 train Loss 68.6385 test Loss 30.6804 with MSE metric 23163.8577\n",
      "Epoch 157 batch 190 train Loss 68.6225 test Loss 30.6740 with MSE metric 23159.9257\n",
      "Epoch 157 batch 200 train Loss 68.6064 test Loss 30.6676 with MSE metric 23155.9520\n",
      "Epoch 157 batch 210 train Loss 68.5904 test Loss 30.6612 with MSE metric 23151.9752\n",
      "Epoch 157 batch 220 train Loss 68.5744 test Loss 30.6548 with MSE metric 23148.0568\n",
      "Epoch 157 batch 230 train Loss 68.5584 test Loss 30.6484 with MSE metric 23144.2259\n",
      "Epoch 157 batch 240 train Loss 68.5423 test Loss 30.6420 with MSE metric 23140.3004\n",
      "Time taken for 1 epoch: 24.155724048614502 secs\n",
      "\n",
      "Epoch 158 batch 0 train Loss 68.5263 test Loss 30.6356 with MSE metric 23136.3992\n",
      "Epoch 158 batch 10 train Loss 68.5104 test Loss 30.6292 with MSE metric 23132.4389\n",
      "Epoch 158 batch 20 train Loss 68.4944 test Loss 30.6228 with MSE metric 23128.6173\n",
      "Epoch 158 batch 30 train Loss 68.4784 test Loss 30.6164 with MSE metric 23124.7894\n",
      "Epoch 158 batch 40 train Loss 68.4624 test Loss 30.6100 with MSE metric 23120.8917\n",
      "Epoch 158 batch 50 train Loss 68.4465 test Loss 30.6037 with MSE metric 23117.0536\n",
      "Epoch 158 batch 60 train Loss 68.4305 test Loss 30.5973 with MSE metric 23113.1978\n",
      "Epoch 158 batch 70 train Loss 68.4146 test Loss 30.5909 with MSE metric 23109.3312\n",
      "Epoch 158 batch 80 train Loss 68.3986 test Loss 30.5846 with MSE metric 23105.4337\n",
      "Epoch 158 batch 90 train Loss 68.3827 test Loss 30.5782 with MSE metric 23101.5837\n",
      "Epoch 158 batch 100 train Loss 68.3668 test Loss 30.5718 with MSE metric 23097.8377\n",
      "Epoch 158 batch 110 train Loss 68.3509 test Loss 30.5655 with MSE metric 23093.9807\n",
      "Epoch 158 batch 120 train Loss 68.3350 test Loss 30.5592 with MSE metric 23090.0350\n",
      "Epoch 158 batch 130 train Loss 68.3191 test Loss 30.5528 with MSE metric 23086.0470\n",
      "Epoch 158 batch 140 train Loss 68.3032 test Loss 30.5465 with MSE metric 23082.2591\n",
      "Epoch 158 batch 150 train Loss 68.2873 test Loss 30.5401 with MSE metric 23078.3839\n",
      "Epoch 158 batch 160 train Loss 68.2714 test Loss 30.5338 with MSE metric 23074.5211\n",
      "Epoch 158 batch 170 train Loss 68.2555 test Loss 30.5275 with MSE metric 23070.6316\n",
      "Epoch 158 batch 180 train Loss 68.2397 test Loss 30.5212 with MSE metric 23066.8400\n",
      "Epoch 158 batch 190 train Loss 68.2238 test Loss 30.5148 with MSE metric 23063.0278\n",
      "Epoch 158 batch 200 train Loss 68.2080 test Loss 30.5086 with MSE metric 23059.2331\n",
      "Epoch 158 batch 210 train Loss 68.1922 test Loss 30.5023 with MSE metric 23055.3258\n",
      "Epoch 158 batch 220 train Loss 68.1763 test Loss 30.4959 with MSE metric 23051.4709\n",
      "Epoch 158 batch 230 train Loss 68.1605 test Loss 30.4896 with MSE metric 23047.6339\n",
      "Epoch 158 batch 240 train Loss 68.1447 test Loss 30.4833 with MSE metric 23043.8099\n",
      "Time taken for 1 epoch: 24.009319067001343 secs\n",
      "\n",
      "Epoch 159 batch 0 train Loss 68.1289 test Loss 30.4770 with MSE metric 23040.0604\n",
      "Epoch 159 batch 10 train Loss 68.1131 test Loss 30.4707 with MSE metric 23036.2372\n",
      "Epoch 159 batch 20 train Loss 68.0973 test Loss 30.4644 with MSE metric 23032.3990\n",
      "Epoch 159 batch 30 train Loss 68.0815 test Loss 30.4581 with MSE metric 23028.5990\n",
      "Epoch 159 batch 40 train Loss 68.0658 test Loss 30.4518 with MSE metric 23024.7443\n",
      "Epoch 159 batch 50 train Loss 68.0500 test Loss 30.4455 with MSE metric 23020.9062\n",
      "Epoch 159 batch 60 train Loss 68.0342 test Loss 30.4392 with MSE metric 23016.9699\n",
      "Epoch 159 batch 70 train Loss 68.0185 test Loss 30.4330 with MSE metric 23013.0733\n",
      "Epoch 159 batch 80 train Loss 68.0028 test Loss 30.4267 with MSE metric 23009.3259\n",
      "Epoch 159 batch 90 train Loss 67.9870 test Loss 30.4204 with MSE metric 23005.4195\n",
      "Epoch 159 batch 100 train Loss 67.9713 test Loss 30.4141 with MSE metric 23001.5464\n",
      "Epoch 159 batch 110 train Loss 67.9556 test Loss 30.4078 with MSE metric 22997.7458\n",
      "Epoch 159 batch 120 train Loss 67.9399 test Loss 30.4016 with MSE metric 22993.8436\n",
      "Epoch 159 batch 130 train Loss 67.9242 test Loss 30.3953 with MSE metric 22990.0151\n",
      "Epoch 159 batch 140 train Loss 67.9085 test Loss 30.3891 with MSE metric 22986.2003\n",
      "Epoch 159 batch 150 train Loss 67.8928 test Loss 30.3828 with MSE metric 22982.3124\n",
      "Epoch 159 batch 160 train Loss 67.8771 test Loss 30.3765 with MSE metric 22978.5184\n",
      "Epoch 159 batch 170 train Loss 67.8614 test Loss 30.3703 with MSE metric 22974.6298\n",
      "Epoch 159 batch 180 train Loss 67.8458 test Loss 30.3640 with MSE metric 22970.7132\n",
      "Epoch 159 batch 190 train Loss 67.8301 test Loss 30.3578 with MSE metric 22966.9077\n",
      "Epoch 159 batch 200 train Loss 67.8145 test Loss 30.3515 with MSE metric 22963.1274\n",
      "Epoch 159 batch 210 train Loss 67.7988 test Loss 30.3453 with MSE metric 22959.2664\n",
      "Epoch 159 batch 220 train Loss 67.7832 test Loss 30.3391 with MSE metric 22955.4348\n",
      "Epoch 159 batch 230 train Loss 67.7676 test Loss 30.3329 with MSE metric 22951.7078\n",
      "Epoch 159 batch 240 train Loss 67.7520 test Loss 30.3266 with MSE metric 22947.8957\n",
      "Time taken for 1 epoch: 23.883249044418335 secs\n",
      "\n",
      "Epoch 160 batch 0 train Loss 67.7363 test Loss 30.3204 with MSE metric 22944.0389\n",
      "Epoch 160 batch 10 train Loss 67.7207 test Loss 30.3141 with MSE metric 22940.2540\n",
      "Epoch 160 batch 20 train Loss 67.7052 test Loss 30.3079 with MSE metric 22936.5004\n",
      "Epoch 160 batch 30 train Loss 67.6896 test Loss 30.3016 with MSE metric 22932.6694\n",
      "Epoch 160 batch 40 train Loss 67.6740 test Loss 30.2954 with MSE metric 22928.8319\n",
      "Epoch 160 batch 50 train Loss 67.6584 test Loss 30.2892 with MSE metric 22925.0088\n",
      "Epoch 160 batch 60 train Loss 67.6429 test Loss 30.2830 with MSE metric 22921.2505\n",
      "Epoch 160 batch 70 train Loss 67.6273 test Loss 30.2768 with MSE metric 22917.4347\n",
      "Epoch 160 batch 80 train Loss 67.6118 test Loss 30.2706 with MSE metric 22913.6286\n",
      "Epoch 160 batch 90 train Loss 67.5962 test Loss 30.2644 with MSE metric 22909.8273\n",
      "Epoch 160 batch 100 train Loss 67.5807 test Loss 30.2582 with MSE metric 22906.0552\n",
      "Epoch 160 batch 110 train Loss 67.5652 test Loss 30.2520 with MSE metric 22902.2805\n",
      "Epoch 160 batch 120 train Loss 67.5497 test Loss 30.2458 with MSE metric 22898.4373\n",
      "Epoch 160 batch 130 train Loss 67.5341 test Loss 30.2397 with MSE metric 22894.4942\n",
      "Epoch 160 batch 140 train Loss 67.5187 test Loss 30.2335 with MSE metric 22890.7573\n",
      "Epoch 160 batch 150 train Loss 67.5032 test Loss 30.2273 with MSE metric 22887.0071\n",
      "Epoch 160 batch 160 train Loss 67.4877 test Loss 30.2211 with MSE metric 22883.2688\n",
      "Epoch 160 batch 170 train Loss 67.4722 test Loss 30.2149 with MSE metric 22879.4845\n",
      "Epoch 160 batch 180 train Loss 67.4567 test Loss 30.2088 with MSE metric 22875.7960\n",
      "Epoch 160 batch 190 train Loss 67.4413 test Loss 30.2026 with MSE metric 22872.0230\n",
      "Epoch 160 batch 200 train Loss 67.4258 test Loss 30.1964 with MSE metric 22868.2713\n",
      "Epoch 160 batch 210 train Loss 67.4104 test Loss 30.1903 with MSE metric 22864.5539\n",
      "Epoch 160 batch 220 train Loss 67.3950 test Loss 30.1841 with MSE metric 22860.8168\n",
      "Epoch 160 batch 230 train Loss 67.3795 test Loss 30.1779 with MSE metric 22857.0311\n",
      "Epoch 160 batch 240 train Loss 67.3641 test Loss 30.1717 with MSE metric 22853.1913\n",
      "Time taken for 1 epoch: 24.21878409385681 secs\n",
      "\n",
      "Epoch 161 batch 0 train Loss 67.3487 test Loss 30.1656 with MSE metric 22849.4607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 batch 10 train Loss 67.3333 test Loss 30.1595 with MSE metric 22845.6386\n",
      "Epoch 161 batch 20 train Loss 67.3179 test Loss 30.1533 with MSE metric 22841.9306\n",
      "Epoch 161 batch 30 train Loss 67.3025 test Loss 30.1472 with MSE metric 22838.1709\n",
      "Epoch 161 batch 40 train Loss 67.2871 test Loss 30.1410 with MSE metric 22834.4269\n",
      "Epoch 161 batch 50 train Loss 67.2717 test Loss 30.1349 with MSE metric 22830.6339\n",
      "Epoch 161 batch 60 train Loss 67.2564 test Loss 30.1287 with MSE metric 22826.8039\n",
      "Epoch 161 batch 70 train Loss 67.2410 test Loss 30.1226 with MSE metric 22822.9866\n",
      "Epoch 161 batch 80 train Loss 67.2256 test Loss 30.1165 with MSE metric 22819.2339\n",
      "Epoch 161 batch 90 train Loss 67.2103 test Loss 30.1103 with MSE metric 22815.4624\n",
      "Epoch 161 batch 100 train Loss 67.1950 test Loss 30.1042 with MSE metric 22811.7143\n",
      "Epoch 161 batch 110 train Loss 67.1796 test Loss 30.0981 with MSE metric 22807.9176\n",
      "Epoch 161 batch 120 train Loss 67.1643 test Loss 30.0920 with MSE metric 22804.2076\n",
      "Epoch 161 batch 130 train Loss 67.1490 test Loss 30.0859 with MSE metric 22800.4630\n",
      "Epoch 161 batch 140 train Loss 67.1337 test Loss 30.0798 with MSE metric 22796.7237\n",
      "Epoch 161 batch 150 train Loss 67.1184 test Loss 30.0737 with MSE metric 22793.0151\n",
      "Epoch 161 batch 160 train Loss 67.1031 test Loss 30.0676 with MSE metric 22789.2473\n",
      "Epoch 161 batch 170 train Loss 67.0878 test Loss 30.0615 with MSE metric 22785.5156\n",
      "Epoch 161 batch 180 train Loss 67.0725 test Loss 30.0554 with MSE metric 22781.7657\n",
      "Epoch 161 batch 190 train Loss 67.0572 test Loss 30.0493 with MSE metric 22777.9770\n",
      "Epoch 161 batch 200 train Loss 67.0420 test Loss 30.0432 with MSE metric 22774.1872\n",
      "Epoch 161 batch 210 train Loss 67.0267 test Loss 30.0372 with MSE metric 22770.4961\n",
      "Epoch 161 batch 220 train Loss 67.0115 test Loss 30.0311 with MSE metric 22766.7989\n",
      "Epoch 161 batch 230 train Loss 66.9962 test Loss 30.0250 with MSE metric 22763.0864\n",
      "Epoch 161 batch 240 train Loss 66.9810 test Loss 30.0189 with MSE metric 22759.4745\n",
      "Time taken for 1 epoch: 24.325680255889893 secs\n",
      "\n",
      "Epoch 162 batch 0 train Loss 66.9658 test Loss 30.0128 with MSE metric 22755.7426\n",
      "Epoch 162 batch 10 train Loss 66.9506 test Loss 30.0067 with MSE metric 22752.0902\n",
      "Epoch 162 batch 20 train Loss 66.9354 test Loss 30.0007 with MSE metric 22748.3765\n",
      "Epoch 162 batch 30 train Loss 66.9202 test Loss 29.9946 with MSE metric 22744.6971\n",
      "Epoch 162 batch 40 train Loss 66.9050 test Loss 29.9885 with MSE metric 22740.9551\n",
      "Epoch 162 batch 50 train Loss 66.8898 test Loss 29.9824 with MSE metric 22737.2148\n",
      "Epoch 162 batch 60 train Loss 66.8746 test Loss 29.9764 with MSE metric 22733.5133\n",
      "Epoch 162 batch 70 train Loss 66.8595 test Loss 29.9703 with MSE metric 22729.7513\n",
      "Epoch 162 batch 80 train Loss 66.8443 test Loss 29.9642 with MSE metric 22726.0466\n",
      "Epoch 162 batch 90 train Loss 66.8291 test Loss 29.9582 with MSE metric 22722.3662\n",
      "Epoch 162 batch 100 train Loss 66.8140 test Loss 29.9521 with MSE metric 22718.6218\n",
      "Epoch 162 batch 110 train Loss 66.7988 test Loss 29.9461 with MSE metric 22714.9296\n",
      "Epoch 162 batch 120 train Loss 66.7837 test Loss 29.9400 with MSE metric 22711.2598\n",
      "Epoch 162 batch 130 train Loss 66.7686 test Loss 29.9340 with MSE metric 22707.5934\n",
      "Epoch 162 batch 140 train Loss 66.7535 test Loss 29.9280 with MSE metric 22703.9225\n",
      "Epoch 162 batch 150 train Loss 66.7384 test Loss 29.9219 with MSE metric 22700.2141\n",
      "Epoch 162 batch 160 train Loss 66.7233 test Loss 29.9159 with MSE metric 22696.5281\n",
      "Epoch 162 batch 170 train Loss 66.7082 test Loss 29.9099 with MSE metric 22692.8802\n",
      "Epoch 162 batch 180 train Loss 66.6931 test Loss 29.9038 with MSE metric 22689.2116\n",
      "Epoch 162 batch 190 train Loss 66.6780 test Loss 29.8978 with MSE metric 22685.3544\n",
      "Epoch 162 batch 200 train Loss 66.6629 test Loss 29.8918 with MSE metric 22681.6581\n",
      "Epoch 162 batch 210 train Loss 66.6478 test Loss 29.8858 with MSE metric 22677.9906\n",
      "Epoch 162 batch 220 train Loss 66.6328 test Loss 29.8798 with MSE metric 22674.1899\n",
      "Epoch 162 batch 230 train Loss 66.6177 test Loss 29.8738 with MSE metric 22670.4509\n",
      "Epoch 162 batch 240 train Loss 66.6027 test Loss 29.8678 with MSE metric 22666.7654\n",
      "Time taken for 1 epoch: 24.28379487991333 secs\n",
      "\n",
      "Epoch 163 batch 0 train Loss 66.5876 test Loss 29.8617 with MSE metric 22663.0148\n",
      "Epoch 163 batch 10 train Loss 66.5726 test Loss 29.8557 with MSE metric 22659.3595\n",
      "Epoch 163 batch 20 train Loss 66.5576 test Loss 29.8497 with MSE metric 22655.6985\n",
      "Epoch 163 batch 30 train Loss 66.5426 test Loss 29.8437 with MSE metric 22652.0766\n",
      "Epoch 163 batch 40 train Loss 66.5275 test Loss 29.8377 with MSE metric 22648.4341\n",
      "Epoch 163 batch 50 train Loss 66.5125 test Loss 29.8317 with MSE metric 22644.7769\n",
      "Epoch 163 batch 60 train Loss 66.4976 test Loss 29.8257 with MSE metric 22641.0899\n",
      "Epoch 163 batch 70 train Loss 66.4826 test Loss 29.8198 with MSE metric 22637.3920\n",
      "Epoch 163 batch 80 train Loss 66.4676 test Loss 29.8138 with MSE metric 22633.7157\n",
      "Epoch 163 batch 90 train Loss 66.4526 test Loss 29.8078 with MSE metric 22630.0685\n",
      "Epoch 163 batch 100 train Loss 66.4377 test Loss 29.8018 with MSE metric 22626.5475\n",
      "Epoch 163 batch 110 train Loss 66.4227 test Loss 29.7958 with MSE metric 22622.8669\n",
      "Epoch 163 batch 120 train Loss 66.4078 test Loss 29.7899 with MSE metric 22619.1989\n",
      "Epoch 163 batch 130 train Loss 66.3928 test Loss 29.7839 with MSE metric 22615.6069\n",
      "Epoch 163 batch 140 train Loss 66.3779 test Loss 29.7780 with MSE metric 22611.9137\n",
      "Epoch 163 batch 150 train Loss 66.3629 test Loss 29.7720 with MSE metric 22608.2159\n",
      "Epoch 163 batch 160 train Loss 66.3480 test Loss 29.7660 with MSE metric 22604.5469\n",
      "Epoch 163 batch 170 train Loss 66.3331 test Loss 29.7601 with MSE metric 22600.9208\n",
      "Epoch 163 batch 180 train Loss 66.3182 test Loss 29.7542 with MSE metric 22597.2763\n",
      "Epoch 163 batch 190 train Loss 66.3033 test Loss 29.7482 with MSE metric 22593.5846\n",
      "Epoch 163 batch 200 train Loss 66.2884 test Loss 29.7423 with MSE metric 22589.8592\n",
      "Epoch 163 batch 210 train Loss 66.2735 test Loss 29.7363 with MSE metric 22586.2360\n",
      "Epoch 163 batch 220 train Loss 66.2586 test Loss 29.7304 with MSE metric 22582.6698\n",
      "Epoch 163 batch 230 train Loss 66.2438 test Loss 29.7245 with MSE metric 22579.0530\n",
      "Epoch 163 batch 240 train Loss 66.2289 test Loss 29.7185 with MSE metric 22575.4663\n",
      "Time taken for 1 epoch: 23.937822103500366 secs\n",
      "\n",
      "Epoch 164 batch 0 train Loss 66.2141 test Loss 29.7126 with MSE metric 22571.9225\n",
      "Epoch 164 batch 10 train Loss 66.1992 test Loss 29.7067 with MSE metric 22568.2227\n",
      "Epoch 164 batch 20 train Loss 66.1844 test Loss 29.7008 with MSE metric 22564.6242\n",
      "Epoch 164 batch 30 train Loss 66.1695 test Loss 29.6949 with MSE metric 22561.0129\n",
      "Epoch 164 batch 40 train Loss 66.1547 test Loss 29.6889 with MSE metric 22557.4192\n",
      "Epoch 164 batch 50 train Loss 66.1399 test Loss 29.6830 with MSE metric 22553.8670\n",
      "Epoch 164 batch 60 train Loss 66.1251 test Loss 29.6771 with MSE metric 22550.2042\n",
      "Epoch 164 batch 70 train Loss 66.1103 test Loss 29.6712 with MSE metric 22546.5645\n",
      "Epoch 164 batch 80 train Loss 66.0955 test Loss 29.6654 with MSE metric 22542.9332\n",
      "Epoch 164 batch 90 train Loss 66.0807 test Loss 29.6595 with MSE metric 22539.2770\n",
      "Epoch 164 batch 100 train Loss 66.0659 test Loss 29.6536 with MSE metric 22535.6088\n",
      "Epoch 164 batch 110 train Loss 66.0511 test Loss 29.6477 with MSE metric 22532.0131\n",
      "Epoch 164 batch 120 train Loss 66.0364 test Loss 29.6418 with MSE metric 22528.4621\n",
      "Epoch 164 batch 130 train Loss 66.0216 test Loss 29.6359 with MSE metric 22524.8104\n",
      "Epoch 164 batch 140 train Loss 66.0068 test Loss 29.6300 with MSE metric 22521.2667\n",
      "Epoch 164 batch 150 train Loss 65.9921 test Loss 29.6241 with MSE metric 22517.7174\n",
      "Epoch 164 batch 160 train Loss 65.9773 test Loss 29.6182 with MSE metric 22514.0987\n",
      "Epoch 164 batch 170 train Loss 65.9626 test Loss 29.6124 with MSE metric 22510.3880\n",
      "Epoch 164 batch 180 train Loss 65.9479 test Loss 29.6065 with MSE metric 22506.8493\n",
      "Epoch 164 batch 190 train Loss 65.9332 test Loss 29.6007 with MSE metric 22503.3160\n",
      "Epoch 164 batch 200 train Loss 65.9185 test Loss 29.5948 with MSE metric 22499.7046\n",
      "Epoch 164 batch 210 train Loss 65.9037 test Loss 29.5890 with MSE metric 22496.1005\n",
      "Epoch 164 batch 220 train Loss 65.8891 test Loss 29.5831 with MSE metric 22492.4714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164 batch 230 train Loss 65.8744 test Loss 29.5772 with MSE metric 22488.8400\n",
      "Epoch 164 batch 240 train Loss 65.8597 test Loss 29.5714 with MSE metric 22485.2462\n",
      "Time taken for 1 epoch: 24.320390939712524 secs\n",
      "\n",
      "Epoch 165 batch 0 train Loss 65.8450 test Loss 29.5655 with MSE metric 22481.7042\n",
      "Epoch 165 batch 10 train Loss 65.8303 test Loss 29.5597 with MSE metric 22478.1385\n",
      "Epoch 165 batch 20 train Loss 65.8157 test Loss 29.5538 with MSE metric 22474.5111\n",
      "Epoch 165 batch 30 train Loss 65.8010 test Loss 29.5479 with MSE metric 22470.9576\n",
      "Epoch 165 batch 40 train Loss 65.7864 test Loss 29.5421 with MSE metric 22467.3898\n",
      "Epoch 165 batch 50 train Loss 65.7717 test Loss 29.5362 with MSE metric 22463.8051\n",
      "Epoch 165 batch 60 train Loss 65.7571 test Loss 29.5304 with MSE metric 22460.2338\n",
      "Epoch 165 batch 70 train Loss 65.7425 test Loss 29.5245 with MSE metric 22456.6139\n",
      "Epoch 165 batch 80 train Loss 65.7278 test Loss 29.5187 with MSE metric 22453.0109\n",
      "Epoch 165 batch 90 train Loss 65.7132 test Loss 29.5129 with MSE metric 22449.4322\n",
      "Epoch 165 batch 100 train Loss 65.6986 test Loss 29.5070 with MSE metric 22445.8253\n",
      "Epoch 165 batch 110 train Loss 65.6840 test Loss 29.5012 with MSE metric 22442.2803\n",
      "Epoch 165 batch 120 train Loss 65.6694 test Loss 29.4954 with MSE metric 22438.7519\n",
      "Epoch 165 batch 130 train Loss 65.6548 test Loss 29.4896 with MSE metric 22435.1376\n",
      "Epoch 165 batch 140 train Loss 65.6403 test Loss 29.4838 with MSE metric 22431.5644\n",
      "Epoch 165 batch 150 train Loss 65.6257 test Loss 29.4779 with MSE metric 22428.0326\n",
      "Epoch 165 batch 160 train Loss 65.6111 test Loss 29.4721 with MSE metric 22424.5276\n",
      "Epoch 165 batch 170 train Loss 65.5966 test Loss 29.4663 with MSE metric 22420.9277\n",
      "Epoch 165 batch 180 train Loss 65.5820 test Loss 29.4605 with MSE metric 22417.4580\n",
      "Epoch 165 batch 190 train Loss 65.5675 test Loss 29.4547 with MSE metric 22413.8517\n",
      "Epoch 165 batch 200 train Loss 65.5530 test Loss 29.4489 with MSE metric 22410.2899\n",
      "Epoch 165 batch 210 train Loss 65.5384 test Loss 29.4431 with MSE metric 22406.7731\n",
      "Epoch 165 batch 220 train Loss 65.5239 test Loss 29.4373 with MSE metric 22403.3158\n",
      "Epoch 165 batch 230 train Loss 65.5094 test Loss 29.4315 with MSE metric 22399.7826\n",
      "Epoch 165 batch 240 train Loss 65.4949 test Loss 29.4257 with MSE metric 22396.1290\n",
      "Time taken for 1 epoch: 24.093408584594727 secs\n",
      "\n",
      "Epoch 166 batch 0 train Loss 65.4804 test Loss 29.4199 with MSE metric 22392.5955\n",
      "Epoch 166 batch 10 train Loss 65.4659 test Loss 29.4141 with MSE metric 22389.0625\n",
      "Epoch 166 batch 20 train Loss 65.4514 test Loss 29.4083 with MSE metric 22385.5751\n",
      "Epoch 166 batch 30 train Loss 65.4370 test Loss 29.4025 with MSE metric 22382.0688\n",
      "Epoch 166 batch 40 train Loss 65.4225 test Loss 29.3968 with MSE metric 22378.5574\n",
      "Epoch 166 batch 50 train Loss 65.4080 test Loss 29.3910 with MSE metric 22374.9777\n",
      "Epoch 166 batch 60 train Loss 65.3936 test Loss 29.3852 with MSE metric 22371.3975\n",
      "Epoch 166 batch 70 train Loss 65.3791 test Loss 29.3795 with MSE metric 22367.8843\n",
      "Epoch 166 batch 80 train Loss 65.3647 test Loss 29.3738 with MSE metric 22364.3854\n",
      "Epoch 166 batch 90 train Loss 65.3502 test Loss 29.3680 with MSE metric 22360.8529\n",
      "Epoch 166 batch 100 train Loss 65.3358 test Loss 29.3622 with MSE metric 22357.4183\n",
      "Epoch 166 batch 110 train Loss 65.3214 test Loss 29.3565 with MSE metric 22353.9372\n",
      "Epoch 166 batch 120 train Loss 65.3070 test Loss 29.3507 with MSE metric 22350.4155\n",
      "Epoch 166 batch 130 train Loss 65.2926 test Loss 29.3449 with MSE metric 22346.9396\n",
      "Epoch 166 batch 140 train Loss 65.2782 test Loss 29.3392 with MSE metric 22343.4393\n",
      "Epoch 166 batch 150 train Loss 65.2638 test Loss 29.3334 with MSE metric 22339.9260\n",
      "Epoch 166 batch 160 train Loss 65.2494 test Loss 29.3276 with MSE metric 22336.3627\n",
      "Epoch 166 batch 170 train Loss 65.2350 test Loss 29.3219 with MSE metric 22332.9111\n",
      "Epoch 166 batch 180 train Loss 65.2206 test Loss 29.3162 with MSE metric 22329.3860\n",
      "Epoch 166 batch 190 train Loss 65.2063 test Loss 29.3105 with MSE metric 22325.9055\n",
      "Epoch 166 batch 200 train Loss 65.1919 test Loss 29.3047 with MSE metric 22322.3775\n",
      "Epoch 166 batch 210 train Loss 65.1775 test Loss 29.2990 with MSE metric 22318.8667\n",
      "Epoch 166 batch 220 train Loss 65.1632 test Loss 29.2933 with MSE metric 22315.4977\n",
      "Epoch 166 batch 230 train Loss 65.1489 test Loss 29.2876 with MSE metric 22311.9951\n",
      "Epoch 166 batch 240 train Loss 65.1345 test Loss 29.2818 with MSE metric 22308.5729\n",
      "Time taken for 1 epoch: 24.05841898918152 secs\n",
      "\n",
      "Epoch 167 batch 0 train Loss 65.1202 test Loss 29.2761 with MSE metric 22305.0837\n",
      "Epoch 167 batch 10 train Loss 65.1059 test Loss 29.2704 with MSE metric 22301.6066\n",
      "Epoch 167 batch 20 train Loss 65.0916 test Loss 29.2647 with MSE metric 22298.1510\n",
      "Epoch 167 batch 30 train Loss 65.0773 test Loss 29.2590 with MSE metric 22294.6773\n",
      "Epoch 167 batch 40 train Loss 65.0630 test Loss 29.2533 with MSE metric 22291.1259\n",
      "Epoch 167 batch 50 train Loss 65.0487 test Loss 29.2476 with MSE metric 22287.6805\n",
      "Epoch 167 batch 60 train Loss 65.0344 test Loss 29.2419 with MSE metric 22284.2511\n",
      "Epoch 167 batch 70 train Loss 65.0201 test Loss 29.2362 with MSE metric 22280.7778\n",
      "Epoch 167 batch 80 train Loss 65.0059 test Loss 29.2305 with MSE metric 22277.3103\n",
      "Epoch 167 batch 90 train Loss 64.9916 test Loss 29.2248 with MSE metric 22273.8908\n",
      "Epoch 167 batch 100 train Loss 64.9773 test Loss 29.2191 with MSE metric 22270.3920\n",
      "Epoch 167 batch 110 train Loss 64.9631 test Loss 29.2134 with MSE metric 22266.9041\n",
      "Epoch 167 batch 120 train Loss 64.9488 test Loss 29.2077 with MSE metric 22263.4395\n",
      "Epoch 167 batch 130 train Loss 64.9346 test Loss 29.2020 with MSE metric 22260.0333\n",
      "Epoch 167 batch 140 train Loss 64.9204 test Loss 29.1963 with MSE metric 22256.5866\n",
      "Epoch 167 batch 150 train Loss 64.9061 test Loss 29.1906 with MSE metric 22253.1533\n",
      "Epoch 167 batch 160 train Loss 64.8919 test Loss 29.1849 with MSE metric 22249.6908\n",
      "Epoch 167 batch 170 train Loss 64.8777 test Loss 29.1792 with MSE metric 22246.2513\n",
      "Epoch 167 batch 180 train Loss 64.8635 test Loss 29.1735 with MSE metric 22242.7528\n",
      "Epoch 167 batch 190 train Loss 64.8493 test Loss 29.1679 with MSE metric 22239.2877\n",
      "Epoch 167 batch 200 train Loss 64.8351 test Loss 29.1622 with MSE metric 22235.8410\n",
      "Epoch 167 batch 210 train Loss 64.8209 test Loss 29.1566 with MSE metric 22232.4484\n",
      "Epoch 167 batch 220 train Loss 64.8068 test Loss 29.1509 with MSE metric 22229.0195\n",
      "Epoch 167 batch 230 train Loss 64.7926 test Loss 29.1452 with MSE metric 22225.5252\n",
      "Epoch 167 batch 240 train Loss 64.7784 test Loss 29.1396 with MSE metric 22222.0996\n",
      "Time taken for 1 epoch: 24.21499991416931 secs\n",
      "\n",
      "Epoch 168 batch 0 train Loss 64.7643 test Loss 29.1340 with MSE metric 22218.5757\n",
      "Epoch 168 batch 10 train Loss 64.7501 test Loss 29.1283 with MSE metric 22215.2017\n",
      "Epoch 168 batch 20 train Loss 64.7360 test Loss 29.1227 with MSE metric 22211.8184\n",
      "Epoch 168 batch 30 train Loss 64.7219 test Loss 29.1170 with MSE metric 22208.4618\n",
      "Epoch 168 batch 40 train Loss 64.7077 test Loss 29.1114 with MSE metric 22205.0229\n",
      "Epoch 168 batch 50 train Loss 64.6936 test Loss 29.1058 with MSE metric 22201.5366\n",
      "Epoch 168 batch 60 train Loss 64.6795 test Loss 29.1001 with MSE metric 22198.1201\n",
      "Epoch 168 batch 70 train Loss 64.6654 test Loss 29.0945 with MSE metric 22194.6741\n",
      "Epoch 168 batch 80 train Loss 64.6513 test Loss 29.0889 with MSE metric 22191.2173\n",
      "Epoch 168 batch 90 train Loss 64.6372 test Loss 29.0833 with MSE metric 22187.7595\n",
      "Epoch 168 batch 100 train Loss 64.6231 test Loss 29.0776 with MSE metric 22184.2944\n",
      "Epoch 168 batch 110 train Loss 64.6090 test Loss 29.0720 with MSE metric 22180.9223\n",
      "Epoch 168 batch 120 train Loss 64.5949 test Loss 29.0664 with MSE metric 22177.5325\n",
      "Epoch 168 batch 130 train Loss 64.5809 test Loss 29.0607 with MSE metric 22174.0596\n",
      "Epoch 168 batch 140 train Loss 64.5668 test Loss 29.0551 with MSE metric 22170.6716\n",
      "Epoch 168 batch 150 train Loss 64.5527 test Loss 29.0495 with MSE metric 22167.2660\n",
      "Epoch 168 batch 160 train Loss 64.5387 test Loss 29.0439 with MSE metric 22163.8224\n",
      "Epoch 168 batch 170 train Loss 64.5247 test Loss 29.0383 with MSE metric 22160.4321\n",
      "Epoch 168 batch 180 train Loss 64.5106 test Loss 29.0327 with MSE metric 22156.9741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168 batch 190 train Loss 64.4966 test Loss 29.0271 with MSE metric 22153.5316\n",
      "Epoch 168 batch 200 train Loss 64.4826 test Loss 29.0215 with MSE metric 22150.0671\n",
      "Epoch 168 batch 210 train Loss 64.4685 test Loss 29.0159 with MSE metric 22146.5325\n",
      "Epoch 168 batch 220 train Loss 64.4545 test Loss 29.0103 with MSE metric 22143.1354\n",
      "Epoch 168 batch 230 train Loss 64.4405 test Loss 29.0047 with MSE metric 22139.7420\n",
      "Epoch 168 batch 240 train Loss 64.4265 test Loss 28.9991 with MSE metric 22136.2298\n",
      "Time taken for 1 epoch: 24.92238998413086 secs\n",
      "\n",
      "Epoch 169 batch 0 train Loss 64.4126 test Loss 28.9936 with MSE metric 22132.8240\n",
      "Epoch 169 batch 10 train Loss 64.3986 test Loss 28.9880 with MSE metric 22129.4537\n",
      "Epoch 169 batch 20 train Loss 64.3846 test Loss 28.9824 with MSE metric 22125.9822\n",
      "Epoch 169 batch 30 train Loss 64.3706 test Loss 28.9768 with MSE metric 22122.6215\n",
      "Epoch 169 batch 40 train Loss 64.3567 test Loss 28.9712 with MSE metric 22119.2511\n",
      "Epoch 169 batch 50 train Loss 64.3427 test Loss 28.9657 with MSE metric 22115.8493\n",
      "Epoch 169 batch 60 train Loss 64.3287 test Loss 28.9601 with MSE metric 22112.4671\n",
      "Epoch 169 batch 70 train Loss 64.3148 test Loss 28.9545 with MSE metric 22108.9966\n",
      "Epoch 169 batch 80 train Loss 64.3009 test Loss 28.9490 with MSE metric 22105.5593\n",
      "Epoch 169 batch 90 train Loss 64.2869 test Loss 28.9434 with MSE metric 22102.1225\n",
      "Epoch 169 batch 100 train Loss 64.2730 test Loss 28.9379 with MSE metric 22098.7523\n",
      "Epoch 169 batch 110 train Loss 64.2591 test Loss 28.9323 with MSE metric 22095.3672\n",
      "Epoch 169 batch 120 train Loss 64.2452 test Loss 28.9267 with MSE metric 22092.0021\n",
      "Epoch 169 batch 130 train Loss 64.2313 test Loss 28.9212 with MSE metric 22088.6933\n",
      "Epoch 169 batch 140 train Loss 64.2174 test Loss 28.9157 with MSE metric 22085.3393\n",
      "Epoch 169 batch 150 train Loss 64.2035 test Loss 28.9101 with MSE metric 22081.9495\n",
      "Epoch 169 batch 160 train Loss 64.1896 test Loss 28.9046 with MSE metric 22078.5756\n",
      "Epoch 169 batch 170 train Loss 64.1757 test Loss 28.8990 with MSE metric 22075.1325\n",
      "Epoch 169 batch 180 train Loss 64.1619 test Loss 28.8935 with MSE metric 22071.6937\n",
      "Epoch 169 batch 190 train Loss 64.1480 test Loss 28.8880 with MSE metric 22068.2809\n",
      "Epoch 169 batch 200 train Loss 64.1341 test Loss 28.8824 with MSE metric 22064.8528\n",
      "Epoch 169 batch 210 train Loss 64.1203 test Loss 28.8769 with MSE metric 22061.5503\n",
      "Epoch 169 batch 220 train Loss 64.1064 test Loss 28.8714 with MSE metric 22058.1351\n",
      "Epoch 169 batch 230 train Loss 64.0926 test Loss 28.8658 with MSE metric 22054.7474\n",
      "Epoch 169 batch 240 train Loss 64.0788 test Loss 28.8603 with MSE metric 22051.3861\n",
      "Time taken for 1 epoch: 24.63450288772583 secs\n",
      "\n",
      "Epoch 170 batch 0 train Loss 64.0649 test Loss 28.8548 with MSE metric 22047.9750\n",
      "Epoch 170 batch 10 train Loss 64.0511 test Loss 28.8493 with MSE metric 22044.5680\n",
      "Epoch 170 batch 20 train Loss 64.0373 test Loss 28.8438 with MSE metric 22041.2621\n",
      "Epoch 170 batch 30 train Loss 64.0235 test Loss 28.8383 with MSE metric 22037.9108\n",
      "Epoch 170 batch 40 train Loss 64.0097 test Loss 28.8328 with MSE metric 22034.5403\n",
      "Epoch 170 batch 50 train Loss 63.9959 test Loss 28.8272 with MSE metric 22031.2570\n",
      "Epoch 170 batch 60 train Loss 63.9821 test Loss 28.8218 with MSE metric 22027.9274\n",
      "Epoch 170 batch 70 train Loss 63.9684 test Loss 28.8163 with MSE metric 22024.5300\n",
      "Epoch 170 batch 80 train Loss 63.9546 test Loss 28.8108 with MSE metric 22021.1598\n",
      "Epoch 170 batch 90 train Loss 63.9408 test Loss 28.8053 with MSE metric 22017.7768\n",
      "Epoch 170 batch 100 train Loss 63.9271 test Loss 28.7998 with MSE metric 22014.4602\n",
      "Epoch 170 batch 110 train Loss 63.9133 test Loss 28.7943 with MSE metric 22011.1623\n",
      "Epoch 170 batch 120 train Loss 63.8996 test Loss 28.7888 with MSE metric 22007.7971\n",
      "Epoch 170 batch 130 train Loss 63.8858 test Loss 28.7833 with MSE metric 22004.4753\n",
      "Epoch 170 batch 140 train Loss 63.8721 test Loss 28.7778 with MSE metric 22001.1066\n",
      "Epoch 170 batch 150 train Loss 63.8584 test Loss 28.7723 with MSE metric 21997.7899\n",
      "Epoch 170 batch 160 train Loss 63.8446 test Loss 28.7668 with MSE metric 21994.4538\n",
      "Epoch 170 batch 170 train Loss 63.8309 test Loss 28.7614 with MSE metric 21991.0794\n",
      "Epoch 170 batch 180 train Loss 63.8172 test Loss 28.7559 with MSE metric 21987.8830\n",
      "Epoch 170 batch 190 train Loss 63.8035 test Loss 28.7505 with MSE metric 21984.5002\n",
      "Epoch 170 batch 200 train Loss 63.7898 test Loss 28.7450 with MSE metric 21981.1581\n",
      "Epoch 170 batch 210 train Loss 63.7761 test Loss 28.7395 with MSE metric 21977.7816\n",
      "Epoch 170 batch 220 train Loss 63.7625 test Loss 28.7340 with MSE metric 21974.4713\n",
      "Epoch 170 batch 230 train Loss 63.7488 test Loss 28.7286 with MSE metric 21971.1796\n",
      "Epoch 170 batch 240 train Loss 63.7351 test Loss 28.7232 with MSE metric 21967.8365\n",
      "Time taken for 1 epoch: 24.551342964172363 secs\n",
      "\n",
      "Epoch 171 batch 0 train Loss 63.7214 test Loss 28.7177 with MSE metric 21964.4636\n",
      "Epoch 171 batch 10 train Loss 63.7078 test Loss 28.7122 with MSE metric 21961.1327\n",
      "Epoch 171 batch 20 train Loss 63.6941 test Loss 28.7068 with MSE metric 21957.8716\n",
      "Epoch 171 batch 30 train Loss 63.6805 test Loss 28.7013 with MSE metric 21954.5773\n",
      "Epoch 171 batch 40 train Loss 63.6669 test Loss 28.6959 with MSE metric 21951.2440\n",
      "Epoch 171 batch 50 train Loss 63.6532 test Loss 28.6904 with MSE metric 21947.9123\n",
      "Epoch 171 batch 60 train Loss 63.6396 test Loss 28.6850 with MSE metric 21944.4687\n",
      "Epoch 171 batch 70 train Loss 63.6260 test Loss 28.6796 with MSE metric 21941.1538\n",
      "Epoch 171 batch 80 train Loss 63.6124 test Loss 28.6742 with MSE metric 21937.7342\n",
      "Epoch 171 batch 90 train Loss 63.5987 test Loss 28.6687 with MSE metric 21934.4491\n",
      "Epoch 171 batch 100 train Loss 63.5851 test Loss 28.6633 with MSE metric 21931.0468\n",
      "Epoch 171 batch 110 train Loss 63.5716 test Loss 28.6579 with MSE metric 21927.7495\n",
      "Epoch 171 batch 120 train Loss 63.5580 test Loss 28.6524 with MSE metric 21924.3653\n",
      "Epoch 171 batch 130 train Loss 63.5444 test Loss 28.6470 with MSE metric 21921.0181\n",
      "Epoch 171 batch 140 train Loss 63.5308 test Loss 28.6416 with MSE metric 21917.6546\n",
      "Epoch 171 batch 150 train Loss 63.5172 test Loss 28.6362 with MSE metric 21914.3001\n",
      "Epoch 171 batch 160 train Loss 63.5037 test Loss 28.6308 with MSE metric 21910.9666\n",
      "Epoch 171 batch 170 train Loss 63.4901 test Loss 28.6253 with MSE metric 21907.6479\n",
      "Epoch 171 batch 180 train Loss 63.4766 test Loss 28.6199 with MSE metric 21904.2517\n",
      "Epoch 171 batch 190 train Loss 63.4630 test Loss 28.6145 with MSE metric 21900.9441\n",
      "Epoch 171 batch 200 train Loss 63.4495 test Loss 28.6091 with MSE metric 21897.6692\n",
      "Epoch 171 batch 210 train Loss 63.4359 test Loss 28.6037 with MSE metric 21894.4024\n",
      "Epoch 171 batch 220 train Loss 63.4224 test Loss 28.5983 with MSE metric 21891.0368\n",
      "Epoch 171 batch 230 train Loss 63.4089 test Loss 28.5929 with MSE metric 21887.7658\n",
      "Epoch 171 batch 240 train Loss 63.3954 test Loss 28.5876 with MSE metric 21884.5018\n",
      "Time taken for 1 epoch: 24.501386165618896 secs\n",
      "\n",
      "Epoch 172 batch 0 train Loss 63.3819 test Loss 28.5822 with MSE metric 21881.2062\n",
      "Epoch 172 batch 10 train Loss 63.3684 test Loss 28.5768 with MSE metric 21877.9320\n",
      "Epoch 172 batch 20 train Loss 63.3549 test Loss 28.5715 with MSE metric 21874.6755\n",
      "Epoch 172 batch 30 train Loss 63.3414 test Loss 28.5661 with MSE metric 21871.3962\n",
      "Epoch 172 batch 40 train Loss 63.3279 test Loss 28.5607 with MSE metric 21868.0618\n",
      "Epoch 172 batch 50 train Loss 63.3144 test Loss 28.5553 with MSE metric 21864.8105\n",
      "Epoch 172 batch 60 train Loss 63.3010 test Loss 28.5499 with MSE metric 21861.4973\n",
      "Epoch 172 batch 70 train Loss 63.2875 test Loss 28.5445 with MSE metric 21858.1929\n",
      "Epoch 172 batch 80 train Loss 63.2741 test Loss 28.5392 with MSE metric 21854.9746\n",
      "Epoch 172 batch 90 train Loss 63.2606 test Loss 28.5338 with MSE metric 21851.6131\n",
      "Epoch 172 batch 100 train Loss 63.2472 test Loss 28.5284 with MSE metric 21848.4519\n",
      "Epoch 172 batch 110 train Loss 63.2337 test Loss 28.5230 with MSE metric 21845.0980\n",
      "Epoch 172 batch 120 train Loss 63.2203 test Loss 28.5177 with MSE metric 21841.8219\n",
      "Epoch 172 batch 130 train Loss 63.2069 test Loss 28.5123 with MSE metric 21838.5559\n",
      "Epoch 172 batch 140 train Loss 63.1935 test Loss 28.5070 with MSE metric 21835.2240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 batch 150 train Loss 63.1800 test Loss 28.5016 with MSE metric 21831.9385\n",
      "Epoch 172 batch 160 train Loss 63.1666 test Loss 28.4962 with MSE metric 21828.6239\n",
      "Epoch 172 batch 170 train Loss 63.1532 test Loss 28.4909 with MSE metric 21825.3135\n",
      "Epoch 172 batch 180 train Loss 63.1398 test Loss 28.4855 with MSE metric 21822.0523\n",
      "Epoch 172 batch 190 train Loss 63.1265 test Loss 28.4802 with MSE metric 21818.7603\n",
      "Epoch 172 batch 200 train Loss 63.1131 test Loss 28.4749 with MSE metric 21815.4802\n",
      "Epoch 172 batch 210 train Loss 63.0997 test Loss 28.4695 with MSE metric 21812.1982\n",
      "Epoch 172 batch 220 train Loss 63.0863 test Loss 28.4642 with MSE metric 21808.9914\n",
      "Epoch 172 batch 230 train Loss 63.0730 test Loss 28.4588 with MSE metric 21805.7404\n",
      "Epoch 172 batch 240 train Loss 63.0596 test Loss 28.4535 with MSE metric 21802.5307\n",
      "Time taken for 1 epoch: 24.78684687614441 secs\n",
      "\n",
      "Epoch 173 batch 0 train Loss 63.0463 test Loss 28.4482 with MSE metric 21799.2634\n",
      "Epoch 173 batch 10 train Loss 63.0329 test Loss 28.4429 with MSE metric 21796.0554\n",
      "Epoch 173 batch 20 train Loss 63.0196 test Loss 28.4375 with MSE metric 21792.7003\n",
      "Epoch 173 batch 30 train Loss 63.0063 test Loss 28.4322 with MSE metric 21789.3698\n",
      "Epoch 173 batch 40 train Loss 62.9929 test Loss 28.4269 with MSE metric 21786.0511\n",
      "Epoch 173 batch 50 train Loss 62.9796 test Loss 28.4216 with MSE metric 21782.7337\n",
      "Epoch 173 batch 60 train Loss 62.9663 test Loss 28.4163 with MSE metric 21779.5106\n",
      "Epoch 173 batch 70 train Loss 62.9530 test Loss 28.4110 with MSE metric 21776.1756\n",
      "Epoch 173 batch 80 train Loss 62.9397 test Loss 28.4057 with MSE metric 21772.9035\n",
      "Epoch 173 batch 90 train Loss 62.9264 test Loss 28.4004 with MSE metric 21769.6634\n",
      "Epoch 173 batch 100 train Loss 62.9131 test Loss 28.3951 with MSE metric 21766.4535\n",
      "Epoch 173 batch 110 train Loss 62.8998 test Loss 28.3898 with MSE metric 21763.2074\n",
      "Epoch 173 batch 120 train Loss 62.8865 test Loss 28.3845 with MSE metric 21759.9477\n",
      "Epoch 173 batch 130 train Loss 62.8733 test Loss 28.3792 with MSE metric 21756.7058\n",
      "Epoch 173 batch 140 train Loss 62.8600 test Loss 28.3739 with MSE metric 21753.4890\n",
      "Epoch 173 batch 150 train Loss 62.8467 test Loss 28.3686 with MSE metric 21750.2743\n",
      "Epoch 173 batch 160 train Loss 62.8335 test Loss 28.3633 with MSE metric 21747.0443\n",
      "Epoch 173 batch 170 train Loss 62.8202 test Loss 28.3580 with MSE metric 21743.7915\n",
      "Epoch 173 batch 180 train Loss 62.8070 test Loss 28.3527 with MSE metric 21740.5233\n",
      "Epoch 173 batch 190 train Loss 62.7938 test Loss 28.3474 with MSE metric 21737.2659\n",
      "Epoch 173 batch 200 train Loss 62.7805 test Loss 28.3422 with MSE metric 21734.0415\n",
      "Epoch 173 batch 210 train Loss 62.7673 test Loss 28.3369 with MSE metric 21730.8955\n",
      "Epoch 173 batch 220 train Loss 62.7541 test Loss 28.3316 with MSE metric 21727.6812\n",
      "Epoch 173 batch 230 train Loss 62.7409 test Loss 28.3263 with MSE metric 21724.4817\n",
      "Epoch 173 batch 240 train Loss 62.7277 test Loss 28.3210 with MSE metric 21721.1606\n",
      "Time taken for 1 epoch: 25.463083028793335 secs\n",
      "\n",
      "Epoch 174 batch 0 train Loss 62.7145 test Loss 28.3158 with MSE metric 21717.8999\n",
      "Epoch 174 batch 10 train Loss 62.7013 test Loss 28.3106 with MSE metric 21714.6923\n",
      "Epoch 174 batch 20 train Loss 62.6881 test Loss 28.3053 with MSE metric 21711.4566\n",
      "Epoch 174 batch 30 train Loss 62.6749 test Loss 28.3001 with MSE metric 21708.2258\n",
      "Epoch 174 batch 40 train Loss 62.6618 test Loss 28.2948 with MSE metric 21705.0801\n",
      "Epoch 174 batch 50 train Loss 62.6486 test Loss 28.2896 with MSE metric 21701.9558\n",
      "Epoch 174 batch 60 train Loss 62.6355 test Loss 28.2843 with MSE metric 21698.7827\n",
      "Epoch 174 batch 70 train Loss 62.6223 test Loss 28.2791 with MSE metric 21695.5972\n",
      "Epoch 174 batch 80 train Loss 62.6092 test Loss 28.2738 with MSE metric 21692.4155\n",
      "Epoch 174 batch 90 train Loss 62.5960 test Loss 28.2686 with MSE metric 21689.2439\n",
      "Epoch 174 batch 100 train Loss 62.5829 test Loss 28.2633 with MSE metric 21686.0498\n",
      "Epoch 174 batch 110 train Loss 62.5698 test Loss 28.2581 with MSE metric 21682.8349\n",
      "Epoch 174 batch 120 train Loss 62.5566 test Loss 28.2528 with MSE metric 21679.5793\n",
      "Epoch 174 batch 130 train Loss 62.5435 test Loss 28.2476 with MSE metric 21676.4051\n",
      "Epoch 174 batch 140 train Loss 62.5304 test Loss 28.2424 with MSE metric 21673.2271\n",
      "Epoch 174 batch 150 train Loss 62.5173 test Loss 28.2371 with MSE metric 21670.1114\n",
      "Epoch 174 batch 160 train Loss 62.5042 test Loss 28.2319 with MSE metric 21666.9554\n",
      "Epoch 174 batch 170 train Loss 62.4911 test Loss 28.2267 with MSE metric 21663.7618\n",
      "Epoch 174 batch 180 train Loss 62.4780 test Loss 28.2215 with MSE metric 21660.5054\n",
      "Epoch 174 batch 190 train Loss 62.4649 test Loss 28.2163 with MSE metric 21657.2547\n",
      "Epoch 174 batch 200 train Loss 62.4518 test Loss 28.2110 with MSE metric 21654.0567\n",
      "Epoch 174 batch 210 train Loss 62.4388 test Loss 28.2058 with MSE metric 21650.8340\n",
      "Epoch 174 batch 220 train Loss 62.4257 test Loss 28.2006 with MSE metric 21647.6750\n",
      "Epoch 174 batch 230 train Loss 62.4126 test Loss 28.1954 with MSE metric 21644.3759\n",
      "Epoch 174 batch 240 train Loss 62.3996 test Loss 28.1901 with MSE metric 21641.1044\n",
      "Time taken for 1 epoch: 25.632454872131348 secs\n",
      "\n",
      "Epoch 175 batch 0 train Loss 62.3865 test Loss 28.1849 with MSE metric 21637.8684\n",
      "Epoch 175 batch 10 train Loss 62.3735 test Loss 28.1797 with MSE metric 21634.6749\n",
      "Epoch 175 batch 20 train Loss 62.3605 test Loss 28.1745 with MSE metric 21631.4660\n",
      "Epoch 175 batch 30 train Loss 62.3474 test Loss 28.1693 with MSE metric 21628.3093\n",
      "Epoch 175 batch 40 train Loss 62.3344 test Loss 28.1641 with MSE metric 21625.1191\n",
      "Epoch 175 batch 50 train Loss 62.3214 test Loss 28.1589 with MSE metric 21621.9417\n",
      "Epoch 175 batch 60 train Loss 62.3084 test Loss 28.1537 with MSE metric 21618.6617\n",
      "Epoch 175 batch 70 train Loss 62.2954 test Loss 28.1485 with MSE metric 21615.4597\n",
      "Epoch 175 batch 80 train Loss 62.2824 test Loss 28.1433 with MSE metric 21612.2827\n",
      "Epoch 175 batch 90 train Loss 62.2694 test Loss 28.1382 with MSE metric 21609.1488\n",
      "Epoch 175 batch 100 train Loss 62.2564 test Loss 28.1330 with MSE metric 21605.9927\n",
      "Epoch 175 batch 110 train Loss 62.2434 test Loss 28.1278 with MSE metric 21602.7868\n",
      "Epoch 175 batch 120 train Loss 62.2304 test Loss 28.1226 with MSE metric 21599.6237\n",
      "Epoch 175 batch 130 train Loss 62.2175 test Loss 28.1174 with MSE metric 21596.4472\n",
      "Epoch 175 batch 140 train Loss 62.2045 test Loss 28.1122 with MSE metric 21593.3352\n",
      "Epoch 175 batch 150 train Loss 62.1915 test Loss 28.1070 with MSE metric 21590.1905\n",
      "Epoch 175 batch 160 train Loss 62.1786 test Loss 28.1018 with MSE metric 21587.0135\n",
      "Epoch 175 batch 170 train Loss 62.1657 test Loss 28.0967 with MSE metric 21583.8675\n",
      "Epoch 175 batch 180 train Loss 62.1527 test Loss 28.0915 with MSE metric 21580.6863\n",
      "Epoch 175 batch 190 train Loss 62.1398 test Loss 28.0863 with MSE metric 21577.5119\n",
      "Epoch 175 batch 200 train Loss 62.1268 test Loss 28.0812 with MSE metric 21574.2762\n",
      "Epoch 175 batch 210 train Loss 62.1139 test Loss 28.0760 with MSE metric 21571.1057\n",
      "Epoch 175 batch 220 train Loss 62.1010 test Loss 28.0709 with MSE metric 21567.9577\n",
      "Epoch 175 batch 230 train Loss 62.0881 test Loss 28.0657 with MSE metric 21564.7969\n",
      "Epoch 175 batch 240 train Loss 62.0752 test Loss 28.0606 with MSE metric 21561.6175\n",
      "Time taken for 1 epoch: 24.643140077590942 secs\n",
      "\n",
      "Epoch 176 batch 0 train Loss 62.0623 test Loss 28.0554 with MSE metric 21558.4397\n",
      "Epoch 176 batch 10 train Loss 62.0494 test Loss 28.0503 with MSE metric 21555.3055\n",
      "Epoch 176 batch 20 train Loss 62.0365 test Loss 28.0451 with MSE metric 21552.1101\n",
      "Epoch 176 batch 30 train Loss 62.0236 test Loss 28.0400 with MSE metric 21548.9502\n",
      "Epoch 176 batch 40 train Loss 62.0107 test Loss 28.0349 with MSE metric 21545.8009\n",
      "Epoch 176 batch 50 train Loss 61.9979 test Loss 28.0297 with MSE metric 21542.6502\n",
      "Epoch 176 batch 60 train Loss 61.9850 test Loss 28.0246 with MSE metric 21539.5497\n",
      "Epoch 176 batch 70 train Loss 61.9722 test Loss 28.0194 with MSE metric 21536.3986\n",
      "Epoch 176 batch 80 train Loss 61.9593 test Loss 28.0143 with MSE metric 21533.1564\n",
      "Epoch 176 batch 90 train Loss 61.9464 test Loss 28.0092 with MSE metric 21529.8785\n",
      "Epoch 176 batch 100 train Loss 61.9336 test Loss 28.0040 with MSE metric 21526.7706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176 batch 110 train Loss 61.9208 test Loss 27.9989 with MSE metric 21523.6445\n",
      "Epoch 176 batch 120 train Loss 61.9079 test Loss 27.9938 with MSE metric 21520.4598\n",
      "Epoch 176 batch 130 train Loss 61.8951 test Loss 27.9887 with MSE metric 21517.2650\n",
      "Epoch 176 batch 140 train Loss 61.8823 test Loss 27.9835 with MSE metric 21514.1856\n",
      "Epoch 176 batch 150 train Loss 61.8695 test Loss 27.9784 with MSE metric 21511.0620\n",
      "Epoch 176 batch 160 train Loss 61.8567 test Loss 27.9733 with MSE metric 21507.9322\n",
      "Epoch 176 batch 170 train Loss 61.8439 test Loss 27.9682 with MSE metric 21504.8479\n",
      "Epoch 176 batch 180 train Loss 61.8311 test Loss 27.9631 with MSE metric 21501.7121\n",
      "Epoch 176 batch 190 train Loss 61.8183 test Loss 27.9580 with MSE metric 21498.5431\n",
      "Epoch 176 batch 200 train Loss 61.8055 test Loss 27.9529 with MSE metric 21495.4333\n",
      "Epoch 176 batch 210 train Loss 61.7927 test Loss 27.9478 with MSE metric 21492.3017\n",
      "Epoch 176 batch 220 train Loss 61.7800 test Loss 27.9427 with MSE metric 21489.1374\n",
      "Epoch 176 batch 230 train Loss 61.7672 test Loss 27.9376 with MSE metric 21486.0406\n",
      "Epoch 176 batch 240 train Loss 61.7545 test Loss 27.9325 with MSE metric 21482.9730\n",
      "Time taken for 1 epoch: 24.948307991027832 secs\n",
      "\n",
      "Epoch 177 batch 0 train Loss 61.7417 test Loss 27.9274 with MSE metric 21479.8303\n",
      "Epoch 177 batch 10 train Loss 61.7290 test Loss 27.9223 with MSE metric 21476.7609\n",
      "Epoch 177 batch 20 train Loss 61.7162 test Loss 27.9172 with MSE metric 21473.5732\n",
      "Epoch 177 batch 30 train Loss 61.7035 test Loss 27.9121 with MSE metric 21470.4048\n",
      "Epoch 177 batch 40 train Loss 61.6907 test Loss 27.9071 with MSE metric 21467.2946\n",
      "Epoch 177 batch 50 train Loss 61.6780 test Loss 27.9020 with MSE metric 21464.1863\n",
      "Epoch 177 batch 60 train Loss 61.6653 test Loss 27.8969 with MSE metric 21461.0934\n",
      "Epoch 177 batch 70 train Loss 61.6526 test Loss 27.8918 with MSE metric 21458.0362\n",
      "Epoch 177 batch 80 train Loss 61.6399 test Loss 27.8868 with MSE metric 21454.9504\n",
      "Epoch 177 batch 90 train Loss 61.6272 test Loss 27.8817 with MSE metric 21451.8862\n",
      "Epoch 177 batch 100 train Loss 61.6145 test Loss 27.8766 with MSE metric 21448.8637\n",
      "Epoch 177 batch 110 train Loss 61.6018 test Loss 27.8715 with MSE metric 21445.7531\n",
      "Epoch 177 batch 120 train Loss 61.5891 test Loss 27.8665 with MSE metric 21442.5969\n",
      "Epoch 177 batch 130 train Loss 61.5764 test Loss 27.8614 with MSE metric 21439.4590\n",
      "Epoch 177 batch 140 train Loss 61.5638 test Loss 27.8564 with MSE metric 21436.2989\n",
      "Epoch 177 batch 150 train Loss 61.5511 test Loss 27.8513 with MSE metric 21433.2069\n",
      "Epoch 177 batch 160 train Loss 61.5384 test Loss 27.8463 with MSE metric 21430.1652\n",
      "Epoch 177 batch 170 train Loss 61.5258 test Loss 27.8412 with MSE metric 21427.1032\n",
      "Epoch 177 batch 180 train Loss 61.5131 test Loss 27.8362 with MSE metric 21423.9769\n",
      "Epoch 177 batch 190 train Loss 61.5005 test Loss 27.8311 with MSE metric 21420.8706\n",
      "Epoch 177 batch 200 train Loss 61.4878 test Loss 27.8261 with MSE metric 21417.8213\n",
      "Epoch 177 batch 210 train Loss 61.4752 test Loss 27.8210 with MSE metric 21414.6823\n",
      "Epoch 177 batch 220 train Loss 61.4626 test Loss 27.8160 with MSE metric 21411.6175\n",
      "Epoch 177 batch 230 train Loss 61.4500 test Loss 27.8109 with MSE metric 21408.6121\n",
      "Epoch 177 batch 240 train Loss 61.4373 test Loss 27.8059 with MSE metric 21405.6064\n",
      "Time taken for 1 epoch: 24.267904043197632 secs\n",
      "\n",
      "Epoch 178 batch 0 train Loss 61.4247 test Loss 27.8009 with MSE metric 21402.5493\n",
      "Epoch 178 batch 10 train Loss 61.4121 test Loss 27.7959 with MSE metric 21399.4806\n",
      "Epoch 178 batch 20 train Loss 61.3995 test Loss 27.7909 with MSE metric 21396.3969\n",
      "Epoch 178 batch 30 train Loss 61.3869 test Loss 27.7859 with MSE metric 21393.3212\n",
      "Epoch 178 batch 40 train Loss 61.3744 test Loss 27.7808 with MSE metric 21390.3226\n",
      "Epoch 178 batch 50 train Loss 61.3618 test Loss 27.7758 with MSE metric 21387.2248\n",
      "Epoch 178 batch 60 train Loss 61.3492 test Loss 27.7708 with MSE metric 21384.1668\n",
      "Epoch 178 batch 70 train Loss 61.3366 test Loss 27.7658 with MSE metric 21381.0848\n",
      "Epoch 178 batch 80 train Loss 61.3241 test Loss 27.7608 with MSE metric 21377.9732\n",
      "Epoch 178 batch 90 train Loss 61.3115 test Loss 27.7558 with MSE metric 21374.8775\n",
      "Epoch 178 batch 100 train Loss 61.2990 test Loss 27.7507 with MSE metric 21371.8391\n",
      "Epoch 178 batch 110 train Loss 61.2864 test Loss 27.7457 with MSE metric 21368.8481\n",
      "Epoch 178 batch 120 train Loss 61.2739 test Loss 27.7407 with MSE metric 21365.8533\n",
      "Epoch 178 batch 130 train Loss 61.2613 test Loss 27.7357 with MSE metric 21362.7707\n",
      "Epoch 178 batch 140 train Loss 61.2488 test Loss 27.7307 with MSE metric 21359.7481\n",
      "Epoch 178 batch 150 train Loss 61.2363 test Loss 27.7257 with MSE metric 21356.7550\n",
      "Epoch 178 batch 160 train Loss 61.2238 test Loss 27.7207 with MSE metric 21353.6677\n",
      "Epoch 178 batch 170 train Loss 61.2112 test Loss 27.7157 with MSE metric 21350.6359\n",
      "Epoch 178 batch 180 train Loss 61.1987 test Loss 27.7107 with MSE metric 21347.5470\n",
      "Epoch 178 batch 190 train Loss 61.1862 test Loss 27.7057 with MSE metric 21344.4858\n",
      "Epoch 178 batch 200 train Loss 61.1737 test Loss 27.7007 with MSE metric 21341.3779\n",
      "Epoch 178 batch 210 train Loss 61.1613 test Loss 27.6957 with MSE metric 21338.3926\n",
      "Epoch 178 batch 220 train Loss 61.1488 test Loss 27.6907 with MSE metric 21335.2734\n",
      "Epoch 178 batch 230 train Loss 61.1363 test Loss 27.6857 with MSE metric 21332.1947\n",
      "Epoch 178 batch 240 train Loss 61.1238 test Loss 27.6807 with MSE metric 21329.1662\n",
      "Time taken for 1 epoch: 24.051868200302124 secs\n",
      "\n",
      "Epoch 179 batch 0 train Loss 61.1113 test Loss 27.6757 with MSE metric 21326.0818\n",
      "Epoch 179 batch 10 train Loss 61.0989 test Loss 27.6708 with MSE metric 21323.0510\n",
      "Epoch 179 batch 20 train Loss 61.0864 test Loss 27.6658 with MSE metric 21319.9941\n",
      "Epoch 179 batch 30 train Loss 61.0740 test Loss 27.6608 with MSE metric 21316.9469\n",
      "Epoch 179 batch 40 train Loss 61.0615 test Loss 27.6559 with MSE metric 21313.9473\n",
      "Epoch 179 batch 50 train Loss 61.0491 test Loss 27.6509 with MSE metric 21310.8950\n",
      "Epoch 179 batch 60 train Loss 61.0367 test Loss 27.6459 with MSE metric 21307.8920\n",
      "Epoch 179 batch 70 train Loss 61.0242 test Loss 27.6410 with MSE metric 21304.8186\n",
      "Epoch 179 batch 80 train Loss 61.0118 test Loss 27.6360 with MSE metric 21301.7145\n",
      "Epoch 179 batch 90 train Loss 60.9994 test Loss 27.6310 with MSE metric 21298.6755\n",
      "Epoch 179 batch 100 train Loss 60.9869 test Loss 27.6261 with MSE metric 21295.6780\n",
      "Epoch 179 batch 110 train Loss 60.9745 test Loss 27.6211 with MSE metric 21292.6246\n",
      "Epoch 179 batch 120 train Loss 60.9621 test Loss 27.6162 with MSE metric 21289.6213\n",
      "Epoch 179 batch 130 train Loss 60.9497 test Loss 27.6112 with MSE metric 21286.5651\n",
      "Epoch 179 batch 140 train Loss 60.9373 test Loss 27.6063 with MSE metric 21283.4481\n",
      "Epoch 179 batch 150 train Loss 60.9249 test Loss 27.6013 with MSE metric 21280.3978\n",
      "Epoch 179 batch 160 train Loss 60.9126 test Loss 27.5964 with MSE metric 21277.4259\n",
      "Epoch 179 batch 170 train Loss 60.9002 test Loss 27.5914 with MSE metric 21274.4021\n",
      "Epoch 179 batch 180 train Loss 60.8878 test Loss 27.5865 with MSE metric 21271.3060\n",
      "Epoch 179 batch 190 train Loss 60.8754 test Loss 27.5815 with MSE metric 21268.2312\n",
      "Epoch 179 batch 200 train Loss 60.8631 test Loss 27.5766 with MSE metric 21265.2351\n",
      "Epoch 179 batch 210 train Loss 60.8507 test Loss 27.5717 with MSE metric 21262.2239\n",
      "Epoch 179 batch 220 train Loss 60.8384 test Loss 27.5668 with MSE metric 21259.1782\n",
      "Epoch 179 batch 230 train Loss 60.8261 test Loss 27.5618 with MSE metric 21256.2022\n",
      "Epoch 179 batch 240 train Loss 60.8137 test Loss 27.5569 with MSE metric 21253.1336\n",
      "Time taken for 1 epoch: 24.79691219329834 secs\n",
      "\n",
      "Epoch 180 batch 0 train Loss 60.8014 test Loss 27.5520 with MSE metric 21250.0988\n",
      "Epoch 180 batch 10 train Loss 60.7891 test Loss 27.5470 with MSE metric 21247.0980\n",
      "Epoch 180 batch 20 train Loss 60.7767 test Loss 27.5421 with MSE metric 21244.0295\n",
      "Epoch 180 batch 30 train Loss 60.7644 test Loss 27.5372 with MSE metric 21241.0594\n",
      "Epoch 180 batch 40 train Loss 60.7521 test Loss 27.5322 with MSE metric 21238.0648\n",
      "Epoch 180 batch 50 train Loss 60.7398 test Loss 27.5273 with MSE metric 21235.0546\n",
      "Epoch 180 batch 60 train Loss 60.7275 test Loss 27.5224 with MSE metric 21232.1280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180 batch 70 train Loss 60.7152 test Loss 27.5176 with MSE metric 21229.1702\n",
      "Epoch 180 batch 80 train Loss 60.7029 test Loss 27.5127 with MSE metric 21226.1590\n",
      "Epoch 180 batch 90 train Loss 60.6907 test Loss 27.5078 with MSE metric 21223.2083\n",
      "Epoch 180 batch 100 train Loss 60.6784 test Loss 27.5028 with MSE metric 21220.2534\n",
      "Epoch 180 batch 110 train Loss 60.6661 test Loss 27.4980 with MSE metric 21217.2534\n",
      "Epoch 180 batch 120 train Loss 60.6538 test Loss 27.4931 with MSE metric 21214.2411\n",
      "Epoch 180 batch 130 train Loss 60.6416 test Loss 27.4882 with MSE metric 21211.2321\n",
      "Epoch 180 batch 140 train Loss 60.6293 test Loss 27.4833 with MSE metric 21208.3549\n",
      "Epoch 180 batch 150 train Loss 60.6171 test Loss 27.4784 with MSE metric 21205.2903\n",
      "Epoch 180 batch 160 train Loss 60.6048 test Loss 27.4735 with MSE metric 21202.3230\n",
      "Epoch 180 batch 170 train Loss 60.5926 test Loss 27.4686 with MSE metric 21199.3990\n",
      "Epoch 180 batch 180 train Loss 60.5803 test Loss 27.4637 with MSE metric 21196.3693\n",
      "Epoch 180 batch 190 train Loss 60.5681 test Loss 27.4588 with MSE metric 21193.3495\n",
      "Epoch 180 batch 200 train Loss 60.5559 test Loss 27.4540 with MSE metric 21190.3173\n",
      "Epoch 180 batch 210 train Loss 60.5437 test Loss 27.4491 with MSE metric 21187.3334\n",
      "Epoch 180 batch 220 train Loss 60.5315 test Loss 27.4442 with MSE metric 21184.2930\n",
      "Epoch 180 batch 230 train Loss 60.5192 test Loss 27.4393 with MSE metric 21181.2940\n",
      "Epoch 180 batch 240 train Loss 60.5071 test Loss 27.4345 with MSE metric 21178.3469\n",
      "Time taken for 1 epoch: 25.46008276939392 secs\n",
      "\n",
      "Epoch 181 batch 0 train Loss 60.4949 test Loss 27.4296 with MSE metric 21175.4073\n",
      "Epoch 181 batch 10 train Loss 60.4827 test Loss 27.4247 with MSE metric 21172.4075\n",
      "Epoch 181 batch 20 train Loss 60.4705 test Loss 27.4199 with MSE metric 21169.4741\n",
      "Epoch 181 batch 30 train Loss 60.4583 test Loss 27.4150 with MSE metric 21166.4967\n",
      "Epoch 181 batch 40 train Loss 60.4461 test Loss 27.4101 with MSE metric 21163.4939\n",
      "Epoch 181 batch 50 train Loss 60.4340 test Loss 27.4053 with MSE metric 21160.5924\n",
      "Epoch 181 batch 60 train Loss 60.4218 test Loss 27.4004 with MSE metric 21157.6141\n",
      "Epoch 181 batch 70 train Loss 60.4097 test Loss 27.3956 with MSE metric 21154.6919\n",
      "Epoch 181 batch 80 train Loss 60.3975 test Loss 27.3907 with MSE metric 21151.6794\n",
      "Epoch 181 batch 90 train Loss 60.3854 test Loss 27.3859 with MSE metric 21148.7919\n",
      "Epoch 181 batch 100 train Loss 60.3732 test Loss 27.3810 with MSE metric 21145.8270\n",
      "Epoch 181 batch 110 train Loss 60.3611 test Loss 27.3762 with MSE metric 21142.8571\n",
      "Epoch 181 batch 120 train Loss 60.3490 test Loss 27.3713 with MSE metric 21139.9179\n",
      "Epoch 181 batch 130 train Loss 60.3368 test Loss 27.3665 with MSE metric 21137.0319\n",
      "Epoch 181 batch 140 train Loss 60.3247 test Loss 27.3616 with MSE metric 21134.0701\n",
      "Epoch 181 batch 150 train Loss 60.3126 test Loss 27.3568 with MSE metric 21131.1196\n",
      "Epoch 181 batch 160 train Loss 60.3005 test Loss 27.3520 with MSE metric 21128.1584\n",
      "Epoch 181 batch 170 train Loss 60.2884 test Loss 27.3471 with MSE metric 21125.1432\n",
      "Epoch 181 batch 180 train Loss 60.2763 test Loss 27.3423 with MSE metric 21122.1262\n",
      "Epoch 181 batch 190 train Loss 60.2642 test Loss 27.3375 with MSE metric 21119.1557\n",
      "Epoch 181 batch 200 train Loss 60.2521 test Loss 27.3327 with MSE metric 21116.2040\n",
      "Epoch 181 batch 210 train Loss 60.2400 test Loss 27.3279 with MSE metric 21113.2802\n",
      "Epoch 181 batch 220 train Loss 60.2279 test Loss 27.3231 with MSE metric 21110.3129\n",
      "Epoch 181 batch 230 train Loss 60.2159 test Loss 27.3182 with MSE metric 21107.3716\n",
      "Epoch 181 batch 240 train Loss 60.2038 test Loss 27.3134 with MSE metric 21104.4174\n",
      "Time taken for 1 epoch: 25.589177131652832 secs\n",
      "\n",
      "Epoch 182 batch 0 train Loss 60.1917 test Loss 27.3086 with MSE metric 21101.4572\n",
      "Epoch 182 batch 10 train Loss 60.1797 test Loss 27.3038 with MSE metric 21098.5236\n",
      "Epoch 182 batch 20 train Loss 60.1676 test Loss 27.2990 with MSE metric 21095.5520\n",
      "Epoch 182 batch 30 train Loss 60.1556 test Loss 27.2942 with MSE metric 21092.5677\n",
      "Epoch 182 batch 40 train Loss 60.1435 test Loss 27.2893 with MSE metric 21089.6309\n",
      "Epoch 182 batch 50 train Loss 60.1315 test Loss 27.2845 with MSE metric 21086.7308\n",
      "Epoch 182 batch 60 train Loss 60.1195 test Loss 27.2797 with MSE metric 21083.7806\n",
      "Epoch 182 batch 70 train Loss 60.1074 test Loss 27.2749 with MSE metric 21080.8245\n",
      "Epoch 182 batch 80 train Loss 60.0954 test Loss 27.2701 with MSE metric 21077.8597\n",
      "Epoch 182 batch 90 train Loss 60.0834 test Loss 27.2653 with MSE metric 21074.8562\n",
      "Epoch 182 batch 100 train Loss 60.0714 test Loss 27.2605 with MSE metric 21071.9031\n",
      "Epoch 182 batch 110 train Loss 60.0594 test Loss 27.2558 with MSE metric 21069.0018\n",
      "Epoch 182 batch 120 train Loss 60.0474 test Loss 27.2510 with MSE metric 21066.0133\n",
      "Epoch 182 batch 130 train Loss 60.0354 test Loss 27.2462 with MSE metric 21063.0759\n",
      "Epoch 182 batch 140 train Loss 60.0234 test Loss 27.2414 with MSE metric 21060.1072\n",
      "Epoch 182 batch 150 train Loss 60.0114 test Loss 27.2366 with MSE metric 21057.1961\n",
      "Epoch 182 batch 160 train Loss 59.9994 test Loss 27.2319 with MSE metric 21054.2777\n",
      "Epoch 182 batch 170 train Loss 59.9875 test Loss 27.2271 with MSE metric 21051.2736\n",
      "Epoch 182 batch 180 train Loss 59.9755 test Loss 27.2223 with MSE metric 21048.3585\n",
      "Epoch 182 batch 190 train Loss 59.9635 test Loss 27.2176 with MSE metric 21045.4141\n",
      "Epoch 182 batch 200 train Loss 59.9516 test Loss 27.2128 with MSE metric 21042.6500\n",
      "Epoch 182 batch 210 train Loss 59.9396 test Loss 27.2080 with MSE metric 21039.6828\n",
      "Epoch 182 batch 220 train Loss 59.9277 test Loss 27.2032 with MSE metric 21036.7271\n",
      "Epoch 182 batch 230 train Loss 59.9157 test Loss 27.1985 with MSE metric 21033.8061\n",
      "Epoch 182 batch 240 train Loss 59.9038 test Loss 27.1937 with MSE metric 21030.9463\n",
      "Time taken for 1 epoch: 25.209463119506836 secs\n",
      "\n",
      "Epoch 183 batch 0 train Loss 59.8919 test Loss 27.1889 with MSE metric 21028.0340\n",
      "Epoch 183 batch 10 train Loss 59.8800 test Loss 27.1842 with MSE metric 21025.1120\n",
      "Epoch 183 batch 20 train Loss 59.8681 test Loss 27.1794 with MSE metric 21022.2449\n",
      "Epoch 183 batch 30 train Loss 59.8561 test Loss 27.1747 with MSE metric 21019.2927\n",
      "Epoch 183 batch 40 train Loss 59.8442 test Loss 27.1699 with MSE metric 21016.3802\n",
      "Epoch 183 batch 50 train Loss 59.8323 test Loss 27.1652 with MSE metric 21013.5640\n",
      "Epoch 183 batch 60 train Loss 59.8204 test Loss 27.1604 with MSE metric 21010.6689\n",
      "Epoch 183 batch 70 train Loss 59.8085 test Loss 27.1557 with MSE metric 21007.7119\n",
      "Epoch 183 batch 80 train Loss 59.7966 test Loss 27.1510 with MSE metric 21004.7779\n",
      "Epoch 183 batch 90 train Loss 59.7848 test Loss 27.1462 with MSE metric 21001.8859\n",
      "Epoch 183 batch 100 train Loss 59.7729 test Loss 27.1415 with MSE metric 20998.9790\n",
      "Epoch 183 batch 110 train Loss 59.7610 test Loss 27.1368 with MSE metric 20996.1119\n",
      "Epoch 183 batch 120 train Loss 59.7491 test Loss 27.1320 with MSE metric 20993.1994\n",
      "Epoch 183 batch 130 train Loss 59.7373 test Loss 27.1273 with MSE metric 20990.1930\n",
      "Epoch 183 batch 140 train Loss 59.7254 test Loss 27.1226 with MSE metric 20987.2997\n",
      "Epoch 183 batch 150 train Loss 59.7136 test Loss 27.1178 with MSE metric 20984.4009\n",
      "Epoch 183 batch 160 train Loss 59.7017 test Loss 27.1131 with MSE metric 20981.4632\n",
      "Epoch 183 batch 170 train Loss 59.6899 test Loss 27.1084 with MSE metric 20978.5925\n",
      "Epoch 183 batch 180 train Loss 59.6780 test Loss 27.1036 with MSE metric 20975.7264\n",
      "Epoch 183 batch 190 train Loss 59.6662 test Loss 27.0989 with MSE metric 20972.8272\n",
      "Epoch 183 batch 200 train Loss 59.6544 test Loss 27.0942 with MSE metric 20969.9261\n",
      "Epoch 183 batch 210 train Loss 59.6425 test Loss 27.0895 with MSE metric 20966.9790\n",
      "Epoch 183 batch 220 train Loss 59.6307 test Loss 27.0848 with MSE metric 20964.0745\n",
      "Epoch 183 batch 230 train Loss 59.6189 test Loss 27.0800 with MSE metric 20961.1738\n",
      "Epoch 183 batch 240 train Loss 59.6071 test Loss 27.0753 with MSE metric 20958.2540\n",
      "Time taken for 1 epoch: 24.637139320373535 secs\n",
      "\n",
      "Epoch 184 batch 0 train Loss 59.5953 test Loss 27.0706 with MSE metric 20955.3073\n",
      "Epoch 184 batch 10 train Loss 59.5835 test Loss 27.0659 with MSE metric 20952.3838\n",
      "Epoch 184 batch 20 train Loss 59.5717 test Loss 27.0612 with MSE metric 20949.5654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184 batch 30 train Loss 59.5599 test Loss 27.0565 with MSE metric 20946.6691\n",
      "Epoch 184 batch 40 train Loss 59.5481 test Loss 27.0518 with MSE metric 20943.8020\n",
      "Epoch 184 batch 50 train Loss 59.5364 test Loss 27.0471 with MSE metric 20940.9595\n",
      "Epoch 184 batch 60 train Loss 59.5246 test Loss 27.0424 with MSE metric 20938.0922\n",
      "Epoch 184 batch 70 train Loss 59.5128 test Loss 27.0377 with MSE metric 20935.2275\n",
      "Epoch 184 batch 80 train Loss 59.5011 test Loss 27.0330 with MSE metric 20932.3323\n",
      "Epoch 184 batch 90 train Loss 59.4893 test Loss 27.0283 with MSE metric 20929.4930\n",
      "Epoch 184 batch 100 train Loss 59.4776 test Loss 27.0236 with MSE metric 20926.6220\n",
      "Epoch 184 batch 110 train Loss 59.4658 test Loss 27.0189 with MSE metric 20923.8405\n",
      "Epoch 184 batch 120 train Loss 59.4541 test Loss 27.0142 with MSE metric 20921.0501\n",
      "Epoch 184 batch 130 train Loss 59.4423 test Loss 27.0095 with MSE metric 20918.1457\n",
      "Epoch 184 batch 140 train Loss 59.4306 test Loss 27.0048 with MSE metric 20915.2586\n",
      "Epoch 184 batch 150 train Loss 59.4189 test Loss 27.0002 with MSE metric 20912.3773\n",
      "Epoch 184 batch 160 train Loss 59.4072 test Loss 26.9955 with MSE metric 20909.5312\n",
      "Epoch 184 batch 170 train Loss 59.3955 test Loss 26.9908 with MSE metric 20906.6149\n",
      "Epoch 184 batch 180 train Loss 59.3837 test Loss 26.9861 with MSE metric 20903.7741\n",
      "Epoch 184 batch 190 train Loss 59.3720 test Loss 26.9815 with MSE metric 20900.9454\n",
      "Epoch 184 batch 200 train Loss 59.3603 test Loss 26.9768 with MSE metric 20898.0243\n",
      "Epoch 184 batch 210 train Loss 59.3487 test Loss 26.9721 with MSE metric 20895.1756\n",
      "Epoch 184 batch 220 train Loss 59.3370 test Loss 26.9675 with MSE metric 20892.2904\n",
      "Epoch 184 batch 230 train Loss 59.3253 test Loss 26.9628 with MSE metric 20889.4144\n",
      "Epoch 184 batch 240 train Loss 59.3136 test Loss 26.9582 with MSE metric 20886.5791\n",
      "Time taken for 1 epoch: 24.24854588508606 secs\n",
      "\n",
      "Epoch 185 batch 0 train Loss 59.3019 test Loss 26.9535 with MSE metric 20883.7192\n",
      "Epoch 185 batch 10 train Loss 59.2903 test Loss 26.9488 with MSE metric 20880.9306\n",
      "Epoch 185 batch 20 train Loss 59.2786 test Loss 26.9442 with MSE metric 20878.1045\n",
      "Epoch 185 batch 30 train Loss 59.2669 test Loss 26.9395 with MSE metric 20875.2261\n",
      "Epoch 185 batch 40 train Loss 59.2553 test Loss 26.9349 with MSE metric 20872.3282\n",
      "Epoch 185 batch 50 train Loss 59.2436 test Loss 26.9302 with MSE metric 20869.3858\n",
      "Epoch 185 batch 60 train Loss 59.2320 test Loss 26.9256 with MSE metric 20866.4888\n",
      "Epoch 185 batch 70 train Loss 59.2203 test Loss 26.9209 with MSE metric 20863.6381\n",
      "Epoch 185 batch 80 train Loss 59.2087 test Loss 26.9163 with MSE metric 20860.7186\n",
      "Epoch 185 batch 90 train Loss 59.1971 test Loss 26.9116 with MSE metric 20857.8866\n",
      "Epoch 185 batch 100 train Loss 59.1854 test Loss 26.9070 with MSE metric 20855.0997\n",
      "Epoch 185 batch 110 train Loss 59.1738 test Loss 26.9023 with MSE metric 20852.2668\n",
      "Epoch 185 batch 120 train Loss 59.1622 test Loss 26.8977 with MSE metric 20849.4632\n",
      "Epoch 185 batch 130 train Loss 59.1506 test Loss 26.8931 with MSE metric 20846.6378\n",
      "Epoch 185 batch 140 train Loss 59.1390 test Loss 26.8884 with MSE metric 20843.7701\n",
      "Epoch 185 batch 150 train Loss 59.1274 test Loss 26.8838 with MSE metric 20840.9040\n",
      "Epoch 185 batch 160 train Loss 59.1158 test Loss 26.8792 with MSE metric 20838.0162\n",
      "Epoch 185 batch 170 train Loss 59.1042 test Loss 26.8746 with MSE metric 20835.1876\n",
      "Epoch 185 batch 180 train Loss 59.0926 test Loss 26.8700 with MSE metric 20832.4114\n",
      "Epoch 185 batch 190 train Loss 59.0810 test Loss 26.8653 with MSE metric 20829.5950\n",
      "Epoch 185 batch 200 train Loss 59.0695 test Loss 26.8607 with MSE metric 20826.7772\n",
      "Epoch 185 batch 210 train Loss 59.0579 test Loss 26.8561 with MSE metric 20823.9501\n",
      "Epoch 185 batch 220 train Loss 59.0463 test Loss 26.8515 with MSE metric 20821.1381\n",
      "Epoch 185 batch 230 train Loss 59.0348 test Loss 26.8469 with MSE metric 20818.3158\n",
      "Epoch 185 batch 240 train Loss 59.0232 test Loss 26.8423 with MSE metric 20815.5065\n",
      "Time taken for 1 epoch: 23.844959020614624 secs\n",
      "\n",
      "Epoch 186 batch 0 train Loss 59.0117 test Loss 26.8377 with MSE metric 20812.6656\n",
      "Epoch 186 batch 10 train Loss 59.0001 test Loss 26.8331 with MSE metric 20809.8560\n",
      "Epoch 186 batch 20 train Loss 58.9886 test Loss 26.8285 with MSE metric 20807.0099\n",
      "Epoch 186 batch 30 train Loss 58.9770 test Loss 26.8239 with MSE metric 20804.1627\n",
      "Epoch 186 batch 40 train Loss 58.9655 test Loss 26.8193 with MSE metric 20801.2777\n",
      "Epoch 186 batch 50 train Loss 58.9540 test Loss 26.8146 with MSE metric 20798.4372\n",
      "Epoch 186 batch 60 train Loss 58.9425 test Loss 26.8101 with MSE metric 20795.6380\n",
      "Epoch 186 batch 70 train Loss 58.9310 test Loss 26.8055 with MSE metric 20792.8398\n",
      "Epoch 186 batch 80 train Loss 58.9195 test Loss 26.8009 with MSE metric 20789.9886\n",
      "Epoch 186 batch 90 train Loss 58.9079 test Loss 26.7963 with MSE metric 20787.1436\n",
      "Epoch 186 batch 100 train Loss 58.8964 test Loss 26.7917 with MSE metric 20784.2945\n",
      "Epoch 186 batch 110 train Loss 58.8850 test Loss 26.7871 with MSE metric 20781.4982\n",
      "Epoch 186 batch 120 train Loss 58.8735 test Loss 26.7825 with MSE metric 20778.7730\n",
      "Epoch 186 batch 130 train Loss 58.8620 test Loss 26.7779 with MSE metric 20775.9506\n",
      "Epoch 186 batch 140 train Loss 58.8505 test Loss 26.7733 with MSE metric 20773.0924\n",
      "Epoch 186 batch 150 train Loss 58.8390 test Loss 26.7687 with MSE metric 20770.2446\n",
      "Epoch 186 batch 160 train Loss 58.8276 test Loss 26.7641 with MSE metric 20767.4178\n",
      "Epoch 186 batch 170 train Loss 58.8161 test Loss 26.7595 with MSE metric 20764.7035\n",
      "Epoch 186 batch 180 train Loss 58.8046 test Loss 26.7550 with MSE metric 20761.9084\n",
      "Epoch 186 batch 190 train Loss 58.7932 test Loss 26.7504 with MSE metric 20759.0731\n",
      "Epoch 186 batch 200 train Loss 58.7817 test Loss 26.7458 with MSE metric 20756.3281\n",
      "Epoch 186 batch 210 train Loss 58.7703 test Loss 26.7412 with MSE metric 20753.4640\n",
      "Epoch 186 batch 220 train Loss 58.7589 test Loss 26.7367 with MSE metric 20750.7152\n",
      "Epoch 186 batch 230 train Loss 58.7474 test Loss 26.7321 with MSE metric 20747.9112\n",
      "Epoch 186 batch 240 train Loss 58.7360 test Loss 26.7276 with MSE metric 20745.1096\n",
      "Time taken for 1 epoch: 24.422606229782104 secs\n",
      "\n",
      "Epoch 187 batch 0 train Loss 58.7246 test Loss 26.7230 with MSE metric 20742.3493\n",
      "Epoch 187 batch 10 train Loss 58.7132 test Loss 26.7184 with MSE metric 20739.5346\n",
      "Epoch 187 batch 20 train Loss 58.7017 test Loss 26.7139 with MSE metric 20736.7892\n",
      "Epoch 187 batch 30 train Loss 58.6903 test Loss 26.7093 with MSE metric 20733.9496\n",
      "Epoch 187 batch 40 train Loss 58.6789 test Loss 26.7048 with MSE metric 20731.1599\n",
      "Epoch 187 batch 50 train Loss 58.6675 test Loss 26.7002 with MSE metric 20728.3225\n",
      "Epoch 187 batch 60 train Loss 58.6561 test Loss 26.6957 with MSE metric 20725.5052\n",
      "Epoch 187 batch 70 train Loss 58.6447 test Loss 26.6912 with MSE metric 20722.7572\n",
      "Epoch 187 batch 80 train Loss 58.6334 test Loss 26.6866 with MSE metric 20719.9283\n",
      "Epoch 187 batch 90 train Loss 58.6220 test Loss 26.6821 with MSE metric 20717.1834\n",
      "Epoch 187 batch 100 train Loss 58.6106 test Loss 26.6775 with MSE metric 20714.4698\n",
      "Epoch 187 batch 110 train Loss 58.5992 test Loss 26.6730 with MSE metric 20711.6449\n",
      "Epoch 187 batch 120 train Loss 58.5879 test Loss 26.6685 with MSE metric 20708.8808\n",
      "Epoch 187 batch 130 train Loss 58.5765 test Loss 26.6639 with MSE metric 20706.1539\n",
      "Epoch 187 batch 140 train Loss 58.5651 test Loss 26.6594 with MSE metric 20703.4305\n",
      "Epoch 187 batch 150 train Loss 58.5538 test Loss 26.6548 with MSE metric 20700.6258\n",
      "Epoch 187 batch 160 train Loss 58.5424 test Loss 26.6503 with MSE metric 20697.8579\n",
      "Epoch 187 batch 170 train Loss 58.5311 test Loss 26.6458 with MSE metric 20695.1296\n",
      "Epoch 187 batch 180 train Loss 58.5198 test Loss 26.6413 with MSE metric 20692.3176\n",
      "Epoch 187 batch 190 train Loss 58.5084 test Loss 26.6367 with MSE metric 20689.5698\n",
      "Epoch 187 batch 200 train Loss 58.4971 test Loss 26.6322 with MSE metric 20686.7802\n",
      "Epoch 187 batch 210 train Loss 58.4858 test Loss 26.6277 with MSE metric 20683.9931\n",
      "Epoch 187 batch 220 train Loss 58.4745 test Loss 26.6232 with MSE metric 20681.2394\n",
      "Epoch 187 batch 230 train Loss 58.4631 test Loss 26.6186 with MSE metric 20678.5382\n",
      "Epoch 187 batch 240 train Loss 58.4518 test Loss 26.6141 with MSE metric 20675.7648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 26.134443998336792 secs\n",
      "\n",
      "Epoch 188 batch 0 train Loss 58.4405 test Loss 26.6096 with MSE metric 20672.9529\n",
      "Epoch 188 batch 10 train Loss 58.4292 test Loss 26.6051 with MSE metric 20670.1348\n",
      "Epoch 188 batch 20 train Loss 58.4179 test Loss 26.6006 with MSE metric 20667.3541\n",
      "Epoch 188 batch 30 train Loss 58.4066 test Loss 26.5961 with MSE metric 20664.5860\n",
      "Epoch 188 batch 40 train Loss 58.3953 test Loss 26.5916 with MSE metric 20661.9005\n",
      "Epoch 188 batch 50 train Loss 58.3841 test Loss 26.5871 with MSE metric 20659.0923\n",
      "Epoch 188 batch 60 train Loss 58.3728 test Loss 26.5826 with MSE metric 20656.3645\n",
      "Epoch 188 batch 70 train Loss 58.3615 test Loss 26.5781 with MSE metric 20653.6077\n",
      "Epoch 188 batch 80 train Loss 58.3503 test Loss 26.5736 with MSE metric 20650.8816\n",
      "Epoch 188 batch 90 train Loss 58.3390 test Loss 26.5691 with MSE metric 20648.1696\n",
      "Epoch 188 batch 100 train Loss 58.3278 test Loss 26.5646 with MSE metric 20645.4166\n",
      "Epoch 188 batch 110 train Loss 58.3165 test Loss 26.5601 with MSE metric 20642.6389\n",
      "Epoch 188 batch 120 train Loss 58.3053 test Loss 26.5556 with MSE metric 20639.9050\n",
      "Epoch 188 batch 130 train Loss 58.2940 test Loss 26.5511 with MSE metric 20637.1079\n",
      "Epoch 188 batch 140 train Loss 58.2828 test Loss 26.5466 with MSE metric 20634.3779\n",
      "Epoch 188 batch 150 train Loss 58.2716 test Loss 26.5422 with MSE metric 20631.6621\n",
      "Epoch 188 batch 160 train Loss 58.2603 test Loss 26.5377 with MSE metric 20628.8256\n",
      "Epoch 188 batch 170 train Loss 58.2491 test Loss 26.5332 with MSE metric 20626.1148\n",
      "Epoch 188 batch 180 train Loss 58.2379 test Loss 26.5287 with MSE metric 20623.3136\n",
      "Epoch 188 batch 190 train Loss 58.2267 test Loss 26.5242 with MSE metric 20620.5800\n",
      "Epoch 188 batch 200 train Loss 58.2155 test Loss 26.5198 with MSE metric 20617.8600\n",
      "Epoch 188 batch 210 train Loss 58.2042 test Loss 26.5153 with MSE metric 20615.0964\n",
      "Epoch 188 batch 220 train Loss 58.1930 test Loss 26.5108 with MSE metric 20612.3885\n",
      "Epoch 188 batch 230 train Loss 58.1819 test Loss 26.5063 with MSE metric 20609.6211\n",
      "Epoch 188 batch 240 train Loss 58.1707 test Loss 26.5018 with MSE metric 20606.8950\n",
      "Time taken for 1 epoch: 25.96453595161438 secs\n",
      "\n",
      "Epoch 189 batch 0 train Loss 58.1595 test Loss 26.4974 with MSE metric 20604.1245\n",
      "Epoch 189 batch 10 train Loss 58.1483 test Loss 26.4929 with MSE metric 20601.4069\n",
      "Epoch 189 batch 20 train Loss 58.1371 test Loss 26.4884 with MSE metric 20598.5978\n",
      "Epoch 189 batch 30 train Loss 58.1259 test Loss 26.4840 with MSE metric 20595.8793\n",
      "Epoch 189 batch 40 train Loss 58.1148 test Loss 26.4795 with MSE metric 20593.1398\n",
      "Epoch 189 batch 50 train Loss 58.1036 test Loss 26.4750 with MSE metric 20590.4323\n",
      "Epoch 189 batch 60 train Loss 58.0925 test Loss 26.4706 with MSE metric 20587.7782\n",
      "Epoch 189 batch 70 train Loss 58.0813 test Loss 26.4661 with MSE metric 20585.1020\n",
      "Epoch 189 batch 80 train Loss 58.0702 test Loss 26.4617 with MSE metric 20582.2867\n",
      "Epoch 189 batch 90 train Loss 58.0590 test Loss 26.4572 with MSE metric 20579.6214\n",
      "Epoch 189 batch 100 train Loss 58.0479 test Loss 26.4528 with MSE metric 20576.9831\n",
      "Epoch 189 batch 110 train Loss 58.0368 test Loss 26.4483 with MSE metric 20574.2655\n",
      "Epoch 189 batch 120 train Loss 58.0256 test Loss 26.4438 with MSE metric 20571.5221\n",
      "Epoch 189 batch 130 train Loss 58.0145 test Loss 26.4394 with MSE metric 20568.7774\n",
      "Epoch 189 batch 140 train Loss 58.0034 test Loss 26.4349 with MSE metric 20565.9702\n",
      "Epoch 189 batch 150 train Loss 57.9923 test Loss 26.4305 with MSE metric 20563.2901\n",
      "Epoch 189 batch 160 train Loss 57.9812 test Loss 26.4261 with MSE metric 20560.5627\n",
      "Epoch 189 batch 170 train Loss 57.9701 test Loss 26.4216 with MSE metric 20557.7658\n",
      "Epoch 189 batch 180 train Loss 57.9590 test Loss 26.4172 with MSE metric 20555.0793\n",
      "Epoch 189 batch 190 train Loss 57.9479 test Loss 26.4128 with MSE metric 20552.3973\n",
      "Epoch 189 batch 200 train Loss 57.9368 test Loss 26.4083 with MSE metric 20549.7353\n",
      "Epoch 189 batch 210 train Loss 57.9257 test Loss 26.4039 with MSE metric 20547.0076\n",
      "Epoch 189 batch 220 train Loss 57.9146 test Loss 26.3995 with MSE metric 20544.2669\n",
      "Epoch 189 batch 230 train Loss 57.9035 test Loss 26.3950 with MSE metric 20541.6068\n",
      "Epoch 189 batch 240 train Loss 57.8924 test Loss 26.3906 with MSE metric 20538.8926\n",
      "Time taken for 1 epoch: 24.430814027786255 secs\n",
      "\n",
      "Epoch 190 batch 0 train Loss 57.8814 test Loss 26.3862 with MSE metric 20536.1987\n",
      "Epoch 190 batch 10 train Loss 57.8703 test Loss 26.3818 with MSE metric 20533.5352\n",
      "Epoch 190 batch 20 train Loss 57.8593 test Loss 26.3774 with MSE metric 20530.8574\n",
      "Epoch 190 batch 30 train Loss 57.8482 test Loss 26.3730 with MSE metric 20528.1620\n",
      "Epoch 190 batch 40 train Loss 57.8372 test Loss 26.3686 with MSE metric 20525.4646\n",
      "Epoch 190 batch 50 train Loss 57.8261 test Loss 26.3642 with MSE metric 20522.7668\n",
      "Epoch 190 batch 60 train Loss 57.8151 test Loss 26.3597 with MSE metric 20520.0731\n",
      "Epoch 190 batch 70 train Loss 57.8040 test Loss 26.3553 with MSE metric 20517.3748\n",
      "Epoch 190 batch 80 train Loss 57.7930 test Loss 26.3509 with MSE metric 20514.5938\n",
      "Epoch 190 batch 90 train Loss 57.7820 test Loss 26.3465 with MSE metric 20511.8692\n",
      "Epoch 190 batch 100 train Loss 57.7710 test Loss 26.3421 with MSE metric 20509.1766\n",
      "Epoch 190 batch 110 train Loss 57.7599 test Loss 26.3377 with MSE metric 20506.4467\n",
      "Epoch 190 batch 120 train Loss 57.7489 test Loss 26.3333 with MSE metric 20503.7463\n",
      "Epoch 190 batch 130 train Loss 57.7379 test Loss 26.3289 with MSE metric 20501.0429\n",
      "Epoch 190 batch 140 train Loss 57.7269 test Loss 26.3246 with MSE metric 20498.3338\n",
      "Epoch 190 batch 150 train Loss 57.7159 test Loss 26.3202 with MSE metric 20495.6336\n",
      "Epoch 190 batch 160 train Loss 57.7049 test Loss 26.3158 with MSE metric 20492.9541\n",
      "Epoch 190 batch 170 train Loss 57.6939 test Loss 26.3114 with MSE metric 20490.2044\n",
      "Epoch 190 batch 180 train Loss 57.6830 test Loss 26.3070 with MSE metric 20487.4453\n",
      "Epoch 190 batch 190 train Loss 57.6720 test Loss 26.3026 with MSE metric 20484.7766\n",
      "Epoch 190 batch 200 train Loss 57.6610 test Loss 26.2983 with MSE metric 20482.1074\n",
      "Epoch 190 batch 210 train Loss 57.6500 test Loss 26.2939 with MSE metric 20479.3232\n",
      "Epoch 190 batch 220 train Loss 57.6391 test Loss 26.2895 with MSE metric 20476.6023\n",
      "Epoch 190 batch 230 train Loss 57.6281 test Loss 26.2852 with MSE metric 20473.9213\n",
      "Epoch 190 batch 240 train Loss 57.6171 test Loss 26.2808 with MSE metric 20471.2290\n",
      "Time taken for 1 epoch: 24.263030767440796 secs\n",
      "\n",
      "Epoch 191 batch 0 train Loss 57.6062 test Loss 26.2764 with MSE metric 20468.5454\n",
      "Epoch 191 batch 10 train Loss 57.5952 test Loss 26.2720 with MSE metric 20465.8505\n",
      "Epoch 191 batch 20 train Loss 57.5843 test Loss 26.2677 with MSE metric 20463.1953\n",
      "Epoch 191 batch 30 train Loss 57.5734 test Loss 26.2633 with MSE metric 20460.4644\n",
      "Epoch 191 batch 40 train Loss 57.5624 test Loss 26.2589 with MSE metric 20457.7649\n",
      "Epoch 191 batch 50 train Loss 57.5515 test Loss 26.2546 with MSE metric 20454.9608\n",
      "Epoch 191 batch 60 train Loss 57.5406 test Loss 26.2502 with MSE metric 20452.2637\n",
      "Epoch 191 batch 70 train Loss 57.5296 test Loss 26.2458 with MSE metric 20449.6173\n",
      "Epoch 191 batch 80 train Loss 57.5187 test Loss 26.2415 with MSE metric 20446.9361\n",
      "Epoch 191 batch 90 train Loss 57.5078 test Loss 26.2371 with MSE metric 20444.2467\n",
      "Epoch 191 batch 100 train Loss 57.4969 test Loss 26.2327 with MSE metric 20441.6159\n",
      "Epoch 191 batch 110 train Loss 57.4860 test Loss 26.2284 with MSE metric 20438.8768\n",
      "Epoch 191 batch 120 train Loss 57.4751 test Loss 26.2240 with MSE metric 20436.2103\n",
      "Epoch 191 batch 130 train Loss 57.4642 test Loss 26.2197 with MSE metric 20433.5342\n",
      "Epoch 191 batch 140 train Loss 57.4533 test Loss 26.2153 with MSE metric 20430.8246\n",
      "Epoch 191 batch 150 train Loss 57.4424 test Loss 26.2110 with MSE metric 20428.1580\n",
      "Epoch 191 batch 160 train Loss 57.4316 test Loss 26.2067 with MSE metric 20425.4463\n",
      "Epoch 191 batch 170 train Loss 57.4207 test Loss 26.2023 with MSE metric 20422.7161\n",
      "Epoch 191 batch 180 train Loss 57.4098 test Loss 26.1980 with MSE metric 20420.0322\n",
      "Epoch 191 batch 190 train Loss 57.3989 test Loss 26.1937 with MSE metric 20417.3759\n",
      "Epoch 191 batch 200 train Loss 57.3881 test Loss 26.1893 with MSE metric 20414.6269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191 batch 210 train Loss 57.3772 test Loss 26.1850 with MSE metric 20412.0044\n",
      "Epoch 191 batch 220 train Loss 57.3664 test Loss 26.1807 with MSE metric 20409.4542\n",
      "Epoch 191 batch 230 train Loss 57.3555 test Loss 26.1763 with MSE metric 20406.7981\n",
      "Epoch 191 batch 240 train Loss 57.3447 test Loss 26.1720 with MSE metric 20404.0900\n",
      "Time taken for 1 epoch: 24.243361949920654 secs\n",
      "\n",
      "Epoch 192 batch 0 train Loss 57.3338 test Loss 26.1677 with MSE metric 20401.4685\n",
      "Epoch 192 batch 10 train Loss 57.3230 test Loss 26.1634 with MSE metric 20398.8579\n",
      "Epoch 192 batch 20 train Loss 57.3122 test Loss 26.1590 with MSE metric 20396.2741\n",
      "Epoch 192 batch 30 train Loss 57.3013 test Loss 26.1547 with MSE metric 20393.5933\n",
      "Epoch 192 batch 40 train Loss 57.2905 test Loss 26.1504 with MSE metric 20390.9795\n",
      "Epoch 192 batch 50 train Loss 57.2797 test Loss 26.1461 with MSE metric 20388.3429\n",
      "Epoch 192 batch 60 train Loss 57.2689 test Loss 26.1417 with MSE metric 20385.6847\n",
      "Epoch 192 batch 70 train Loss 57.2581 test Loss 26.1374 with MSE metric 20383.0678\n",
      "Epoch 192 batch 80 train Loss 57.2473 test Loss 26.1331 with MSE metric 20380.4439\n",
      "Epoch 192 batch 90 train Loss 57.2365 test Loss 26.1288 with MSE metric 20377.8663\n",
      "Epoch 192 batch 100 train Loss 57.2257 test Loss 26.1245 with MSE metric 20375.2167\n",
      "Epoch 192 batch 110 train Loss 57.2149 test Loss 26.1202 with MSE metric 20372.5819\n",
      "Epoch 192 batch 120 train Loss 57.2041 test Loss 26.1159 with MSE metric 20369.8799\n",
      "Epoch 192 batch 130 train Loss 57.1934 test Loss 26.1116 with MSE metric 20367.1993\n",
      "Epoch 192 batch 140 train Loss 57.1826 test Loss 26.1073 with MSE metric 20364.5502\n",
      "Epoch 192 batch 150 train Loss 57.1718 test Loss 26.1030 with MSE metric 20361.9595\n",
      "Epoch 192 batch 160 train Loss 57.1610 test Loss 26.0987 with MSE metric 20359.3449\n",
      "Epoch 192 batch 170 train Loss 57.1503 test Loss 26.0944 with MSE metric 20356.7768\n",
      "Epoch 192 batch 180 train Loss 57.1395 test Loss 26.0901 with MSE metric 20354.1927\n",
      "Epoch 192 batch 190 train Loss 57.1288 test Loss 26.0858 with MSE metric 20351.5487\n",
      "Epoch 192 batch 200 train Loss 57.1180 test Loss 26.0815 with MSE metric 20348.9641\n",
      "Epoch 192 batch 210 train Loss 57.1073 test Loss 26.0772 with MSE metric 20346.3399\n",
      "Epoch 192 batch 220 train Loss 57.0965 test Loss 26.0729 with MSE metric 20343.6773\n",
      "Epoch 192 batch 230 train Loss 57.0858 test Loss 26.0686 with MSE metric 20341.0344\n",
      "Epoch 192 batch 240 train Loss 57.0751 test Loss 26.0643 with MSE metric 20338.4342\n",
      "Time taken for 1 epoch: 24.230401039123535 secs\n",
      "\n",
      "Epoch 193 batch 0 train Loss 57.0644 test Loss 26.0601 with MSE metric 20335.8255\n",
      "Epoch 193 batch 10 train Loss 57.0536 test Loss 26.0558 with MSE metric 20333.1541\n",
      "Epoch 193 batch 20 train Loss 57.0429 test Loss 26.0515 with MSE metric 20330.5280\n",
      "Epoch 193 batch 30 train Loss 57.0322 test Loss 26.0472 with MSE metric 20327.9621\n",
      "Epoch 193 batch 40 train Loss 57.0215 test Loss 26.0429 with MSE metric 20325.3720\n",
      "Epoch 193 batch 50 train Loss 57.0108 test Loss 26.0387 with MSE metric 20322.7443\n",
      "Epoch 193 batch 60 train Loss 57.0001 test Loss 26.0344 with MSE metric 20320.0888\n",
      "Epoch 193 batch 70 train Loss 56.9894 test Loss 26.0301 with MSE metric 20317.4887\n",
      "Epoch 193 batch 80 train Loss 56.9787 test Loss 26.0259 with MSE metric 20314.9302\n",
      "Epoch 193 batch 90 train Loss 56.9680 test Loss 26.0216 with MSE metric 20312.2871\n",
      "Epoch 193 batch 100 train Loss 56.9573 test Loss 26.0173 with MSE metric 20309.6809\n",
      "Epoch 193 batch 110 train Loss 56.9467 test Loss 26.0131 with MSE metric 20307.0512\n",
      "Epoch 193 batch 120 train Loss 56.9360 test Loss 26.0088 with MSE metric 20304.5424\n",
      "Epoch 193 batch 130 train Loss 56.9253 test Loss 26.0045 with MSE metric 20301.9306\n",
      "Epoch 193 batch 140 train Loss 56.9147 test Loss 26.0003 with MSE metric 20299.4088\n",
      "Epoch 193 batch 150 train Loss 56.9040 test Loss 25.9960 with MSE metric 20296.7919\n",
      "Epoch 193 batch 160 train Loss 56.8933 test Loss 25.9918 with MSE metric 20294.1626\n",
      "Epoch 193 batch 170 train Loss 56.8827 test Loss 25.9875 with MSE metric 20291.4993\n",
      "Epoch 193 batch 180 train Loss 56.8720 test Loss 25.9833 with MSE metric 20288.8744\n",
      "Epoch 193 batch 190 train Loss 56.8614 test Loss 25.9791 with MSE metric 20286.3040\n",
      "Epoch 193 batch 200 train Loss 56.8508 test Loss 25.9748 with MSE metric 20283.6331\n",
      "Epoch 193 batch 210 train Loss 56.8401 test Loss 25.9706 with MSE metric 20280.9601\n",
      "Epoch 193 batch 220 train Loss 56.8295 test Loss 25.9663 with MSE metric 20278.3895\n",
      "Epoch 193 batch 230 train Loss 56.8189 test Loss 25.9621 with MSE metric 20275.8267\n",
      "Epoch 193 batch 240 train Loss 56.8082 test Loss 25.9579 with MSE metric 20273.2410\n",
      "Time taken for 1 epoch: 24.26750898361206 secs\n",
      "\n",
      "Epoch 194 batch 0 train Loss 56.7976 test Loss 25.9536 with MSE metric 20270.6847\n",
      "Epoch 194 batch 10 train Loss 56.7870 test Loss 25.9494 with MSE metric 20268.1279\n",
      "Epoch 194 batch 20 train Loss 56.7764 test Loss 25.9451 with MSE metric 20265.5448\n",
      "Epoch 194 batch 30 train Loss 56.7658 test Loss 25.9409 with MSE metric 20262.9539\n",
      "Epoch 194 batch 40 train Loss 56.7552 test Loss 25.9367 with MSE metric 20260.3191\n",
      "Epoch 194 batch 50 train Loss 56.7446 test Loss 25.9325 with MSE metric 20257.7141\n",
      "Epoch 194 batch 60 train Loss 56.7340 test Loss 25.9282 with MSE metric 20255.1422\n",
      "Epoch 194 batch 70 train Loss 56.7235 test Loss 25.9240 with MSE metric 20252.5647\n",
      "Epoch 194 batch 80 train Loss 56.7129 test Loss 25.9198 with MSE metric 20249.9103\n",
      "Epoch 194 batch 90 train Loss 56.7023 test Loss 25.9156 with MSE metric 20247.2259\n",
      "Epoch 194 batch 100 train Loss 56.6917 test Loss 25.9113 with MSE metric 20244.5945\n",
      "Epoch 194 batch 110 train Loss 56.6811 test Loss 25.9071 with MSE metric 20242.0056\n",
      "Epoch 194 batch 120 train Loss 56.6706 test Loss 25.9029 with MSE metric 20239.4346\n",
      "Epoch 194 batch 130 train Loss 56.6600 test Loss 25.8987 with MSE metric 20236.8301\n",
      "Epoch 194 batch 140 train Loss 56.6495 test Loss 25.8945 with MSE metric 20234.2718\n",
      "Epoch 194 batch 150 train Loss 56.6389 test Loss 25.8902 with MSE metric 20231.7776\n",
      "Epoch 194 batch 160 train Loss 56.6284 test Loss 25.8860 with MSE metric 20229.1884\n",
      "Epoch 194 batch 170 train Loss 56.6178 test Loss 25.8818 with MSE metric 20226.6039\n",
      "Epoch 194 batch 180 train Loss 56.6073 test Loss 25.8776 with MSE metric 20224.1593\n",
      "Epoch 194 batch 190 train Loss 56.5968 test Loss 25.8734 with MSE metric 20221.6257\n",
      "Epoch 194 batch 200 train Loss 56.5862 test Loss 25.8692 with MSE metric 20219.0100\n",
      "Epoch 194 batch 210 train Loss 56.5757 test Loss 25.8650 with MSE metric 20216.4537\n",
      "Epoch 194 batch 220 train Loss 56.5652 test Loss 25.8608 with MSE metric 20213.9147\n",
      "Epoch 194 batch 230 train Loss 56.5547 test Loss 25.8566 with MSE metric 20211.3628\n",
      "Epoch 194 batch 240 train Loss 56.5442 test Loss 25.8524 with MSE metric 20208.8274\n",
      "Time taken for 1 epoch: 24.530982971191406 secs\n",
      "\n",
      "Epoch 195 batch 0 train Loss 56.5337 test Loss 25.8481 with MSE metric 20206.2729\n",
      "Epoch 195 batch 10 train Loss 56.5232 test Loss 25.8440 with MSE metric 20203.7101\n",
      "Epoch 195 batch 20 train Loss 56.5127 test Loss 25.8398 with MSE metric 20201.1666\n",
      "Epoch 195 batch 30 train Loss 56.5022 test Loss 25.8356 with MSE metric 20198.6146\n",
      "Epoch 195 batch 40 train Loss 56.4917 test Loss 25.8314 with MSE metric 20196.1252\n",
      "Epoch 195 batch 50 train Loss 56.4812 test Loss 25.8272 with MSE metric 20193.5416\n",
      "Epoch 195 batch 60 train Loss 56.4707 test Loss 25.8230 with MSE metric 20191.0162\n",
      "Epoch 195 batch 70 train Loss 56.4603 test Loss 25.8188 with MSE metric 20188.4888\n",
      "Epoch 195 batch 80 train Loss 56.4498 test Loss 25.8147 with MSE metric 20185.8978\n",
      "Epoch 195 batch 90 train Loss 56.4393 test Loss 25.8105 with MSE metric 20183.3431\n",
      "Epoch 195 batch 100 train Loss 56.4288 test Loss 25.8063 with MSE metric 20180.7905\n",
      "Epoch 195 batch 110 train Loss 56.4184 test Loss 25.8021 with MSE metric 20178.1928\n",
      "Epoch 195 batch 120 train Loss 56.4079 test Loss 25.7980 with MSE metric 20175.6581\n",
      "Epoch 195 batch 130 train Loss 56.3975 test Loss 25.7938 with MSE metric 20173.0683\n",
      "Epoch 195 batch 140 train Loss 56.3870 test Loss 25.7896 with MSE metric 20170.4449\n",
      "Epoch 195 batch 150 train Loss 56.3766 test Loss 25.7855 with MSE metric 20167.9199\n",
      "Epoch 195 batch 160 train Loss 56.3662 test Loss 25.7813 with MSE metric 20165.4160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195 batch 170 train Loss 56.3557 test Loss 25.7772 with MSE metric 20162.8661\n",
      "Epoch 195 batch 180 train Loss 56.3453 test Loss 25.7730 with MSE metric 20160.3229\n",
      "Epoch 195 batch 190 train Loss 56.3349 test Loss 25.7688 with MSE metric 20157.7472\n",
      "Epoch 195 batch 200 train Loss 56.3245 test Loss 25.7647 with MSE metric 20155.1717\n",
      "Epoch 195 batch 210 train Loss 56.3140 test Loss 25.7605 with MSE metric 20152.6080\n",
      "Epoch 195 batch 220 train Loss 56.3036 test Loss 25.7563 with MSE metric 20150.0517\n",
      "Epoch 195 batch 230 train Loss 56.2932 test Loss 25.7522 with MSE metric 20147.4624\n",
      "Epoch 195 batch 240 train Loss 56.2828 test Loss 25.7480 with MSE metric 20144.9190\n",
      "Time taken for 1 epoch: 24.31041717529297 secs\n",
      "\n",
      "Epoch 196 batch 0 train Loss 56.2724 test Loss 25.7439 with MSE metric 20142.3433\n",
      "Epoch 196 batch 10 train Loss 56.2620 test Loss 25.7397 with MSE metric 20139.8556\n",
      "Epoch 196 batch 20 train Loss 56.2516 test Loss 25.7355 with MSE metric 20137.3563\n",
      "Epoch 196 batch 30 train Loss 56.2412 test Loss 25.7314 with MSE metric 20134.7485\n",
      "Epoch 196 batch 40 train Loss 56.2309 test Loss 25.7273 with MSE metric 20132.1683\n",
      "Epoch 196 batch 50 train Loss 56.2205 test Loss 25.7231 with MSE metric 20129.6556\n",
      "Epoch 196 batch 60 train Loss 56.2101 test Loss 25.7190 with MSE metric 20127.1117\n",
      "Epoch 196 batch 70 train Loss 56.1997 test Loss 25.7148 with MSE metric 20124.6448\n",
      "Epoch 196 batch 80 train Loss 56.1894 test Loss 25.7107 with MSE metric 20122.0844\n",
      "Epoch 196 batch 90 train Loss 56.1790 test Loss 25.7066 with MSE metric 20119.5223\n",
      "Epoch 196 batch 100 train Loss 56.1687 test Loss 25.7024 with MSE metric 20116.9881\n",
      "Epoch 196 batch 110 train Loss 56.1583 test Loss 25.6983 with MSE metric 20114.5368\n",
      "Epoch 196 batch 120 train Loss 56.1480 test Loss 25.6942 with MSE metric 20112.0092\n",
      "Epoch 196 batch 130 train Loss 56.1376 test Loss 25.6900 with MSE metric 20109.5432\n",
      "Epoch 196 batch 140 train Loss 56.1273 test Loss 25.6859 with MSE metric 20106.9664\n",
      "Epoch 196 batch 150 train Loss 56.1169 test Loss 25.6818 with MSE metric 20104.4591\n",
      "Epoch 196 batch 160 train Loss 56.1066 test Loss 25.6777 with MSE metric 20101.9000\n",
      "Epoch 196 batch 170 train Loss 56.0963 test Loss 25.6735 with MSE metric 20099.3686\n",
      "Epoch 196 batch 180 train Loss 56.0859 test Loss 25.6694 with MSE metric 20096.8630\n",
      "Epoch 196 batch 190 train Loss 56.0756 test Loss 25.6653 with MSE metric 20094.3491\n",
      "Epoch 196 batch 200 train Loss 56.0653 test Loss 25.6612 with MSE metric 20091.8198\n",
      "Epoch 196 batch 210 train Loss 56.0550 test Loss 25.6571 with MSE metric 20089.3188\n",
      "Epoch 196 batch 220 train Loss 56.0447 test Loss 25.6529 with MSE metric 20086.7408\n",
      "Epoch 196 batch 230 train Loss 56.0344 test Loss 25.6488 with MSE metric 20084.2879\n",
      "Epoch 196 batch 240 train Loss 56.0241 test Loss 25.6447 with MSE metric 20081.7934\n",
      "Time taken for 1 epoch: 24.481933116912842 secs\n",
      "\n",
      "Epoch 197 batch 0 train Loss 56.0138 test Loss 25.6406 with MSE metric 20079.2348\n",
      "Epoch 197 batch 10 train Loss 56.0035 test Loss 25.6365 with MSE metric 20076.7157\n",
      "Epoch 197 batch 20 train Loss 55.9932 test Loss 25.6324 with MSE metric 20074.2094\n",
      "Epoch 197 batch 30 train Loss 55.9829 test Loss 25.6283 with MSE metric 20071.6926\n",
      "Epoch 197 batch 40 train Loss 55.9726 test Loss 25.6242 with MSE metric 20069.1073\n",
      "Epoch 197 batch 50 train Loss 55.9624 test Loss 25.6201 with MSE metric 20066.5851\n",
      "Epoch 197 batch 60 train Loss 55.9521 test Loss 25.6160 with MSE metric 20064.0629\n",
      "Epoch 197 batch 70 train Loss 55.9418 test Loss 25.6119 with MSE metric 20061.5778\n",
      "Epoch 197 batch 80 train Loss 55.9316 test Loss 25.6078 with MSE metric 20059.0626\n",
      "Epoch 197 batch 90 train Loss 55.9213 test Loss 25.6037 with MSE metric 20056.5598\n",
      "Epoch 197 batch 100 train Loss 55.9111 test Loss 25.5996 with MSE metric 20054.0910\n",
      "Epoch 197 batch 110 train Loss 55.9008 test Loss 25.5956 with MSE metric 20051.5860\n",
      "Epoch 197 batch 120 train Loss 55.8906 test Loss 25.5915 with MSE metric 20049.0470\n",
      "Epoch 197 batch 130 train Loss 55.8803 test Loss 25.5874 with MSE metric 20046.4438\n",
      "Epoch 197 batch 140 train Loss 55.8701 test Loss 25.5833 with MSE metric 20043.9270\n",
      "Epoch 197 batch 150 train Loss 55.8598 test Loss 25.5792 with MSE metric 20041.4079\n",
      "Epoch 197 batch 160 train Loss 55.8496 test Loss 25.5751 with MSE metric 20038.8576\n",
      "Epoch 197 batch 170 train Loss 55.8394 test Loss 25.5710 with MSE metric 20036.3059\n",
      "Epoch 197 batch 180 train Loss 55.8292 test Loss 25.5670 with MSE metric 20033.8330\n",
      "Epoch 197 batch 190 train Loss 55.8189 test Loss 25.5629 with MSE metric 20031.3449\n",
      "Epoch 197 batch 200 train Loss 55.8087 test Loss 25.5588 with MSE metric 20028.8093\n",
      "Epoch 197 batch 210 train Loss 55.7985 test Loss 25.5547 with MSE metric 20026.3092\n",
      "Epoch 197 batch 220 train Loss 55.7883 test Loss 25.5506 with MSE metric 20023.8829\n",
      "Epoch 197 batch 230 train Loss 55.7781 test Loss 25.5466 with MSE metric 20021.3625\n",
      "Epoch 197 batch 240 train Loss 55.7679 test Loss 25.5425 with MSE metric 20018.8731\n",
      "Time taken for 1 epoch: 24.362313985824585 secs\n",
      "\n",
      "Epoch 198 batch 0 train Loss 55.7577 test Loss 25.5384 with MSE metric 20016.3083\n",
      "Epoch 198 batch 10 train Loss 55.7475 test Loss 25.5343 with MSE metric 20013.8461\n",
      "Epoch 198 batch 20 train Loss 55.7374 test Loss 25.5303 with MSE metric 20011.3804\n",
      "Epoch 198 batch 30 train Loss 55.7272 test Loss 25.5262 with MSE metric 20008.8760\n",
      "Epoch 198 batch 40 train Loss 55.7170 test Loss 25.5221 with MSE metric 20006.3650\n",
      "Epoch 198 batch 50 train Loss 55.7068 test Loss 25.5180 with MSE metric 20003.8765\n",
      "Epoch 198 batch 60 train Loss 55.6967 test Loss 25.5140 with MSE metric 20001.4042\n",
      "Epoch 198 batch 70 train Loss 55.6865 test Loss 25.5100 with MSE metric 19998.8638\n",
      "Epoch 198 batch 80 train Loss 55.6763 test Loss 25.5059 with MSE metric 19996.3492\n",
      "Epoch 198 batch 90 train Loss 55.6662 test Loss 25.5019 with MSE metric 19993.8726\n",
      "Epoch 198 batch 100 train Loss 55.6560 test Loss 25.4978 with MSE metric 19991.3755\n",
      "Epoch 198 batch 110 train Loss 55.6459 test Loss 25.4938 with MSE metric 19988.8758\n",
      "Epoch 198 batch 120 train Loss 55.6357 test Loss 25.4897 with MSE metric 19986.4495\n",
      "Epoch 198 batch 130 train Loss 55.6256 test Loss 25.4856 with MSE metric 19983.9784\n",
      "Epoch 198 batch 140 train Loss 55.6155 test Loss 25.4816 with MSE metric 19981.4586\n",
      "Epoch 198 batch 150 train Loss 55.6053 test Loss 25.4776 with MSE metric 19978.9398\n",
      "Epoch 198 batch 160 train Loss 55.5952 test Loss 25.4735 with MSE metric 19976.4848\n",
      "Epoch 198 batch 170 train Loss 55.5851 test Loss 25.4695 with MSE metric 19974.0016\n",
      "Epoch 198 batch 180 train Loss 55.5750 test Loss 25.4654 with MSE metric 19971.6033\n",
      "Epoch 198 batch 190 train Loss 55.5649 test Loss 25.4614 with MSE metric 19969.0896\n",
      "Epoch 198 batch 200 train Loss 55.5547 test Loss 25.4574 with MSE metric 19966.5734\n",
      "Epoch 198 batch 210 train Loss 55.5446 test Loss 25.4533 with MSE metric 19964.1301\n",
      "Epoch 198 batch 220 train Loss 55.5345 test Loss 25.4493 with MSE metric 19961.6308\n",
      "Epoch 198 batch 230 train Loss 55.5244 test Loss 25.4453 with MSE metric 19959.1564\n",
      "Epoch 198 batch 240 train Loss 55.5143 test Loss 25.4412 with MSE metric 19956.6874\n",
      "Time taken for 1 epoch: 24.719050884246826 secs\n",
      "\n",
      "Epoch 199 batch 0 train Loss 55.5043 test Loss 25.4372 with MSE metric 19954.2348\n",
      "Epoch 199 batch 10 train Loss 55.4942 test Loss 25.4331 with MSE metric 19951.7661\n",
      "Epoch 199 batch 20 train Loss 55.4841 test Loss 25.4291 with MSE metric 19949.2742\n",
      "Epoch 199 batch 30 train Loss 55.4740 test Loss 25.4250 with MSE metric 19946.8178\n",
      "Epoch 199 batch 40 train Loss 55.4639 test Loss 25.4210 with MSE metric 19944.3181\n",
      "Epoch 199 batch 50 train Loss 55.4539 test Loss 25.4170 with MSE metric 19941.8578\n",
      "Epoch 199 batch 60 train Loss 55.4438 test Loss 25.4130 with MSE metric 19939.4239\n",
      "Epoch 199 batch 70 train Loss 55.4337 test Loss 25.4090 with MSE metric 19937.0503\n",
      "Epoch 199 batch 80 train Loss 55.4237 test Loss 25.4049 with MSE metric 19934.6023\n",
      "Epoch 199 batch 90 train Loss 55.4136 test Loss 25.4009 with MSE metric 19932.1678\n",
      "Epoch 199 batch 100 train Loss 55.4036 test Loss 25.3969 with MSE metric 19929.6758\n",
      "Epoch 199 batch 110 train Loss 55.3935 test Loss 25.3929 with MSE metric 19927.1681\n",
      "Epoch 199 batch 120 train Loss 55.3835 test Loss 25.3889 with MSE metric 19924.7397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199 batch 130 train Loss 55.3735 test Loss 25.3849 with MSE metric 19922.2934\n",
      "Epoch 199 batch 140 train Loss 55.3634 test Loss 25.3809 with MSE metric 19919.8637\n",
      "Epoch 199 batch 150 train Loss 55.3534 test Loss 25.3769 with MSE metric 19917.4139\n",
      "Epoch 199 batch 160 train Loss 55.3434 test Loss 25.3729 with MSE metric 19914.9843\n",
      "Epoch 199 batch 170 train Loss 55.3334 test Loss 25.3689 with MSE metric 19912.5261\n",
      "Epoch 199 batch 180 train Loss 55.3233 test Loss 25.3649 with MSE metric 19910.0578\n",
      "Epoch 199 batch 190 train Loss 55.3133 test Loss 25.3608 with MSE metric 19907.6350\n",
      "Epoch 199 batch 200 train Loss 55.3033 test Loss 25.3568 with MSE metric 19905.2199\n",
      "Epoch 199 batch 210 train Loss 55.2933 test Loss 25.3528 with MSE metric 19902.7515\n",
      "Epoch 199 batch 220 train Loss 55.2833 test Loss 25.3488 with MSE metric 19900.2799\n",
      "Epoch 199 batch 230 train Loss 55.2733 test Loss 25.3448 with MSE metric 19897.8117\n",
      "Epoch 199 batch 240 train Loss 55.2633 test Loss 25.3409 with MSE metric 19895.3139\n",
      "Time taken for 1 epoch: 26.155203819274902 secs\n",
      "\n",
      "Epoch 200 batch 0 train Loss 55.2533 test Loss 25.3369 with MSE metric 19892.8330\n",
      "Epoch 200 batch 10 train Loss 55.2433 test Loss 25.3329 with MSE metric 19890.4540\n",
      "Epoch 200 batch 20 train Loss 55.2334 test Loss 25.3289 with MSE metric 19887.9695\n",
      "Epoch 200 batch 30 train Loss 55.2234 test Loss 25.3249 with MSE metric 19885.5159\n",
      "Epoch 200 batch 40 train Loss 55.2134 test Loss 25.3209 with MSE metric 19883.0327\n",
      "Epoch 200 batch 50 train Loss 55.2034 test Loss 25.3170 with MSE metric 19880.5826\n",
      "Epoch 200 batch 60 train Loss 55.1935 test Loss 25.3130 with MSE metric 19878.1009\n",
      "Epoch 200 batch 70 train Loss 55.1835 test Loss 25.3090 with MSE metric 19875.5989\n",
      "Epoch 200 batch 80 train Loss 55.1735 test Loss 25.3050 with MSE metric 19873.1343\n",
      "Epoch 200 batch 90 train Loss 55.1636 test Loss 25.3011 with MSE metric 19870.7105\n",
      "Epoch 200 batch 100 train Loss 55.1536 test Loss 25.2971 with MSE metric 19868.1917\n",
      "Epoch 200 batch 110 train Loss 55.1437 test Loss 25.2931 with MSE metric 19865.8165\n",
      "Epoch 200 batch 120 train Loss 55.1337 test Loss 25.2892 with MSE metric 19863.3538\n",
      "Epoch 200 batch 130 train Loss 55.1238 test Loss 25.2852 with MSE metric 19860.9383\n",
      "Epoch 200 batch 140 train Loss 55.1139 test Loss 25.2812 with MSE metric 19858.5004\n",
      "Epoch 200 batch 150 train Loss 55.1039 test Loss 25.2773 with MSE metric 19856.0863\n",
      "Epoch 200 batch 160 train Loss 55.0940 test Loss 25.2733 with MSE metric 19853.6174\n",
      "Epoch 200 batch 170 train Loss 55.0841 test Loss 25.2694 with MSE metric 19851.1673\n",
      "Epoch 200 batch 180 train Loss 55.0742 test Loss 25.2654 with MSE metric 19848.7288\n",
      "Epoch 200 batch 190 train Loss 55.0643 test Loss 25.2614 with MSE metric 19846.2108\n",
      "Epoch 200 batch 200 train Loss 55.0544 test Loss 25.2575 with MSE metric 19843.7719\n",
      "Epoch 200 batch 210 train Loss 55.0444 test Loss 25.2535 with MSE metric 19841.3640\n",
      "Epoch 200 batch 220 train Loss 55.0345 test Loss 25.2496 with MSE metric 19838.9539\n",
      "Epoch 200 batch 230 train Loss 55.0247 test Loss 25.2456 with MSE metric 19836.5290\n",
      "Epoch 200 batch 240 train Loss 55.0148 test Loss 25.2416 with MSE metric 19834.1162\n",
      "Time taken for 1 epoch: 24.97882318496704 secs\n",
      "\n",
      "Epoch 201 batch 0 train Loss 55.0049 test Loss 25.2377 with MSE metric 19831.6756\n",
      "Epoch 201 batch 10 train Loss 54.9950 test Loss 25.2337 with MSE metric 19829.2019\n",
      "Epoch 201 batch 20 train Loss 54.9851 test Loss 25.2298 with MSE metric 19826.7845\n",
      "Epoch 201 batch 30 train Loss 54.9752 test Loss 25.2259 with MSE metric 19824.3943\n",
      "Epoch 201 batch 40 train Loss 54.9653 test Loss 25.2219 with MSE metric 19822.0187\n",
      "Epoch 201 batch 50 train Loss 54.9555 test Loss 25.2180 with MSE metric 19819.5099\n",
      "Epoch 201 batch 60 train Loss 54.9456 test Loss 25.2140 with MSE metric 19817.0146\n",
      "Epoch 201 batch 70 train Loss 54.9357 test Loss 25.2101 with MSE metric 19814.5964\n",
      "Epoch 201 batch 80 train Loss 54.9259 test Loss 25.2062 with MSE metric 19812.2126\n",
      "Epoch 201 batch 90 train Loss 54.9160 test Loss 25.2022 with MSE metric 19809.8300\n",
      "Epoch 201 batch 100 train Loss 54.9062 test Loss 25.1983 with MSE metric 19807.4369\n",
      "Epoch 201 batch 110 train Loss 54.8963 test Loss 25.1943 with MSE metric 19805.0447\n",
      "Epoch 201 batch 120 train Loss 54.8865 test Loss 25.1904 with MSE metric 19802.6610\n",
      "Epoch 201 batch 130 train Loss 54.8767 test Loss 25.1865 with MSE metric 19800.2356\n",
      "Epoch 201 batch 140 train Loss 54.8668 test Loss 25.1826 with MSE metric 19797.7948\n",
      "Epoch 201 batch 150 train Loss 54.8570 test Loss 25.1787 with MSE metric 19795.3773\n",
      "Epoch 201 batch 160 train Loss 54.8472 test Loss 25.1747 with MSE metric 19793.0010\n",
      "Epoch 201 batch 170 train Loss 54.8373 test Loss 25.1708 with MSE metric 19790.5826\n",
      "Epoch 201 batch 180 train Loss 54.8275 test Loss 25.1669 with MSE metric 19788.2614\n",
      "Epoch 201 batch 190 train Loss 54.8177 test Loss 25.1630 with MSE metric 19785.8233\n",
      "Epoch 201 batch 200 train Loss 54.8079 test Loss 25.1591 with MSE metric 19783.4008\n",
      "Epoch 201 batch 210 train Loss 54.7981 test Loss 25.1551 with MSE metric 19781.0223\n",
      "Epoch 201 batch 220 train Loss 54.7883 test Loss 25.1512 with MSE metric 19778.6033\n",
      "Epoch 201 batch 230 train Loss 54.7785 test Loss 25.1473 with MSE metric 19776.1775\n",
      "Epoch 201 batch 240 train Loss 54.7687 test Loss 25.1434 with MSE metric 19773.7540\n",
      "Time taken for 1 epoch: 25.70627999305725 secs\n",
      "\n",
      "Epoch 202 batch 0 train Loss 54.7589 test Loss 25.1395 with MSE metric 19771.3931\n",
      "Epoch 202 batch 10 train Loss 54.7491 test Loss 25.1355 with MSE metric 19768.9956\n",
      "Epoch 202 batch 20 train Loss 54.7393 test Loss 25.1316 with MSE metric 19766.6247\n",
      "Epoch 202 batch 30 train Loss 54.7295 test Loss 25.1277 with MSE metric 19764.2366\n",
      "Epoch 202 batch 40 train Loss 54.7198 test Loss 25.1238 with MSE metric 19761.8262\n",
      "Epoch 202 batch 50 train Loss 54.7100 test Loss 25.1199 with MSE metric 19759.4528\n",
      "Epoch 202 batch 60 train Loss 54.7002 test Loss 25.1160 with MSE metric 19757.0095\n",
      "Epoch 202 batch 70 train Loss 54.6905 test Loss 25.1121 with MSE metric 19754.6411\n",
      "Epoch 202 batch 80 train Loss 54.6807 test Loss 25.1082 with MSE metric 19752.2467\n",
      "Epoch 202 batch 90 train Loss 54.6709 test Loss 25.1043 with MSE metric 19749.9261\n",
      "Epoch 202 batch 100 train Loss 54.6612 test Loss 25.1004 with MSE metric 19747.5296\n",
      "Epoch 202 batch 110 train Loss 54.6514 test Loss 25.0965 with MSE metric 19745.1008\n",
      "Epoch 202 batch 120 train Loss 54.6417 test Loss 25.0927 with MSE metric 19742.7833\n",
      "Epoch 202 batch 130 train Loss 54.6320 test Loss 25.0888 with MSE metric 19740.3325\n",
      "Epoch 202 batch 140 train Loss 54.6222 test Loss 25.0849 with MSE metric 19737.9543\n",
      "Epoch 202 batch 150 train Loss 54.6125 test Loss 25.0810 with MSE metric 19735.6366\n",
      "Epoch 202 batch 160 train Loss 54.6027 test Loss 25.0772 with MSE metric 19733.2118\n",
      "Epoch 202 batch 170 train Loss 54.5930 test Loss 25.0733 with MSE metric 19730.7966\n",
      "Epoch 202 batch 180 train Loss 54.5833 test Loss 25.0694 with MSE metric 19728.3693\n",
      "Epoch 202 batch 190 train Loss 54.5736 test Loss 25.0655 with MSE metric 19725.9500\n",
      "Epoch 202 batch 200 train Loss 54.5639 test Loss 25.0616 with MSE metric 19723.6594\n",
      "Epoch 202 batch 210 train Loss 54.5542 test Loss 25.0577 with MSE metric 19721.2929\n",
      "Epoch 202 batch 220 train Loss 54.5444 test Loss 25.0539 with MSE metric 19718.9016\n",
      "Epoch 202 batch 230 train Loss 54.5347 test Loss 25.0500 with MSE metric 19716.5748\n",
      "Epoch 202 batch 240 train Loss 54.5250 test Loss 25.0461 with MSE metric 19714.1746\n",
      "Time taken for 1 epoch: 24.562053203582764 secs\n",
      "\n",
      "Epoch 203 batch 0 train Loss 54.5153 test Loss 25.0422 with MSE metric 19711.7376\n",
      "Epoch 203 batch 10 train Loss 54.5056 test Loss 25.0384 with MSE metric 19709.2778\n",
      "Epoch 203 batch 20 train Loss 54.4960 test Loss 25.0345 with MSE metric 19706.8812\n",
      "Epoch 203 batch 30 train Loss 54.4863 test Loss 25.0306 with MSE metric 19704.6026\n",
      "Epoch 203 batch 40 train Loss 54.4766 test Loss 25.0268 with MSE metric 19702.3462\n",
      "Epoch 203 batch 50 train Loss 54.4669 test Loss 25.0229 with MSE metric 19699.9885\n",
      "Epoch 203 batch 60 train Loss 54.4572 test Loss 25.0190 with MSE metric 19697.5750\n",
      "Epoch 203 batch 70 train Loss 54.4476 test Loss 25.0152 with MSE metric 19695.1787\n",
      "Epoch 203 batch 80 train Loss 54.4379 test Loss 25.0113 with MSE metric 19692.7707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 batch 90 train Loss 54.4282 test Loss 25.0075 with MSE metric 19690.3766\n",
      "Epoch 203 batch 100 train Loss 54.4186 test Loss 25.0036 with MSE metric 19687.9998\n",
      "Epoch 203 batch 110 train Loss 54.4089 test Loss 24.9997 with MSE metric 19685.6136\n",
      "Epoch 203 batch 120 train Loss 54.3993 test Loss 24.9959 with MSE metric 19683.3174\n",
      "Epoch 203 batch 130 train Loss 54.3896 test Loss 24.9920 with MSE metric 19681.0014\n",
      "Epoch 203 batch 140 train Loss 54.3800 test Loss 24.9882 with MSE metric 19678.5475\n",
      "Epoch 203 batch 150 train Loss 54.3704 test Loss 24.9843 with MSE metric 19676.2407\n",
      "Epoch 203 batch 160 train Loss 54.3607 test Loss 24.9805 with MSE metric 19673.8426\n",
      "Epoch 203 batch 170 train Loss 54.3511 test Loss 24.9766 with MSE metric 19671.4559\n",
      "Epoch 203 batch 180 train Loss 54.3415 test Loss 24.9728 with MSE metric 19669.0698\n",
      "Epoch 203 batch 190 train Loss 54.3318 test Loss 24.9690 with MSE metric 19666.7179\n",
      "Epoch 203 batch 200 train Loss 54.3222 test Loss 24.9651 with MSE metric 19664.3456\n",
      "Epoch 203 batch 210 train Loss 54.3126 test Loss 24.9613 with MSE metric 19661.9841\n",
      "Epoch 203 batch 220 train Loss 54.3030 test Loss 24.9574 with MSE metric 19659.6304\n",
      "Epoch 203 batch 230 train Loss 54.2934 test Loss 24.9536 with MSE metric 19657.3036\n",
      "Epoch 203 batch 240 train Loss 54.2838 test Loss 24.9497 with MSE metric 19654.9857\n",
      "Time taken for 1 epoch: 24.432424068450928 secs\n",
      "\n",
      "Epoch 204 batch 0 train Loss 54.2742 test Loss 24.9459 with MSE metric 19652.6890\n",
      "Epoch 204 batch 10 train Loss 54.2646 test Loss 24.9421 with MSE metric 19650.2902\n",
      "Epoch 204 batch 20 train Loss 54.2550 test Loss 24.9382 with MSE metric 19647.9165\n",
      "Epoch 204 batch 30 train Loss 54.2454 test Loss 24.9344 with MSE metric 19645.6082\n",
      "Epoch 204 batch 40 train Loss 54.2358 test Loss 24.9305 with MSE metric 19643.2278\n",
      "Epoch 204 batch 50 train Loss 54.2262 test Loss 24.9267 with MSE metric 19640.8947\n",
      "Epoch 204 batch 60 train Loss 54.2166 test Loss 24.9229 with MSE metric 19638.4899\n",
      "Epoch 204 batch 70 train Loss 54.2071 test Loss 24.9190 with MSE metric 19636.0603\n",
      "Epoch 204 batch 80 train Loss 54.1975 test Loss 24.9152 with MSE metric 19633.6959\n",
      "Epoch 204 batch 90 train Loss 54.1879 test Loss 24.9114 with MSE metric 19631.3903\n",
      "Epoch 204 batch 100 train Loss 54.1784 test Loss 24.9075 with MSE metric 19629.0576\n",
      "Epoch 204 batch 110 train Loss 54.1688 test Loss 24.9037 with MSE metric 19626.6215\n",
      "Epoch 204 batch 120 train Loss 54.1592 test Loss 24.8999 with MSE metric 19624.1756\n",
      "Epoch 204 batch 130 train Loss 54.1497 test Loss 24.8961 with MSE metric 19621.8758\n",
      "Epoch 204 batch 140 train Loss 54.1401 test Loss 24.8923 with MSE metric 19619.5061\n",
      "Epoch 204 batch 150 train Loss 54.1306 test Loss 24.8885 with MSE metric 19617.1239\n",
      "Epoch 204 batch 160 train Loss 54.1210 test Loss 24.8847 with MSE metric 19614.7116\n",
      "Epoch 204 batch 170 train Loss 54.1115 test Loss 24.8809 with MSE metric 19612.3890\n",
      "Epoch 204 batch 180 train Loss 54.1020 test Loss 24.8771 with MSE metric 19610.0344\n",
      "Epoch 204 batch 190 train Loss 54.0924 test Loss 24.8733 with MSE metric 19607.7239\n",
      "Epoch 204 batch 200 train Loss 54.0829 test Loss 24.8695 with MSE metric 19605.3974\n",
      "Epoch 204 batch 210 train Loss 54.0734 test Loss 24.8656 with MSE metric 19602.9977\n",
      "Epoch 204 batch 220 train Loss 54.0639 test Loss 24.8619 with MSE metric 19600.6295\n",
      "Epoch 204 batch 230 train Loss 54.0544 test Loss 24.8581 with MSE metric 19598.3642\n",
      "Epoch 204 batch 240 train Loss 54.0448 test Loss 24.8543 with MSE metric 19595.9973\n",
      "Time taken for 1 epoch: 25.12465000152588 secs\n",
      "\n",
      "Epoch 205 batch 0 train Loss 54.0353 test Loss 24.8505 with MSE metric 19593.6738\n",
      "Epoch 205 batch 10 train Loss 54.0258 test Loss 24.8467 with MSE metric 19591.3643\n",
      "Epoch 205 batch 20 train Loss 54.0163 test Loss 24.8429 with MSE metric 19589.0385\n",
      "Epoch 205 batch 30 train Loss 54.0068 test Loss 24.8391 with MSE metric 19586.7595\n",
      "Epoch 205 batch 40 train Loss 53.9973 test Loss 24.8353 with MSE metric 19584.4523\n",
      "Epoch 205 batch 50 train Loss 53.9878 test Loss 24.8315 with MSE metric 19582.1461\n",
      "Epoch 205 batch 60 train Loss 53.9784 test Loss 24.8277 with MSE metric 19579.8855\n",
      "Epoch 205 batch 70 train Loss 53.9689 test Loss 24.8239 with MSE metric 19577.6059\n",
      "Epoch 205 batch 80 train Loss 53.9594 test Loss 24.8201 with MSE metric 19575.2814\n",
      "Epoch 205 batch 90 train Loss 53.9499 test Loss 24.8163 with MSE metric 19572.8956\n",
      "Epoch 205 batch 100 train Loss 53.9405 test Loss 24.8126 with MSE metric 19570.6326\n",
      "Epoch 205 batch 110 train Loss 53.9310 test Loss 24.8088 with MSE metric 19568.3508\n",
      "Epoch 205 batch 120 train Loss 53.9215 test Loss 24.8050 with MSE metric 19566.0562\n",
      "Epoch 205 batch 130 train Loss 53.9121 test Loss 24.8012 with MSE metric 19563.7226\n",
      "Epoch 205 batch 140 train Loss 53.9026 test Loss 24.7975 with MSE metric 19561.3264\n",
      "Epoch 205 batch 150 train Loss 53.8932 test Loss 24.7937 with MSE metric 19558.9694\n",
      "Epoch 205 batch 160 train Loss 53.8837 test Loss 24.7899 with MSE metric 19556.6912\n",
      "Epoch 205 batch 170 train Loss 53.8743 test Loss 24.7862 with MSE metric 19554.4182\n",
      "Epoch 205 batch 180 train Loss 53.8648 test Loss 24.7824 with MSE metric 19552.0702\n",
      "Epoch 205 batch 190 train Loss 53.8554 test Loss 24.7786 with MSE metric 19549.7564\n",
      "Epoch 205 batch 200 train Loss 53.8460 test Loss 24.7748 with MSE metric 19547.4668\n",
      "Epoch 205 batch 210 train Loss 53.8365 test Loss 24.7711 with MSE metric 19545.1168\n",
      "Epoch 205 batch 220 train Loss 53.8271 test Loss 24.7673 with MSE metric 19542.8217\n",
      "Epoch 205 batch 230 train Loss 53.8177 test Loss 24.7635 with MSE metric 19540.5419\n",
      "Epoch 205 batch 240 train Loss 53.8083 test Loss 24.7598 with MSE metric 19538.2024\n",
      "Time taken for 1 epoch: 24.55048704147339 secs\n",
      "\n",
      "Epoch 206 batch 0 train Loss 53.7989 test Loss 24.7560 with MSE metric 19535.8421\n",
      "Epoch 206 batch 10 train Loss 53.7894 test Loss 24.7522 with MSE metric 19533.5335\n",
      "Epoch 206 batch 20 train Loss 53.7800 test Loss 24.7485 with MSE metric 19531.2818\n",
      "Epoch 206 batch 30 train Loss 53.7706 test Loss 24.7447 with MSE metric 19528.9334\n",
      "Epoch 206 batch 40 train Loss 53.7612 test Loss 24.7409 with MSE metric 19526.6837\n",
      "Epoch 206 batch 50 train Loss 53.7518 test Loss 24.7372 with MSE metric 19524.3655\n",
      "Epoch 206 batch 60 train Loss 53.7424 test Loss 24.7334 with MSE metric 19522.0509\n",
      "Epoch 206 batch 70 train Loss 53.7330 test Loss 24.7297 with MSE metric 19519.7519\n",
      "Epoch 206 batch 80 train Loss 53.7237 test Loss 24.7259 with MSE metric 19517.4575\n",
      "Epoch 206 batch 90 train Loss 53.7143 test Loss 24.7222 with MSE metric 19515.1365\n",
      "Epoch 206 batch 100 train Loss 53.7049 test Loss 24.7184 with MSE metric 19512.8446\n",
      "Epoch 206 batch 110 train Loss 53.6955 test Loss 24.7147 with MSE metric 19510.5143\n",
      "Epoch 206 batch 120 train Loss 53.6861 test Loss 24.7110 with MSE metric 19508.1921\n",
      "Epoch 206 batch 130 train Loss 53.6768 test Loss 24.7072 with MSE metric 19505.9096\n",
      "Epoch 206 batch 140 train Loss 53.6674 test Loss 24.7035 with MSE metric 19503.5572\n",
      "Epoch 206 batch 150 train Loss 53.6580 test Loss 24.6997 with MSE metric 19501.1753\n",
      "Epoch 206 batch 160 train Loss 53.6487 test Loss 24.6960 with MSE metric 19498.9363\n",
      "Epoch 206 batch 170 train Loss 53.6393 test Loss 24.6923 with MSE metric 19496.6435\n",
      "Epoch 206 batch 180 train Loss 53.6300 test Loss 24.6885 with MSE metric 19494.3739\n",
      "Epoch 206 batch 190 train Loss 53.6206 test Loss 24.6848 with MSE metric 19492.0767\n",
      "Epoch 206 batch 200 train Loss 53.6113 test Loss 24.6810 with MSE metric 19489.7868\n",
      "Epoch 206 batch 210 train Loss 53.6020 test Loss 24.6773 with MSE metric 19487.5471\n",
      "Epoch 206 batch 220 train Loss 53.5926 test Loss 24.6736 with MSE metric 19485.2606\n",
      "Epoch 206 batch 230 train Loss 53.5833 test Loss 24.6698 with MSE metric 19482.9355\n",
      "Epoch 206 batch 240 train Loss 53.5740 test Loss 24.6661 with MSE metric 19480.6269\n",
      "Time taken for 1 epoch: 24.480300188064575 secs\n",
      "\n",
      "Epoch 207 batch 0 train Loss 53.5646 test Loss 24.6624 with MSE metric 19478.2553\n",
      "Epoch 207 batch 10 train Loss 53.5553 test Loss 24.6587 with MSE metric 19475.9710\n",
      "Epoch 207 batch 20 train Loss 53.5460 test Loss 24.6550 with MSE metric 19473.6770\n",
      "Epoch 207 batch 30 train Loss 53.5367 test Loss 24.6512 with MSE metric 19471.3755\n",
      "Epoch 207 batch 40 train Loss 53.5274 test Loss 24.6475 with MSE metric 19469.1280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207 batch 50 train Loss 53.5180 test Loss 24.6438 with MSE metric 19466.9093\n",
      "Epoch 207 batch 60 train Loss 53.5087 test Loss 24.6401 with MSE metric 19464.6308\n",
      "Epoch 207 batch 70 train Loss 53.4994 test Loss 24.6364 with MSE metric 19462.3212\n",
      "Epoch 207 batch 80 train Loss 53.4901 test Loss 24.6326 with MSE metric 19460.0058\n",
      "Epoch 207 batch 90 train Loss 53.4808 test Loss 24.6289 with MSE metric 19457.7171\n",
      "Epoch 207 batch 100 train Loss 53.4716 test Loss 24.6252 with MSE metric 19455.4603\n",
      "Epoch 207 batch 110 train Loss 53.4623 test Loss 24.6215 with MSE metric 19453.1755\n",
      "Epoch 207 batch 120 train Loss 53.4530 test Loss 24.6178 with MSE metric 19450.8925\n",
      "Epoch 207 batch 130 train Loss 53.4437 test Loss 24.6141 with MSE metric 19448.5944\n",
      "Epoch 207 batch 140 train Loss 53.4344 test Loss 24.6104 with MSE metric 19446.3420\n",
      "Epoch 207 batch 150 train Loss 53.4252 test Loss 24.6067 with MSE metric 19444.0937\n",
      "Epoch 207 batch 160 train Loss 53.4159 test Loss 24.6030 with MSE metric 19441.8396\n",
      "Epoch 207 batch 170 train Loss 53.4066 test Loss 24.5993 with MSE metric 19439.5295\n",
      "Epoch 207 batch 180 train Loss 53.3974 test Loss 24.5956 with MSE metric 19437.2462\n",
      "Epoch 207 batch 190 train Loss 53.3881 test Loss 24.5919 with MSE metric 19435.0038\n",
      "Epoch 207 batch 200 train Loss 53.3789 test Loss 24.5882 with MSE metric 19432.7385\n",
      "Epoch 207 batch 210 train Loss 53.3696 test Loss 24.5845 with MSE metric 19430.4785\n",
      "Epoch 207 batch 220 train Loss 53.3604 test Loss 24.5808 with MSE metric 19428.2196\n",
      "Epoch 207 batch 230 train Loss 53.3511 test Loss 24.5771 with MSE metric 19425.9542\n",
      "Epoch 207 batch 240 train Loss 53.3419 test Loss 24.5735 with MSE metric 19423.7183\n",
      "Time taken for 1 epoch: 24.167922973632812 secs\n",
      "\n",
      "Epoch 208 batch 0 train Loss 53.3327 test Loss 24.5698 with MSE metric 19421.4514\n",
      "Epoch 208 batch 10 train Loss 53.3234 test Loss 24.5661 with MSE metric 19419.1796\n",
      "Epoch 208 batch 20 train Loss 53.3142 test Loss 24.5624 with MSE metric 19416.9435\n",
      "Epoch 208 batch 30 train Loss 53.3050 test Loss 24.5587 with MSE metric 19414.6514\n",
      "Epoch 208 batch 40 train Loss 53.2957 test Loss 24.5550 with MSE metric 19412.3930\n",
      "Epoch 208 batch 50 train Loss 53.2865 test Loss 24.5514 with MSE metric 19410.0587\n",
      "Epoch 208 batch 60 train Loss 53.2773 test Loss 24.5477 with MSE metric 19407.8869\n",
      "Epoch 208 batch 70 train Loss 53.2681 test Loss 24.5440 with MSE metric 19405.5651\n",
      "Epoch 208 batch 80 train Loss 53.2589 test Loss 24.5403 with MSE metric 19403.3840\n",
      "Epoch 208 batch 90 train Loss 53.2497 test Loss 24.5367 with MSE metric 19401.0919\n",
      "Epoch 208 batch 100 train Loss 53.2405 test Loss 24.5330 with MSE metric 19398.8220\n",
      "Epoch 208 batch 110 train Loss 53.2313 test Loss 24.5293 with MSE metric 19396.6105\n",
      "Epoch 208 batch 120 train Loss 53.2221 test Loss 24.5256 with MSE metric 19394.3615\n",
      "Epoch 208 batch 130 train Loss 53.2129 test Loss 24.5220 with MSE metric 19392.0438\n",
      "Epoch 208 batch 140 train Loss 53.2037 test Loss 24.5183 with MSE metric 19389.8249\n",
      "Epoch 208 batch 150 train Loss 53.1945 test Loss 24.5147 with MSE metric 19387.5514\n",
      "Epoch 208 batch 160 train Loss 53.1854 test Loss 24.5110 with MSE metric 19385.2616\n",
      "Epoch 208 batch 170 train Loss 53.1762 test Loss 24.5073 with MSE metric 19383.0146\n",
      "Epoch 208 batch 180 train Loss 53.1670 test Loss 24.5036 with MSE metric 19380.7593\n",
      "Epoch 208 batch 190 train Loss 53.1578 test Loss 24.5000 with MSE metric 19378.4517\n",
      "Epoch 208 batch 200 train Loss 53.1487 test Loss 24.4963 with MSE metric 19376.2146\n",
      "Epoch 208 batch 210 train Loss 53.1395 test Loss 24.4927 with MSE metric 19373.9182\n",
      "Epoch 208 batch 220 train Loss 53.1304 test Loss 24.4890 with MSE metric 19371.6967\n",
      "Epoch 208 batch 230 train Loss 53.1212 test Loss 24.4853 with MSE metric 19369.4586\n",
      "Epoch 208 batch 240 train Loss 53.1121 test Loss 24.4817 with MSE metric 19367.2593\n",
      "Time taken for 1 epoch: 23.947566986083984 secs\n",
      "\n",
      "Epoch 209 batch 0 train Loss 53.1029 test Loss 24.4780 with MSE metric 19365.0184\n",
      "Epoch 209 batch 10 train Loss 53.0938 test Loss 24.4744 with MSE metric 19362.7561\n",
      "Epoch 209 batch 20 train Loss 53.0846 test Loss 24.4707 with MSE metric 19360.4746\n",
      "Epoch 209 batch 30 train Loss 53.0755 test Loss 24.4671 with MSE metric 19358.1894\n",
      "Epoch 209 batch 40 train Loss 53.0664 test Loss 24.4634 with MSE metric 19355.9116\n",
      "Epoch 209 batch 50 train Loss 53.0572 test Loss 24.4598 with MSE metric 19353.7062\n",
      "Epoch 209 batch 60 train Loss 53.0481 test Loss 24.4561 with MSE metric 19351.4793\n",
      "Epoch 209 batch 70 train Loss 53.0390 test Loss 24.4525 with MSE metric 19349.2354\n",
      "Epoch 209 batch 80 train Loss 53.0299 test Loss 24.4489 with MSE metric 19346.9879\n",
      "Epoch 209 batch 90 train Loss 53.0207 test Loss 24.4452 with MSE metric 19344.7611\n",
      "Epoch 209 batch 100 train Loss 53.0116 test Loss 24.4416 with MSE metric 19342.5511\n",
      "Epoch 209 batch 110 train Loss 53.0025 test Loss 24.4379 with MSE metric 19340.3231\n",
      "Epoch 209 batch 120 train Loss 52.9934 test Loss 24.4343 with MSE metric 19338.1658\n",
      "Epoch 209 batch 130 train Loss 52.9843 test Loss 24.4307 with MSE metric 19335.9861\n",
      "Epoch 209 batch 140 train Loss 52.9752 test Loss 24.4271 with MSE metric 19333.7554\n",
      "Epoch 209 batch 150 train Loss 52.9661 test Loss 24.4234 with MSE metric 19331.6070\n",
      "Epoch 209 batch 160 train Loss 52.9570 test Loss 24.4198 with MSE metric 19329.4192\n",
      "Epoch 209 batch 170 train Loss 52.9480 test Loss 24.4161 with MSE metric 19327.2021\n",
      "Epoch 209 batch 180 train Loss 52.9389 test Loss 24.4125 with MSE metric 19324.9504\n",
      "Epoch 209 batch 190 train Loss 52.9298 test Loss 24.4089 with MSE metric 19322.7427\n",
      "Epoch 209 batch 200 train Loss 52.9207 test Loss 24.4053 with MSE metric 19320.4633\n",
      "Epoch 209 batch 210 train Loss 52.9116 test Loss 24.4017 with MSE metric 19318.2214\n",
      "Epoch 209 batch 220 train Loss 52.9026 test Loss 24.3980 with MSE metric 19316.0379\n",
      "Epoch 209 batch 230 train Loss 52.8935 test Loss 24.3944 with MSE metric 19313.7769\n",
      "Epoch 209 batch 240 train Loss 52.8844 test Loss 24.3908 with MSE metric 19311.4982\n",
      "Time taken for 1 epoch: 23.84233593940735 secs\n",
      "\n",
      "Epoch 210 batch 0 train Loss 52.8754 test Loss 24.3872 with MSE metric 19309.2990\n",
      "Epoch 210 batch 10 train Loss 52.8663 test Loss 24.3836 with MSE metric 19307.0846\n",
      "Epoch 210 batch 20 train Loss 52.8573 test Loss 24.3800 with MSE metric 19304.9178\n",
      "Epoch 210 batch 30 train Loss 52.8482 test Loss 24.3763 with MSE metric 19302.6829\n",
      "Epoch 210 batch 40 train Loss 52.8392 test Loss 24.3727 with MSE metric 19300.4684\n",
      "Epoch 210 batch 50 train Loss 52.8301 test Loss 24.3691 with MSE metric 19298.2120\n",
      "Epoch 210 batch 60 train Loss 52.8211 test Loss 24.3655 with MSE metric 19296.0681\n",
      "Epoch 210 batch 70 train Loss 52.8121 test Loss 24.3619 with MSE metric 19293.8749\n",
      "Epoch 210 batch 80 train Loss 52.8030 test Loss 24.3583 with MSE metric 19291.6535\n",
      "Epoch 210 batch 90 train Loss 52.7940 test Loss 24.3547 with MSE metric 19289.4454\n",
      "Epoch 210 batch 100 train Loss 52.7850 test Loss 24.3511 with MSE metric 19287.2100\n",
      "Epoch 210 batch 110 train Loss 52.7759 test Loss 24.3475 with MSE metric 19285.0097\n",
      "Epoch 210 batch 120 train Loss 52.7669 test Loss 24.3439 with MSE metric 19282.7930\n",
      "Epoch 210 batch 130 train Loss 52.7579 test Loss 24.3403 with MSE metric 19280.5749\n",
      "Epoch 210 batch 140 train Loss 52.7489 test Loss 24.3367 with MSE metric 19278.3022\n",
      "Epoch 210 batch 150 train Loss 52.7399 test Loss 24.3331 with MSE metric 19276.0933\n",
      "Epoch 210 batch 160 train Loss 52.7309 test Loss 24.3295 with MSE metric 19273.8498\n",
      "Epoch 210 batch 170 train Loss 52.7219 test Loss 24.3259 with MSE metric 19271.6296\n",
      "Epoch 210 batch 180 train Loss 52.7129 test Loss 24.3223 with MSE metric 19269.3848\n",
      "Epoch 210 batch 190 train Loss 52.7039 test Loss 24.3187 with MSE metric 19267.1566\n",
      "Epoch 210 batch 200 train Loss 52.6949 test Loss 24.3151 with MSE metric 19264.9111\n",
      "Epoch 210 batch 210 train Loss 52.6859 test Loss 24.3115 with MSE metric 19262.6892\n",
      "Epoch 210 batch 220 train Loss 52.6769 test Loss 24.3079 with MSE metric 19260.4792\n",
      "Epoch 210 batch 230 train Loss 52.6679 test Loss 24.3043 with MSE metric 19258.2441\n",
      "Epoch 210 batch 240 train Loss 52.6590 test Loss 24.3008 with MSE metric 19256.0317\n",
      "Time taken for 1 epoch: 23.73912811279297 secs\n",
      "\n",
      "Epoch 211 batch 0 train Loss 52.6500 test Loss 24.2972 with MSE metric 19253.8069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211 batch 10 train Loss 52.6410 test Loss 24.2936 with MSE metric 19251.6501\n",
      "Epoch 211 batch 20 train Loss 52.6320 test Loss 24.2900 with MSE metric 19249.4707\n",
      "Epoch 211 batch 30 train Loss 52.6231 test Loss 24.2865 with MSE metric 19247.2499\n",
      "Epoch 211 batch 40 train Loss 52.6141 test Loss 24.2829 with MSE metric 19245.0466\n",
      "Epoch 211 batch 50 train Loss 52.6052 test Loss 24.2793 with MSE metric 19242.8685\n",
      "Epoch 211 batch 60 train Loss 52.5962 test Loss 24.2757 with MSE metric 19240.6418\n",
      "Epoch 211 batch 70 train Loss 52.5873 test Loss 24.2722 with MSE metric 19238.4054\n",
      "Epoch 211 batch 80 train Loss 52.5783 test Loss 24.2686 with MSE metric 19236.1809\n",
      "Epoch 211 batch 90 train Loss 52.5694 test Loss 24.2650 with MSE metric 19233.9924\n",
      "Epoch 211 batch 100 train Loss 52.5604 test Loss 24.2615 with MSE metric 19231.7586\n",
      "Epoch 211 batch 110 train Loss 52.5515 test Loss 24.2579 with MSE metric 19229.5781\n",
      "Epoch 211 batch 120 train Loss 52.5425 test Loss 24.2543 with MSE metric 19227.4312\n",
      "Epoch 211 batch 130 train Loss 52.5336 test Loss 24.2507 with MSE metric 19225.2816\n",
      "Epoch 211 batch 140 train Loss 52.5247 test Loss 24.2472 with MSE metric 19223.0758\n",
      "Epoch 211 batch 150 train Loss 52.5158 test Loss 24.2436 with MSE metric 19220.9088\n",
      "Epoch 211 batch 160 train Loss 52.5068 test Loss 24.2400 with MSE metric 19218.7334\n",
      "Epoch 211 batch 170 train Loss 52.4979 test Loss 24.2365 with MSE metric 19216.5969\n",
      "Epoch 211 batch 180 train Loss 52.4890 test Loss 24.2329 with MSE metric 19214.4492\n",
      "Epoch 211 batch 190 train Loss 52.4801 test Loss 24.2293 with MSE metric 19212.2940\n",
      "Epoch 211 batch 200 train Loss 52.4712 test Loss 24.2258 with MSE metric 19210.0939\n",
      "Epoch 211 batch 210 train Loss 52.4623 test Loss 24.2222 with MSE metric 19207.9003\n",
      "Epoch 211 batch 220 train Loss 52.4534 test Loss 24.2187 with MSE metric 19205.7296\n",
      "Epoch 211 batch 230 train Loss 52.4445 test Loss 24.2151 with MSE metric 19203.5334\n",
      "Epoch 211 batch 240 train Loss 52.4356 test Loss 24.2116 with MSE metric 19201.3844\n",
      "Time taken for 1 epoch: 24.103047847747803 secs\n",
      "\n",
      "Epoch 212 batch 0 train Loss 52.4267 test Loss 24.2080 with MSE metric 19199.1629\n",
      "Epoch 212 batch 10 train Loss 52.4178 test Loss 24.2045 with MSE metric 19196.9695\n",
      "Epoch 212 batch 20 train Loss 52.4089 test Loss 24.2009 with MSE metric 19194.8266\n",
      "Epoch 212 batch 30 train Loss 52.4001 test Loss 24.1974 with MSE metric 19192.6371\n",
      "Epoch 212 batch 40 train Loss 52.3912 test Loss 24.1938 with MSE metric 19190.4436\n",
      "Epoch 212 batch 50 train Loss 52.3823 test Loss 24.1903 with MSE metric 19188.3159\n",
      "Epoch 212 batch 60 train Loss 52.3734 test Loss 24.1868 with MSE metric 19186.1518\n",
      "Epoch 212 batch 70 train Loss 52.3646 test Loss 24.1832 with MSE metric 19183.9805\n",
      "Epoch 212 batch 80 train Loss 52.3557 test Loss 24.1797 with MSE metric 19181.8369\n",
      "Epoch 212 batch 90 train Loss 52.3469 test Loss 24.1761 with MSE metric 19179.6846\n",
      "Epoch 212 batch 100 train Loss 52.3380 test Loss 24.1726 with MSE metric 19177.4850\n",
      "Epoch 212 batch 110 train Loss 52.3291 test Loss 24.1691 with MSE metric 19175.2861\n",
      "Epoch 212 batch 120 train Loss 52.3203 test Loss 24.1655 with MSE metric 19173.0609\n",
      "Epoch 212 batch 130 train Loss 52.3114 test Loss 24.1620 with MSE metric 19170.9021\n",
      "Epoch 212 batch 140 train Loss 52.3026 test Loss 24.1585 with MSE metric 19168.7649\n",
      "Epoch 212 batch 150 train Loss 52.2938 test Loss 24.1549 with MSE metric 19166.5562\n",
      "Epoch 212 batch 160 train Loss 52.2849 test Loss 24.1514 with MSE metric 19164.3671\n",
      "Epoch 212 batch 170 train Loss 52.2761 test Loss 24.1479 with MSE metric 19162.1924\n",
      "Epoch 212 batch 180 train Loss 52.2673 test Loss 24.1444 with MSE metric 19160.0384\n",
      "Epoch 212 batch 190 train Loss 52.2584 test Loss 24.1409 with MSE metric 19157.9397\n",
      "Epoch 212 batch 200 train Loss 52.2496 test Loss 24.1373 with MSE metric 19155.7265\n",
      "Epoch 212 batch 210 train Loss 52.2408 test Loss 24.1338 with MSE metric 19153.5351\n",
      "Epoch 212 batch 220 train Loss 52.2320 test Loss 24.1303 with MSE metric 19151.4026\n",
      "Epoch 212 batch 230 train Loss 52.2232 test Loss 24.1268 with MSE metric 19149.2798\n",
      "Epoch 212 batch 240 train Loss 52.2143 test Loss 24.1233 with MSE metric 19147.1134\n",
      "Time taken for 1 epoch: 24.39232897758484 secs\n",
      "\n",
      "Epoch 213 batch 0 train Loss 52.2055 test Loss 24.1197 with MSE metric 19144.9459\n",
      "Epoch 213 batch 10 train Loss 52.1967 test Loss 24.1162 with MSE metric 19142.8370\n",
      "Epoch 213 batch 20 train Loss 52.1879 test Loss 24.1127 with MSE metric 19140.6319\n",
      "Epoch 213 batch 30 train Loss 52.1791 test Loss 24.1092 with MSE metric 19138.5206\n",
      "Epoch 213 batch 40 train Loss 52.1703 test Loss 24.1057 with MSE metric 19136.3824\n",
      "Epoch 213 batch 50 train Loss 52.1616 test Loss 24.1022 with MSE metric 19134.2102\n",
      "Epoch 213 batch 60 train Loss 52.1528 test Loss 24.0987 with MSE metric 19132.0259\n",
      "Epoch 213 batch 70 train Loss 52.1440 test Loss 24.0952 with MSE metric 19129.8169\n",
      "Epoch 213 batch 80 train Loss 52.1352 test Loss 24.0916 with MSE metric 19127.6793\n",
      "Epoch 213 batch 90 train Loss 52.1264 test Loss 24.0881 with MSE metric 19125.5506\n",
      "Epoch 213 batch 100 train Loss 52.1176 test Loss 24.0846 with MSE metric 19123.3837\n",
      "Epoch 213 batch 110 train Loss 52.1089 test Loss 24.0811 with MSE metric 19121.2143\n",
      "Epoch 213 batch 120 train Loss 52.1001 test Loss 24.0776 with MSE metric 19119.0844\n",
      "Epoch 213 batch 130 train Loss 52.0913 test Loss 24.0741 with MSE metric 19116.9818\n",
      "Epoch 213 batch 140 train Loss 52.0826 test Loss 24.0706 with MSE metric 19114.8277\n",
      "Epoch 213 batch 150 train Loss 52.0738 test Loss 24.0671 with MSE metric 19112.8004\n",
      "Epoch 213 batch 160 train Loss 52.0651 test Loss 24.0636 with MSE metric 19110.6658\n",
      "Epoch 213 batch 170 train Loss 52.0563 test Loss 24.0601 with MSE metric 19108.5124\n",
      "Epoch 213 batch 180 train Loss 52.0476 test Loss 24.0566 with MSE metric 19106.3623\n",
      "Epoch 213 batch 190 train Loss 52.0388 test Loss 24.0531 with MSE metric 19104.1824\n",
      "Epoch 213 batch 200 train Loss 52.0301 test Loss 24.0496 with MSE metric 19101.9909\n",
      "Epoch 213 batch 210 train Loss 52.0213 test Loss 24.0461 with MSE metric 19099.8298\n",
      "Epoch 213 batch 220 train Loss 52.0126 test Loss 24.0427 with MSE metric 19097.7125\n",
      "Epoch 213 batch 230 train Loss 52.0039 test Loss 24.0392 with MSE metric 19095.5521\n",
      "Epoch 213 batch 240 train Loss 51.9952 test Loss 24.0357 with MSE metric 19093.3587\n",
      "Time taken for 1 epoch: 24.536451816558838 secs\n",
      "\n",
      "Epoch 214 batch 0 train Loss 51.9864 test Loss 24.0322 with MSE metric 19091.2085\n",
      "Epoch 214 batch 10 train Loss 51.9777 test Loss 24.0287 with MSE metric 19089.0499\n",
      "Epoch 214 batch 20 train Loss 51.9690 test Loss 24.0253 with MSE metric 19086.9422\n",
      "Epoch 214 batch 30 train Loss 51.9603 test Loss 24.0218 with MSE metric 19084.7709\n",
      "Epoch 214 batch 40 train Loss 51.9516 test Loss 24.0183 with MSE metric 19082.5933\n",
      "Epoch 214 batch 50 train Loss 51.9428 test Loss 24.0149 with MSE metric 19080.4652\n",
      "Epoch 214 batch 60 train Loss 51.9341 test Loss 24.0114 with MSE metric 19078.3254\n",
      "Epoch 214 batch 70 train Loss 51.9254 test Loss 24.0079 with MSE metric 19076.1969\n",
      "Epoch 214 batch 80 train Loss 51.9167 test Loss 24.0044 with MSE metric 19073.9991\n",
      "Epoch 214 batch 90 train Loss 51.9081 test Loss 24.0009 with MSE metric 19071.8901\n",
      "Epoch 214 batch 100 train Loss 51.8994 test Loss 23.9974 with MSE metric 19069.7836\n",
      "Epoch 214 batch 110 train Loss 51.8907 test Loss 23.9940 with MSE metric 19067.6694\n",
      "Epoch 214 batch 120 train Loss 51.8820 test Loss 23.9905 with MSE metric 19065.5603\n",
      "Epoch 214 batch 130 train Loss 51.8733 test Loss 23.9870 with MSE metric 19063.4061\n",
      "Epoch 214 batch 140 train Loss 51.8646 test Loss 23.9836 with MSE metric 19061.2889\n",
      "Epoch 214 batch 150 train Loss 51.8559 test Loss 23.9801 with MSE metric 19059.1896\n",
      "Epoch 214 batch 160 train Loss 51.8473 test Loss 23.9766 with MSE metric 19057.0303\n",
      "Epoch 214 batch 170 train Loss 51.8386 test Loss 23.9731 with MSE metric 19054.9515\n",
      "Epoch 214 batch 180 train Loss 51.8299 test Loss 23.9697 with MSE metric 19052.9113\n",
      "Epoch 214 batch 190 train Loss 51.8213 test Loss 23.9662 with MSE metric 19050.7573\n",
      "Epoch 214 batch 200 train Loss 51.8126 test Loss 23.9627 with MSE metric 19048.6680\n",
      "Epoch 214 batch 210 train Loss 51.8040 test Loss 23.9593 with MSE metric 19046.5286\n",
      "Epoch 214 batch 220 train Loss 51.7953 test Loss 23.9558 with MSE metric 19044.4161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 batch 230 train Loss 51.7867 test Loss 23.9524 with MSE metric 19042.2261\n",
      "Epoch 214 batch 240 train Loss 51.7780 test Loss 23.9489 with MSE metric 19040.0889\n",
      "Time taken for 1 epoch: 24.47244906425476 secs\n",
      "\n",
      "Epoch 215 batch 0 train Loss 51.7694 test Loss 23.9455 with MSE metric 19037.9834\n",
      "Epoch 215 batch 10 train Loss 51.7607 test Loss 23.9420 with MSE metric 19035.9704\n",
      "Epoch 215 batch 20 train Loss 51.7521 test Loss 23.9386 with MSE metric 19033.7975\n",
      "Epoch 215 batch 30 train Loss 51.7435 test Loss 23.9351 with MSE metric 19031.6647\n",
      "Epoch 215 batch 40 train Loss 51.7348 test Loss 23.9317 with MSE metric 19029.5907\n",
      "Epoch 215 batch 50 train Loss 51.7262 test Loss 23.9282 with MSE metric 19027.5356\n",
      "Epoch 215 batch 60 train Loss 51.7176 test Loss 23.9248 with MSE metric 19025.3811\n",
      "Epoch 215 batch 70 train Loss 51.7090 test Loss 23.9213 with MSE metric 19023.2808\n",
      "Epoch 215 batch 80 train Loss 51.7003 test Loss 23.9179 with MSE metric 19021.1074\n",
      "Epoch 215 batch 90 train Loss 51.6917 test Loss 23.9145 with MSE metric 19018.9890\n",
      "Epoch 215 batch 100 train Loss 51.6831 test Loss 23.9110 with MSE metric 19016.9374\n",
      "Epoch 215 batch 110 train Loss 51.6745 test Loss 23.9076 with MSE metric 19014.8770\n",
      "Epoch 215 batch 120 train Loss 51.6659 test Loss 23.9042 with MSE metric 19012.7754\n",
      "Epoch 215 batch 130 train Loss 51.6573 test Loss 23.9007 with MSE metric 19010.6160\n",
      "Epoch 215 batch 140 train Loss 51.6487 test Loss 23.8973 with MSE metric 19008.5422\n",
      "Epoch 215 batch 150 train Loss 51.6401 test Loss 23.8939 with MSE metric 19006.4418\n",
      "Epoch 215 batch 160 train Loss 51.6315 test Loss 23.8904 with MSE metric 19004.3481\n",
      "Epoch 215 batch 170 train Loss 51.6229 test Loss 23.8870 with MSE metric 19002.2155\n",
      "Epoch 215 batch 180 train Loss 51.6143 test Loss 23.8835 with MSE metric 19000.0808\n",
      "Epoch 215 batch 190 train Loss 51.6057 test Loss 23.8801 with MSE metric 18997.9708\n",
      "Epoch 215 batch 200 train Loss 51.5971 test Loss 23.8767 with MSE metric 18995.8027\n",
      "Epoch 215 batch 210 train Loss 51.5886 test Loss 23.8733 with MSE metric 18993.6443\n",
      "Epoch 215 batch 220 train Loss 51.5800 test Loss 23.8698 with MSE metric 18991.5088\n",
      "Epoch 215 batch 230 train Loss 51.5714 test Loss 23.8664 with MSE metric 18989.3620\n",
      "Epoch 215 batch 240 train Loss 51.5629 test Loss 23.8630 with MSE metric 18987.2772\n",
      "Time taken for 1 epoch: 24.336596965789795 secs\n",
      "\n",
      "Epoch 216 batch 0 train Loss 51.5543 test Loss 23.8596 with MSE metric 18985.1398\n",
      "Epoch 216 batch 10 train Loss 51.5457 test Loss 23.8562 with MSE metric 18983.0621\n",
      "Epoch 216 batch 20 train Loss 51.5372 test Loss 23.8527 with MSE metric 18980.8984\n",
      "Epoch 216 batch 30 train Loss 51.5286 test Loss 23.8493 with MSE metric 18978.7355\n",
      "Epoch 216 batch 40 train Loss 51.5201 test Loss 23.8459 with MSE metric 18976.6098\n",
      "Epoch 216 batch 50 train Loss 51.5115 test Loss 23.8425 with MSE metric 18974.5123\n",
      "Epoch 216 batch 60 train Loss 51.5030 test Loss 23.8391 with MSE metric 18972.4309\n",
      "Epoch 216 batch 70 train Loss 51.4944 test Loss 23.8357 with MSE metric 18970.3282\n",
      "Epoch 216 batch 80 train Loss 51.4859 test Loss 23.8323 with MSE metric 18968.2927\n",
      "Epoch 216 batch 90 train Loss 51.4774 test Loss 23.8289 with MSE metric 18966.2097\n",
      "Epoch 216 batch 100 train Loss 51.4688 test Loss 23.8255 with MSE metric 18964.0568\n",
      "Epoch 216 batch 110 train Loss 51.4603 test Loss 23.8220 with MSE metric 18961.9664\n",
      "Epoch 216 batch 120 train Loss 51.4518 test Loss 23.8186 with MSE metric 18959.8923\n",
      "Epoch 216 batch 130 train Loss 51.4432 test Loss 23.8152 with MSE metric 18957.8186\n",
      "Epoch 216 batch 140 train Loss 51.4347 test Loss 23.8118 with MSE metric 18955.7646\n",
      "Epoch 216 batch 150 train Loss 51.4262 test Loss 23.8084 with MSE metric 18953.6158\n",
      "Epoch 216 batch 160 train Loss 51.4177 test Loss 23.8050 with MSE metric 18951.5334\n",
      "Epoch 216 batch 170 train Loss 51.4092 test Loss 23.8016 with MSE metric 18949.4554\n",
      "Epoch 216 batch 180 train Loss 51.4007 test Loss 23.7982 with MSE metric 18947.3533\n",
      "Epoch 216 batch 190 train Loss 51.3922 test Loss 23.7948 with MSE metric 18945.2821\n",
      "Epoch 216 batch 200 train Loss 51.3837 test Loss 23.7914 with MSE metric 18943.2057\n",
      "Epoch 216 batch 210 train Loss 51.3752 test Loss 23.7880 with MSE metric 18941.1360\n",
      "Epoch 216 batch 220 train Loss 51.3667 test Loss 23.7847 with MSE metric 18938.9998\n",
      "Epoch 216 batch 230 train Loss 51.3582 test Loss 23.7813 with MSE metric 18936.9005\n",
      "Epoch 216 batch 240 train Loss 51.3497 test Loss 23.7779 with MSE metric 18934.8130\n",
      "Time taken for 1 epoch: 24.941506147384644 secs\n",
      "\n",
      "Epoch 217 batch 0 train Loss 51.3412 test Loss 23.7745 with MSE metric 18932.7600\n",
      "Epoch 217 batch 10 train Loss 51.3327 test Loss 23.7711 with MSE metric 18930.7436\n",
      "Epoch 217 batch 20 train Loss 51.3242 test Loss 23.7677 with MSE metric 18928.6948\n",
      "Epoch 217 batch 30 train Loss 51.3158 test Loss 23.7643 with MSE metric 18926.6561\n",
      "Epoch 217 batch 40 train Loss 51.3073 test Loss 23.7609 with MSE metric 18924.5654\n",
      "Epoch 217 batch 50 train Loss 51.2988 test Loss 23.7576 with MSE metric 18922.4926\n",
      "Epoch 217 batch 60 train Loss 51.2904 test Loss 23.7542 with MSE metric 18920.4099\n",
      "Epoch 217 batch 70 train Loss 51.2819 test Loss 23.7508 with MSE metric 18918.3918\n",
      "Epoch 217 batch 80 train Loss 51.2735 test Loss 23.7474 with MSE metric 18916.3357\n",
      "Epoch 217 batch 90 train Loss 51.2650 test Loss 23.7440 with MSE metric 18914.3205\n",
      "Epoch 217 batch 100 train Loss 51.2565 test Loss 23.7406 with MSE metric 18912.2388\n",
      "Epoch 217 batch 110 train Loss 51.2481 test Loss 23.7372 with MSE metric 18910.0549\n",
      "Epoch 217 batch 120 train Loss 51.2396 test Loss 23.7338 with MSE metric 18908.0204\n",
      "Epoch 217 batch 130 train Loss 51.2312 test Loss 23.7305 with MSE metric 18905.9776\n",
      "Epoch 217 batch 140 train Loss 51.2228 test Loss 23.7271 with MSE metric 18903.9097\n",
      "Epoch 217 batch 150 train Loss 51.2143 test Loss 23.7237 with MSE metric 18901.8769\n",
      "Epoch 217 batch 160 train Loss 51.2059 test Loss 23.7203 with MSE metric 18899.7832\n",
      "Epoch 217 batch 170 train Loss 51.1974 test Loss 23.7170 with MSE metric 18897.6905\n",
      "Epoch 217 batch 180 train Loss 51.1890 test Loss 23.7136 with MSE metric 18895.5770\n",
      "Epoch 217 batch 190 train Loss 51.1806 test Loss 23.7102 with MSE metric 18893.5802\n",
      "Epoch 217 batch 200 train Loss 51.1722 test Loss 23.7069 with MSE metric 18891.5131\n",
      "Epoch 217 batch 210 train Loss 51.1637 test Loss 23.7035 with MSE metric 18889.4319\n",
      "Epoch 217 batch 220 train Loss 51.1553 test Loss 23.7001 with MSE metric 18887.3747\n",
      "Epoch 217 batch 230 train Loss 51.1469 test Loss 23.6968 with MSE metric 18885.3672\n",
      "Epoch 217 batch 240 train Loss 51.1385 test Loss 23.6934 with MSE metric 18883.2937\n",
      "Time taken for 1 epoch: 26.831255674362183 secs\n",
      "\n",
      "Epoch 218 batch 0 train Loss 51.1301 test Loss 23.6901 with MSE metric 18881.1787\n",
      "Epoch 218 batch 10 train Loss 51.1217 test Loss 23.6867 with MSE metric 18879.0408\n",
      "Epoch 218 batch 20 train Loss 51.1133 test Loss 23.6833 with MSE metric 18876.9435\n",
      "Epoch 218 batch 30 train Loss 51.1049 test Loss 23.6800 with MSE metric 18874.9138\n",
      "Epoch 218 batch 40 train Loss 51.0965 test Loss 23.6766 with MSE metric 18872.8806\n",
      "Epoch 218 batch 50 train Loss 51.0881 test Loss 23.6733 with MSE metric 18870.8325\n",
      "Epoch 218 batch 60 train Loss 51.0797 test Loss 23.6699 with MSE metric 18868.7592\n",
      "Epoch 218 batch 70 train Loss 51.0713 test Loss 23.6666 with MSE metric 18866.6476\n",
      "Epoch 218 batch 80 train Loss 51.0630 test Loss 23.6632 with MSE metric 18864.6601\n",
      "Epoch 218 batch 90 train Loss 51.0546 test Loss 23.6599 with MSE metric 18862.5822\n",
      "Epoch 218 batch 100 train Loss 51.0462 test Loss 23.6566 with MSE metric 18860.4807\n",
      "Epoch 218 batch 110 train Loss 51.0378 test Loss 23.6532 with MSE metric 18858.3871\n",
      "Epoch 218 batch 120 train Loss 51.0294 test Loss 23.6499 with MSE metric 18856.3366\n",
      "Epoch 218 batch 130 train Loss 51.0211 test Loss 23.6465 with MSE metric 18854.2133\n",
      "Epoch 218 batch 140 train Loss 51.0127 test Loss 23.6431 with MSE metric 18852.1590\n",
      "Epoch 218 batch 150 train Loss 51.0043 test Loss 23.6398 with MSE metric 18850.0211\n",
      "Epoch 218 batch 160 train Loss 50.9960 test Loss 23.6365 with MSE metric 18847.9512\n",
      "Epoch 218 batch 170 train Loss 50.9876 test Loss 23.6331 with MSE metric 18845.9224\n",
      "Epoch 218 batch 180 train Loss 50.9793 test Loss 23.6298 with MSE metric 18843.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218 batch 190 train Loss 50.9709 test Loss 23.6265 with MSE metric 18841.7145\n",
      "Epoch 218 batch 200 train Loss 50.9626 test Loss 23.6231 with MSE metric 18839.6726\n",
      "Epoch 218 batch 210 train Loss 50.9542 test Loss 23.6198 with MSE metric 18837.6689\n",
      "Epoch 218 batch 220 train Loss 50.9459 test Loss 23.6165 with MSE metric 18835.6531\n",
      "Epoch 218 batch 230 train Loss 50.9376 test Loss 23.6131 with MSE metric 18833.6200\n",
      "Epoch 218 batch 240 train Loss 50.9292 test Loss 23.6098 with MSE metric 18831.5739\n",
      "Time taken for 1 epoch: 24.92932677268982 secs\n",
      "\n",
      "Epoch 219 batch 0 train Loss 50.9209 test Loss 23.6065 with MSE metric 18829.5741\n",
      "Epoch 219 batch 10 train Loss 50.9126 test Loss 23.6031 with MSE metric 18827.4662\n",
      "Epoch 219 batch 20 train Loss 50.9042 test Loss 23.5998 with MSE metric 18825.3941\n",
      "Epoch 219 batch 30 train Loss 50.8959 test Loss 23.5965 with MSE metric 18823.3353\n",
      "Epoch 219 batch 40 train Loss 50.8876 test Loss 23.5932 with MSE metric 18821.2770\n",
      "Epoch 219 batch 50 train Loss 50.8793 test Loss 23.5898 with MSE metric 18819.2195\n",
      "Epoch 219 batch 60 train Loss 50.8710 test Loss 23.5865 with MSE metric 18817.2118\n",
      "Epoch 219 batch 70 train Loss 50.8627 test Loss 23.5832 with MSE metric 18815.1725\n",
      "Epoch 219 batch 80 train Loss 50.8543 test Loss 23.5799 with MSE metric 18813.0959\n",
      "Epoch 219 batch 90 train Loss 50.8460 test Loss 23.5765 with MSE metric 18811.0087\n",
      "Epoch 219 batch 100 train Loss 50.8377 test Loss 23.5732 with MSE metric 18808.9047\n",
      "Epoch 219 batch 110 train Loss 50.8294 test Loss 23.5699 with MSE metric 18806.8897\n",
      "Epoch 219 batch 120 train Loss 50.8211 test Loss 23.5666 with MSE metric 18804.8802\n",
      "Epoch 219 batch 130 train Loss 50.8128 test Loss 23.5633 with MSE metric 18802.8654\n",
      "Epoch 219 batch 140 train Loss 50.8046 test Loss 23.5599 with MSE metric 18800.7989\n",
      "Epoch 219 batch 150 train Loss 50.7963 test Loss 23.5566 with MSE metric 18798.8463\n",
      "Epoch 219 batch 160 train Loss 50.7880 test Loss 23.5533 with MSE metric 18796.7512\n",
      "Epoch 219 batch 170 train Loss 50.7797 test Loss 23.5500 with MSE metric 18794.7080\n",
      "Epoch 219 batch 180 train Loss 50.7714 test Loss 23.5467 with MSE metric 18792.6935\n",
      "Epoch 219 batch 190 train Loss 50.7632 test Loss 23.5434 with MSE metric 18790.5921\n",
      "Epoch 219 batch 200 train Loss 50.7549 test Loss 23.5401 with MSE metric 18788.5771\n",
      "Epoch 219 batch 210 train Loss 50.7466 test Loss 23.5368 with MSE metric 18786.5078\n",
      "Epoch 219 batch 220 train Loss 50.7384 test Loss 23.5335 with MSE metric 18784.4691\n",
      "Epoch 219 batch 230 train Loss 50.7301 test Loss 23.5303 with MSE metric 18782.4255\n",
      "Epoch 219 batch 240 train Loss 50.7218 test Loss 23.5270 with MSE metric 18780.4171\n",
      "Time taken for 1 epoch: 24.427538871765137 secs\n",
      "\n",
      "Epoch 220 batch 0 train Loss 50.7136 test Loss 23.5237 with MSE metric 18778.4518\n",
      "Epoch 220 batch 10 train Loss 50.7053 test Loss 23.5204 with MSE metric 18776.4356\n",
      "Epoch 220 batch 20 train Loss 50.6971 test Loss 23.5171 with MSE metric 18774.4007\n",
      "Epoch 220 batch 30 train Loss 50.6889 test Loss 23.5138 with MSE metric 18772.3409\n",
      "Epoch 220 batch 40 train Loss 50.6806 test Loss 23.5105 with MSE metric 18770.3392\n",
      "Epoch 220 batch 50 train Loss 50.6724 test Loss 23.5072 with MSE metric 18768.3217\n",
      "Epoch 220 batch 60 train Loss 50.6641 test Loss 23.5039 with MSE metric 18766.3007\n",
      "Epoch 220 batch 70 train Loss 50.6559 test Loss 23.5006 with MSE metric 18764.2859\n",
      "Epoch 220 batch 80 train Loss 50.6477 test Loss 23.4973 with MSE metric 18762.2471\n",
      "Epoch 220 batch 90 train Loss 50.6394 test Loss 23.4940 with MSE metric 18760.2728\n",
      "Epoch 220 batch 100 train Loss 50.6312 test Loss 23.4908 with MSE metric 18758.2446\n",
      "Epoch 220 batch 110 train Loss 50.6230 test Loss 23.4875 with MSE metric 18756.2623\n",
      "Epoch 220 batch 120 train Loss 50.6148 test Loss 23.4842 with MSE metric 18754.2894\n",
      "Epoch 220 batch 130 train Loss 50.6066 test Loss 23.4809 with MSE metric 18752.2104\n",
      "Epoch 220 batch 140 train Loss 50.5983 test Loss 23.4776 with MSE metric 18750.1597\n",
      "Epoch 220 batch 150 train Loss 50.5901 test Loss 23.4743 with MSE metric 18748.1506\n",
      "Epoch 220 batch 160 train Loss 50.5819 test Loss 23.4710 with MSE metric 18746.1209\n",
      "Epoch 220 batch 170 train Loss 50.5737 test Loss 23.4678 with MSE metric 18744.0705\n",
      "Epoch 220 batch 180 train Loss 50.5655 test Loss 23.4645 with MSE metric 18742.0223\n",
      "Epoch 220 batch 190 train Loss 50.5573 test Loss 23.4612 with MSE metric 18740.0380\n",
      "Epoch 220 batch 200 train Loss 50.5491 test Loss 23.4579 with MSE metric 18737.9855\n",
      "Epoch 220 batch 210 train Loss 50.5409 test Loss 23.4547 with MSE metric 18735.9815\n",
      "Epoch 220 batch 220 train Loss 50.5327 test Loss 23.4514 with MSE metric 18733.9326\n",
      "Epoch 220 batch 230 train Loss 50.5245 test Loss 23.4481 with MSE metric 18731.8993\n",
      "Epoch 220 batch 240 train Loss 50.5163 test Loss 23.4449 with MSE metric 18729.8580\n",
      "Time taken for 1 epoch: 24.271629810333252 secs\n",
      "\n",
      "Epoch 221 batch 0 train Loss 50.5082 test Loss 23.4416 with MSE metric 18727.8686\n",
      "Epoch 221 batch 10 train Loss 50.5000 test Loss 23.4383 with MSE metric 18725.8046\n",
      "Epoch 221 batch 20 train Loss 50.4918 test Loss 23.4350 with MSE metric 18723.8283\n",
      "Epoch 221 batch 30 train Loss 50.4836 test Loss 23.4318 with MSE metric 18721.8719\n",
      "Epoch 221 batch 40 train Loss 50.4755 test Loss 23.4285 with MSE metric 18719.8452\n",
      "Epoch 221 batch 50 train Loss 50.4673 test Loss 23.4252 with MSE metric 18717.8840\n",
      "Epoch 221 batch 60 train Loss 50.4592 test Loss 23.4220 with MSE metric 18715.8058\n",
      "Epoch 221 batch 70 train Loss 50.4510 test Loss 23.4187 with MSE metric 18713.8798\n",
      "Epoch 221 batch 80 train Loss 50.4428 test Loss 23.4155 with MSE metric 18711.8855\n",
      "Epoch 221 batch 90 train Loss 50.4347 test Loss 23.4122 with MSE metric 18709.8787\n",
      "Epoch 221 batch 100 train Loss 50.4265 test Loss 23.4090 with MSE metric 18707.8578\n",
      "Epoch 221 batch 110 train Loss 50.4184 test Loss 23.4057 with MSE metric 18705.8911\n",
      "Epoch 221 batch 120 train Loss 50.4102 test Loss 23.4025 with MSE metric 18703.8591\n",
      "Epoch 221 batch 130 train Loss 50.4021 test Loss 23.3992 with MSE metric 18701.8553\n",
      "Epoch 221 batch 140 train Loss 50.3940 test Loss 23.3959 with MSE metric 18699.8417\n",
      "Epoch 221 batch 150 train Loss 50.3858 test Loss 23.3927 with MSE metric 18697.8525\n",
      "Epoch 221 batch 160 train Loss 50.3777 test Loss 23.3894 with MSE metric 18695.8250\n",
      "Epoch 221 batch 170 train Loss 50.3695 test Loss 23.3862 with MSE metric 18693.7656\n",
      "Epoch 221 batch 180 train Loss 50.3614 test Loss 23.3830 with MSE metric 18691.7620\n",
      "Epoch 221 batch 190 train Loss 50.3533 test Loss 23.3797 with MSE metric 18689.8013\n",
      "Epoch 221 batch 200 train Loss 50.3452 test Loss 23.3765 with MSE metric 18687.7426\n",
      "Epoch 221 batch 210 train Loss 50.3371 test Loss 23.3732 with MSE metric 18685.7214\n",
      "Epoch 221 batch 220 train Loss 50.3289 test Loss 23.3700 with MSE metric 18683.7548\n",
      "Epoch 221 batch 230 train Loss 50.3208 test Loss 23.3667 with MSE metric 18681.7444\n",
      "Epoch 221 batch 240 train Loss 50.3127 test Loss 23.3635 with MSE metric 18679.6955\n",
      "Time taken for 1 epoch: 24.446473121643066 secs\n",
      "\n",
      "Epoch 222 batch 0 train Loss 50.3046 test Loss 23.3603 with MSE metric 18677.7221\n",
      "Epoch 222 batch 10 train Loss 50.2965 test Loss 23.3570 with MSE metric 18675.7141\n",
      "Epoch 222 batch 20 train Loss 50.2884 test Loss 23.3538 with MSE metric 18673.7367\n",
      "Epoch 222 batch 30 train Loss 50.2803 test Loss 23.3506 with MSE metric 18671.7600\n",
      "Epoch 222 batch 40 train Loss 50.2722 test Loss 23.3473 with MSE metric 18669.7986\n",
      "Epoch 222 batch 50 train Loss 50.2641 test Loss 23.3441 with MSE metric 18667.8031\n",
      "Epoch 222 batch 60 train Loss 50.2560 test Loss 23.3408 with MSE metric 18665.7920\n",
      "Epoch 222 batch 70 train Loss 50.2479 test Loss 23.3376 with MSE metric 18663.8093\n",
      "Epoch 222 batch 80 train Loss 50.2398 test Loss 23.3344 with MSE metric 18661.7626\n",
      "Epoch 222 batch 90 train Loss 50.2318 test Loss 23.3312 with MSE metric 18659.7519\n",
      "Epoch 222 batch 100 train Loss 50.2237 test Loss 23.3280 with MSE metric 18657.7396\n",
      "Epoch 222 batch 110 train Loss 50.2156 test Loss 23.3248 with MSE metric 18655.6810\n",
      "Epoch 222 batch 120 train Loss 50.2075 test Loss 23.3215 with MSE metric 18653.7059\n",
      "Epoch 222 batch 130 train Loss 50.1995 test Loss 23.3183 with MSE metric 18651.7934\n",
      "Epoch 222 batch 140 train Loss 50.1914 test Loss 23.3151 with MSE metric 18649.8305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222 batch 150 train Loss 50.1833 test Loss 23.3119 with MSE metric 18647.8743\n",
      "Epoch 222 batch 160 train Loss 50.1753 test Loss 23.3087 with MSE metric 18645.8876\n",
      "Epoch 222 batch 170 train Loss 50.1672 test Loss 23.3054 with MSE metric 18643.8782\n",
      "Epoch 222 batch 180 train Loss 50.1592 test Loss 23.3022 with MSE metric 18641.8681\n",
      "Epoch 222 batch 190 train Loss 50.1511 test Loss 23.2990 with MSE metric 18639.9615\n",
      "Epoch 222 batch 200 train Loss 50.1431 test Loss 23.2958 with MSE metric 18637.9689\n",
      "Epoch 222 batch 210 train Loss 50.1350 test Loss 23.2926 with MSE metric 18635.9690\n",
      "Epoch 222 batch 220 train Loss 50.1270 test Loss 23.2893 with MSE metric 18633.9877\n",
      "Epoch 222 batch 230 train Loss 50.1189 test Loss 23.2861 with MSE metric 18632.0284\n",
      "Epoch 222 batch 240 train Loss 50.1109 test Loss 23.2829 with MSE metric 18630.0406\n",
      "Time taken for 1 epoch: 24.86866283416748 secs\n",
      "\n",
      "Epoch 223 batch 0 train Loss 50.1028 test Loss 23.2797 with MSE metric 18628.0436\n",
      "Epoch 223 batch 10 train Loss 50.0948 test Loss 23.2765 with MSE metric 18626.0339\n",
      "Epoch 223 batch 20 train Loss 50.0868 test Loss 23.2733 with MSE metric 18624.0167\n",
      "Epoch 223 batch 30 train Loss 50.0788 test Loss 23.2701 with MSE metric 18622.0786\n",
      "Epoch 223 batch 40 train Loss 50.0707 test Loss 23.2669 with MSE metric 18620.1506\n",
      "Epoch 223 batch 50 train Loss 50.0627 test Loss 23.2637 with MSE metric 18618.2416\n",
      "Epoch 223 batch 60 train Loss 50.0547 test Loss 23.2604 with MSE metric 18616.2756\n",
      "Epoch 223 batch 70 train Loss 50.0467 test Loss 23.2572 with MSE metric 18614.3274\n",
      "Epoch 223 batch 80 train Loss 50.0387 test Loss 23.2540 with MSE metric 18612.3766\n",
      "Epoch 223 batch 90 train Loss 50.0307 test Loss 23.2508 with MSE metric 18610.4302\n",
      "Epoch 223 batch 100 train Loss 50.0227 test Loss 23.2476 with MSE metric 18608.4354\n",
      "Epoch 223 batch 110 train Loss 50.0147 test Loss 23.2444 with MSE metric 18606.4920\n",
      "Epoch 223 batch 120 train Loss 50.0067 test Loss 23.2412 with MSE metric 18604.6069\n",
      "Epoch 223 batch 130 train Loss 49.9987 test Loss 23.2380 with MSE metric 18602.6777\n",
      "Epoch 223 batch 140 train Loss 49.9907 test Loss 23.2348 with MSE metric 18600.7396\n",
      "Epoch 223 batch 150 train Loss 49.9827 test Loss 23.2316 with MSE metric 18598.7593\n",
      "Epoch 223 batch 160 train Loss 49.9747 test Loss 23.2284 with MSE metric 18596.7805\n",
      "Epoch 223 batch 170 train Loss 49.9667 test Loss 23.2253 with MSE metric 18594.7934\n",
      "Epoch 223 batch 180 train Loss 49.9587 test Loss 23.2221 with MSE metric 18592.7955\n",
      "Epoch 223 batch 190 train Loss 49.9507 test Loss 23.2189 with MSE metric 18590.8283\n",
      "Epoch 223 batch 200 train Loss 49.9428 test Loss 23.2157 with MSE metric 18588.8677\n",
      "Epoch 223 batch 210 train Loss 49.9348 test Loss 23.2125 with MSE metric 18586.9619\n",
      "Epoch 223 batch 220 train Loss 49.9268 test Loss 23.2093 with MSE metric 18585.0609\n",
      "Epoch 223 batch 230 train Loss 49.9189 test Loss 23.2061 with MSE metric 18583.1087\n",
      "Epoch 223 batch 240 train Loss 49.9109 test Loss 23.2029 with MSE metric 18581.1761\n",
      "Time taken for 1 epoch: 24.868847846984863 secs\n",
      "\n",
      "Epoch 224 batch 0 train Loss 49.9029 test Loss 23.1997 with MSE metric 18579.2661\n",
      "Epoch 224 batch 10 train Loss 49.8950 test Loss 23.1966 with MSE metric 18577.3238\n",
      "Epoch 224 batch 20 train Loss 49.8870 test Loss 23.1934 with MSE metric 18575.4301\n",
      "Epoch 224 batch 30 train Loss 49.8791 test Loss 23.1902 with MSE metric 18573.3702\n",
      "Epoch 224 batch 40 train Loss 49.8711 test Loss 23.1870 with MSE metric 18571.3659\n",
      "Epoch 224 batch 50 train Loss 49.8632 test Loss 23.1838 with MSE metric 18569.4104\n",
      "Epoch 224 batch 60 train Loss 49.8552 test Loss 23.1806 with MSE metric 18567.4793\n",
      "Epoch 224 batch 70 train Loss 49.8473 test Loss 23.1775 with MSE metric 18565.5483\n",
      "Epoch 224 batch 80 train Loss 49.8393 test Loss 23.1743 with MSE metric 18563.5792\n",
      "Epoch 224 batch 90 train Loss 49.8314 test Loss 23.1711 with MSE metric 18561.6330\n",
      "Epoch 224 batch 100 train Loss 49.8235 test Loss 23.1680 with MSE metric 18559.7039\n",
      "Epoch 224 batch 110 train Loss 49.8155 test Loss 23.1648 with MSE metric 18557.7302\n",
      "Epoch 224 batch 120 train Loss 49.8076 test Loss 23.1616 with MSE metric 18555.7190\n",
      "Epoch 224 batch 130 train Loss 49.7997 test Loss 23.1584 with MSE metric 18553.7615\n",
      "Epoch 224 batch 140 train Loss 49.7917 test Loss 23.1553 with MSE metric 18551.8347\n",
      "Epoch 224 batch 150 train Loss 49.7838 test Loss 23.1521 with MSE metric 18549.9076\n",
      "Epoch 224 batch 160 train Loss 49.7759 test Loss 23.1489 with MSE metric 18547.9429\n",
      "Epoch 224 batch 170 train Loss 49.7680 test Loss 23.1458 with MSE metric 18545.9394\n",
      "Epoch 224 batch 180 train Loss 49.7601 test Loss 23.1426 with MSE metric 18543.9955\n",
      "Epoch 224 batch 190 train Loss 49.7521 test Loss 23.1395 with MSE metric 18541.9575\n",
      "Epoch 224 batch 200 train Loss 49.7442 test Loss 23.1363 with MSE metric 18539.9987\n",
      "Epoch 224 batch 210 train Loss 49.7363 test Loss 23.1331 with MSE metric 18538.0557\n",
      "Epoch 224 batch 220 train Loss 49.7284 test Loss 23.1300 with MSE metric 18536.0648\n",
      "Epoch 224 batch 230 train Loss 49.7205 test Loss 23.1268 with MSE metric 18534.1466\n",
      "Epoch 224 batch 240 train Loss 49.7126 test Loss 23.1237 with MSE metric 18532.1650\n",
      "Time taken for 1 epoch: 24.86346697807312 secs\n",
      "\n",
      "Epoch 225 batch 0 train Loss 49.7047 test Loss 23.1205 with MSE metric 18530.2019\n",
      "Epoch 225 batch 10 train Loss 49.6968 test Loss 23.1174 with MSE metric 18528.2969\n",
      "Epoch 225 batch 20 train Loss 49.6890 test Loss 23.1142 with MSE metric 18526.3775\n",
      "Epoch 225 batch 30 train Loss 49.6811 test Loss 23.1111 with MSE metric 18524.4343\n",
      "Epoch 225 batch 40 train Loss 49.6732 test Loss 23.1080 with MSE metric 18522.5606\n",
      "Epoch 225 batch 50 train Loss 49.6653 test Loss 23.1048 with MSE metric 18520.6270\n",
      "Epoch 225 batch 60 train Loss 49.6574 test Loss 23.1017 with MSE metric 18518.7198\n",
      "Epoch 225 batch 70 train Loss 49.6496 test Loss 23.0985 with MSE metric 18516.7613\n",
      "Epoch 225 batch 80 train Loss 49.6417 test Loss 23.0954 with MSE metric 18514.8520\n",
      "Epoch 225 batch 90 train Loss 49.6338 test Loss 23.0922 with MSE metric 18512.9908\n",
      "Epoch 225 batch 100 train Loss 49.6260 test Loss 23.0891 with MSE metric 18511.0654\n",
      "Epoch 225 batch 110 train Loss 49.6181 test Loss 23.0859 with MSE metric 18509.1549\n",
      "Epoch 225 batch 120 train Loss 49.6103 test Loss 23.0828 with MSE metric 18507.2484\n",
      "Epoch 225 batch 130 train Loss 49.6024 test Loss 23.0796 with MSE metric 18505.3477\n",
      "Epoch 225 batch 140 train Loss 49.5945 test Loss 23.0765 with MSE metric 18503.3975\n",
      "Epoch 225 batch 150 train Loss 49.5867 test Loss 23.0734 with MSE metric 18501.4631\n",
      "Epoch 225 batch 160 train Loss 49.5788 test Loss 23.0703 with MSE metric 18499.5635\n",
      "Epoch 225 batch 170 train Loss 49.5710 test Loss 23.0671 with MSE metric 18497.6551\n",
      "Epoch 225 batch 180 train Loss 49.5632 test Loss 23.0640 with MSE metric 18495.7181\n",
      "Epoch 225 batch 190 train Loss 49.5553 test Loss 23.0608 with MSE metric 18493.7888\n",
      "Epoch 225 batch 200 train Loss 49.5475 test Loss 23.0577 with MSE metric 18491.8025\n",
      "Epoch 225 batch 210 train Loss 49.5396 test Loss 23.0546 with MSE metric 18489.8479\n",
      "Epoch 225 batch 220 train Loss 49.5318 test Loss 23.0514 with MSE metric 18487.9524\n",
      "Epoch 225 batch 230 train Loss 49.5240 test Loss 23.0483 with MSE metric 18485.9569\n",
      "Epoch 225 batch 240 train Loss 49.5162 test Loss 23.0452 with MSE metric 18484.0316\n",
      "Time taken for 1 epoch: 24.348795890808105 secs\n",
      "\n",
      "Epoch 226 batch 0 train Loss 49.5083 test Loss 23.0421 with MSE metric 18482.0409\n",
      "Epoch 226 batch 10 train Loss 49.5005 test Loss 23.0389 with MSE metric 18480.0939\n",
      "Epoch 226 batch 20 train Loss 49.4927 test Loss 23.0358 with MSE metric 18478.1638\n",
      "Epoch 226 batch 30 train Loss 49.4849 test Loss 23.0327 with MSE metric 18476.2523\n",
      "Epoch 226 batch 40 train Loss 49.4771 test Loss 23.0296 with MSE metric 18474.3338\n",
      "Epoch 226 batch 50 train Loss 49.4693 test Loss 23.0265 with MSE metric 18472.4075\n",
      "Epoch 226 batch 60 train Loss 49.4614 test Loss 23.0234 with MSE metric 18470.4520\n",
      "Epoch 226 batch 70 train Loss 49.4536 test Loss 23.0203 with MSE metric 18468.5034\n",
      "Epoch 226 batch 80 train Loss 49.4458 test Loss 23.0171 with MSE metric 18466.5078\n",
      "Epoch 226 batch 90 train Loss 49.4380 test Loss 23.0140 with MSE metric 18464.6112\n",
      "Epoch 226 batch 100 train Loss 49.4302 test Loss 23.0109 with MSE metric 18462.6612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226 batch 110 train Loss 49.4224 test Loss 23.0078 with MSE metric 18460.7700\n",
      "Epoch 226 batch 120 train Loss 49.4147 test Loss 23.0047 with MSE metric 18458.8159\n",
      "Epoch 226 batch 130 train Loss 49.4069 test Loss 23.0016 with MSE metric 18456.9075\n",
      "Epoch 226 batch 140 train Loss 49.3991 test Loss 22.9985 with MSE metric 18455.0264\n",
      "Epoch 226 batch 150 train Loss 49.3913 test Loss 22.9954 with MSE metric 18453.1644\n",
      "Epoch 226 batch 160 train Loss 49.3835 test Loss 22.9923 with MSE metric 18451.2706\n",
      "Epoch 226 batch 170 train Loss 49.3758 test Loss 22.9891 with MSE metric 18449.3714\n",
      "Epoch 226 batch 180 train Loss 49.3680 test Loss 22.9860 with MSE metric 18447.4469\n",
      "Epoch 226 batch 190 train Loss 49.3602 test Loss 22.9829 with MSE metric 18445.5378\n",
      "Epoch 226 batch 200 train Loss 49.3524 test Loss 22.9798 with MSE metric 18443.6312\n",
      "Epoch 226 batch 210 train Loss 49.3447 test Loss 22.9767 with MSE metric 18441.6976\n",
      "Epoch 226 batch 220 train Loss 49.3369 test Loss 22.9736 with MSE metric 18439.7475\n",
      "Epoch 226 batch 230 train Loss 49.3291 test Loss 22.9705 with MSE metric 18437.8302\n",
      "Epoch 226 batch 240 train Loss 49.3214 test Loss 22.9674 with MSE metric 18435.8689\n",
      "Time taken for 1 epoch: 24.54081678390503 secs\n",
      "\n",
      "Epoch 227 batch 0 train Loss 49.3136 test Loss 22.9643 with MSE metric 18433.9501\n",
      "Epoch 227 batch 10 train Loss 49.3059 test Loss 22.9612 with MSE metric 18432.1016\n",
      "Epoch 227 batch 20 train Loss 49.2981 test Loss 22.9581 with MSE metric 18430.2173\n",
      "Epoch 227 batch 30 train Loss 49.2904 test Loss 22.9550 with MSE metric 18428.3444\n",
      "Epoch 227 batch 40 train Loss 49.2826 test Loss 22.9519 with MSE metric 18426.4146\n",
      "Epoch 227 batch 50 train Loss 49.2749 test Loss 22.9488 with MSE metric 18424.5395\n",
      "Epoch 227 batch 60 train Loss 49.2672 test Loss 22.9457 with MSE metric 18422.6221\n",
      "Epoch 227 batch 70 train Loss 49.2594 test Loss 22.9426 with MSE metric 18420.6627\n",
      "Epoch 227 batch 80 train Loss 49.2517 test Loss 22.9395 with MSE metric 18418.7878\n",
      "Epoch 227 batch 90 train Loss 49.2440 test Loss 22.9364 with MSE metric 18416.8550\n",
      "Epoch 227 batch 100 train Loss 49.2362 test Loss 22.9333 with MSE metric 18414.9806\n",
      "Epoch 227 batch 110 train Loss 49.2285 test Loss 22.9303 with MSE metric 18413.0944\n",
      "Epoch 227 batch 120 train Loss 49.2208 test Loss 22.9272 with MSE metric 18411.2233\n",
      "Epoch 227 batch 130 train Loss 49.2131 test Loss 22.9241 with MSE metric 18409.3525\n",
      "Epoch 227 batch 140 train Loss 49.2054 test Loss 22.9210 with MSE metric 18407.4078\n",
      "Epoch 227 batch 150 train Loss 49.1977 test Loss 22.9179 with MSE metric 18405.4978\n",
      "Epoch 227 batch 160 train Loss 49.1899 test Loss 22.9148 with MSE metric 18403.6113\n",
      "Epoch 227 batch 170 train Loss 49.1822 test Loss 22.9118 with MSE metric 18401.6843\n",
      "Epoch 227 batch 180 train Loss 49.1745 test Loss 22.9087 with MSE metric 18399.7768\n",
      "Epoch 227 batch 190 train Loss 49.1668 test Loss 22.9056 with MSE metric 18397.8708\n",
      "Epoch 227 batch 200 train Loss 49.1591 test Loss 22.9025 with MSE metric 18395.9815\n",
      "Epoch 227 batch 210 train Loss 49.1514 test Loss 22.8995 with MSE metric 18394.0810\n",
      "Epoch 227 batch 220 train Loss 49.1437 test Loss 22.8964 with MSE metric 18392.1768\n",
      "Epoch 227 batch 230 train Loss 49.1360 test Loss 22.8933 with MSE metric 18390.3314\n",
      "Epoch 227 batch 240 train Loss 49.1284 test Loss 22.8902 with MSE metric 18388.4389\n",
      "Time taken for 1 epoch: 24.942822217941284 secs\n",
      "\n",
      "Epoch 228 batch 0 train Loss 49.1207 test Loss 22.8871 with MSE metric 18386.5245\n",
      "Epoch 228 batch 10 train Loss 49.1130 test Loss 22.8841 with MSE metric 18384.6766\n",
      "Epoch 228 batch 20 train Loss 49.1053 test Loss 22.8810 with MSE metric 18382.8168\n",
      "Epoch 228 batch 30 train Loss 49.0976 test Loss 22.8779 with MSE metric 18381.0004\n",
      "Epoch 228 batch 40 train Loss 49.0900 test Loss 22.8748 with MSE metric 18379.1360\n",
      "Epoch 228 batch 50 train Loss 49.0823 test Loss 22.8718 with MSE metric 18377.2606\n",
      "Epoch 228 batch 60 train Loss 49.0746 test Loss 22.8687 with MSE metric 18375.3777\n",
      "Epoch 228 batch 70 train Loss 49.0669 test Loss 22.8657 with MSE metric 18373.4784\n",
      "Epoch 228 batch 80 train Loss 49.0593 test Loss 22.8626 with MSE metric 18371.5731\n",
      "Epoch 228 batch 90 train Loss 49.0516 test Loss 22.8595 with MSE metric 18369.6992\n",
      "Epoch 228 batch 100 train Loss 49.0440 test Loss 22.8564 with MSE metric 18367.8582\n",
      "Epoch 228 batch 110 train Loss 49.0363 test Loss 22.8534 with MSE metric 18365.9724\n",
      "Epoch 228 batch 120 train Loss 49.0286 test Loss 22.8503 with MSE metric 18364.0542\n",
      "Epoch 228 batch 130 train Loss 49.0210 test Loss 22.8473 with MSE metric 18362.1344\n",
      "Epoch 228 batch 140 train Loss 49.0133 test Loss 22.8442 with MSE metric 18360.2064\n",
      "Epoch 228 batch 150 train Loss 49.0057 test Loss 22.8412 with MSE metric 18358.3111\n",
      "Epoch 228 batch 160 train Loss 48.9981 test Loss 22.8381 with MSE metric 18356.4672\n",
      "Epoch 228 batch 170 train Loss 48.9904 test Loss 22.8351 with MSE metric 18354.5887\n",
      "Epoch 228 batch 180 train Loss 48.9828 test Loss 22.8320 with MSE metric 18352.7034\n",
      "Epoch 228 batch 190 train Loss 48.9751 test Loss 22.8290 with MSE metric 18350.8349\n",
      "Epoch 228 batch 200 train Loss 48.9675 test Loss 22.8259 with MSE metric 18348.9449\n",
      "Epoch 228 batch 210 train Loss 48.9599 test Loss 22.8229 with MSE metric 18347.0531\n",
      "Epoch 228 batch 220 train Loss 48.9522 test Loss 22.8198 with MSE metric 18345.1429\n",
      "Epoch 228 batch 230 train Loss 48.9446 test Loss 22.8168 with MSE metric 18343.3150\n",
      "Epoch 228 batch 240 train Loss 48.9370 test Loss 22.8137 with MSE metric 18341.4584\n",
      "Time taken for 1 epoch: 23.965537071228027 secs\n",
      "\n",
      "Epoch 229 batch 0 train Loss 48.9294 test Loss 22.8107 with MSE metric 18339.5765\n",
      "Epoch 229 batch 10 train Loss 48.9218 test Loss 22.8077 with MSE metric 18337.7313\n",
      "Epoch 229 batch 20 train Loss 48.9142 test Loss 22.8046 with MSE metric 18335.8389\n",
      "Epoch 229 batch 30 train Loss 48.9065 test Loss 22.8015 with MSE metric 18333.9444\n",
      "Epoch 229 batch 40 train Loss 48.8989 test Loss 22.7985 with MSE metric 18332.0544\n",
      "Epoch 229 batch 50 train Loss 48.8913 test Loss 22.7955 with MSE metric 18330.1917\n",
      "Epoch 229 batch 60 train Loss 48.8837 test Loss 22.7925 with MSE metric 18328.3268\n",
      "Epoch 229 batch 70 train Loss 48.8761 test Loss 22.7894 with MSE metric 18326.4688\n",
      "Epoch 229 batch 80 train Loss 48.8685 test Loss 22.7864 with MSE metric 18324.6116\n",
      "Epoch 229 batch 90 train Loss 48.8609 test Loss 22.7833 with MSE metric 18322.7287\n",
      "Epoch 229 batch 100 train Loss 48.8533 test Loss 22.7803 with MSE metric 18320.8755\n",
      "Epoch 229 batch 110 train Loss 48.8457 test Loss 22.7773 with MSE metric 18318.9507\n",
      "Epoch 229 batch 120 train Loss 48.8381 test Loss 22.7742 with MSE metric 18317.0896\n",
      "Epoch 229 batch 130 train Loss 48.8306 test Loss 22.7712 with MSE metric 18315.2489\n",
      "Epoch 229 batch 140 train Loss 48.8230 test Loss 22.7682 with MSE metric 18313.4142\n",
      "Epoch 229 batch 150 train Loss 48.8154 test Loss 22.7651 with MSE metric 18311.5063\n",
      "Epoch 229 batch 160 train Loss 48.8078 test Loss 22.7621 with MSE metric 18309.6315\n",
      "Epoch 229 batch 170 train Loss 48.8003 test Loss 22.7591 with MSE metric 18307.7600\n",
      "Epoch 229 batch 180 train Loss 48.7927 test Loss 22.7561 with MSE metric 18305.8244\n",
      "Epoch 229 batch 190 train Loss 48.7851 test Loss 22.7530 with MSE metric 18303.9986\n",
      "Epoch 229 batch 200 train Loss 48.7775 test Loss 22.7500 with MSE metric 18302.1532\n",
      "Epoch 229 batch 210 train Loss 48.7700 test Loss 22.7470 with MSE metric 18300.2932\n",
      "Epoch 229 batch 220 train Loss 48.7624 test Loss 22.7440 with MSE metric 18298.3825\n",
      "Epoch 229 batch 230 train Loss 48.7549 test Loss 22.7409 with MSE metric 18296.5152\n",
      "Epoch 229 batch 240 train Loss 48.7473 test Loss 22.7380 with MSE metric 18294.6253\n",
      "Time taken for 1 epoch: 24.206366062164307 secs\n",
      "\n",
      "Epoch 230 batch 0 train Loss 48.7397 test Loss 22.7349 with MSE metric 18292.7504\n",
      "Epoch 230 batch 10 train Loss 48.7322 test Loss 22.7319 with MSE metric 18290.9067\n",
      "Epoch 230 batch 20 train Loss 48.7246 test Loss 22.7289 with MSE metric 18289.0541\n",
      "Epoch 230 batch 30 train Loss 48.7171 test Loss 22.7259 with MSE metric 18287.1929\n",
      "Epoch 230 batch 40 train Loss 48.7095 test Loss 22.7229 with MSE metric 18285.3706\n",
      "Epoch 230 batch 50 train Loss 48.7020 test Loss 22.7198 with MSE metric 18283.5184\n",
      "Epoch 230 batch 60 train Loss 48.6945 test Loss 22.7168 with MSE metric 18281.6571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230 batch 70 train Loss 48.6869 test Loss 22.7138 with MSE metric 18279.7845\n",
      "Epoch 230 batch 80 train Loss 48.6794 test Loss 22.7108 with MSE metric 18277.9849\n",
      "Epoch 230 batch 90 train Loss 48.6719 test Loss 22.7078 with MSE metric 18276.1036\n",
      "Epoch 230 batch 100 train Loss 48.6643 test Loss 22.7048 with MSE metric 18274.2226\n",
      "Epoch 230 batch 110 train Loss 48.6568 test Loss 22.7018 with MSE metric 18272.3394\n",
      "Epoch 230 batch 120 train Loss 48.6493 test Loss 22.6988 with MSE metric 18270.4974\n",
      "Epoch 230 batch 130 train Loss 48.6418 test Loss 22.6957 with MSE metric 18268.6970\n",
      "Epoch 230 batch 140 train Loss 48.6343 test Loss 22.6928 with MSE metric 18266.8527\n",
      "Epoch 230 batch 150 train Loss 48.6267 test Loss 22.6897 with MSE metric 18265.0717\n",
      "Epoch 230 batch 160 train Loss 48.6192 test Loss 22.6867 with MSE metric 18263.2597\n",
      "Epoch 230 batch 170 train Loss 48.6117 test Loss 22.6837 with MSE metric 18261.3579\n",
      "Epoch 230 batch 180 train Loss 48.6042 test Loss 22.6807 with MSE metric 18259.4925\n",
      "Epoch 230 batch 190 train Loss 48.5967 test Loss 22.6777 with MSE metric 18257.6761\n",
      "Epoch 230 batch 200 train Loss 48.5892 test Loss 22.6748 with MSE metric 18255.8178\n",
      "Epoch 230 batch 210 train Loss 48.5817 test Loss 22.6718 with MSE metric 18253.9610\n",
      "Epoch 230 batch 220 train Loss 48.5742 test Loss 22.6688 with MSE metric 18252.1552\n",
      "Epoch 230 batch 230 train Loss 48.5667 test Loss 22.6658 with MSE metric 18250.2991\n",
      "Epoch 230 batch 240 train Loss 48.5592 test Loss 22.6628 with MSE metric 18248.4618\n",
      "Time taken for 1 epoch: 24.427266836166382 secs\n",
      "\n",
      "Epoch 231 batch 0 train Loss 48.5518 test Loss 22.6598 with MSE metric 18246.6751\n",
      "Epoch 231 batch 10 train Loss 48.5443 test Loss 22.6568 with MSE metric 18244.8345\n",
      "Epoch 231 batch 20 train Loss 48.5368 test Loss 22.6538 with MSE metric 18243.0008\n",
      "Epoch 231 batch 30 train Loss 48.5293 test Loss 22.6508 with MSE metric 18241.1576\n",
      "Epoch 231 batch 40 train Loss 48.5218 test Loss 22.6479 with MSE metric 18239.2866\n",
      "Epoch 231 batch 50 train Loss 48.5144 test Loss 22.6449 with MSE metric 18237.4668\n",
      "Epoch 231 batch 60 train Loss 48.5069 test Loss 22.6419 with MSE metric 18235.7220\n",
      "Epoch 231 batch 70 train Loss 48.4994 test Loss 22.6389 with MSE metric 18233.8186\n",
      "Epoch 231 batch 80 train Loss 48.4920 test Loss 22.6359 with MSE metric 18231.9514\n",
      "Epoch 231 batch 90 train Loss 48.4845 test Loss 22.6329 with MSE metric 18230.1337\n",
      "Epoch 231 batch 100 train Loss 48.4770 test Loss 22.6300 with MSE metric 18228.3072\n",
      "Epoch 231 batch 110 train Loss 48.4696 test Loss 22.6270 with MSE metric 18226.4618\n",
      "Epoch 231 batch 120 train Loss 48.4621 test Loss 22.6240 with MSE metric 18224.6189\n",
      "Epoch 231 batch 130 train Loss 48.4547 test Loss 22.6210 with MSE metric 18222.7622\n",
      "Epoch 231 batch 140 train Loss 48.4472 test Loss 22.6180 with MSE metric 18220.9144\n",
      "Epoch 231 batch 150 train Loss 48.4397 test Loss 22.6151 with MSE metric 18219.0974\n",
      "Epoch 231 batch 160 train Loss 48.4323 test Loss 22.6121 with MSE metric 18217.3227\n",
      "Epoch 231 batch 170 train Loss 48.4249 test Loss 22.6091 with MSE metric 18215.4387\n",
      "Epoch 231 batch 180 train Loss 48.4174 test Loss 22.6062 with MSE metric 18213.5706\n",
      "Epoch 231 batch 190 train Loss 48.4100 test Loss 22.6032 with MSE metric 18211.7757\n",
      "Epoch 231 batch 200 train Loss 48.4025 test Loss 22.6002 with MSE metric 18210.0025\n",
      "Epoch 231 batch 210 train Loss 48.3951 test Loss 22.5973 with MSE metric 18208.1362\n",
      "Epoch 231 batch 220 train Loss 48.3877 test Loss 22.5943 with MSE metric 18206.2985\n",
      "Epoch 231 batch 230 train Loss 48.3802 test Loss 22.5914 with MSE metric 18204.5036\n",
      "Epoch 231 batch 240 train Loss 48.3728 test Loss 22.5884 with MSE metric 18202.6382\n",
      "Time taken for 1 epoch: 27.46484398841858 secs\n",
      "\n",
      "Epoch 232 batch 0 train Loss 48.3654 test Loss 22.5854 with MSE metric 18200.8637\n",
      "Epoch 232 batch 10 train Loss 48.3580 test Loss 22.5824 with MSE metric 18199.0905\n",
      "Epoch 232 batch 20 train Loss 48.3506 test Loss 22.5795 with MSE metric 18197.2615\n",
      "Epoch 232 batch 30 train Loss 48.3431 test Loss 22.5765 with MSE metric 18195.4550\n",
      "Epoch 232 batch 40 train Loss 48.3357 test Loss 22.5736 with MSE metric 18193.5605\n",
      "Epoch 232 batch 50 train Loss 48.3283 test Loss 22.5706 with MSE metric 18191.7071\n",
      "Epoch 232 batch 60 train Loss 48.3209 test Loss 22.5677 with MSE metric 18189.9187\n",
      "Epoch 232 batch 70 train Loss 48.3135 test Loss 22.5647 with MSE metric 18188.0611\n",
      "Epoch 232 batch 80 train Loss 48.3061 test Loss 22.5617 with MSE metric 18186.2304\n",
      "Epoch 232 batch 90 train Loss 48.2987 test Loss 22.5588 with MSE metric 18184.4003\n",
      "Epoch 232 batch 100 train Loss 48.2913 test Loss 22.5558 with MSE metric 18182.5817\n",
      "Epoch 232 batch 110 train Loss 48.2839 test Loss 22.5529 with MSE metric 18180.7032\n",
      "Epoch 232 batch 120 train Loss 48.2765 test Loss 22.5499 with MSE metric 18178.8556\n",
      "Epoch 232 batch 130 train Loss 48.2691 test Loss 22.5470 with MSE metric 18177.0272\n",
      "Epoch 232 batch 140 train Loss 48.2617 test Loss 22.5440 with MSE metric 18175.2061\n",
      "Epoch 232 batch 150 train Loss 48.2543 test Loss 22.5411 with MSE metric 18173.3965\n",
      "Epoch 232 batch 160 train Loss 48.2470 test Loss 22.5381 with MSE metric 18171.5666\n",
      "Epoch 232 batch 170 train Loss 48.2396 test Loss 22.5352 with MSE metric 18169.7592\n",
      "Epoch 232 batch 180 train Loss 48.2322 test Loss 22.5322 with MSE metric 18167.9187\n",
      "Epoch 232 batch 190 train Loss 48.2248 test Loss 22.5293 with MSE metric 18166.1124\n",
      "Epoch 232 batch 200 train Loss 48.2174 test Loss 22.5264 with MSE metric 18164.2915\n",
      "Epoch 232 batch 210 train Loss 48.2101 test Loss 22.5234 with MSE metric 18162.4785\n",
      "Epoch 232 batch 220 train Loss 48.2027 test Loss 22.5205 with MSE metric 18160.7099\n",
      "Epoch 232 batch 230 train Loss 48.1953 test Loss 22.5175 with MSE metric 18158.8645\n",
      "Epoch 232 batch 240 train Loss 48.1880 test Loss 22.5146 with MSE metric 18157.0387\n",
      "Time taken for 1 epoch: 28.858561992645264 secs\n",
      "\n",
      "Epoch 233 batch 0 train Loss 48.1806 test Loss 22.5116 with MSE metric 18155.2197\n",
      "Epoch 233 batch 10 train Loss 48.1733 test Loss 22.5087 with MSE metric 18153.4238\n",
      "Epoch 233 batch 20 train Loss 48.1659 test Loss 22.5058 with MSE metric 18151.5688\n",
      "Epoch 233 batch 30 train Loss 48.1586 test Loss 22.5028 with MSE metric 18149.8229\n",
      "Epoch 233 batch 40 train Loss 48.1512 test Loss 22.4999 with MSE metric 18148.0239\n",
      "Epoch 233 batch 50 train Loss 48.1439 test Loss 22.4969 with MSE metric 18146.1965\n",
      "Epoch 233 batch 60 train Loss 48.1365 test Loss 22.4940 with MSE metric 18144.3393\n",
      "Epoch 233 batch 70 train Loss 48.1292 test Loss 22.4911 with MSE metric 18142.5481\n",
      "Epoch 233 batch 80 train Loss 48.1218 test Loss 22.4881 with MSE metric 18140.7961\n",
      "Epoch 233 batch 90 train Loss 48.1145 test Loss 22.4852 with MSE metric 18138.9967\n",
      "Epoch 233 batch 100 train Loss 48.1072 test Loss 22.4823 with MSE metric 18137.2123\n",
      "Epoch 233 batch 110 train Loss 48.0998 test Loss 22.4793 with MSE metric 18135.4133\n",
      "Epoch 233 batch 120 train Loss 48.0925 test Loss 22.4764 with MSE metric 18133.6145\n",
      "Epoch 233 batch 130 train Loss 48.0852 test Loss 22.4735 with MSE metric 18131.7939\n",
      "Epoch 233 batch 140 train Loss 48.0779 test Loss 22.4706 with MSE metric 18130.0357\n",
      "Epoch 233 batch 150 train Loss 48.0705 test Loss 22.4677 with MSE metric 18128.2267\n",
      "Epoch 233 batch 160 train Loss 48.0632 test Loss 22.4647 with MSE metric 18126.4349\n",
      "Epoch 233 batch 170 train Loss 48.0559 test Loss 22.4618 with MSE metric 18124.6398\n",
      "Epoch 233 batch 180 train Loss 48.0486 test Loss 22.4589 with MSE metric 18122.9028\n",
      "Epoch 233 batch 190 train Loss 48.0413 test Loss 22.4560 with MSE metric 18121.0712\n",
      "Epoch 233 batch 200 train Loss 48.0340 test Loss 22.4530 with MSE metric 18119.3174\n",
      "Epoch 233 batch 210 train Loss 48.0266 test Loss 22.4501 with MSE metric 18117.4635\n",
      "Epoch 233 batch 220 train Loss 48.0193 test Loss 22.4472 with MSE metric 18115.6934\n",
      "Epoch 233 batch 230 train Loss 48.0120 test Loss 22.4443 with MSE metric 18113.8962\n",
      "Epoch 233 batch 240 train Loss 48.0047 test Loss 22.4414 with MSE metric 18112.0526\n",
      "Time taken for 1 epoch: 29.10159397125244 secs\n",
      "\n",
      "Epoch 234 batch 0 train Loss 47.9974 test Loss 22.4384 with MSE metric 18110.2985\n",
      "Epoch 234 batch 10 train Loss 47.9901 test Loss 22.4355 with MSE metric 18108.4738\n",
      "Epoch 234 batch 20 train Loss 47.9829 test Loss 22.4326 with MSE metric 18106.6423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234 batch 30 train Loss 47.9756 test Loss 22.4297 with MSE metric 18104.8516\n",
      "Epoch 234 batch 40 train Loss 47.9683 test Loss 22.4268 with MSE metric 18103.0342\n",
      "Epoch 234 batch 50 train Loss 47.9610 test Loss 22.4239 with MSE metric 18101.3285\n",
      "Epoch 234 batch 60 train Loss 47.9537 test Loss 22.4210 with MSE metric 18099.5609\n",
      "Epoch 234 batch 70 train Loss 47.9464 test Loss 22.4181 with MSE metric 18097.7139\n",
      "Epoch 234 batch 80 train Loss 47.9392 test Loss 22.4152 with MSE metric 18095.9320\n",
      "Epoch 234 batch 90 train Loss 47.9319 test Loss 22.4123 with MSE metric 18094.1762\n",
      "Epoch 234 batch 100 train Loss 47.9246 test Loss 22.4094 with MSE metric 18092.3733\n",
      "Epoch 234 batch 110 train Loss 47.9173 test Loss 22.4065 with MSE metric 18090.5772\n",
      "Epoch 234 batch 120 train Loss 47.9101 test Loss 22.4036 with MSE metric 18088.8113\n",
      "Epoch 234 batch 130 train Loss 47.9028 test Loss 22.4006 with MSE metric 18086.9855\n",
      "Epoch 234 batch 140 train Loss 47.8955 test Loss 22.3977 with MSE metric 18085.1560\n",
      "Epoch 234 batch 150 train Loss 47.8883 test Loss 22.3948 with MSE metric 18083.3658\n",
      "Epoch 234 batch 160 train Loss 47.8810 test Loss 22.3919 with MSE metric 18081.5703\n",
      "Epoch 234 batch 170 train Loss 47.8738 test Loss 22.3891 with MSE metric 18079.8342\n",
      "Epoch 234 batch 180 train Loss 47.8665 test Loss 22.3862 with MSE metric 18078.0593\n",
      "Epoch 234 batch 190 train Loss 47.8593 test Loss 22.3833 with MSE metric 18076.3289\n",
      "Epoch 234 batch 200 train Loss 47.8520 test Loss 22.3804 with MSE metric 18074.5525\n",
      "Epoch 234 batch 210 train Loss 47.8448 test Loss 22.3775 with MSE metric 18072.7681\n",
      "Epoch 234 batch 220 train Loss 47.8375 test Loss 22.3746 with MSE metric 18070.9718\n",
      "Epoch 234 batch 230 train Loss 47.8303 test Loss 22.3717 with MSE metric 18069.1835\n",
      "Epoch 234 batch 240 train Loss 47.8231 test Loss 22.3688 with MSE metric 18067.4500\n",
      "Time taken for 1 epoch: 28.96806001663208 secs\n",
      "\n",
      "Epoch 235 batch 0 train Loss 47.8158 test Loss 22.3659 with MSE metric 18065.6807\n",
      "Epoch 235 batch 10 train Loss 47.8086 test Loss 22.3630 with MSE metric 18063.9568\n",
      "Epoch 235 batch 20 train Loss 47.8014 test Loss 22.3601 with MSE metric 18062.1892\n",
      "Epoch 235 batch 30 train Loss 47.7941 test Loss 22.3572 with MSE metric 18060.4199\n",
      "Epoch 235 batch 40 train Loss 47.7869 test Loss 22.3544 with MSE metric 18058.6333\n",
      "Epoch 235 batch 50 train Loss 47.7797 test Loss 22.3515 with MSE metric 18056.8240\n",
      "Epoch 235 batch 60 train Loss 47.7725 test Loss 22.3486 with MSE metric 18055.1154\n",
      "Epoch 235 batch 70 train Loss 47.7652 test Loss 22.3457 with MSE metric 18053.3329\n",
      "Epoch 235 batch 80 train Loss 47.7580 test Loss 22.3429 with MSE metric 18051.5758\n",
      "Epoch 235 batch 90 train Loss 47.7508 test Loss 22.3400 with MSE metric 18049.7848\n",
      "Epoch 235 batch 100 train Loss 47.7436 test Loss 22.3371 with MSE metric 18047.9515\n",
      "Epoch 235 batch 110 train Loss 47.7364 test Loss 22.3342 with MSE metric 18046.1286\n",
      "Epoch 235 batch 120 train Loss 47.7292 test Loss 22.3313 with MSE metric 18044.3723\n",
      "Epoch 235 batch 130 train Loss 47.7220 test Loss 22.3284 with MSE metric 18042.6710\n",
      "Epoch 235 batch 140 train Loss 47.7148 test Loss 22.3256 with MSE metric 18040.9189\n",
      "Epoch 235 batch 150 train Loss 47.7076 test Loss 22.3227 with MSE metric 18039.1125\n",
      "Epoch 235 batch 160 train Loss 47.7004 test Loss 22.3198 with MSE metric 18037.3060\n",
      "Epoch 235 batch 170 train Loss 47.6932 test Loss 22.3169 with MSE metric 18035.4398\n",
      "Epoch 235 batch 180 train Loss 47.6860 test Loss 22.3141 with MSE metric 18033.5952\n",
      "Epoch 235 batch 190 train Loss 47.6788 test Loss 22.3112 with MSE metric 18031.8547\n",
      "Epoch 235 batch 200 train Loss 47.6716 test Loss 22.3083 with MSE metric 18030.0864\n",
      "Epoch 235 batch 210 train Loss 47.6644 test Loss 22.3055 with MSE metric 18028.2811\n",
      "Epoch 235 batch 220 train Loss 47.6572 test Loss 22.3026 with MSE metric 18026.4936\n",
      "Epoch 235 batch 230 train Loss 47.6501 test Loss 22.2997 with MSE metric 18024.7196\n",
      "Epoch 235 batch 240 train Loss 47.6429 test Loss 22.2969 with MSE metric 18022.9415\n",
      "Time taken for 1 epoch: 32.15312933921814 secs\n",
      "\n",
      "Epoch 236 batch 0 train Loss 47.6357 test Loss 22.2940 with MSE metric 18021.1702\n",
      "Epoch 236 batch 10 train Loss 47.6285 test Loss 22.2912 with MSE metric 18019.3922\n",
      "Epoch 236 batch 20 train Loss 47.6214 test Loss 22.2883 with MSE metric 18017.6506\n",
      "Epoch 236 batch 30 train Loss 47.6142 test Loss 22.2854 with MSE metric 18015.8604\n",
      "Epoch 236 batch 40 train Loss 47.6070 test Loss 22.2826 with MSE metric 18014.0627\n",
      "Epoch 236 batch 50 train Loss 47.5999 test Loss 22.2797 with MSE metric 18012.3159\n",
      "Epoch 236 batch 60 train Loss 47.5927 test Loss 22.2769 with MSE metric 18010.6138\n",
      "Epoch 236 batch 70 train Loss 47.5856 test Loss 22.2740 with MSE metric 18008.8731\n",
      "Epoch 236 batch 80 train Loss 47.5784 test Loss 22.2711 with MSE metric 18007.1389\n",
      "Epoch 236 batch 90 train Loss 47.5713 test Loss 22.2683 with MSE metric 18005.3765\n",
      "Epoch 236 batch 100 train Loss 47.5641 test Loss 22.2654 with MSE metric 18003.5606\n",
      "Epoch 236 batch 110 train Loss 47.5570 test Loss 22.2626 with MSE metric 18001.8565\n",
      "Epoch 236 batch 120 train Loss 47.5498 test Loss 22.2597 with MSE metric 18000.1091\n",
      "Epoch 236 batch 130 train Loss 47.5427 test Loss 22.2569 with MSE metric 17998.3649\n",
      "Epoch 236 batch 140 train Loss 47.5355 test Loss 22.2541 with MSE metric 17996.5738\n",
      "Epoch 236 batch 150 train Loss 47.5284 test Loss 22.2512 with MSE metric 17994.9160\n",
      "Epoch 236 batch 160 train Loss 47.5213 test Loss 22.2484 with MSE metric 17993.1724\n",
      "Epoch 236 batch 170 train Loss 47.5141 test Loss 22.2455 with MSE metric 17991.4523\n",
      "Epoch 236 batch 180 train Loss 47.5070 test Loss 22.2427 with MSE metric 17989.7531\n",
      "Epoch 236 batch 190 train Loss 47.4999 test Loss 22.2398 with MSE metric 17988.0156\n",
      "Epoch 236 batch 200 train Loss 47.4927 test Loss 22.2370 with MSE metric 17986.2960\n",
      "Epoch 236 batch 210 train Loss 47.4856 test Loss 22.2341 with MSE metric 17984.5490\n",
      "Epoch 236 batch 220 train Loss 47.4785 test Loss 22.2313 with MSE metric 17982.8054\n",
      "Epoch 236 batch 230 train Loss 47.4714 test Loss 22.2284 with MSE metric 17981.0641\n",
      "Epoch 236 batch 240 train Loss 47.4643 test Loss 22.2256 with MSE metric 17979.3662\n",
      "Time taken for 1 epoch: 29.30556082725525 secs\n",
      "\n",
      "Epoch 237 batch 0 train Loss 47.4572 test Loss 22.2228 with MSE metric 17977.6136\n",
      "Epoch 237 batch 10 train Loss 47.4501 test Loss 22.2199 with MSE metric 17975.8657\n",
      "Epoch 237 batch 20 train Loss 47.4429 test Loss 22.2171 with MSE metric 17974.1298\n",
      "Epoch 237 batch 30 train Loss 47.4358 test Loss 22.2143 with MSE metric 17972.3546\n",
      "Epoch 237 batch 40 train Loss 47.4287 test Loss 22.2114 with MSE metric 17970.6372\n",
      "Epoch 237 batch 50 train Loss 47.4216 test Loss 22.2086 with MSE metric 17968.9007\n",
      "Epoch 237 batch 60 train Loss 47.4145 test Loss 22.2057 with MSE metric 17967.0873\n",
      "Epoch 237 batch 70 train Loss 47.4074 test Loss 22.2029 with MSE metric 17965.3551\n",
      "Epoch 237 batch 80 train Loss 47.4003 test Loss 22.2000 with MSE metric 17963.6396\n",
      "Epoch 237 batch 90 train Loss 47.3932 test Loss 22.1972 with MSE metric 17961.8553\n",
      "Epoch 237 batch 100 train Loss 47.3862 test Loss 22.1944 with MSE metric 17960.1039\n",
      "Epoch 237 batch 110 train Loss 47.3791 test Loss 22.1915 with MSE metric 17958.3967\n",
      "Epoch 237 batch 120 train Loss 47.3720 test Loss 22.1887 with MSE metric 17956.6949\n",
      "Epoch 237 batch 130 train Loss 47.3649 test Loss 22.1859 with MSE metric 17954.9360\n",
      "Epoch 237 batch 140 train Loss 47.3578 test Loss 22.1830 with MSE metric 17953.2733\n",
      "Epoch 237 batch 150 train Loss 47.3508 test Loss 22.1802 with MSE metric 17951.5570\n",
      "Epoch 237 batch 160 train Loss 47.3437 test Loss 22.1774 with MSE metric 17949.8479\n",
      "Epoch 237 batch 170 train Loss 47.3366 test Loss 22.1746 with MSE metric 17948.1202\n",
      "Epoch 237 batch 180 train Loss 47.3295 test Loss 22.1717 with MSE metric 17946.3372\n",
      "Epoch 237 batch 190 train Loss 47.3225 test Loss 22.1689 with MSE metric 17944.6090\n",
      "Epoch 237 batch 200 train Loss 47.3154 test Loss 22.1661 with MSE metric 17942.8472\n",
      "Epoch 237 batch 210 train Loss 47.3083 test Loss 22.1633 with MSE metric 17941.0748\n",
      "Epoch 237 batch 220 train Loss 47.3013 test Loss 22.1605 with MSE metric 17939.3558\n",
      "Epoch 237 batch 230 train Loss 47.2942 test Loss 22.1576 with MSE metric 17937.6696\n",
      "Epoch 237 batch 240 train Loss 47.2872 test Loss 22.1548 with MSE metric 17935.9421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 30.929280996322632 secs\n",
      "\n",
      "Epoch 238 batch 0 train Loss 47.2801 test Loss 22.1520 with MSE metric 17934.2647\n",
      "Epoch 238 batch 10 train Loss 47.2731 test Loss 22.1492 with MSE metric 17932.4923\n",
      "Epoch 238 batch 20 train Loss 47.2660 test Loss 22.1464 with MSE metric 17930.7743\n",
      "Epoch 238 batch 30 train Loss 47.2590 test Loss 22.1435 with MSE metric 17929.0491\n",
      "Epoch 238 batch 40 train Loss 47.2519 test Loss 22.1407 with MSE metric 17927.3859\n",
      "Epoch 238 batch 50 train Loss 47.2449 test Loss 22.1379 with MSE metric 17925.6194\n",
      "Epoch 238 batch 60 train Loss 47.2378 test Loss 22.1351 with MSE metric 17923.8536\n",
      "Epoch 238 batch 70 train Loss 47.2308 test Loss 22.1323 with MSE metric 17922.2029\n",
      "Epoch 238 batch 80 train Loss 47.2238 test Loss 22.1295 with MSE metric 17920.5072\n",
      "Epoch 238 batch 90 train Loss 47.2167 test Loss 22.1267 with MSE metric 17918.7359\n",
      "Epoch 238 batch 100 train Loss 47.2097 test Loss 22.1239 with MSE metric 17916.9645\n",
      "Epoch 238 batch 110 train Loss 47.2027 test Loss 22.1211 with MSE metric 17915.2268\n",
      "Epoch 238 batch 120 train Loss 47.1956 test Loss 22.1183 with MSE metric 17913.5332\n",
      "Epoch 238 batch 130 train Loss 47.1886 test Loss 22.1155 with MSE metric 17911.8074\n",
      "Epoch 238 batch 140 train Loss 47.1816 test Loss 22.1127 with MSE metric 17910.0438\n",
      "Epoch 238 batch 150 train Loss 47.1746 test Loss 22.1099 with MSE metric 17908.2969\n",
      "Epoch 238 batch 160 train Loss 47.1675 test Loss 22.1071 with MSE metric 17906.5183\n",
      "Epoch 238 batch 170 train Loss 47.1605 test Loss 22.1043 with MSE metric 17904.8603\n",
      "Epoch 238 batch 180 train Loss 47.1535 test Loss 22.1015 with MSE metric 17903.1030\n",
      "Epoch 238 batch 190 train Loss 47.1465 test Loss 22.0987 with MSE metric 17901.3492\n",
      "Epoch 238 batch 200 train Loss 47.1395 test Loss 22.0959 with MSE metric 17899.6479\n",
      "Epoch 238 batch 210 train Loss 47.1325 test Loss 22.0931 with MSE metric 17897.9060\n",
      "Epoch 238 batch 220 train Loss 47.1255 test Loss 22.0904 with MSE metric 17896.1039\n",
      "Epoch 238 batch 230 train Loss 47.1185 test Loss 22.0876 with MSE metric 17894.3620\n",
      "Epoch 238 batch 240 train Loss 47.1115 test Loss 22.0848 with MSE metric 17892.6272\n",
      "Time taken for 1 epoch: 30.410068035125732 secs\n",
      "\n",
      "Epoch 239 batch 0 train Loss 47.1045 test Loss 22.0820 with MSE metric 17890.9234\n",
      "Epoch 239 batch 10 train Loss 47.0975 test Loss 22.0793 with MSE metric 17889.1916\n",
      "Epoch 239 batch 20 train Loss 47.0905 test Loss 22.0764 with MSE metric 17887.4026\n",
      "Epoch 239 batch 30 train Loss 47.0835 test Loss 22.0737 with MSE metric 17885.6399\n",
      "Epoch 239 batch 40 train Loss 47.0765 test Loss 22.0709 with MSE metric 17883.9116\n",
      "Epoch 239 batch 50 train Loss 47.0695 test Loss 22.0681 with MSE metric 17882.1781\n",
      "Epoch 239 batch 60 train Loss 47.0626 test Loss 22.0653 with MSE metric 17880.4260\n",
      "Epoch 239 batch 70 train Loss 47.0556 test Loss 22.0625 with MSE metric 17878.7019\n",
      "Epoch 239 batch 80 train Loss 47.0486 test Loss 22.0597 with MSE metric 17876.9508\n",
      "Epoch 239 batch 90 train Loss 47.0416 test Loss 22.0569 with MSE metric 17875.2151\n",
      "Epoch 239 batch 100 train Loss 47.0347 test Loss 22.0541 with MSE metric 17873.5121\n",
      "Epoch 239 batch 110 train Loss 47.0277 test Loss 22.0513 with MSE metric 17871.7911\n",
      "Epoch 239 batch 120 train Loss 47.0207 test Loss 22.0486 with MSE metric 17870.1277\n",
      "Epoch 239 batch 130 train Loss 47.0138 test Loss 22.0458 with MSE metric 17868.4012\n",
      "Epoch 239 batch 140 train Loss 47.0068 test Loss 22.0430 with MSE metric 17866.6987\n",
      "Epoch 239 batch 150 train Loss 46.9998 test Loss 22.0402 with MSE metric 17864.9670\n",
      "Epoch 239 batch 160 train Loss 46.9929 test Loss 22.0374 with MSE metric 17863.2903\n",
      "Epoch 239 batch 170 train Loss 46.9859 test Loss 22.0346 with MSE metric 17861.5941\n",
      "Epoch 239 batch 180 train Loss 46.9790 test Loss 22.0319 with MSE metric 17859.9305\n",
      "Epoch 239 batch 190 train Loss 46.9720 test Loss 22.0291 with MSE metric 17858.2464\n",
      "Epoch 239 batch 200 train Loss 46.9651 test Loss 22.0263 with MSE metric 17856.4967\n",
      "Epoch 239 batch 210 train Loss 46.9581 test Loss 22.0235 with MSE metric 17854.7784\n",
      "Epoch 239 batch 220 train Loss 46.9512 test Loss 22.0208 with MSE metric 17853.1517\n",
      "Epoch 239 batch 230 train Loss 46.9442 test Loss 22.0180 with MSE metric 17851.3948\n",
      "Epoch 239 batch 240 train Loss 46.9373 test Loss 22.0153 with MSE metric 17849.7317\n",
      "Time taken for 1 epoch: 29.774513006210327 secs\n",
      "\n",
      "Epoch 240 batch 0 train Loss 46.9304 test Loss 22.0125 with MSE metric 17848.0426\n",
      "Epoch 240 batch 10 train Loss 46.9234 test Loss 22.0097 with MSE metric 17846.3487\n",
      "Epoch 240 batch 20 train Loss 46.9165 test Loss 22.0069 with MSE metric 17844.6401\n",
      "Epoch 240 batch 30 train Loss 46.9096 test Loss 22.0042 with MSE metric 17842.9185\n",
      "Epoch 240 batch 40 train Loss 46.9026 test Loss 22.0014 with MSE metric 17841.1957\n",
      "Epoch 240 batch 50 train Loss 46.8957 test Loss 21.9986 with MSE metric 17839.4483\n",
      "Epoch 240 batch 60 train Loss 46.8888 test Loss 21.9959 with MSE metric 17837.7462\n",
      "Epoch 240 batch 70 train Loss 46.8819 test Loss 21.9931 with MSE metric 17836.0681\n",
      "Epoch 240 batch 80 train Loss 46.8750 test Loss 21.9903 with MSE metric 17834.3944\n",
      "Epoch 240 batch 90 train Loss 46.8680 test Loss 21.9876 with MSE metric 17832.7135\n",
      "Epoch 240 batch 100 train Loss 46.8611 test Loss 21.9848 with MSE metric 17831.0230\n",
      "Epoch 240 batch 110 train Loss 46.8542 test Loss 21.9821 with MSE metric 17829.3121\n",
      "Epoch 240 batch 120 train Loss 46.8473 test Loss 21.9793 with MSE metric 17827.6024\n",
      "Epoch 240 batch 130 train Loss 46.8404 test Loss 21.9766 with MSE metric 17825.8563\n",
      "Epoch 240 batch 140 train Loss 46.8335 test Loss 21.9738 with MSE metric 17824.1259\n",
      "Epoch 240 batch 150 train Loss 46.8266 test Loss 21.9711 with MSE metric 17822.4008\n",
      "Epoch 240 batch 160 train Loss 46.8197 test Loss 21.9683 with MSE metric 17820.6823\n",
      "Epoch 240 batch 170 train Loss 46.8128 test Loss 21.9655 with MSE metric 17818.9960\n",
      "Epoch 240 batch 180 train Loss 46.8059 test Loss 21.9628 with MSE metric 17817.3374\n",
      "Epoch 240 batch 190 train Loss 46.7990 test Loss 21.9600 with MSE metric 17815.7029\n",
      "Epoch 240 batch 200 train Loss 46.7921 test Loss 21.9573 with MSE metric 17813.9542\n",
      "Epoch 240 batch 210 train Loss 46.7852 test Loss 21.9545 with MSE metric 17812.3214\n",
      "Epoch 240 batch 220 train Loss 46.7783 test Loss 21.9518 with MSE metric 17810.5927\n",
      "Epoch 240 batch 230 train Loss 46.7714 test Loss 21.9491 with MSE metric 17808.8726\n",
      "Epoch 240 batch 240 train Loss 46.7646 test Loss 21.9463 with MSE metric 17807.1691\n",
      "Time taken for 1 epoch: 29.432066917419434 secs\n",
      "\n",
      "Epoch 241 batch 0 train Loss 46.7577 test Loss 21.9435 with MSE metric 17805.4839\n",
      "Epoch 241 batch 10 train Loss 46.7508 test Loss 21.9408 with MSE metric 17803.7418\n",
      "Epoch 241 batch 20 train Loss 46.7439 test Loss 21.9380 with MSE metric 17802.0509\n",
      "Epoch 241 batch 30 train Loss 46.7371 test Loss 21.9353 with MSE metric 17800.3315\n",
      "Epoch 241 batch 40 train Loss 46.7302 test Loss 21.9326 with MSE metric 17798.6038\n",
      "Epoch 241 batch 50 train Loss 46.7233 test Loss 21.9298 with MSE metric 17796.9566\n",
      "Epoch 241 batch 60 train Loss 46.7165 test Loss 21.9271 with MSE metric 17795.2292\n",
      "Epoch 241 batch 70 train Loss 46.7096 test Loss 21.9243 with MSE metric 17793.5239\n",
      "Epoch 241 batch 80 train Loss 46.7027 test Loss 21.9216 with MSE metric 17791.8281\n",
      "Epoch 241 batch 90 train Loss 46.6959 test Loss 21.9188 with MSE metric 17790.1213\n",
      "Epoch 241 batch 100 train Loss 46.6890 test Loss 21.9161 with MSE metric 17788.4921\n",
      "Epoch 241 batch 110 train Loss 46.6822 test Loss 21.9133 with MSE metric 17786.8011\n",
      "Epoch 241 batch 120 train Loss 46.6753 test Loss 21.9106 with MSE metric 17785.0807\n",
      "Epoch 241 batch 130 train Loss 46.6684 test Loss 21.9079 with MSE metric 17783.3895\n",
      "Epoch 241 batch 140 train Loss 46.6616 test Loss 21.9051 with MSE metric 17781.7586\n",
      "Epoch 241 batch 150 train Loss 46.6548 test Loss 21.9024 with MSE metric 17780.1611\n",
      "Epoch 241 batch 160 train Loss 46.6479 test Loss 21.8997 with MSE metric 17778.5119\n",
      "Epoch 241 batch 170 train Loss 46.6411 test Loss 21.8970 with MSE metric 17776.8038\n",
      "Epoch 241 batch 180 train Loss 46.6342 test Loss 21.8942 with MSE metric 17775.1326\n",
      "Epoch 241 batch 190 train Loss 46.6274 test Loss 21.8915 with MSE metric 17773.4585\n",
      "Epoch 241 batch 200 train Loss 46.6206 test Loss 21.8888 with MSE metric 17771.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241 batch 210 train Loss 46.6137 test Loss 21.8860 with MSE metric 17770.1446\n",
      "Epoch 241 batch 220 train Loss 46.6069 test Loss 21.8833 with MSE metric 17768.4386\n",
      "Epoch 241 batch 230 train Loss 46.6001 test Loss 21.8806 with MSE metric 17766.7777\n",
      "Epoch 241 batch 240 train Loss 46.5933 test Loss 21.8778 with MSE metric 17765.0795\n",
      "Time taken for 1 epoch: 29.473978281021118 secs\n",
      "\n",
      "Epoch 242 batch 0 train Loss 46.5864 test Loss 21.8751 with MSE metric 17763.3826\n",
      "Epoch 242 batch 10 train Loss 46.5796 test Loss 21.8724 with MSE metric 17761.7321\n",
      "Epoch 242 batch 20 train Loss 46.5728 test Loss 21.8696 with MSE metric 17760.0578\n",
      "Epoch 242 batch 30 train Loss 46.5660 test Loss 21.8669 with MSE metric 17758.3264\n",
      "Epoch 242 batch 40 train Loss 46.5592 test Loss 21.8642 with MSE metric 17756.6753\n",
      "Epoch 242 batch 50 train Loss 46.5523 test Loss 21.8615 with MSE metric 17754.9663\n",
      "Epoch 242 batch 60 train Loss 46.5455 test Loss 21.8588 with MSE metric 17753.3582\n",
      "Epoch 242 batch 70 train Loss 46.5387 test Loss 21.8561 with MSE metric 17751.6856\n",
      "Epoch 242 batch 80 train Loss 46.5319 test Loss 21.8533 with MSE metric 17750.0170\n",
      "Epoch 242 batch 90 train Loss 46.5251 test Loss 21.8506 with MSE metric 17748.3300\n",
      "Epoch 242 batch 100 train Loss 46.5183 test Loss 21.8479 with MSE metric 17746.6366\n",
      "Epoch 242 batch 110 train Loss 46.5115 test Loss 21.8452 with MSE metric 17744.9532\n",
      "Epoch 242 batch 120 train Loss 46.5047 test Loss 21.8424 with MSE metric 17743.2422\n",
      "Epoch 242 batch 130 train Loss 46.4979 test Loss 21.8397 with MSE metric 17741.5797\n",
      "Epoch 242 batch 140 train Loss 46.4911 test Loss 21.8370 with MSE metric 17739.8989\n",
      "Epoch 242 batch 150 train Loss 46.4843 test Loss 21.8343 with MSE metric 17738.2274\n",
      "Epoch 242 batch 160 train Loss 46.4776 test Loss 21.8316 with MSE metric 17736.5163\n",
      "Epoch 242 batch 170 train Loss 46.4708 test Loss 21.8289 with MSE metric 17734.8120\n",
      "Epoch 242 batch 180 train Loss 46.4640 test Loss 21.8262 with MSE metric 17733.1655\n",
      "Epoch 242 batch 190 train Loss 46.4572 test Loss 21.8235 with MSE metric 17731.5159\n",
      "Epoch 242 batch 200 train Loss 46.4504 test Loss 21.8208 with MSE metric 17729.8696\n",
      "Epoch 242 batch 210 train Loss 46.4437 test Loss 21.8181 with MSE metric 17728.1772\n",
      "Epoch 242 batch 220 train Loss 46.4369 test Loss 21.8154 with MSE metric 17726.5079\n",
      "Epoch 242 batch 230 train Loss 46.4301 test Loss 21.8127 with MSE metric 17724.8737\n",
      "Epoch 242 batch 240 train Loss 46.4233 test Loss 21.8100 with MSE metric 17723.2963\n",
      "Time taken for 1 epoch: 29.88199806213379 secs\n",
      "\n",
      "Epoch 243 batch 0 train Loss 46.4166 test Loss 21.8073 with MSE metric 17721.6055\n",
      "Epoch 243 batch 10 train Loss 46.4098 test Loss 21.8046 with MSE metric 17719.8910\n",
      "Epoch 243 batch 20 train Loss 46.4030 test Loss 21.8019 with MSE metric 17718.1863\n",
      "Epoch 243 batch 30 train Loss 46.3963 test Loss 21.7992 with MSE metric 17716.5584\n",
      "Epoch 243 batch 40 train Loss 46.3895 test Loss 21.7965 with MSE metric 17714.8795\n",
      "Epoch 243 batch 50 train Loss 46.3828 test Loss 21.7938 with MSE metric 17713.2151\n",
      "Epoch 243 batch 60 train Loss 46.3760 test Loss 21.7911 with MSE metric 17711.5849\n",
      "Epoch 243 batch 70 train Loss 46.3693 test Loss 21.7884 with MSE metric 17709.9843\n",
      "Epoch 243 batch 80 train Loss 46.3625 test Loss 21.7857 with MSE metric 17708.3418\n",
      "Epoch 243 batch 90 train Loss 46.3558 test Loss 21.7829 with MSE metric 17706.7075\n",
      "Epoch 243 batch 100 train Loss 46.3490 test Loss 21.7803 with MSE metric 17705.0148\n",
      "Epoch 243 batch 110 train Loss 46.3423 test Loss 21.7776 with MSE metric 17703.3568\n",
      "Epoch 243 batch 120 train Loss 46.3355 test Loss 21.7749 with MSE metric 17701.6847\n",
      "Epoch 243 batch 130 train Loss 46.3288 test Loss 21.7722 with MSE metric 17699.9710\n",
      "Epoch 243 batch 140 train Loss 46.3221 test Loss 21.7695 with MSE metric 17698.3170\n",
      "Epoch 243 batch 150 train Loss 46.3153 test Loss 21.7668 with MSE metric 17696.6687\n",
      "Epoch 243 batch 160 train Loss 46.3086 test Loss 21.7641 with MSE metric 17695.0149\n",
      "Epoch 243 batch 170 train Loss 46.3019 test Loss 21.7615 with MSE metric 17693.3584\n",
      "Epoch 243 batch 180 train Loss 46.2952 test Loss 21.7587 with MSE metric 17691.6958\n",
      "Epoch 243 batch 190 train Loss 46.2884 test Loss 21.7561 with MSE metric 17690.0287\n",
      "Epoch 243 batch 200 train Loss 46.2817 test Loss 21.7534 with MSE metric 17688.3954\n",
      "Epoch 243 batch 210 train Loss 46.2750 test Loss 21.7507 with MSE metric 17686.8065\n",
      "Epoch 243 batch 220 train Loss 46.2683 test Loss 21.7480 with MSE metric 17685.1988\n",
      "Epoch 243 batch 230 train Loss 46.2616 test Loss 21.7453 with MSE metric 17683.4684\n",
      "Epoch 243 batch 240 train Loss 46.2548 test Loss 21.7427 with MSE metric 17681.8313\n",
      "Time taken for 1 epoch: 29.180109977722168 secs\n",
      "\n",
      "Epoch 244 batch 0 train Loss 46.2481 test Loss 21.7400 with MSE metric 17680.2208\n",
      "Epoch 244 batch 10 train Loss 46.2414 test Loss 21.7373 with MSE metric 17678.6180\n",
      "Epoch 244 batch 20 train Loss 46.2347 test Loss 21.7346 with MSE metric 17676.9719\n",
      "Epoch 244 batch 30 train Loss 46.2280 test Loss 21.7319 with MSE metric 17675.3092\n",
      "Epoch 244 batch 40 train Loss 46.2213 test Loss 21.7293 with MSE metric 17673.6446\n",
      "Epoch 244 batch 50 train Loss 46.2146 test Loss 21.7266 with MSE metric 17671.9654\n",
      "Epoch 244 batch 60 train Loss 46.2079 test Loss 21.7239 with MSE metric 17670.2670\n",
      "Epoch 244 batch 70 train Loss 46.2012 test Loss 21.7213 with MSE metric 17668.6128\n",
      "Epoch 244 batch 80 train Loss 46.1945 test Loss 21.7186 with MSE metric 17666.9766\n",
      "Epoch 244 batch 90 train Loss 46.1878 test Loss 21.7159 with MSE metric 17665.3193\n",
      "Epoch 244 batch 100 train Loss 46.1811 test Loss 21.7133 with MSE metric 17663.6314\n",
      "Epoch 244 batch 110 train Loss 46.1744 test Loss 21.7106 with MSE metric 17662.0047\n",
      "Epoch 244 batch 120 train Loss 46.1678 test Loss 21.7079 with MSE metric 17660.3424\n",
      "Epoch 244 batch 130 train Loss 46.1611 test Loss 21.7052 with MSE metric 17658.6807\n",
      "Epoch 244 batch 140 train Loss 46.1544 test Loss 21.7026 with MSE metric 17657.0453\n",
      "Epoch 244 batch 150 train Loss 46.1477 test Loss 21.6999 with MSE metric 17655.4622\n",
      "Epoch 244 batch 160 train Loss 46.1410 test Loss 21.6972 with MSE metric 17653.7918\n",
      "Epoch 244 batch 170 train Loss 46.1344 test Loss 21.6946 with MSE metric 17652.1583\n",
      "Epoch 244 batch 180 train Loss 46.1277 test Loss 21.6919 with MSE metric 17650.4664\n",
      "Epoch 244 batch 190 train Loss 46.1210 test Loss 21.6892 with MSE metric 17648.8077\n",
      "Epoch 244 batch 200 train Loss 46.1144 test Loss 21.6866 with MSE metric 17647.2067\n",
      "Epoch 244 batch 210 train Loss 46.1077 test Loss 21.6839 with MSE metric 17645.5156\n",
      "Epoch 244 batch 220 train Loss 46.1010 test Loss 21.6813 with MSE metric 17643.8363\n",
      "Epoch 244 batch 230 train Loss 46.0944 test Loss 21.6786 with MSE metric 17642.1996\n",
      "Epoch 244 batch 240 train Loss 46.0877 test Loss 21.6759 with MSE metric 17640.5272\n",
      "Time taken for 1 epoch: 28.765380859375 secs\n",
      "\n",
      "Epoch 245 batch 0 train Loss 46.0810 test Loss 21.6733 with MSE metric 17638.9035\n",
      "Epoch 245 batch 10 train Loss 46.0744 test Loss 21.6706 with MSE metric 17637.2235\n",
      "Epoch 245 batch 20 train Loss 46.0677 test Loss 21.6680 with MSE metric 17635.6661\n",
      "Epoch 245 batch 30 train Loss 46.0611 test Loss 21.6653 with MSE metric 17634.0161\n",
      "Epoch 245 batch 40 train Loss 46.0544 test Loss 21.6626 with MSE metric 17632.4084\n",
      "Epoch 245 batch 50 train Loss 46.0478 test Loss 21.6600 with MSE metric 17630.7811\n",
      "Epoch 245 batch 60 train Loss 46.0412 test Loss 21.6573 with MSE metric 17629.1533\n",
      "Epoch 245 batch 70 train Loss 46.0345 test Loss 21.6547 with MSE metric 17627.5144\n",
      "Epoch 245 batch 80 train Loss 46.0279 test Loss 21.6520 with MSE metric 17625.8213\n",
      "Epoch 245 batch 90 train Loss 46.0212 test Loss 21.6494 with MSE metric 17624.2395\n",
      "Epoch 245 batch 100 train Loss 46.0146 test Loss 21.6467 with MSE metric 17622.6325\n",
      "Epoch 245 batch 110 train Loss 46.0080 test Loss 21.6441 with MSE metric 17621.0034\n",
      "Epoch 245 batch 120 train Loss 46.0013 test Loss 21.6414 with MSE metric 17619.3060\n",
      "Epoch 245 batch 130 train Loss 45.9947 test Loss 21.6388 with MSE metric 17617.7021\n",
      "Epoch 245 batch 140 train Loss 45.9881 test Loss 21.6361 with MSE metric 17616.0201\n",
      "Epoch 245 batch 150 train Loss 45.9815 test Loss 21.6335 with MSE metric 17614.4458\n",
      "Epoch 245 batch 160 train Loss 45.9748 test Loss 21.6308 with MSE metric 17612.8203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245 batch 170 train Loss 45.9682 test Loss 21.6282 with MSE metric 17611.1689\n",
      "Epoch 245 batch 180 train Loss 45.9616 test Loss 21.6255 with MSE metric 17609.5246\n",
      "Epoch 245 batch 190 train Loss 45.9550 test Loss 21.6229 with MSE metric 17607.9538\n",
      "Epoch 245 batch 200 train Loss 45.9484 test Loss 21.6202 with MSE metric 17606.3292\n",
      "Epoch 245 batch 210 train Loss 45.9418 test Loss 21.6176 with MSE metric 17604.7086\n",
      "Epoch 245 batch 220 train Loss 45.9352 test Loss 21.6150 with MSE metric 17603.0676\n",
      "Epoch 245 batch 230 train Loss 45.9285 test Loss 21.6124 with MSE metric 17601.3933\n",
      "Epoch 245 batch 240 train Loss 45.9219 test Loss 21.6097 with MSE metric 17599.7641\n",
      "Time taken for 1 epoch: 27.911991119384766 secs\n",
      "\n",
      "Epoch 246 batch 0 train Loss 45.9153 test Loss 21.6071 with MSE metric 17598.1434\n",
      "Epoch 246 batch 10 train Loss 45.9087 test Loss 21.6044 with MSE metric 17596.4817\n",
      "Epoch 246 batch 20 train Loss 45.9021 test Loss 21.6018 with MSE metric 17594.8499\n",
      "Epoch 246 batch 30 train Loss 45.8955 test Loss 21.5991 with MSE metric 17593.2518\n",
      "Epoch 246 batch 40 train Loss 45.8889 test Loss 21.5965 with MSE metric 17591.5719\n",
      "Epoch 246 batch 50 train Loss 45.8823 test Loss 21.5939 with MSE metric 17589.8997\n",
      "Epoch 246 batch 60 train Loss 45.8757 test Loss 21.5913 with MSE metric 17588.2497\n",
      "Epoch 246 batch 70 train Loss 45.8692 test Loss 21.5886 with MSE metric 17586.5707\n",
      "Epoch 246 batch 80 train Loss 45.8626 test Loss 21.5860 with MSE metric 17584.9439\n",
      "Epoch 246 batch 90 train Loss 45.8560 test Loss 21.5834 with MSE metric 17583.3208\n",
      "Epoch 246 batch 100 train Loss 45.8494 test Loss 21.5807 with MSE metric 17581.7492\n",
      "Epoch 246 batch 110 train Loss 45.8428 test Loss 21.5781 with MSE metric 17580.1391\n",
      "Epoch 246 batch 120 train Loss 45.8363 test Loss 21.5755 with MSE metric 17578.5362\n",
      "Epoch 246 batch 130 train Loss 45.8297 test Loss 21.5728 with MSE metric 17576.8728\n",
      "Epoch 246 batch 140 train Loss 45.8231 test Loss 21.5702 with MSE metric 17575.2819\n",
      "Epoch 246 batch 150 train Loss 45.8165 test Loss 21.5676 with MSE metric 17573.6831\n",
      "Epoch 246 batch 160 train Loss 45.8100 test Loss 21.5649 with MSE metric 17572.1074\n",
      "Epoch 246 batch 170 train Loss 45.8034 test Loss 21.5623 with MSE metric 17570.4400\n",
      "Epoch 246 batch 180 train Loss 45.7968 test Loss 21.5597 with MSE metric 17568.8442\n",
      "Epoch 246 batch 190 train Loss 45.7903 test Loss 21.5571 with MSE metric 17567.2775\n",
      "Epoch 246 batch 200 train Loss 45.7837 test Loss 21.5544 with MSE metric 17565.6719\n",
      "Epoch 246 batch 210 train Loss 45.7772 test Loss 21.5518 with MSE metric 17564.1058\n",
      "Epoch 246 batch 220 train Loss 45.7706 test Loss 21.5492 with MSE metric 17562.4097\n",
      "Epoch 246 batch 230 train Loss 45.7641 test Loss 21.5466 with MSE metric 17560.7611\n",
      "Epoch 246 batch 240 train Loss 45.7575 test Loss 21.5440 with MSE metric 17559.1359\n",
      "Time taken for 1 epoch: 31.217116832733154 secs\n",
      "\n",
      "Epoch 247 batch 0 train Loss 45.7510 test Loss 21.5413 with MSE metric 17557.5556\n",
      "Epoch 247 batch 10 train Loss 45.7444 test Loss 21.5387 with MSE metric 17555.9909\n",
      "Epoch 247 batch 20 train Loss 45.7379 test Loss 21.5361 with MSE metric 17554.4283\n",
      "Epoch 247 batch 30 train Loss 45.7313 test Loss 21.5335 with MSE metric 17552.7935\n",
      "Epoch 247 batch 40 train Loss 45.7248 test Loss 21.5309 with MSE metric 17551.1759\n",
      "Epoch 247 batch 50 train Loss 45.7182 test Loss 21.5283 with MSE metric 17549.5182\n",
      "Epoch 247 batch 60 train Loss 45.7117 test Loss 21.5257 with MSE metric 17547.9469\n",
      "Epoch 247 batch 70 train Loss 45.7052 test Loss 21.5231 with MSE metric 17546.3413\n",
      "Epoch 247 batch 80 train Loss 45.6986 test Loss 21.5204 with MSE metric 17544.7244\n",
      "Epoch 247 batch 90 train Loss 45.6921 test Loss 21.5178 with MSE metric 17543.0906\n",
      "Epoch 247 batch 100 train Loss 45.6856 test Loss 21.5152 with MSE metric 17541.5450\n",
      "Epoch 247 batch 110 train Loss 45.6791 test Loss 21.5126 with MSE metric 17539.9998\n",
      "Epoch 247 batch 120 train Loss 45.6726 test Loss 21.5100 with MSE metric 17538.4223\n",
      "Epoch 247 batch 130 train Loss 45.6660 test Loss 21.5074 with MSE metric 17536.8457\n",
      "Epoch 247 batch 140 train Loss 45.6595 test Loss 21.5048 with MSE metric 17535.2706\n",
      "Epoch 247 batch 150 train Loss 45.6530 test Loss 21.5022 with MSE metric 17533.6823\n",
      "Epoch 247 batch 160 train Loss 45.6465 test Loss 21.4995 with MSE metric 17532.1074\n",
      "Epoch 247 batch 170 train Loss 45.6400 test Loss 21.4969 with MSE metric 17530.5202\n",
      "Epoch 247 batch 180 train Loss 45.6335 test Loss 21.4943 with MSE metric 17528.8960\n",
      "Epoch 247 batch 190 train Loss 45.6270 test Loss 21.4917 with MSE metric 17527.3048\n",
      "Epoch 247 batch 200 train Loss 45.6204 test Loss 21.4891 with MSE metric 17525.7189\n",
      "Epoch 247 batch 210 train Loss 45.6139 test Loss 21.4865 with MSE metric 17524.1338\n",
      "Epoch 247 batch 220 train Loss 45.6074 test Loss 21.4839 with MSE metric 17522.5663\n",
      "Epoch 247 batch 230 train Loss 45.6009 test Loss 21.4813 with MSE metric 17520.9456\n",
      "Epoch 247 batch 240 train Loss 45.5944 test Loss 21.4788 with MSE metric 17519.2674\n",
      "Time taken for 1 epoch: 29.949848175048828 secs\n",
      "\n",
      "Epoch 248 batch 0 train Loss 45.5879 test Loss 21.4762 with MSE metric 17517.6594\n",
      "Epoch 248 batch 10 train Loss 45.5814 test Loss 21.4736 with MSE metric 17516.0640\n",
      "Epoch 248 batch 20 train Loss 45.5749 test Loss 21.4710 with MSE metric 17514.4883\n",
      "Epoch 248 batch 30 train Loss 45.5685 test Loss 21.4684 with MSE metric 17512.9008\n",
      "Epoch 248 batch 40 train Loss 45.5620 test Loss 21.4658 with MSE metric 17511.3269\n",
      "Epoch 248 batch 50 train Loss 45.5555 test Loss 21.4632 with MSE metric 17509.7058\n",
      "Epoch 248 batch 60 train Loss 45.5490 test Loss 21.4606 with MSE metric 17508.1071\n",
      "Epoch 248 batch 70 train Loss 45.5425 test Loss 21.4580 with MSE metric 17506.5407\n",
      "Epoch 248 batch 80 train Loss 45.5360 test Loss 21.4554 with MSE metric 17504.9658\n",
      "Epoch 248 batch 90 train Loss 45.5296 test Loss 21.4528 with MSE metric 17503.4175\n",
      "Epoch 248 batch 100 train Loss 45.5231 test Loss 21.4502 with MSE metric 17501.8517\n",
      "Epoch 248 batch 110 train Loss 45.5166 test Loss 21.4476 with MSE metric 17500.2695\n",
      "Epoch 248 batch 120 train Loss 45.5101 test Loss 21.4451 with MSE metric 17498.6999\n",
      "Epoch 248 batch 130 train Loss 45.5037 test Loss 21.4425 with MSE metric 17497.1227\n",
      "Epoch 248 batch 140 train Loss 45.4972 test Loss 21.4399 with MSE metric 17495.4967\n",
      "Epoch 248 batch 150 train Loss 45.4907 test Loss 21.4373 with MSE metric 17493.8874\n",
      "Epoch 248 batch 160 train Loss 45.4843 test Loss 21.4347 with MSE metric 17492.3819\n",
      "Epoch 248 batch 170 train Loss 45.4778 test Loss 21.4321 with MSE metric 17490.8015\n",
      "Epoch 248 batch 180 train Loss 45.4714 test Loss 21.4296 with MSE metric 17489.1677\n",
      "Epoch 248 batch 190 train Loss 45.4649 test Loss 21.4270 with MSE metric 17487.5851\n",
      "Epoch 248 batch 200 train Loss 45.4584 test Loss 21.4244 with MSE metric 17486.0530\n",
      "Epoch 248 batch 210 train Loss 45.4520 test Loss 21.4218 with MSE metric 17484.4749\n",
      "Epoch 248 batch 220 train Loss 45.4455 test Loss 21.4193 with MSE metric 17482.8679\n",
      "Epoch 248 batch 230 train Loss 45.4391 test Loss 21.4167 with MSE metric 17481.2893\n",
      "Epoch 248 batch 240 train Loss 45.4326 test Loss 21.4141 with MSE metric 17479.7098\n",
      "Time taken for 1 epoch: 27.687521934509277 secs\n",
      "\n",
      "Epoch 249 batch 0 train Loss 45.4262 test Loss 21.4115 with MSE metric 17478.1159\n",
      "Epoch 249 batch 10 train Loss 45.4198 test Loss 21.4089 with MSE metric 17476.5294\n",
      "Epoch 249 batch 20 train Loss 45.4133 test Loss 21.4064 with MSE metric 17474.9329\n",
      "Epoch 249 batch 30 train Loss 45.4069 test Loss 21.4038 with MSE metric 17473.3775\n",
      "Epoch 249 batch 40 train Loss 45.4004 test Loss 21.4012 with MSE metric 17471.8257\n",
      "Epoch 249 batch 50 train Loss 45.3940 test Loss 21.3987 with MSE metric 17470.0951\n",
      "Epoch 249 batch 60 train Loss 45.3876 test Loss 21.3961 with MSE metric 17468.5308\n",
      "Epoch 249 batch 70 train Loss 45.3811 test Loss 21.3935 with MSE metric 17466.9706\n",
      "Epoch 249 batch 80 train Loss 45.3747 test Loss 21.3910 with MSE metric 17465.4119\n",
      "Epoch 249 batch 90 train Loss 45.3683 test Loss 21.3884 with MSE metric 17463.8469\n",
      "Epoch 249 batch 100 train Loss 45.3619 test Loss 21.3858 with MSE metric 17462.2418\n",
      "Epoch 249 batch 110 train Loss 45.3554 test Loss 21.3832 with MSE metric 17460.6834\n",
      "Epoch 249 batch 120 train Loss 45.3490 test Loss 21.3807 with MSE metric 17459.0609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249 batch 130 train Loss 45.3426 test Loss 21.3781 with MSE metric 17457.5049\n",
      "Epoch 249 batch 140 train Loss 45.3362 test Loss 21.3756 with MSE metric 17455.9776\n",
      "Epoch 249 batch 150 train Loss 45.3298 test Loss 21.3730 with MSE metric 17454.3847\n",
      "Epoch 249 batch 160 train Loss 45.3234 test Loss 21.3705 with MSE metric 17452.7957\n",
      "Epoch 249 batch 170 train Loss 45.3169 test Loss 21.3679 with MSE metric 17451.1789\n",
      "Epoch 249 batch 180 train Loss 45.3105 test Loss 21.3653 with MSE metric 17449.6354\n",
      "Epoch 249 batch 190 train Loss 45.3041 test Loss 21.3628 with MSE metric 17448.0767\n",
      "Epoch 249 batch 200 train Loss 45.2977 test Loss 21.3602 with MSE metric 17446.4880\n",
      "Epoch 249 batch 210 train Loss 45.2913 test Loss 21.3576 with MSE metric 17444.8587\n",
      "Epoch 249 batch 220 train Loss 45.2849 test Loss 21.3551 with MSE metric 17443.2754\n",
      "Epoch 249 batch 230 train Loss 45.2785 test Loss 21.3525 with MSE metric 17441.7247\n",
      "Epoch 249 batch 240 train Loss 45.2721 test Loss 21.3500 with MSE metric 17440.1750\n",
      "Time taken for 1 epoch: 27.508673191070557 secs\n",
      "\n",
      "Epoch 250 batch 0 train Loss 45.2657 test Loss 21.3474 with MSE metric 17438.5717\n",
      "Epoch 250 batch 10 train Loss 45.2593 test Loss 21.3448 with MSE metric 17436.9903\n",
      "Epoch 250 batch 20 train Loss 45.2530 test Loss 21.3423 with MSE metric 17435.4322\n",
      "Epoch 250 batch 30 train Loss 45.2466 test Loss 21.3397 with MSE metric 17433.8559\n",
      "Epoch 250 batch 40 train Loss 45.2402 test Loss 21.3372 with MSE metric 17432.2460\n",
      "Epoch 250 batch 50 train Loss 45.2338 test Loss 21.3346 with MSE metric 17430.6571\n",
      "Epoch 250 batch 60 train Loss 45.2274 test Loss 21.3321 with MSE metric 17429.1863\n",
      "Epoch 250 batch 70 train Loss 45.2210 test Loss 21.3295 with MSE metric 17427.6208\n",
      "Epoch 250 batch 80 train Loss 45.2147 test Loss 21.3270 with MSE metric 17426.0620\n",
      "Epoch 250 batch 90 train Loss 45.2083 test Loss 21.3245 with MSE metric 17424.5064\n",
      "Epoch 250 batch 100 train Loss 45.2019 test Loss 21.3219 with MSE metric 17422.9819\n",
      "Epoch 250 batch 110 train Loss 45.1956 test Loss 21.3193 with MSE metric 17421.3970\n",
      "Epoch 250 batch 120 train Loss 45.1892 test Loss 21.3168 with MSE metric 17419.7856\n",
      "Epoch 250 batch 130 train Loss 45.1828 test Loss 21.3143 with MSE metric 17418.2539\n",
      "Epoch 250 batch 140 train Loss 45.1765 test Loss 21.3117 with MSE metric 17416.6733\n",
      "Epoch 250 batch 150 train Loss 45.1701 test Loss 21.3092 with MSE metric 17415.1002\n",
      "Epoch 250 batch 160 train Loss 45.1637 test Loss 21.3066 with MSE metric 17413.5217\n",
      "Epoch 250 batch 170 train Loss 45.1574 test Loss 21.3041 with MSE metric 17411.9224\n",
      "Epoch 250 batch 180 train Loss 45.1510 test Loss 21.3016 with MSE metric 17410.3327\n",
      "Epoch 250 batch 190 train Loss 45.1447 test Loss 21.2990 with MSE metric 17408.8085\n",
      "Epoch 250 batch 200 train Loss 45.1383 test Loss 21.2965 with MSE metric 17407.2147\n",
      "Epoch 250 batch 210 train Loss 45.1320 test Loss 21.2939 with MSE metric 17405.6520\n",
      "Epoch 250 batch 220 train Loss 45.1256 test Loss 21.2914 with MSE metric 17404.1267\n",
      "Epoch 250 batch 230 train Loss 45.1193 test Loss 21.2888 with MSE metric 17402.5718\n",
      "Epoch 250 batch 240 train Loss 45.1129 test Loss 21.2863 with MSE metric 17400.9836\n",
      "Time taken for 1 epoch: 31.14138913154602 secs\n",
      "\n",
      "Epoch 251 batch 0 train Loss 45.1066 test Loss 21.2838 with MSE metric 17399.4248\n",
      "Epoch 251 batch 10 train Loss 45.1002 test Loss 21.2813 with MSE metric 17397.7744\n",
      "Epoch 251 batch 20 train Loss 45.0939 test Loss 21.2787 with MSE metric 17396.2303\n",
      "Epoch 251 batch 30 train Loss 45.0876 test Loss 21.2762 with MSE metric 17394.6667\n",
      "Epoch 251 batch 40 train Loss 45.0812 test Loss 21.2737 with MSE metric 17393.0661\n",
      "Epoch 251 batch 50 train Loss 45.0749 test Loss 21.2711 with MSE metric 17391.5867\n",
      "Epoch 251 batch 60 train Loss 45.0686 test Loss 21.2686 with MSE metric 17390.0549\n",
      "Epoch 251 batch 70 train Loss 45.0622 test Loss 21.2661 with MSE metric 17388.5290\n",
      "Epoch 251 batch 80 train Loss 45.0559 test Loss 21.2635 with MSE metric 17386.9911\n",
      "Epoch 251 batch 90 train Loss 45.0496 test Loss 21.2610 with MSE metric 17385.4193\n",
      "Epoch 251 batch 100 train Loss 45.0433 test Loss 21.2584 with MSE metric 17383.8843\n",
      "Epoch 251 batch 110 train Loss 45.0370 test Loss 21.2559 with MSE metric 17382.3448\n",
      "Epoch 251 batch 120 train Loss 45.0306 test Loss 21.2534 with MSE metric 17380.7328\n",
      "Epoch 251 batch 130 train Loss 45.0243 test Loss 21.2509 with MSE metric 17379.1757\n",
      "Epoch 251 batch 140 train Loss 45.0180 test Loss 21.2484 with MSE metric 17377.6717\n",
      "Epoch 251 batch 150 train Loss 45.0117 test Loss 21.2459 with MSE metric 17376.1686\n",
      "Epoch 251 batch 160 train Loss 45.0054 test Loss 21.2433 with MSE metric 17374.5683\n",
      "Epoch 251 batch 170 train Loss 44.9991 test Loss 21.2408 with MSE metric 17373.0759\n",
      "Epoch 251 batch 180 train Loss 44.9928 test Loss 21.2383 with MSE metric 17371.4864\n",
      "Epoch 251 batch 190 train Loss 44.9865 test Loss 21.2358 with MSE metric 17369.9276\n",
      "Epoch 251 batch 200 train Loss 44.9802 test Loss 21.2333 with MSE metric 17368.4257\n",
      "Epoch 251 batch 210 train Loss 44.9739 test Loss 21.2308 with MSE metric 17366.9483\n",
      "Epoch 251 batch 220 train Loss 44.9676 test Loss 21.2282 with MSE metric 17365.3919\n",
      "Epoch 251 batch 230 train Loss 44.9613 test Loss 21.2257 with MSE metric 17363.8213\n",
      "Epoch 251 batch 240 train Loss 44.9550 test Loss 21.2232 with MSE metric 17362.2771\n",
      "Time taken for 1 epoch: 33.983306884765625 secs\n",
      "\n",
      "Epoch 252 batch 0 train Loss 44.9487 test Loss 21.2207 with MSE metric 17360.7039\n",
      "Epoch 252 batch 10 train Loss 44.9424 test Loss 21.2182 with MSE metric 17359.2106\n",
      "Epoch 252 batch 20 train Loss 44.9361 test Loss 21.2157 with MSE metric 17357.6489\n",
      "Epoch 252 batch 30 train Loss 44.9298 test Loss 21.2131 with MSE metric 17356.1363\n",
      "Epoch 252 batch 40 train Loss 44.9235 test Loss 21.2106 with MSE metric 17354.5383\n",
      "Epoch 252 batch 50 train Loss 44.9173 test Loss 21.2081 with MSE metric 17352.9925\n",
      "Epoch 252 batch 60 train Loss 44.9110 test Loss 21.2056 with MSE metric 17351.4773\n",
      "Epoch 252 batch 70 train Loss 44.9047 test Loss 21.2031 with MSE metric 17349.9167\n",
      "Epoch 252 batch 80 train Loss 44.8984 test Loss 21.2006 with MSE metric 17348.3338\n",
      "Epoch 252 batch 90 train Loss 44.8922 test Loss 21.1981 with MSE metric 17346.7665\n",
      "Epoch 252 batch 100 train Loss 44.8859 test Loss 21.1956 with MSE metric 17345.1961\n",
      "Epoch 252 batch 110 train Loss 44.8796 test Loss 21.1930 with MSE metric 17343.6929\n",
      "Epoch 252 batch 120 train Loss 44.8733 test Loss 21.1906 with MSE metric 17342.1268\n",
      "Epoch 252 batch 130 train Loss 44.8671 test Loss 21.1881 with MSE metric 17340.5531\n",
      "Epoch 252 batch 140 train Loss 44.8608 test Loss 21.1856 with MSE metric 17339.0275\n",
      "Epoch 252 batch 150 train Loss 44.8545 test Loss 21.1831 with MSE metric 17337.4852\n",
      "Epoch 252 batch 160 train Loss 44.8483 test Loss 21.1806 with MSE metric 17335.9431\n",
      "Epoch 252 batch 170 train Loss 44.8420 test Loss 21.1780 with MSE metric 17334.3943\n",
      "Epoch 252 batch 180 train Loss 44.8358 test Loss 21.1755 with MSE metric 17332.8512\n",
      "Epoch 252 batch 190 train Loss 44.8295 test Loss 21.1730 with MSE metric 17331.3393\n",
      "Epoch 252 batch 200 train Loss 44.8233 test Loss 21.1705 with MSE metric 17329.8531\n",
      "Epoch 252 batch 210 train Loss 44.8170 test Loss 21.1680 with MSE metric 17328.3158\n",
      "Epoch 252 batch 220 train Loss 44.8108 test Loss 21.1655 with MSE metric 17326.8551\n",
      "Epoch 252 batch 230 train Loss 44.8045 test Loss 21.1630 with MSE metric 17325.2653\n",
      "Epoch 252 batch 240 train Loss 44.7983 test Loss 21.1605 with MSE metric 17323.7616\n",
      "Time taken for 1 epoch: 28.937411785125732 secs\n",
      "\n",
      "Epoch 253 batch 0 train Loss 44.7920 test Loss 21.1580 with MSE metric 17322.2484\n",
      "Epoch 253 batch 10 train Loss 44.7858 test Loss 21.1555 with MSE metric 17320.6963\n",
      "Epoch 253 batch 20 train Loss 44.7796 test Loss 21.1531 with MSE metric 17319.2356\n",
      "Epoch 253 batch 30 train Loss 44.7733 test Loss 21.1506 with MSE metric 17317.6926\n",
      "Epoch 253 batch 40 train Loss 44.7671 test Loss 21.1481 with MSE metric 17316.2312\n",
      "Epoch 253 batch 50 train Loss 44.7609 test Loss 21.1456 with MSE metric 17314.7004\n",
      "Epoch 253 batch 60 train Loss 44.7546 test Loss 21.1431 with MSE metric 17313.1568\n",
      "Epoch 253 batch 70 train Loss 44.7484 test Loss 21.1406 with MSE metric 17311.5702\n",
      "Epoch 253 batch 80 train Loss 44.7422 test Loss 21.1381 with MSE metric 17310.0333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253 batch 90 train Loss 44.7360 test Loss 21.1357 with MSE metric 17308.4163\n",
      "Epoch 253 batch 100 train Loss 44.7297 test Loss 21.1332 with MSE metric 17306.8449\n",
      "Epoch 253 batch 110 train Loss 44.7235 test Loss 21.1307 with MSE metric 17305.3171\n",
      "Epoch 253 batch 120 train Loss 44.7173 test Loss 21.1282 with MSE metric 17303.7242\n",
      "Epoch 253 batch 130 train Loss 44.7111 test Loss 21.1258 with MSE metric 17302.2268\n",
      "Epoch 253 batch 140 train Loss 44.7048 test Loss 21.1233 with MSE metric 17300.6638\n",
      "Epoch 253 batch 150 train Loss 44.6986 test Loss 21.1208 with MSE metric 17299.1694\n",
      "Epoch 253 batch 160 train Loss 44.6924 test Loss 21.1183 with MSE metric 17297.6437\n",
      "Epoch 253 batch 170 train Loss 44.6862 test Loss 21.1158 with MSE metric 17296.0642\n",
      "Epoch 253 batch 180 train Loss 44.6800 test Loss 21.1134 with MSE metric 17294.5664\n",
      "Epoch 253 batch 190 train Loss 44.6738 test Loss 21.1109 with MSE metric 17293.0380\n",
      "Epoch 253 batch 200 train Loss 44.6676 test Loss 21.1084 with MSE metric 17291.5037\n",
      "Epoch 253 batch 210 train Loss 44.6614 test Loss 21.1059 with MSE metric 17289.9571\n",
      "Epoch 253 batch 220 train Loss 44.6552 test Loss 21.1034 with MSE metric 17288.4580\n",
      "Epoch 253 batch 230 train Loss 44.6490 test Loss 21.1010 with MSE metric 17286.9686\n",
      "Epoch 253 batch 240 train Loss 44.6428 test Loss 21.0985 with MSE metric 17285.4636\n",
      "Time taken for 1 epoch: 27.169073820114136 secs\n",
      "\n",
      "Epoch 254 batch 0 train Loss 44.6366 test Loss 21.0960 with MSE metric 17283.9558\n",
      "Epoch 254 batch 10 train Loss 44.6304 test Loss 21.0935 with MSE metric 17282.4456\n",
      "Epoch 254 batch 20 train Loss 44.6243 test Loss 21.0911 with MSE metric 17280.9841\n",
      "Epoch 254 batch 30 train Loss 44.6181 test Loss 21.0886 with MSE metric 17279.4707\n",
      "Epoch 254 batch 40 train Loss 44.6119 test Loss 21.0861 with MSE metric 17277.9817\n",
      "Epoch 254 batch 50 train Loss 44.6057 test Loss 21.0837 with MSE metric 17276.4448\n",
      "Epoch 254 batch 60 train Loss 44.5995 test Loss 21.0812 with MSE metric 17274.9689\n",
      "Epoch 254 batch 70 train Loss 44.5933 test Loss 21.0787 with MSE metric 17273.4234\n",
      "Epoch 254 batch 80 train Loss 44.5872 test Loss 21.0763 with MSE metric 17271.8911\n",
      "Epoch 254 batch 90 train Loss 44.5810 test Loss 21.0738 with MSE metric 17270.4607\n",
      "Epoch 254 batch 100 train Loss 44.5748 test Loss 21.0713 with MSE metric 17268.9391\n",
      "Epoch 254 batch 110 train Loss 44.5686 test Loss 21.0689 with MSE metric 17267.4301\n",
      "Epoch 254 batch 120 train Loss 44.5625 test Loss 21.0664 with MSE metric 17265.8971\n",
      "Epoch 254 batch 130 train Loss 44.5563 test Loss 21.0639 with MSE metric 17264.3600\n",
      "Epoch 254 batch 140 train Loss 44.5501 test Loss 21.0615 with MSE metric 17262.8433\n",
      "Epoch 254 batch 150 train Loss 44.5440 test Loss 21.0590 with MSE metric 17261.3700\n",
      "Epoch 254 batch 160 train Loss 44.5378 test Loss 21.0565 with MSE metric 17259.8384\n",
      "Epoch 254 batch 170 train Loss 44.5316 test Loss 21.0541 with MSE metric 17258.2908\n",
      "Epoch 254 batch 180 train Loss 44.5255 test Loss 21.0516 with MSE metric 17256.7389\n",
      "Epoch 254 batch 190 train Loss 44.5193 test Loss 21.0491 with MSE metric 17255.2075\n",
      "Epoch 254 batch 200 train Loss 44.5132 test Loss 21.0467 with MSE metric 17253.7049\n",
      "Epoch 254 batch 210 train Loss 44.5070 test Loss 21.0442 with MSE metric 17252.1185\n",
      "Epoch 254 batch 220 train Loss 44.5009 test Loss 21.0418 with MSE metric 17250.5440\n",
      "Epoch 254 batch 230 train Loss 44.4947 test Loss 21.0393 with MSE metric 17249.0711\n",
      "Epoch 254 batch 240 train Loss 44.4886 test Loss 21.0369 with MSE metric 17247.5354\n",
      "Time taken for 1 epoch: 28.64540386199951 secs\n",
      "\n",
      "Epoch 255 batch 0 train Loss 44.4824 test Loss 21.0344 with MSE metric 17246.0584\n",
      "Epoch 255 batch 10 train Loss 44.4763 test Loss 21.0320 with MSE metric 17244.4925\n",
      "Epoch 255 batch 20 train Loss 44.4701 test Loss 21.0295 with MSE metric 17242.9566\n",
      "Epoch 255 batch 30 train Loss 44.4640 test Loss 21.0270 with MSE metric 17241.4189\n",
      "Epoch 255 batch 40 train Loss 44.4579 test Loss 21.0246 with MSE metric 17239.8991\n",
      "Epoch 255 batch 50 train Loss 44.4517 test Loss 21.0221 with MSE metric 17238.3497\n",
      "Epoch 255 batch 60 train Loss 44.4456 test Loss 21.0197 with MSE metric 17236.8840\n",
      "Epoch 255 batch 70 train Loss 44.4395 test Loss 21.0172 with MSE metric 17235.3757\n",
      "Epoch 255 batch 80 train Loss 44.4333 test Loss 21.0148 with MSE metric 17233.8870\n",
      "Epoch 255 batch 90 train Loss 44.4272 test Loss 21.0123 with MSE metric 17232.3608\n",
      "Epoch 255 batch 100 train Loss 44.4211 test Loss 21.0099 with MSE metric 17230.8258\n",
      "Epoch 255 batch 110 train Loss 44.4149 test Loss 21.0074 with MSE metric 17229.3323\n",
      "Epoch 255 batch 120 train Loss 44.4088 test Loss 21.0050 with MSE metric 17227.7574\n",
      "Epoch 255 batch 130 train Loss 44.4027 test Loss 21.0025 with MSE metric 17226.2673\n",
      "Epoch 255 batch 140 train Loss 44.3966 test Loss 21.0001 with MSE metric 17224.7305\n",
      "Epoch 255 batch 150 train Loss 44.3905 test Loss 20.9977 with MSE metric 17223.2644\n",
      "Epoch 255 batch 160 train Loss 44.3844 test Loss 20.9952 with MSE metric 17221.7381\n",
      "Epoch 255 batch 170 train Loss 44.3783 test Loss 20.9928 with MSE metric 17220.2966\n",
      "Epoch 255 batch 180 train Loss 44.3721 test Loss 20.9903 with MSE metric 17218.8095\n",
      "Epoch 255 batch 190 train Loss 44.3660 test Loss 20.9879 with MSE metric 17217.3711\n",
      "Epoch 255 batch 200 train Loss 44.3599 test Loss 20.9855 with MSE metric 17215.8841\n",
      "Epoch 255 batch 210 train Loss 44.3538 test Loss 20.9830 with MSE metric 17214.3881\n",
      "Epoch 255 batch 220 train Loss 44.3477 test Loss 20.9806 with MSE metric 17212.8585\n",
      "Epoch 255 batch 230 train Loss 44.3416 test Loss 20.9781 with MSE metric 17211.3313\n",
      "Epoch 255 batch 240 train Loss 44.3355 test Loss 20.9757 with MSE metric 17209.8588\n",
      "Time taken for 1 epoch: 28.48142910003662 secs\n",
      "\n",
      "Epoch 256 batch 0 train Loss 44.3294 test Loss 20.9733 with MSE metric 17208.3476\n",
      "Epoch 256 batch 10 train Loss 44.3233 test Loss 20.9708 with MSE metric 17206.9146\n",
      "Epoch 256 batch 20 train Loss 44.3172 test Loss 20.9684 with MSE metric 17205.3804\n",
      "Epoch 256 batch 30 train Loss 44.3112 test Loss 20.9660 with MSE metric 17203.9404\n",
      "Epoch 256 batch 40 train Loss 44.3051 test Loss 20.9636 with MSE metric 17202.4403\n",
      "Epoch 256 batch 50 train Loss 44.2990 test Loss 20.9611 with MSE metric 17200.9573\n",
      "Epoch 256 batch 60 train Loss 44.2929 test Loss 20.9587 with MSE metric 17199.4380\n",
      "Epoch 256 batch 70 train Loss 44.2868 test Loss 20.9563 with MSE metric 17197.9991\n",
      "Epoch 256 batch 80 train Loss 44.2807 test Loss 20.9538 with MSE metric 17196.5278\n",
      "Epoch 256 batch 90 train Loss 44.2746 test Loss 20.9514 with MSE metric 17195.0034\n",
      "Epoch 256 batch 100 train Loss 44.2686 test Loss 20.9489 with MSE metric 17193.4985\n",
      "Epoch 256 batch 110 train Loss 44.2625 test Loss 20.9465 with MSE metric 17192.0092\n",
      "Epoch 256 batch 120 train Loss 44.2564 test Loss 20.9441 with MSE metric 17190.5146\n",
      "Epoch 256 batch 130 train Loss 44.2503 test Loss 20.9417 with MSE metric 17189.0234\n",
      "Epoch 256 batch 140 train Loss 44.2443 test Loss 20.9392 with MSE metric 17187.5336\n",
      "Epoch 256 batch 150 train Loss 44.2382 test Loss 20.9368 with MSE metric 17186.0951\n",
      "Epoch 256 batch 160 train Loss 44.2321 test Loss 20.9344 with MSE metric 17184.6022\n",
      "Epoch 256 batch 170 train Loss 44.2261 test Loss 20.9320 with MSE metric 17183.1006\n",
      "Epoch 256 batch 180 train Loss 44.2200 test Loss 20.9296 with MSE metric 17181.5781\n",
      "Epoch 256 batch 190 train Loss 44.2140 test Loss 20.9272 with MSE metric 17180.1611\n",
      "Epoch 256 batch 200 train Loss 44.2079 test Loss 20.9247 with MSE metric 17178.6445\n",
      "Epoch 256 batch 210 train Loss 44.2018 test Loss 20.9223 with MSE metric 17177.2058\n",
      "Epoch 256 batch 220 train Loss 44.1958 test Loss 20.9199 with MSE metric 17175.7398\n",
      "Epoch 256 batch 230 train Loss 44.1897 test Loss 20.9174 with MSE metric 17174.2531\n",
      "Epoch 256 batch 240 train Loss 44.1837 test Loss 20.9150 with MSE metric 17172.7592\n",
      "Time taken for 1 epoch: 30.427921772003174 secs\n",
      "\n",
      "Epoch 257 batch 0 train Loss 44.1776 test Loss 20.9126 with MSE metric 17171.2330\n",
      "Epoch 257 batch 10 train Loss 44.1716 test Loss 20.9102 with MSE metric 17169.7796\n",
      "Epoch 257 batch 20 train Loss 44.1655 test Loss 20.9078 with MSE metric 17168.2913\n",
      "Epoch 257 batch 30 train Loss 44.1595 test Loss 20.9054 with MSE metric 17166.8206\n",
      "Epoch 257 batch 40 train Loss 44.1535 test Loss 20.9030 with MSE metric 17165.3589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 batch 50 train Loss 44.1474 test Loss 20.9005 with MSE metric 17163.8286\n",
      "Epoch 257 batch 60 train Loss 44.1414 test Loss 20.8981 with MSE metric 17162.3303\n",
      "Epoch 257 batch 70 train Loss 44.1353 test Loss 20.8957 with MSE metric 17160.8169\n",
      "Epoch 257 batch 80 train Loss 44.1293 test Loss 20.8933 with MSE metric 17159.3610\n",
      "Epoch 257 batch 90 train Loss 44.1233 test Loss 20.8909 with MSE metric 17157.8703\n",
      "Epoch 257 batch 100 train Loss 44.1172 test Loss 20.8885 with MSE metric 17156.3907\n",
      "Epoch 257 batch 110 train Loss 44.1112 test Loss 20.8861 with MSE metric 17154.8694\n",
      "Epoch 257 batch 120 train Loss 44.1052 test Loss 20.8837 with MSE metric 17153.4071\n",
      "Epoch 257 batch 130 train Loss 44.0992 test Loss 20.8812 with MSE metric 17151.9570\n",
      "Epoch 257 batch 140 train Loss 44.0931 test Loss 20.8788 with MSE metric 17150.4511\n",
      "Epoch 257 batch 150 train Loss 44.0871 test Loss 20.8764 with MSE metric 17148.9913\n",
      "Epoch 257 batch 160 train Loss 44.0811 test Loss 20.8740 with MSE metric 17147.5398\n",
      "Epoch 257 batch 170 train Loss 44.0751 test Loss 20.8716 with MSE metric 17146.0202\n",
      "Epoch 257 batch 180 train Loss 44.0691 test Loss 20.8692 with MSE metric 17144.5716\n",
      "Epoch 257 batch 190 train Loss 44.0631 test Loss 20.8668 with MSE metric 17143.1329\n",
      "Epoch 257 batch 200 train Loss 44.0570 test Loss 20.8644 with MSE metric 17141.6726\n",
      "Epoch 257 batch 210 train Loss 44.0510 test Loss 20.8620 with MSE metric 17140.1404\n",
      "Epoch 257 batch 220 train Loss 44.0450 test Loss 20.8596 with MSE metric 17138.6755\n",
      "Epoch 257 batch 230 train Loss 44.0390 test Loss 20.8572 with MSE metric 17137.1640\n",
      "Epoch 257 batch 240 train Loss 44.0330 test Loss 20.8548 with MSE metric 17135.7420\n",
      "Time taken for 1 epoch: 27.451735973358154 secs\n",
      "\n",
      "Epoch 258 batch 0 train Loss 44.0270 test Loss 20.8524 with MSE metric 17134.2675\n",
      "Epoch 258 batch 10 train Loss 44.0210 test Loss 20.8500 with MSE metric 17132.8122\n",
      "Epoch 258 batch 20 train Loss 44.0150 test Loss 20.8476 with MSE metric 17131.3625\n",
      "Epoch 258 batch 30 train Loss 44.0090 test Loss 20.8452 with MSE metric 17129.9535\n",
      "Epoch 258 batch 40 train Loss 44.0030 test Loss 20.8428 with MSE metric 17128.4787\n",
      "Epoch 258 batch 50 train Loss 43.9970 test Loss 20.8404 with MSE metric 17127.0062\n",
      "Epoch 258 batch 60 train Loss 43.9910 test Loss 20.8380 with MSE metric 17125.5304\n",
      "Epoch 258 batch 70 train Loss 43.9851 test Loss 20.8357 with MSE metric 17124.0220\n",
      "Epoch 258 batch 80 train Loss 43.9791 test Loss 20.8333 with MSE metric 17122.5339\n",
      "Epoch 258 batch 90 train Loss 43.9731 test Loss 20.8309 with MSE metric 17121.1035\n",
      "Epoch 258 batch 100 train Loss 43.9671 test Loss 20.8285 with MSE metric 17119.6836\n",
      "Epoch 258 batch 110 train Loss 43.9611 test Loss 20.8261 with MSE metric 17118.2838\n",
      "Epoch 258 batch 120 train Loss 43.9551 test Loss 20.8238 with MSE metric 17116.8021\n",
      "Epoch 258 batch 130 train Loss 43.9492 test Loss 20.8214 with MSE metric 17115.3122\n",
      "Epoch 258 batch 140 train Loss 43.9432 test Loss 20.8190 with MSE metric 17113.8827\n",
      "Epoch 258 batch 150 train Loss 43.9372 test Loss 20.8166 with MSE metric 17112.4100\n",
      "Epoch 258 batch 160 train Loss 43.9312 test Loss 20.8142 with MSE metric 17110.9473\n",
      "Epoch 258 batch 170 train Loss 43.9253 test Loss 20.8118 with MSE metric 17109.4474\n",
      "Epoch 258 batch 180 train Loss 43.9193 test Loss 20.8095 with MSE metric 17107.9886\n",
      "Epoch 258 batch 190 train Loss 43.9133 test Loss 20.8071 with MSE metric 17106.5157\n",
      "Epoch 258 batch 200 train Loss 43.9074 test Loss 20.8047 with MSE metric 17105.0498\n",
      "Epoch 258 batch 210 train Loss 43.9014 test Loss 20.8023 with MSE metric 17103.5778\n",
      "Epoch 258 batch 220 train Loss 43.8955 test Loss 20.7999 with MSE metric 17102.1085\n",
      "Epoch 258 batch 230 train Loss 43.8895 test Loss 20.7976 with MSE metric 17100.6028\n",
      "Epoch 258 batch 240 train Loss 43.8835 test Loss 20.7952 with MSE metric 17099.1548\n",
      "Time taken for 1 epoch: 27.475603818893433 secs\n",
      "\n",
      "Epoch 259 batch 0 train Loss 43.8776 test Loss 20.7928 with MSE metric 17097.7549\n",
      "Epoch 259 batch 10 train Loss 43.8716 test Loss 20.7904 with MSE metric 17096.2902\n",
      "Epoch 259 batch 20 train Loss 43.8657 test Loss 20.7881 with MSE metric 17094.7885\n",
      "Epoch 259 batch 30 train Loss 43.8597 test Loss 20.7857 with MSE metric 17093.3138\n",
      "Epoch 259 batch 40 train Loss 43.8538 test Loss 20.7833 with MSE metric 17091.8438\n",
      "Epoch 259 batch 50 train Loss 43.8478 test Loss 20.7809 with MSE metric 17090.4226\n",
      "Epoch 259 batch 60 train Loss 43.8419 test Loss 20.7786 with MSE metric 17088.9439\n",
      "Epoch 259 batch 70 train Loss 43.8359 test Loss 20.7762 with MSE metric 17087.4717\n",
      "Epoch 259 batch 80 train Loss 43.8300 test Loss 20.7738 with MSE metric 17086.0319\n",
      "Epoch 259 batch 90 train Loss 43.8240 test Loss 20.7714 with MSE metric 17084.5396\n",
      "Epoch 259 batch 100 train Loss 43.8181 test Loss 20.7690 with MSE metric 17083.0994\n",
      "Epoch 259 batch 110 train Loss 43.8122 test Loss 20.7667 with MSE metric 17081.6316\n",
      "Epoch 259 batch 120 train Loss 43.8062 test Loss 20.7643 with MSE metric 17080.1602\n",
      "Epoch 259 batch 130 train Loss 43.8003 test Loss 20.7619 with MSE metric 17078.7029\n",
      "Epoch 259 batch 140 train Loss 43.7944 test Loss 20.7596 with MSE metric 17077.2287\n",
      "Epoch 259 batch 150 train Loss 43.7884 test Loss 20.7572 with MSE metric 17075.7853\n",
      "Epoch 259 batch 160 train Loss 43.7825 test Loss 20.7548 with MSE metric 17074.3786\n",
      "Epoch 259 batch 170 train Loss 43.7766 test Loss 20.7524 with MSE metric 17072.8765\n",
      "Epoch 259 batch 180 train Loss 43.7707 test Loss 20.7501 with MSE metric 17071.4263\n",
      "Epoch 259 batch 190 train Loss 43.7647 test Loss 20.7477 with MSE metric 17069.9236\n",
      "Epoch 259 batch 200 train Loss 43.7588 test Loss 20.7454 with MSE metric 17068.4796\n",
      "Epoch 259 batch 210 train Loss 43.7529 test Loss 20.7430 with MSE metric 17066.9828\n",
      "Epoch 259 batch 220 train Loss 43.7470 test Loss 20.7406 with MSE metric 17065.4871\n",
      "Epoch 259 batch 230 train Loss 43.7411 test Loss 20.7383 with MSE metric 17064.0231\n",
      "Epoch 259 batch 240 train Loss 43.7352 test Loss 20.7359 with MSE metric 17062.5458\n",
      "Time taken for 1 epoch: 27.856539249420166 secs\n",
      "\n",
      "Epoch 260 batch 0 train Loss 43.7292 test Loss 20.7336 with MSE metric 17061.0601\n",
      "Epoch 260 batch 10 train Loss 43.7233 test Loss 20.7312 with MSE metric 17059.6120\n",
      "Epoch 260 batch 20 train Loss 43.7174 test Loss 20.7289 with MSE metric 17058.1622\n",
      "Epoch 260 batch 30 train Loss 43.7115 test Loss 20.7265 with MSE metric 17056.7292\n",
      "Epoch 260 batch 40 train Loss 43.7056 test Loss 20.7241 with MSE metric 17055.2647\n",
      "Epoch 260 batch 50 train Loss 43.6997 test Loss 20.7218 with MSE metric 17053.8100\n",
      "Epoch 260 batch 60 train Loss 43.6938 test Loss 20.7194 with MSE metric 17052.3559\n",
      "Epoch 260 batch 70 train Loss 43.6879 test Loss 20.7171 with MSE metric 17050.8089\n",
      "Epoch 260 batch 80 train Loss 43.6820 test Loss 20.7147 with MSE metric 17049.2928\n",
      "Epoch 260 batch 90 train Loss 43.6761 test Loss 20.7124 with MSE metric 17047.8326\n",
      "Epoch 260 batch 100 train Loss 43.6702 test Loss 20.7100 with MSE metric 17046.3884\n",
      "Epoch 260 batch 110 train Loss 43.6643 test Loss 20.7076 with MSE metric 17044.9830\n",
      "Epoch 260 batch 120 train Loss 43.6584 test Loss 20.7053 with MSE metric 17043.5461\n",
      "Epoch 260 batch 130 train Loss 43.6526 test Loss 20.7030 with MSE metric 17042.1185\n",
      "Epoch 260 batch 140 train Loss 43.6467 test Loss 20.7006 with MSE metric 17040.7217\n",
      "Epoch 260 batch 150 train Loss 43.6408 test Loss 20.6983 with MSE metric 17039.2866\n",
      "Epoch 260 batch 160 train Loss 43.6349 test Loss 20.6959 with MSE metric 17037.8821\n",
      "Epoch 260 batch 170 train Loss 43.6290 test Loss 20.6936 with MSE metric 17036.3954\n",
      "Epoch 260 batch 180 train Loss 43.6232 test Loss 20.6912 with MSE metric 17034.9606\n",
      "Epoch 260 batch 190 train Loss 43.6173 test Loss 20.6889 with MSE metric 17033.5435\n",
      "Epoch 260 batch 200 train Loss 43.6114 test Loss 20.6865 with MSE metric 17032.1143\n",
      "Epoch 260 batch 210 train Loss 43.6055 test Loss 20.6842 with MSE metric 17030.6960\n",
      "Epoch 260 batch 220 train Loss 43.5997 test Loss 20.6818 with MSE metric 17029.2985\n",
      "Epoch 260 batch 230 train Loss 43.5938 test Loss 20.6795 with MSE metric 17027.8529\n",
      "Epoch 260 batch 240 train Loss 43.5879 test Loss 20.6771 with MSE metric 17026.4627\n",
      "Time taken for 1 epoch: 28.575037956237793 secs\n",
      "\n",
      "Epoch 261 batch 0 train Loss 43.5821 test Loss 20.6748 with MSE metric 17025.0231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261 batch 10 train Loss 43.5762 test Loss 20.6725 with MSE metric 17023.6035\n",
      "Epoch 261 batch 20 train Loss 43.5703 test Loss 20.6701 with MSE metric 17022.2169\n",
      "Epoch 261 batch 30 train Loss 43.5645 test Loss 20.6678 with MSE metric 17020.7652\n",
      "Epoch 261 batch 40 train Loss 43.5586 test Loss 20.6654 with MSE metric 17019.3340\n",
      "Epoch 261 batch 50 train Loss 43.5528 test Loss 20.6631 with MSE metric 17017.8479\n",
      "Epoch 261 batch 60 train Loss 43.5469 test Loss 20.6608 with MSE metric 17016.4269\n",
      "Epoch 261 batch 70 train Loss 43.5411 test Loss 20.6584 with MSE metric 17015.0795\n",
      "Epoch 261 batch 80 train Loss 43.5352 test Loss 20.6561 with MSE metric 17013.6621\n",
      "Epoch 261 batch 90 train Loss 43.5294 test Loss 20.6538 with MSE metric 17012.2225\n",
      "Epoch 261 batch 100 train Loss 43.5235 test Loss 20.6514 with MSE metric 17010.7596\n",
      "Epoch 261 batch 110 train Loss 43.5177 test Loss 20.6491 with MSE metric 17009.3164\n",
      "Epoch 261 batch 120 train Loss 43.5118 test Loss 20.6468 with MSE metric 17007.8464\n",
      "Epoch 261 batch 130 train Loss 43.5060 test Loss 20.6444 with MSE metric 17006.4382\n",
      "Epoch 261 batch 140 train Loss 43.5001 test Loss 20.6421 with MSE metric 17004.9914\n",
      "Epoch 261 batch 150 train Loss 43.4943 test Loss 20.6398 with MSE metric 17003.5523\n",
      "Epoch 261 batch 160 train Loss 43.4885 test Loss 20.6374 with MSE metric 17002.1036\n",
      "Epoch 261 batch 170 train Loss 43.4826 test Loss 20.6351 with MSE metric 17000.7072\n",
      "Epoch 261 batch 180 train Loss 43.4768 test Loss 20.6328 with MSE metric 16999.2998\n",
      "Epoch 261 batch 190 train Loss 43.4710 test Loss 20.6305 with MSE metric 16997.8316\n",
      "Epoch 261 batch 200 train Loss 43.4651 test Loss 20.6281 with MSE metric 16996.3769\n",
      "Epoch 261 batch 210 train Loss 43.4593 test Loss 20.6258 with MSE metric 16994.9816\n",
      "Epoch 261 batch 220 train Loss 43.4535 test Loss 20.6235 with MSE metric 16993.6197\n",
      "Epoch 261 batch 230 train Loss 43.4477 test Loss 20.6212 with MSE metric 16992.1833\n",
      "Epoch 261 batch 240 train Loss 43.4418 test Loss 20.6188 with MSE metric 16990.7543\n",
      "Time taken for 1 epoch: 27.239919185638428 secs\n",
      "\n",
      "Epoch 262 batch 0 train Loss 43.4360 test Loss 20.6165 with MSE metric 16989.3379\n",
      "Epoch 262 batch 10 train Loss 43.4302 test Loss 20.6142 with MSE metric 16987.9316\n",
      "Epoch 262 batch 20 train Loss 43.4244 test Loss 20.6119 with MSE metric 16986.4543\n",
      "Epoch 262 batch 30 train Loss 43.4186 test Loss 20.6095 with MSE metric 16985.0156\n",
      "Epoch 262 batch 40 train Loss 43.4128 test Loss 20.6072 with MSE metric 16983.5958\n",
      "Epoch 262 batch 50 train Loss 43.4069 test Loss 20.6049 with MSE metric 16982.1689\n",
      "Epoch 262 batch 60 train Loss 43.4011 test Loss 20.6026 with MSE metric 16980.7587\n",
      "Epoch 262 batch 70 train Loss 43.3953 test Loss 20.6002 with MSE metric 16979.3186\n",
      "Epoch 262 batch 80 train Loss 43.3895 test Loss 20.5979 with MSE metric 16977.9069\n",
      "Epoch 262 batch 90 train Loss 43.3837 test Loss 20.5956 with MSE metric 16976.4969\n",
      "Epoch 262 batch 100 train Loss 43.3779 test Loss 20.5933 with MSE metric 16975.1805\n",
      "Epoch 262 batch 110 train Loss 43.3721 test Loss 20.5910 with MSE metric 16973.7170\n",
      "Epoch 262 batch 120 train Loss 43.3663 test Loss 20.5887 with MSE metric 16972.3160\n",
      "Epoch 262 batch 130 train Loss 43.3605 test Loss 20.5863 with MSE metric 16970.8771\n",
      "Epoch 262 batch 140 train Loss 43.3547 test Loss 20.5840 with MSE metric 16969.4115\n",
      "Epoch 262 batch 150 train Loss 43.3489 test Loss 20.5817 with MSE metric 16967.9829\n",
      "Epoch 262 batch 160 train Loss 43.3431 test Loss 20.5794 with MSE metric 16966.5251\n",
      "Epoch 262 batch 170 train Loss 43.3373 test Loss 20.5771 with MSE metric 16965.0965\n",
      "Epoch 262 batch 180 train Loss 43.3315 test Loss 20.5748 with MSE metric 16963.6546\n",
      "Epoch 262 batch 190 train Loss 43.3258 test Loss 20.5725 with MSE metric 16962.2839\n",
      "Epoch 262 batch 200 train Loss 43.3200 test Loss 20.5701 with MSE metric 16960.8796\n",
      "Epoch 262 batch 210 train Loss 43.3142 test Loss 20.5678 with MSE metric 16959.4649\n",
      "Epoch 262 batch 220 train Loss 43.3084 test Loss 20.5655 with MSE metric 16958.0975\n",
      "Epoch 262 batch 230 train Loss 43.3026 test Loss 20.5632 with MSE metric 16956.6432\n",
      "Epoch 262 batch 240 train Loss 43.2969 test Loss 20.5609 with MSE metric 16955.1920\n",
      "Time taken for 1 epoch: 27.324551820755005 secs\n",
      "\n",
      "Epoch 263 batch 0 train Loss 43.2911 test Loss 20.5586 with MSE metric 16953.7411\n",
      "Epoch 263 batch 10 train Loss 43.2853 test Loss 20.5563 with MSE metric 16952.3365\n",
      "Epoch 263 batch 20 train Loss 43.2795 test Loss 20.5540 with MSE metric 16950.8687\n",
      "Epoch 263 batch 30 train Loss 43.2738 test Loss 20.5517 with MSE metric 16949.4848\n",
      "Epoch 263 batch 40 train Loss 43.2680 test Loss 20.5494 with MSE metric 16948.0861\n",
      "Epoch 263 batch 50 train Loss 43.2622 test Loss 20.5471 with MSE metric 16946.7299\n",
      "Epoch 263 batch 60 train Loss 43.2565 test Loss 20.5448 with MSE metric 16945.2666\n",
      "Epoch 263 batch 70 train Loss 43.2507 test Loss 20.5425 with MSE metric 16943.8369\n",
      "Epoch 263 batch 80 train Loss 43.2449 test Loss 20.5402 with MSE metric 16942.4051\n",
      "Epoch 263 batch 90 train Loss 43.2392 test Loss 20.5379 with MSE metric 16940.9682\n",
      "Epoch 263 batch 100 train Loss 43.2334 test Loss 20.5356 with MSE metric 16939.5784\n",
      "Epoch 263 batch 110 train Loss 43.2277 test Loss 20.5333 with MSE metric 16938.1274\n",
      "Epoch 263 batch 120 train Loss 43.2219 test Loss 20.5310 with MSE metric 16936.7010\n",
      "Epoch 263 batch 130 train Loss 43.2161 test Loss 20.5287 with MSE metric 16935.2744\n",
      "Epoch 263 batch 140 train Loss 43.2104 test Loss 20.5264 with MSE metric 16933.8805\n",
      "Epoch 263 batch 150 train Loss 43.2046 test Loss 20.5241 with MSE metric 16932.4495\n",
      "Epoch 263 batch 160 train Loss 43.1989 test Loss 20.5218 with MSE metric 16931.0692\n",
      "Epoch 263 batch 170 train Loss 43.1932 test Loss 20.5195 with MSE metric 16929.6656\n",
      "Epoch 263 batch 180 train Loss 43.1874 test Loss 20.5172 with MSE metric 16928.2345\n",
      "Epoch 263 batch 190 train Loss 43.1817 test Loss 20.5149 with MSE metric 16926.7933\n",
      "Epoch 263 batch 200 train Loss 43.1759 test Loss 20.5126 with MSE metric 16925.3829\n",
      "Epoch 263 batch 210 train Loss 43.1702 test Loss 20.5103 with MSE metric 16924.0100\n",
      "Epoch 263 batch 220 train Loss 43.1644 test Loss 20.5080 with MSE metric 16922.5967\n",
      "Epoch 263 batch 230 train Loss 43.1587 test Loss 20.5057 with MSE metric 16921.2089\n",
      "Epoch 263 batch 240 train Loss 43.1530 test Loss 20.5034 with MSE metric 16919.8322\n",
      "Time taken for 1 epoch: 27.838039875030518 secs\n",
      "\n",
      "Epoch 264 batch 0 train Loss 43.1472 test Loss 20.5012 with MSE metric 16918.4548\n",
      "Epoch 264 batch 10 train Loss 43.1415 test Loss 20.4989 with MSE metric 16917.0155\n",
      "Epoch 264 batch 20 train Loss 43.1358 test Loss 20.4966 with MSE metric 16915.5854\n",
      "Epoch 264 batch 30 train Loss 43.1301 test Loss 20.4943 with MSE metric 16914.1508\n",
      "Epoch 264 batch 40 train Loss 43.1243 test Loss 20.4920 with MSE metric 16912.7430\n",
      "Epoch 264 batch 50 train Loss 43.1186 test Loss 20.4897 with MSE metric 16911.3625\n",
      "Epoch 264 batch 60 train Loss 43.1129 test Loss 20.4874 with MSE metric 16909.9448\n",
      "Epoch 264 batch 70 train Loss 43.1072 test Loss 20.4851 with MSE metric 16908.5325\n",
      "Epoch 264 batch 80 train Loss 43.1014 test Loss 20.4828 with MSE metric 16907.0497\n",
      "Epoch 264 batch 90 train Loss 43.0957 test Loss 20.4806 with MSE metric 16905.6277\n",
      "Epoch 264 batch 100 train Loss 43.0900 test Loss 20.4783 with MSE metric 16904.2030\n",
      "Epoch 264 batch 110 train Loss 43.0843 test Loss 20.4760 with MSE metric 16902.7671\n",
      "Epoch 264 batch 120 train Loss 43.0786 test Loss 20.4737 with MSE metric 16901.3930\n",
      "Epoch 264 batch 130 train Loss 43.0729 test Loss 20.4715 with MSE metric 16899.9729\n",
      "Epoch 264 batch 140 train Loss 43.0672 test Loss 20.4692 with MSE metric 16898.5734\n",
      "Epoch 264 batch 150 train Loss 43.0614 test Loss 20.4669 with MSE metric 16897.1603\n",
      "Epoch 264 batch 160 train Loss 43.0557 test Loss 20.4646 with MSE metric 16895.7986\n",
      "Epoch 264 batch 170 train Loss 43.0500 test Loss 20.4623 with MSE metric 16894.4261\n",
      "Epoch 264 batch 180 train Loss 43.0443 test Loss 20.4600 with MSE metric 16893.0247\n",
      "Epoch 264 batch 190 train Loss 43.0386 test Loss 20.4578 with MSE metric 16891.6356\n",
      "Epoch 264 batch 200 train Loss 43.0329 test Loss 20.4555 with MSE metric 16890.2401\n",
      "Epoch 264 batch 210 train Loss 43.0272 test Loss 20.4532 with MSE metric 16888.8320\n",
      "Epoch 264 batch 220 train Loss 43.0215 test Loss 20.4509 with MSE metric 16887.4466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264 batch 230 train Loss 43.0159 test Loss 20.4486 with MSE metric 16886.0424\n",
      "Epoch 264 batch 240 train Loss 43.0102 test Loss 20.4464 with MSE metric 16884.6448\n",
      "Time taken for 1 epoch: 28.601216077804565 secs\n",
      "\n",
      "Epoch 265 batch 0 train Loss 43.0045 test Loss 20.4441 with MSE metric 16883.2220\n",
      "Epoch 265 batch 10 train Loss 42.9988 test Loss 20.4418 with MSE metric 16881.8257\n",
      "Epoch 265 batch 20 train Loss 42.9931 test Loss 20.4396 with MSE metric 16880.4234\n",
      "Epoch 265 batch 30 train Loss 42.9874 test Loss 20.4373 with MSE metric 16879.0103\n",
      "Epoch 265 batch 40 train Loss 42.9817 test Loss 20.4350 with MSE metric 16877.6391\n",
      "Epoch 265 batch 50 train Loss 42.9761 test Loss 20.4327 with MSE metric 16876.2358\n",
      "Epoch 265 batch 60 train Loss 42.9704 test Loss 20.4304 with MSE metric 16874.8300\n",
      "Epoch 265 batch 70 train Loss 42.9647 test Loss 20.4282 with MSE metric 16873.4676\n",
      "Epoch 265 batch 80 train Loss 42.9590 test Loss 20.4259 with MSE metric 16872.0314\n",
      "Epoch 265 batch 90 train Loss 42.9533 test Loss 20.4236 with MSE metric 16870.6324\n",
      "Epoch 265 batch 100 train Loss 42.9477 test Loss 20.4214 with MSE metric 16869.1744\n",
      "Epoch 265 batch 110 train Loss 42.9420 test Loss 20.4191 with MSE metric 16867.7908\n",
      "Epoch 265 batch 120 train Loss 42.9363 test Loss 20.4169 with MSE metric 16866.3910\n",
      "Epoch 265 batch 130 train Loss 42.9307 test Loss 20.4146 with MSE metric 16865.0252\n",
      "Epoch 265 batch 140 train Loss 42.9250 test Loss 20.4123 with MSE metric 16863.6884\n",
      "Epoch 265 batch 150 train Loss 42.9193 test Loss 20.4101 with MSE metric 16862.2613\n",
      "Epoch 265 batch 160 train Loss 42.9137 test Loss 20.4078 with MSE metric 16860.9149\n",
      "Epoch 265 batch 170 train Loss 42.9080 test Loss 20.4055 with MSE metric 16859.5143\n",
      "Epoch 265 batch 180 train Loss 42.9024 test Loss 20.4033 with MSE metric 16858.0939\n",
      "Epoch 265 batch 190 train Loss 42.8967 test Loss 20.4010 with MSE metric 16856.7428\n",
      "Epoch 265 batch 200 train Loss 42.8910 test Loss 20.3988 with MSE metric 16855.3071\n",
      "Epoch 265 batch 210 train Loss 42.8854 test Loss 20.3965 with MSE metric 16853.9383\n",
      "Epoch 265 batch 220 train Loss 42.8797 test Loss 20.3942 with MSE metric 16852.5342\n",
      "Epoch 265 batch 230 train Loss 42.8741 test Loss 20.3920 with MSE metric 16851.0547\n",
      "Epoch 265 batch 240 train Loss 42.8684 test Loss 20.3897 with MSE metric 16849.6500\n",
      "Time taken for 1 epoch: 27.930124044418335 secs\n",
      "\n",
      "Epoch 266 batch 0 train Loss 42.8628 test Loss 20.3875 with MSE metric 16848.2705\n",
      "Epoch 266 batch 10 train Loss 42.8571 test Loss 20.3852 with MSE metric 16846.9249\n",
      "Epoch 266 batch 20 train Loss 42.8515 test Loss 20.3830 with MSE metric 16845.5163\n",
      "Epoch 266 batch 30 train Loss 42.8458 test Loss 20.3807 with MSE metric 16844.1100\n",
      "Epoch 266 batch 40 train Loss 42.8402 test Loss 20.3785 with MSE metric 16842.7379\n",
      "Epoch 266 batch 50 train Loss 42.8346 test Loss 20.3762 with MSE metric 16841.3531\n",
      "Epoch 266 batch 60 train Loss 42.8289 test Loss 20.3740 with MSE metric 16839.9215\n",
      "Epoch 266 batch 70 train Loss 42.8233 test Loss 20.3717 with MSE metric 16838.5015\n",
      "Epoch 266 batch 80 train Loss 42.8177 test Loss 20.3695 with MSE metric 16837.0814\n",
      "Epoch 266 batch 90 train Loss 42.8120 test Loss 20.3672 with MSE metric 16835.7221\n",
      "Epoch 266 batch 100 train Loss 42.8064 test Loss 20.3650 with MSE metric 16834.3205\n",
      "Epoch 266 batch 110 train Loss 42.8008 test Loss 20.3627 with MSE metric 16832.9218\n",
      "Epoch 266 batch 120 train Loss 42.7951 test Loss 20.3605 with MSE metric 16831.4988\n",
      "Epoch 266 batch 130 train Loss 42.7895 test Loss 20.3582 with MSE metric 16830.1235\n",
      "Epoch 266 batch 140 train Loss 42.7839 test Loss 20.3560 with MSE metric 16828.7346\n",
      "Epoch 266 batch 150 train Loss 42.7783 test Loss 20.3537 with MSE metric 16827.3335\n",
      "Epoch 266 batch 160 train Loss 42.7726 test Loss 20.3515 with MSE metric 16825.9634\n",
      "Epoch 266 batch 170 train Loss 42.7670 test Loss 20.3492 with MSE metric 16824.5575\n",
      "Epoch 266 batch 180 train Loss 42.7614 test Loss 20.3470 with MSE metric 16823.2416\n",
      "Epoch 266 batch 190 train Loss 42.7558 test Loss 20.3448 with MSE metric 16821.9062\n",
      "Epoch 266 batch 200 train Loss 42.7502 test Loss 20.3425 with MSE metric 16820.5623\n",
      "Epoch 266 batch 210 train Loss 42.7446 test Loss 20.3403 with MSE metric 16819.1811\n",
      "Epoch 266 batch 220 train Loss 42.7390 test Loss 20.3380 with MSE metric 16817.8132\n",
      "Epoch 266 batch 230 train Loss 42.7334 test Loss 20.3358 with MSE metric 16816.4376\n",
      "Epoch 266 batch 240 train Loss 42.7277 test Loss 20.3335 with MSE metric 16815.0423\n",
      "Time taken for 1 epoch: 30.759555101394653 secs\n",
      "\n",
      "Epoch 267 batch 0 train Loss 42.7221 test Loss 20.3313 with MSE metric 16813.6512\n",
      "Epoch 267 batch 10 train Loss 42.7165 test Loss 20.3291 with MSE metric 16812.1956\n",
      "Epoch 267 batch 20 train Loss 42.7109 test Loss 20.3268 with MSE metric 16810.7992\n",
      "Epoch 267 batch 30 train Loss 42.7053 test Loss 20.3246 with MSE metric 16809.4021\n",
      "Epoch 267 batch 40 train Loss 42.6997 test Loss 20.3224 with MSE metric 16807.9568\n",
      "Epoch 267 batch 50 train Loss 42.6941 test Loss 20.3201 with MSE metric 16806.6238\n",
      "Epoch 267 batch 60 train Loss 42.6885 test Loss 20.3179 with MSE metric 16805.2242\n",
      "Epoch 267 batch 70 train Loss 42.6830 test Loss 20.3157 with MSE metric 16803.8670\n",
      "Epoch 267 batch 80 train Loss 42.6774 test Loss 20.3134 with MSE metric 16802.5013\n",
      "Epoch 267 batch 90 train Loss 42.6718 test Loss 20.3112 with MSE metric 16801.1647\n",
      "Epoch 267 batch 100 train Loss 42.6662 test Loss 20.3090 with MSE metric 16799.7807\n",
      "Epoch 267 batch 110 train Loss 42.6606 test Loss 20.3068 with MSE metric 16798.4344\n",
      "Epoch 267 batch 120 train Loss 42.6550 test Loss 20.3045 with MSE metric 16797.0430\n",
      "Epoch 267 batch 130 train Loss 42.6494 test Loss 20.3023 with MSE metric 16795.6721\n",
      "Epoch 267 batch 140 train Loss 42.6438 test Loss 20.3001 with MSE metric 16794.2729\n",
      "Epoch 267 batch 150 train Loss 42.6383 test Loss 20.2978 with MSE metric 16792.8528\n",
      "Epoch 267 batch 160 train Loss 42.6327 test Loss 20.2956 with MSE metric 16791.4959\n",
      "Epoch 267 batch 170 train Loss 42.6271 test Loss 20.2934 with MSE metric 16790.1629\n",
      "Epoch 267 batch 180 train Loss 42.6215 test Loss 20.2912 with MSE metric 16788.8003\n",
      "Epoch 267 batch 190 train Loss 42.6160 test Loss 20.2889 with MSE metric 16787.3662\n",
      "Epoch 267 batch 200 train Loss 42.6104 test Loss 20.2867 with MSE metric 16785.9933\n",
      "Epoch 267 batch 210 train Loss 42.6048 test Loss 20.2845 with MSE metric 16784.6037\n",
      "Epoch 267 batch 220 train Loss 42.5992 test Loss 20.2822 with MSE metric 16783.2017\n",
      "Epoch 267 batch 230 train Loss 42.5937 test Loss 20.2800 with MSE metric 16781.7538\n",
      "Epoch 267 batch 240 train Loss 42.5881 test Loss 20.2778 with MSE metric 16780.3438\n",
      "Time taken for 1 epoch: 27.002851247787476 secs\n",
      "\n",
      "Epoch 268 batch 0 train Loss 42.5825 test Loss 20.2756 with MSE metric 16778.9827\n",
      "Epoch 268 batch 10 train Loss 42.5770 test Loss 20.2734 with MSE metric 16777.5593\n",
      "Epoch 268 batch 20 train Loss 42.5714 test Loss 20.2712 with MSE metric 16776.2334\n",
      "Epoch 268 batch 30 train Loss 42.5659 test Loss 20.2689 with MSE metric 16774.8698\n",
      "Epoch 268 batch 40 train Loss 42.5603 test Loss 20.2667 with MSE metric 16773.5009\n",
      "Epoch 268 batch 50 train Loss 42.5547 test Loss 20.2645 with MSE metric 16772.0792\n",
      "Epoch 268 batch 60 train Loss 42.5492 test Loss 20.2623 with MSE metric 16770.6710\n",
      "Epoch 268 batch 70 train Loss 42.5436 test Loss 20.2601 with MSE metric 16769.2407\n",
      "Epoch 268 batch 80 train Loss 42.5381 test Loss 20.2578 with MSE metric 16767.8828\n",
      "Epoch 268 batch 90 train Loss 42.5325 test Loss 20.2556 with MSE metric 16766.5041\n",
      "Epoch 268 batch 100 train Loss 42.5270 test Loss 20.2534 with MSE metric 16765.1664\n",
      "Epoch 268 batch 110 train Loss 42.5214 test Loss 20.2512 with MSE metric 16763.7915\n",
      "Epoch 268 batch 120 train Loss 42.5159 test Loss 20.2490 with MSE metric 16762.4460\n",
      "Epoch 268 batch 130 train Loss 42.5104 test Loss 20.2468 with MSE metric 16761.0501\n",
      "Epoch 268 batch 140 train Loss 42.5048 test Loss 20.2445 with MSE metric 16759.6742\n",
      "Epoch 268 batch 150 train Loss 42.4993 test Loss 20.2423 with MSE metric 16758.2608\n",
      "Epoch 268 batch 160 train Loss 42.4937 test Loss 20.2401 with MSE metric 16756.8650\n",
      "Epoch 268 batch 170 train Loss 42.4882 test Loss 20.2379 with MSE metric 16755.4531\n",
      "Epoch 268 batch 180 train Loss 42.4827 test Loss 20.2357 with MSE metric 16754.0872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268 batch 190 train Loss 42.4771 test Loss 20.2335 with MSE metric 16752.7072\n",
      "Epoch 268 batch 200 train Loss 42.4716 test Loss 20.2313 with MSE metric 16751.3131\n",
      "Epoch 268 batch 210 train Loss 42.4661 test Loss 20.2291 with MSE metric 16749.8773\n",
      "Epoch 268 batch 220 train Loss 42.4605 test Loss 20.2269 with MSE metric 16748.5168\n",
      "Epoch 268 batch 230 train Loss 42.4550 test Loss 20.2246 with MSE metric 16747.1549\n",
      "Epoch 268 batch 240 train Loss 42.4495 test Loss 20.2225 with MSE metric 16745.8016\n",
      "Time taken for 1 epoch: 29.01894998550415 secs\n",
      "\n",
      "Epoch 269 batch 0 train Loss 42.4440 test Loss 20.2203 with MSE metric 16744.4439\n",
      "Epoch 269 batch 10 train Loss 42.4385 test Loss 20.2181 with MSE metric 16743.0543\n",
      "Epoch 269 batch 20 train Loss 42.4329 test Loss 20.2159 with MSE metric 16741.6822\n",
      "Epoch 269 batch 30 train Loss 42.4274 test Loss 20.2136 with MSE metric 16740.3692\n",
      "Epoch 269 batch 40 train Loss 42.4219 test Loss 20.2114 with MSE metric 16738.9905\n",
      "Epoch 269 batch 50 train Loss 42.4164 test Loss 20.2092 with MSE metric 16737.6487\n",
      "Epoch 269 batch 60 train Loss 42.4109 test Loss 20.2070 with MSE metric 16736.2563\n",
      "Epoch 269 batch 70 train Loss 42.4054 test Loss 20.2048 with MSE metric 16734.9145\n",
      "Epoch 269 batch 80 train Loss 42.3999 test Loss 20.2026 with MSE metric 16733.5597\n",
      "Epoch 269 batch 90 train Loss 42.3944 test Loss 20.2004 with MSE metric 16732.1964\n",
      "Epoch 269 batch 100 train Loss 42.3888 test Loss 20.1982 with MSE metric 16730.8363\n",
      "Epoch 269 batch 110 train Loss 42.3833 test Loss 20.1960 with MSE metric 16729.4320\n",
      "Epoch 269 batch 120 train Loss 42.3778 test Loss 20.1938 with MSE metric 16728.1296\n",
      "Epoch 269 batch 130 train Loss 42.3723 test Loss 20.1916 with MSE metric 16726.7265\n",
      "Epoch 269 batch 140 train Loss 42.3668 test Loss 20.1894 with MSE metric 16725.3660\n",
      "Epoch 269 batch 150 train Loss 42.3613 test Loss 20.1872 with MSE metric 16724.0246\n",
      "Epoch 269 batch 160 train Loss 42.3558 test Loss 20.1850 with MSE metric 16722.6451\n",
      "Epoch 269 batch 170 train Loss 42.3503 test Loss 20.1828 with MSE metric 16721.2982\n",
      "Epoch 269 batch 180 train Loss 42.3449 test Loss 20.1806 with MSE metric 16719.9139\n",
      "Epoch 269 batch 190 train Loss 42.3394 test Loss 20.1784 with MSE metric 16718.6228\n",
      "Epoch 269 batch 200 train Loss 42.3339 test Loss 20.1763 with MSE metric 16717.2723\n",
      "Epoch 269 batch 210 train Loss 42.3284 test Loss 20.1741 with MSE metric 16715.9478\n",
      "Epoch 269 batch 220 train Loss 42.3229 test Loss 20.1719 with MSE metric 16714.6045\n",
      "Epoch 269 batch 230 train Loss 42.3174 test Loss 20.1697 with MSE metric 16713.2872\n",
      "Epoch 269 batch 240 train Loss 42.3119 test Loss 20.1675 with MSE metric 16711.9302\n",
      "Time taken for 1 epoch: 27.587809085845947 secs\n",
      "\n",
      "Epoch 270 batch 0 train Loss 42.3065 test Loss 20.1653 with MSE metric 16710.5891\n",
      "Epoch 270 batch 10 train Loss 42.3010 test Loss 20.1631 with MSE metric 16709.2412\n",
      "Epoch 270 batch 20 train Loss 42.2955 test Loss 20.1609 with MSE metric 16707.8785\n",
      "Epoch 270 batch 30 train Loss 42.2900 test Loss 20.1587 with MSE metric 16706.5269\n",
      "Epoch 270 batch 40 train Loss 42.2846 test Loss 20.1566 with MSE metric 16705.1440\n",
      "Epoch 270 batch 50 train Loss 42.2791 test Loss 20.1544 with MSE metric 16703.8326\n",
      "Epoch 270 batch 60 train Loss 42.2736 test Loss 20.1522 with MSE metric 16702.5011\n",
      "Epoch 270 batch 70 train Loss 42.2681 test Loss 20.1500 with MSE metric 16701.1293\n",
      "Epoch 270 batch 80 train Loss 42.2627 test Loss 20.1478 with MSE metric 16699.8014\n",
      "Epoch 270 batch 90 train Loss 42.2572 test Loss 20.1456 with MSE metric 16698.4569\n",
      "Epoch 270 batch 100 train Loss 42.2517 test Loss 20.1435 with MSE metric 16697.1336\n",
      "Epoch 270 batch 110 train Loss 42.2463 test Loss 20.1413 with MSE metric 16695.7377\n",
      "Epoch 270 batch 120 train Loss 42.2408 test Loss 20.1391 with MSE metric 16694.3399\n",
      "Epoch 270 batch 130 train Loss 42.2353 test Loss 20.1369 with MSE metric 16692.9656\n",
      "Epoch 270 batch 140 train Loss 42.2299 test Loss 20.1347 with MSE metric 16691.5923\n",
      "Epoch 270 batch 150 train Loss 42.2244 test Loss 20.1325 with MSE metric 16690.2952\n",
      "Epoch 270 batch 160 train Loss 42.2190 test Loss 20.1303 with MSE metric 16688.9744\n",
      "Epoch 270 batch 170 train Loss 42.2135 test Loss 20.1282 with MSE metric 16687.6355\n",
      "Epoch 270 batch 180 train Loss 42.2081 test Loss 20.1260 with MSE metric 16686.2931\n",
      "Epoch 270 batch 190 train Loss 42.2026 test Loss 20.1238 with MSE metric 16684.9095\n",
      "Epoch 270 batch 200 train Loss 42.1972 test Loss 20.1216 with MSE metric 16683.6274\n",
      "Epoch 270 batch 210 train Loss 42.1917 test Loss 20.1195 with MSE metric 16682.2988\n",
      "Epoch 270 batch 220 train Loss 42.1863 test Loss 20.1173 with MSE metric 16680.9804\n",
      "Epoch 270 batch 230 train Loss 42.1808 test Loss 20.1151 with MSE metric 16679.6457\n",
      "Epoch 270 batch 240 train Loss 42.1754 test Loss 20.1129 with MSE metric 16678.2909\n",
      "Time taken for 1 epoch: 27.226439237594604 secs\n",
      "\n",
      "Epoch 271 batch 0 train Loss 42.1700 test Loss 20.1107 with MSE metric 16676.9760\n",
      "Epoch 271 batch 10 train Loss 42.1645 test Loss 20.1085 with MSE metric 16675.5941\n",
      "Epoch 271 batch 20 train Loss 42.1591 test Loss 20.1064 with MSE metric 16674.2753\n",
      "Epoch 271 batch 30 train Loss 42.1536 test Loss 20.1042 with MSE metric 16672.9263\n",
      "Epoch 271 batch 40 train Loss 42.1482 test Loss 20.1020 with MSE metric 16671.5940\n",
      "Epoch 271 batch 50 train Loss 42.1428 test Loss 20.0999 with MSE metric 16670.2594\n",
      "Epoch 271 batch 60 train Loss 42.1373 test Loss 20.0977 with MSE metric 16668.8598\n",
      "Epoch 271 batch 70 train Loss 42.1319 test Loss 20.0955 with MSE metric 16667.5632\n",
      "Epoch 271 batch 80 train Loss 42.1265 test Loss 20.0933 with MSE metric 16666.1794\n",
      "Epoch 271 batch 90 train Loss 42.1210 test Loss 20.0912 with MSE metric 16664.8376\n",
      "Epoch 271 batch 100 train Loss 42.1156 test Loss 20.0890 with MSE metric 16663.5066\n",
      "Epoch 271 batch 110 train Loss 42.1102 test Loss 20.0869 with MSE metric 16662.1674\n",
      "Epoch 271 batch 120 train Loss 42.1048 test Loss 20.0847 with MSE metric 16660.8201\n",
      "Epoch 271 batch 130 train Loss 42.0994 test Loss 20.0825 with MSE metric 16659.4319\n",
      "Epoch 271 batch 140 train Loss 42.0939 test Loss 20.0803 with MSE metric 16658.1068\n",
      "Epoch 271 batch 150 train Loss 42.0885 test Loss 20.0782 with MSE metric 16656.7477\n",
      "Epoch 271 batch 160 train Loss 42.0831 test Loss 20.0760 with MSE metric 16655.4129\n",
      "Epoch 271 batch 170 train Loss 42.0777 test Loss 20.0739 with MSE metric 16654.0873\n",
      "Epoch 271 batch 180 train Loss 42.0723 test Loss 20.0717 with MSE metric 16652.7608\n",
      "Epoch 271 batch 190 train Loss 42.0669 test Loss 20.0695 with MSE metric 16651.4250\n",
      "Epoch 271 batch 200 train Loss 42.0615 test Loss 20.0674 with MSE metric 16650.0895\n",
      "Epoch 271 batch 210 train Loss 42.0561 test Loss 20.0652 with MSE metric 16648.8206\n",
      "Epoch 271 batch 220 train Loss 42.0507 test Loss 20.0631 with MSE metric 16647.4923\n",
      "Epoch 271 batch 230 train Loss 42.0452 test Loss 20.0609 with MSE metric 16646.1540\n",
      "Epoch 271 batch 240 train Loss 42.0398 test Loss 20.0588 with MSE metric 16644.8254\n",
      "Time taken for 1 epoch: 26.894988775253296 secs\n",
      "\n",
      "Epoch 272 batch 0 train Loss 42.0344 test Loss 20.0566 with MSE metric 16643.5108\n",
      "Epoch 272 batch 10 train Loss 42.0290 test Loss 20.0544 with MSE metric 16642.2486\n",
      "Epoch 272 batch 20 train Loss 42.0236 test Loss 20.0523 with MSE metric 16640.8955\n",
      "Epoch 272 batch 30 train Loss 42.0183 test Loss 20.0501 with MSE metric 16639.5538\n",
      "Epoch 272 batch 40 train Loss 42.0129 test Loss 20.0480 with MSE metric 16638.2440\n",
      "Epoch 272 batch 50 train Loss 42.0075 test Loss 20.0458 with MSE metric 16636.8648\n",
      "Epoch 272 batch 60 train Loss 42.0021 test Loss 20.0437 with MSE metric 16635.5091\n",
      "Epoch 272 batch 70 train Loss 41.9967 test Loss 20.0415 with MSE metric 16634.2294\n",
      "Epoch 272 batch 80 train Loss 41.9913 test Loss 20.0394 with MSE metric 16632.8994\n",
      "Epoch 272 batch 90 train Loss 41.9859 test Loss 20.0372 with MSE metric 16631.5708\n",
      "Epoch 272 batch 100 train Loss 41.9805 test Loss 20.0351 with MSE metric 16630.2309\n",
      "Epoch 272 batch 110 train Loss 41.9751 test Loss 20.0329 with MSE metric 16628.9128\n",
      "Epoch 272 batch 120 train Loss 41.9698 test Loss 20.0308 with MSE metric 16627.5560\n",
      "Epoch 272 batch 130 train Loss 41.9644 test Loss 20.0286 with MSE metric 16626.2037\n",
      "Epoch 272 batch 140 train Loss 41.9590 test Loss 20.0264 with MSE metric 16624.9020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272 batch 150 train Loss 41.9536 test Loss 20.0243 with MSE metric 16623.5856\n",
      "Epoch 272 batch 160 train Loss 41.9482 test Loss 20.0222 with MSE metric 16622.2305\n",
      "Epoch 272 batch 170 train Loss 41.9429 test Loss 20.0200 with MSE metric 16620.8774\n",
      "Epoch 272 batch 180 train Loss 41.9375 test Loss 20.0179 with MSE metric 16619.5548\n",
      "Epoch 272 batch 190 train Loss 41.9321 test Loss 20.0157 with MSE metric 16618.1974\n",
      "Epoch 272 batch 200 train Loss 41.9268 test Loss 20.0136 with MSE metric 16616.9101\n",
      "Epoch 272 batch 210 train Loss 41.9214 test Loss 20.0114 with MSE metric 16615.6066\n",
      "Epoch 272 batch 220 train Loss 41.9160 test Loss 20.0093 with MSE metric 16614.2735\n",
      "Epoch 272 batch 230 train Loss 41.9107 test Loss 20.0071 with MSE metric 16612.9334\n",
      "Epoch 272 batch 240 train Loss 41.9053 test Loss 20.0050 with MSE metric 16611.6136\n",
      "Time taken for 1 epoch: 26.58998680114746 secs\n",
      "\n",
      "Epoch 273 batch 0 train Loss 41.8999 test Loss 20.0028 with MSE metric 16610.2818\n",
      "Epoch 273 batch 10 train Loss 41.8946 test Loss 20.0007 with MSE metric 16608.9964\n",
      "Epoch 273 batch 20 train Loss 41.8892 test Loss 19.9985 with MSE metric 16607.6823\n",
      "Epoch 273 batch 30 train Loss 41.8839 test Loss 19.9964 with MSE metric 16606.3752\n",
      "Epoch 273 batch 40 train Loss 41.8785 test Loss 19.9943 with MSE metric 16605.0067\n",
      "Epoch 273 batch 50 train Loss 41.8732 test Loss 19.9921 with MSE metric 16603.7121\n",
      "Epoch 273 batch 60 train Loss 41.8678 test Loss 19.9900 with MSE metric 16602.3760\n",
      "Epoch 273 batch 70 train Loss 41.8624 test Loss 19.9878 with MSE metric 16601.1069\n",
      "Epoch 273 batch 80 train Loss 41.8571 test Loss 19.9857 with MSE metric 16599.8518\n",
      "Epoch 273 batch 90 train Loss 41.8518 test Loss 19.9835 with MSE metric 16598.5436\n",
      "Epoch 273 batch 100 train Loss 41.8464 test Loss 19.9814 with MSE metric 16597.2769\n",
      "Epoch 273 batch 110 train Loss 41.8411 test Loss 19.9792 with MSE metric 16595.9664\n",
      "Epoch 273 batch 120 train Loss 41.8357 test Loss 19.9771 with MSE metric 16594.6528\n",
      "Epoch 273 batch 130 train Loss 41.8304 test Loss 19.9750 with MSE metric 16593.3136\n",
      "Epoch 273 batch 140 train Loss 41.8250 test Loss 19.9728 with MSE metric 16591.9576\n",
      "Epoch 273 batch 150 train Loss 41.8197 test Loss 19.9707 with MSE metric 16590.6464\n",
      "Epoch 273 batch 160 train Loss 41.8144 test Loss 19.9686 with MSE metric 16589.3057\n",
      "Epoch 273 batch 170 train Loss 41.8090 test Loss 19.9664 with MSE metric 16587.9952\n",
      "Epoch 273 batch 180 train Loss 41.8037 test Loss 19.9643 with MSE metric 16586.6721\n",
      "Epoch 273 batch 190 train Loss 41.7984 test Loss 19.9622 with MSE metric 16585.3726\n",
      "Epoch 273 batch 200 train Loss 41.7930 test Loss 19.9601 with MSE metric 16584.1251\n",
      "Epoch 273 batch 210 train Loss 41.7877 test Loss 19.9579 with MSE metric 16582.7979\n",
      "Epoch 273 batch 220 train Loss 41.7824 test Loss 19.9558 with MSE metric 16581.4615\n",
      "Epoch 273 batch 230 train Loss 41.7771 test Loss 19.9537 with MSE metric 16580.1213\n",
      "Epoch 273 batch 240 train Loss 41.7717 test Loss 19.9515 with MSE metric 16578.8257\n",
      "Time taken for 1 epoch: 28.314034938812256 secs\n",
      "\n",
      "Epoch 274 batch 0 train Loss 41.7664 test Loss 19.9494 with MSE metric 16577.5476\n",
      "Epoch 274 batch 10 train Loss 41.7611 test Loss 19.9473 with MSE metric 16576.2446\n",
      "Epoch 274 batch 20 train Loss 41.7558 test Loss 19.9452 with MSE metric 16574.9162\n",
      "Epoch 274 batch 30 train Loss 41.7505 test Loss 19.9430 with MSE metric 16573.6320\n",
      "Epoch 274 batch 40 train Loss 41.7451 test Loss 19.9409 with MSE metric 16572.2982\n",
      "Epoch 274 batch 50 train Loss 41.7398 test Loss 19.9388 with MSE metric 16570.9851\n",
      "Epoch 274 batch 60 train Loss 41.7345 test Loss 19.9367 with MSE metric 16569.6870\n",
      "Epoch 274 batch 70 train Loss 41.7292 test Loss 19.9345 with MSE metric 16568.3538\n",
      "Epoch 274 batch 80 train Loss 41.7239 test Loss 19.9324 with MSE metric 16567.0252\n",
      "Epoch 274 batch 90 train Loss 41.7186 test Loss 19.9303 with MSE metric 16565.7400\n",
      "Epoch 274 batch 100 train Loss 41.7133 test Loss 19.9282 with MSE metric 16564.4427\n",
      "Epoch 274 batch 110 train Loss 41.7080 test Loss 19.9261 with MSE metric 16563.1565\n",
      "Epoch 274 batch 120 train Loss 41.7027 test Loss 19.9239 with MSE metric 16561.8340\n",
      "Epoch 274 batch 130 train Loss 41.6974 test Loss 19.9218 with MSE metric 16560.5614\n",
      "Epoch 274 batch 140 train Loss 41.6921 test Loss 19.9197 with MSE metric 16559.2907\n",
      "Epoch 274 batch 150 train Loss 41.6868 test Loss 19.9176 with MSE metric 16557.9836\n",
      "Epoch 274 batch 160 train Loss 41.6815 test Loss 19.9155 with MSE metric 16556.6885\n",
      "Epoch 274 batch 170 train Loss 41.6762 test Loss 19.9134 with MSE metric 16555.3438\n",
      "Epoch 274 batch 180 train Loss 41.6709 test Loss 19.9112 with MSE metric 16554.0232\n",
      "Epoch 274 batch 190 train Loss 41.6656 test Loss 19.9091 with MSE metric 16552.7148\n",
      "Epoch 274 batch 200 train Loss 41.6603 test Loss 19.9070 with MSE metric 16551.3817\n",
      "Epoch 274 batch 210 train Loss 41.6550 test Loss 19.9049 with MSE metric 16550.0326\n",
      "Epoch 274 batch 220 train Loss 41.6497 test Loss 19.9028 with MSE metric 16548.7282\n",
      "Epoch 274 batch 230 train Loss 41.6444 test Loss 19.9007 with MSE metric 16547.4161\n",
      "Epoch 274 batch 240 train Loss 41.6391 test Loss 19.8986 with MSE metric 16546.1409\n",
      "Time taken for 1 epoch: 26.475327968597412 secs\n",
      "\n",
      "Epoch 275 batch 0 train Loss 41.6339 test Loss 19.8964 with MSE metric 16544.8236\n",
      "Epoch 275 batch 10 train Loss 41.6286 test Loss 19.8943 with MSE metric 16543.5187\n",
      "Epoch 275 batch 20 train Loss 41.6233 test Loss 19.8922 with MSE metric 16542.1663\n",
      "Epoch 275 batch 30 train Loss 41.6180 test Loss 19.8901 with MSE metric 16540.8821\n",
      "Epoch 275 batch 40 train Loss 41.6127 test Loss 19.8880 with MSE metric 16539.5968\n",
      "Epoch 275 batch 50 train Loss 41.6075 test Loss 19.8859 with MSE metric 16538.2719\n",
      "Epoch 275 batch 60 train Loss 41.6022 test Loss 19.8838 with MSE metric 16536.9886\n",
      "Epoch 275 batch 70 train Loss 41.5969 test Loss 19.8817 with MSE metric 16535.6705\n",
      "Epoch 275 batch 80 train Loss 41.5916 test Loss 19.8796 with MSE metric 16534.3583\n",
      "Epoch 275 batch 90 train Loss 41.5864 test Loss 19.8775 with MSE metric 16533.0319\n",
      "Epoch 275 batch 100 train Loss 41.5811 test Loss 19.8754 with MSE metric 16531.6971\n",
      "Epoch 275 batch 110 train Loss 41.5758 test Loss 19.8733 with MSE metric 16530.3923\n",
      "Epoch 275 batch 120 train Loss 41.5706 test Loss 19.8712 with MSE metric 16529.0883\n",
      "Epoch 275 batch 130 train Loss 41.5653 test Loss 19.8691 with MSE metric 16527.7828\n",
      "Epoch 275 batch 140 train Loss 41.5600 test Loss 19.8670 with MSE metric 16526.5046\n",
      "Epoch 275 batch 150 train Loss 41.5548 test Loss 19.8649 with MSE metric 16525.2230\n",
      "Epoch 275 batch 160 train Loss 41.5495 test Loss 19.8628 with MSE metric 16523.9355\n",
      "Epoch 275 batch 170 train Loss 41.5443 test Loss 19.8607 with MSE metric 16522.7192\n",
      "Epoch 275 batch 180 train Loss 41.5390 test Loss 19.8586 with MSE metric 16521.4007\n",
      "Epoch 275 batch 190 train Loss 41.5338 test Loss 19.8565 with MSE metric 16520.0481\n",
      "Epoch 275 batch 200 train Loss 41.5285 test Loss 19.8544 with MSE metric 16518.7474\n",
      "Epoch 275 batch 210 train Loss 41.5233 test Loss 19.8523 with MSE metric 16517.4387\n",
      "Epoch 275 batch 220 train Loss 41.5180 test Loss 19.8502 with MSE metric 16516.1168\n",
      "Epoch 275 batch 230 train Loss 41.5128 test Loss 19.8481 with MSE metric 16514.8423\n",
      "Epoch 275 batch 240 train Loss 41.5075 test Loss 19.8460 with MSE metric 16513.4973\n",
      "Time taken for 1 epoch: 27.826008796691895 secs\n",
      "\n",
      "Epoch 276 batch 0 train Loss 41.5023 test Loss 19.8439 with MSE metric 16512.2109\n",
      "Epoch 276 batch 10 train Loss 41.4970 test Loss 19.8418 with MSE metric 16510.8874\n",
      "Epoch 276 batch 20 train Loss 41.4918 test Loss 19.8397 with MSE metric 16509.6233\n",
      "Epoch 276 batch 30 train Loss 41.4865 test Loss 19.8376 with MSE metric 16508.3776\n",
      "Epoch 276 batch 40 train Loss 41.4813 test Loss 19.8356 with MSE metric 16507.0377\n",
      "Epoch 276 batch 50 train Loss 41.4761 test Loss 19.8335 with MSE metric 16505.7650\n",
      "Epoch 276 batch 60 train Loss 41.4708 test Loss 19.8314 with MSE metric 16504.5208\n",
      "Epoch 276 batch 70 train Loss 41.4656 test Loss 19.8293 with MSE metric 16503.2156\n",
      "Epoch 276 batch 80 train Loss 41.4604 test Loss 19.8272 with MSE metric 16501.8970\n",
      "Epoch 276 batch 90 train Loss 41.4551 test Loss 19.8251 with MSE metric 16500.6378\n",
      "Epoch 276 batch 100 train Loss 41.4499 test Loss 19.8230 with MSE metric 16499.3441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276 batch 110 train Loss 41.4447 test Loss 19.8209 with MSE metric 16498.0104\n",
      "Epoch 276 batch 120 train Loss 41.4394 test Loss 19.8188 with MSE metric 16496.7774\n",
      "Epoch 276 batch 130 train Loss 41.4342 test Loss 19.8168 with MSE metric 16495.4698\n",
      "Epoch 276 batch 140 train Loss 41.4290 test Loss 19.8147 with MSE metric 16494.2120\n",
      "Epoch 276 batch 150 train Loss 41.4238 test Loss 19.8126 with MSE metric 16492.8902\n",
      "Epoch 276 batch 160 train Loss 41.4185 test Loss 19.8105 with MSE metric 16491.6132\n",
      "Epoch 276 batch 170 train Loss 41.4133 test Loss 19.8084 with MSE metric 16490.3268\n",
      "Epoch 276 batch 180 train Loss 41.4081 test Loss 19.8063 with MSE metric 16489.0434\n",
      "Epoch 276 batch 190 train Loss 41.4029 test Loss 19.8043 with MSE metric 16487.7589\n",
      "Epoch 276 batch 200 train Loss 41.3977 test Loss 19.8022 with MSE metric 16486.4500\n",
      "Epoch 276 batch 210 train Loss 41.3925 test Loss 19.8001 with MSE metric 16485.1586\n",
      "Epoch 276 batch 220 train Loss 41.3872 test Loss 19.7980 with MSE metric 16483.8666\n",
      "Epoch 276 batch 230 train Loss 41.3820 test Loss 19.7959 with MSE metric 16482.6053\n",
      "Epoch 276 batch 240 train Loss 41.3768 test Loss 19.7938 with MSE metric 16481.3393\n",
      "Time taken for 1 epoch: 27.31356692314148 secs\n",
      "\n",
      "Epoch 277 batch 0 train Loss 41.3716 test Loss 19.7918 with MSE metric 16480.0573\n",
      "Epoch 277 batch 10 train Loss 41.3664 test Loss 19.7897 with MSE metric 16478.7724\n",
      "Epoch 277 batch 20 train Loss 41.3612 test Loss 19.7876 with MSE metric 16477.4438\n",
      "Epoch 277 batch 30 train Loss 41.3560 test Loss 19.7855 with MSE metric 16476.1828\n",
      "Epoch 277 batch 40 train Loss 41.3508 test Loss 19.7834 with MSE metric 16474.8894\n",
      "Epoch 277 batch 50 train Loss 41.3456 test Loss 19.7814 with MSE metric 16473.5941\n",
      "Epoch 277 batch 60 train Loss 41.3404 test Loss 19.7793 with MSE metric 16472.2473\n",
      "Epoch 277 batch 70 train Loss 41.3352 test Loss 19.7772 with MSE metric 16470.9665\n",
      "Epoch 277 batch 80 train Loss 41.3300 test Loss 19.7751 with MSE metric 16469.6968\n",
      "Epoch 277 batch 90 train Loss 41.3248 test Loss 19.7731 with MSE metric 16468.4401\n",
      "Epoch 277 batch 100 train Loss 41.3196 test Loss 19.7710 with MSE metric 16467.1771\n",
      "Epoch 277 batch 110 train Loss 41.3144 test Loss 19.7689 with MSE metric 16465.8928\n",
      "Epoch 277 batch 120 train Loss 41.3092 test Loss 19.7668 with MSE metric 16464.6211\n",
      "Epoch 277 batch 130 train Loss 41.3041 test Loss 19.7648 with MSE metric 16463.3914\n",
      "Epoch 277 batch 140 train Loss 41.2989 test Loss 19.7627 with MSE metric 16462.1122\n",
      "Epoch 277 batch 150 train Loss 41.2937 test Loss 19.7606 with MSE metric 16460.7928\n",
      "Epoch 277 batch 160 train Loss 41.2885 test Loss 19.7585 with MSE metric 16459.5141\n",
      "Epoch 277 batch 170 train Loss 41.2833 test Loss 19.7565 with MSE metric 16458.2492\n",
      "Epoch 277 batch 180 train Loss 41.2781 test Loss 19.7544 with MSE metric 16456.9633\n",
      "Epoch 277 batch 190 train Loss 41.2729 test Loss 19.7523 with MSE metric 16455.6627\n",
      "Epoch 277 batch 200 train Loss 41.2678 test Loss 19.7503 with MSE metric 16454.3948\n",
      "Epoch 277 batch 210 train Loss 41.2626 test Loss 19.7482 with MSE metric 16453.1398\n",
      "Epoch 277 batch 220 train Loss 41.2574 test Loss 19.7461 with MSE metric 16451.8923\n",
      "Epoch 277 batch 230 train Loss 41.2523 test Loss 19.7440 with MSE metric 16450.6349\n",
      "Epoch 277 batch 240 train Loss 41.2471 test Loss 19.7420 with MSE metric 16449.3515\n",
      "Time taken for 1 epoch: 30.730117797851562 secs\n",
      "\n",
      "Epoch 278 batch 0 train Loss 41.2419 test Loss 19.7399 with MSE metric 16448.0862\n",
      "Epoch 278 batch 10 train Loss 41.2367 test Loss 19.7378 with MSE metric 16446.8423\n",
      "Epoch 278 batch 20 train Loss 41.2316 test Loss 19.7358 with MSE metric 16445.5947\n",
      "Epoch 278 batch 30 train Loss 41.2264 test Loss 19.7337 with MSE metric 16444.3639\n",
      "Epoch 278 batch 40 train Loss 41.2213 test Loss 19.7317 with MSE metric 16443.1009\n",
      "Epoch 278 batch 50 train Loss 41.2161 test Loss 19.7296 with MSE metric 16441.7938\n",
      "Epoch 278 batch 60 train Loss 41.2109 test Loss 19.7275 with MSE metric 16440.5331\n",
      "Epoch 278 batch 70 train Loss 41.2058 test Loss 19.7255 with MSE metric 16439.2656\n",
      "Epoch 278 batch 80 train Loss 41.2006 test Loss 19.7234 with MSE metric 16437.9674\n",
      "Epoch 278 batch 90 train Loss 41.1955 test Loss 19.7213 with MSE metric 16436.7716\n",
      "Epoch 278 batch 100 train Loss 41.1903 test Loss 19.7193 with MSE metric 16435.4946\n",
      "Epoch 278 batch 110 train Loss 41.1852 test Loss 19.7172 with MSE metric 16434.2644\n",
      "Epoch 278 batch 120 train Loss 41.1800 test Loss 19.7152 with MSE metric 16432.9793\n",
      "Epoch 278 batch 130 train Loss 41.1748 test Loss 19.7131 with MSE metric 16431.6939\n",
      "Epoch 278 batch 140 train Loss 41.1697 test Loss 19.7110 with MSE metric 16430.4565\n",
      "Epoch 278 batch 150 train Loss 41.1646 test Loss 19.7090 with MSE metric 16429.1949\n",
      "Epoch 278 batch 160 train Loss 41.1594 test Loss 19.7069 with MSE metric 16427.9061\n",
      "Epoch 278 batch 170 train Loss 41.1543 test Loss 19.7049 with MSE metric 16426.6456\n",
      "Epoch 278 batch 180 train Loss 41.1491 test Loss 19.7028 with MSE metric 16425.3074\n",
      "Epoch 278 batch 190 train Loss 41.1440 test Loss 19.7008 with MSE metric 16424.0401\n",
      "Epoch 278 batch 200 train Loss 41.1388 test Loss 19.6987 with MSE metric 16422.7528\n",
      "Epoch 278 batch 210 train Loss 41.1337 test Loss 19.6967 with MSE metric 16421.4832\n",
      "Epoch 278 batch 220 train Loss 41.1285 test Loss 19.6946 with MSE metric 16420.1718\n",
      "Epoch 278 batch 230 train Loss 41.1234 test Loss 19.6925 with MSE metric 16418.8480\n",
      "Epoch 278 batch 240 train Loss 41.1183 test Loss 19.6905 with MSE metric 16417.5855\n",
      "Time taken for 1 epoch: 28.21616005897522 secs\n",
      "\n",
      "Epoch 279 batch 0 train Loss 41.1131 test Loss 19.6884 with MSE metric 16416.3316\n",
      "Epoch 279 batch 10 train Loss 41.1080 test Loss 19.6864 with MSE metric 16415.0477\n",
      "Epoch 279 batch 20 train Loss 41.1029 test Loss 19.6843 with MSE metric 16413.8536\n",
      "Epoch 279 batch 30 train Loss 41.0977 test Loss 19.6823 with MSE metric 16412.5906\n",
      "Epoch 279 batch 40 train Loss 41.0926 test Loss 19.6802 with MSE metric 16411.3300\n",
      "Epoch 279 batch 50 train Loss 41.0875 test Loss 19.6782 with MSE metric 16410.0681\n",
      "Epoch 279 batch 60 train Loss 41.0824 test Loss 19.6761 with MSE metric 16408.8322\n",
      "Epoch 279 batch 70 train Loss 41.0772 test Loss 19.6741 with MSE metric 16407.5355\n",
      "Epoch 279 batch 80 train Loss 41.0721 test Loss 19.6721 with MSE metric 16406.2658\n",
      "Epoch 279 batch 90 train Loss 41.0670 test Loss 19.6700 with MSE metric 16405.0152\n",
      "Epoch 279 batch 100 train Loss 41.0619 test Loss 19.6680 with MSE metric 16403.7913\n",
      "Epoch 279 batch 110 train Loss 41.0568 test Loss 19.6659 with MSE metric 16402.5408\n",
      "Epoch 279 batch 120 train Loss 41.0517 test Loss 19.6639 with MSE metric 16401.2657\n",
      "Epoch 279 batch 130 train Loss 41.0465 test Loss 19.6618 with MSE metric 16399.9929\n",
      "Epoch 279 batch 140 train Loss 41.0414 test Loss 19.6598 with MSE metric 16398.7286\n",
      "Epoch 279 batch 150 train Loss 41.0363 test Loss 19.6577 with MSE metric 16397.4452\n",
      "Epoch 279 batch 160 train Loss 41.0312 test Loss 19.6557 with MSE metric 16396.1246\n",
      "Epoch 279 batch 170 train Loss 41.0261 test Loss 19.6537 with MSE metric 16394.8633\n",
      "Epoch 279 batch 180 train Loss 41.0210 test Loss 19.6516 with MSE metric 16393.5610\n",
      "Epoch 279 batch 190 train Loss 41.0159 test Loss 19.6496 with MSE metric 16392.3366\n",
      "Epoch 279 batch 200 train Loss 41.0108 test Loss 19.6476 with MSE metric 16391.1109\n",
      "Epoch 279 batch 210 train Loss 41.0057 test Loss 19.6455 with MSE metric 16389.9137\n",
      "Epoch 279 batch 220 train Loss 41.0006 test Loss 19.6435 with MSE metric 16388.6910\n",
      "Epoch 279 batch 230 train Loss 40.9955 test Loss 19.6414 with MSE metric 16387.4891\n",
      "Epoch 279 batch 240 train Loss 40.9904 test Loss 19.6394 with MSE metric 16386.2311\n",
      "Time taken for 1 epoch: 27.047151803970337 secs\n",
      "\n",
      "Epoch 280 batch 0 train Loss 40.9853 test Loss 19.6374 with MSE metric 16384.9758\n",
      "Epoch 280 batch 10 train Loss 40.9802 test Loss 19.6354 with MSE metric 16383.6814\n",
      "Epoch 280 batch 20 train Loss 40.9751 test Loss 19.6333 with MSE metric 16382.4282\n",
      "Epoch 280 batch 30 train Loss 40.9700 test Loss 19.6313 with MSE metric 16381.1666\n",
      "Epoch 280 batch 40 train Loss 40.9649 test Loss 19.6292 with MSE metric 16379.9106\n",
      "Epoch 280 batch 50 train Loss 40.9598 test Loss 19.6272 with MSE metric 16378.6444\n",
      "Epoch 280 batch 60 train Loss 40.9547 test Loss 19.6252 with MSE metric 16377.3759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280 batch 70 train Loss 40.9497 test Loss 19.6232 with MSE metric 16376.1621\n",
      "Epoch 280 batch 80 train Loss 40.9446 test Loss 19.6211 with MSE metric 16374.9732\n",
      "Epoch 280 batch 90 train Loss 40.9395 test Loss 19.6191 with MSE metric 16373.6278\n",
      "Epoch 280 batch 100 train Loss 40.9344 test Loss 19.6171 with MSE metric 16372.3762\n",
      "Epoch 280 batch 110 train Loss 40.9293 test Loss 19.6150 with MSE metric 16371.1120\n",
      "Epoch 280 batch 120 train Loss 40.9242 test Loss 19.6130 with MSE metric 16369.8732\n",
      "Epoch 280 batch 130 train Loss 40.9192 test Loss 19.6110 with MSE metric 16368.6680\n",
      "Epoch 280 batch 140 train Loss 40.9141 test Loss 19.6090 with MSE metric 16367.4091\n",
      "Epoch 280 batch 150 train Loss 40.9090 test Loss 19.6069 with MSE metric 16366.1316\n",
      "Epoch 280 batch 160 train Loss 40.9039 test Loss 19.6049 with MSE metric 16364.8669\n",
      "Epoch 280 batch 170 train Loss 40.8989 test Loss 19.6029 with MSE metric 16363.5995\n",
      "Epoch 280 batch 180 train Loss 40.8938 test Loss 19.6008 with MSE metric 16362.3207\n",
      "Epoch 280 batch 190 train Loss 40.8887 test Loss 19.5988 with MSE metric 16360.9832\n",
      "Epoch 280 batch 200 train Loss 40.8836 test Loss 19.5968 with MSE metric 16359.7444\n",
      "Epoch 280 batch 210 train Loss 40.8786 test Loss 19.5948 with MSE metric 16358.4607\n",
      "Epoch 280 batch 220 train Loss 40.8735 test Loss 19.5927 with MSE metric 16357.1909\n",
      "Epoch 280 batch 230 train Loss 40.8684 test Loss 19.5907 with MSE metric 16355.9240\n",
      "Epoch 280 batch 240 train Loss 40.8634 test Loss 19.5887 with MSE metric 16354.7058\n",
      "Time taken for 1 epoch: 27.44135594367981 secs\n",
      "\n",
      "Epoch 281 batch 0 train Loss 40.8583 test Loss 19.5866 with MSE metric 16353.4294\n",
      "Epoch 281 batch 10 train Loss 40.8533 test Loss 19.5846 with MSE metric 16352.2542\n",
      "Epoch 281 batch 20 train Loss 40.8482 test Loss 19.5826 with MSE metric 16350.9922\n",
      "Epoch 281 batch 30 train Loss 40.8432 test Loss 19.5806 with MSE metric 16349.7357\n",
      "Epoch 281 batch 40 train Loss 40.8381 test Loss 19.5785 with MSE metric 16348.5092\n",
      "Epoch 281 batch 50 train Loss 40.8330 test Loss 19.5765 with MSE metric 16347.2369\n",
      "Epoch 281 batch 60 train Loss 40.8280 test Loss 19.5745 with MSE metric 16345.9992\n",
      "Epoch 281 batch 70 train Loss 40.8229 test Loss 19.5725 with MSE metric 16344.6655\n",
      "Epoch 281 batch 80 train Loss 40.8179 test Loss 19.5705 with MSE metric 16343.4414\n",
      "Epoch 281 batch 90 train Loss 40.8128 test Loss 19.5685 with MSE metric 16342.2183\n",
      "Epoch 281 batch 100 train Loss 40.8078 test Loss 19.5665 with MSE metric 16340.9499\n",
      "Epoch 281 batch 110 train Loss 40.8028 test Loss 19.5644 with MSE metric 16339.7158\n",
      "Epoch 281 batch 120 train Loss 40.7977 test Loss 19.5624 with MSE metric 16338.4465\n",
      "Epoch 281 batch 130 train Loss 40.7927 test Loss 19.5604 with MSE metric 16337.2054\n",
      "Epoch 281 batch 140 train Loss 40.7876 test Loss 19.5584 with MSE metric 16335.9464\n",
      "Epoch 281 batch 150 train Loss 40.7826 test Loss 19.5564 with MSE metric 16334.7597\n",
      "Epoch 281 batch 160 train Loss 40.7776 test Loss 19.5544 with MSE metric 16333.5225\n",
      "Epoch 281 batch 170 train Loss 40.7725 test Loss 19.5524 with MSE metric 16332.3234\n",
      "Epoch 281 batch 180 train Loss 40.7675 test Loss 19.5503 with MSE metric 16331.0617\n",
      "Epoch 281 batch 190 train Loss 40.7625 test Loss 19.5483 with MSE metric 16329.8411\n",
      "Epoch 281 batch 200 train Loss 40.7574 test Loss 19.5463 with MSE metric 16328.5849\n",
      "Epoch 281 batch 210 train Loss 40.7524 test Loss 19.5443 with MSE metric 16327.3559\n",
      "Epoch 281 batch 220 train Loss 40.7474 test Loss 19.5423 with MSE metric 16326.0760\n",
      "Epoch 281 batch 230 train Loss 40.7423 test Loss 19.5403 with MSE metric 16324.8653\n",
      "Epoch 281 batch 240 train Loss 40.7373 test Loss 19.5383 with MSE metric 16323.6060\n",
      "Time taken for 1 epoch: 27.458128929138184 secs\n",
      "\n",
      "Epoch 282 batch 0 train Loss 40.7323 test Loss 19.5363 with MSE metric 16322.3643\n",
      "Epoch 282 batch 10 train Loss 40.7273 test Loss 19.5343 with MSE metric 16321.1509\n",
      "Epoch 282 batch 20 train Loss 40.7222 test Loss 19.5323 with MSE metric 16319.9090\n",
      "Epoch 282 batch 30 train Loss 40.7172 test Loss 19.5303 with MSE metric 16318.6561\n",
      "Epoch 282 batch 40 train Loss 40.7122 test Loss 19.5282 with MSE metric 16317.3851\n",
      "Epoch 282 batch 50 train Loss 40.7072 test Loss 19.5262 with MSE metric 16316.1392\n",
      "Epoch 282 batch 60 train Loss 40.7022 test Loss 19.5242 with MSE metric 16314.9085\n",
      "Epoch 282 batch 70 train Loss 40.6971 test Loss 19.5222 with MSE metric 16313.6735\n",
      "Epoch 282 batch 80 train Loss 40.6921 test Loss 19.5202 with MSE metric 16312.4379\n",
      "Epoch 282 batch 90 train Loss 40.6871 test Loss 19.5182 with MSE metric 16311.1951\n",
      "Epoch 282 batch 100 train Loss 40.6821 test Loss 19.5162 with MSE metric 16309.9241\n",
      "Epoch 282 batch 110 train Loss 40.6771 test Loss 19.5142 with MSE metric 16308.6888\n",
      "Epoch 282 batch 120 train Loss 40.6721 test Loss 19.5122 with MSE metric 16307.4412\n",
      "Epoch 282 batch 130 train Loss 40.6671 test Loss 19.5102 with MSE metric 16306.2100\n",
      "Epoch 282 batch 140 train Loss 40.6621 test Loss 19.5082 with MSE metric 16304.9941\n",
      "Epoch 282 batch 150 train Loss 40.6571 test Loss 19.5062 with MSE metric 16303.7697\n",
      "Epoch 282 batch 160 train Loss 40.6521 test Loss 19.5042 with MSE metric 16302.4991\n",
      "Epoch 282 batch 170 train Loss 40.6471 test Loss 19.5022 with MSE metric 16301.2116\n",
      "Epoch 282 batch 180 train Loss 40.6421 test Loss 19.5002 with MSE metric 16299.9720\n",
      "Epoch 282 batch 190 train Loss 40.6371 test Loss 19.4982 with MSE metric 16298.7074\n",
      "Epoch 282 batch 200 train Loss 40.6321 test Loss 19.4962 with MSE metric 16297.5134\n",
      "Epoch 282 batch 210 train Loss 40.6271 test Loss 19.4942 with MSE metric 16296.3425\n",
      "Epoch 282 batch 220 train Loss 40.6221 test Loss 19.4922 with MSE metric 16295.1128\n",
      "Epoch 282 batch 230 train Loss 40.6171 test Loss 19.4902 with MSE metric 16293.8811\n",
      "Epoch 282 batch 240 train Loss 40.6121 test Loss 19.4883 with MSE metric 16292.6255\n",
      "Time taken for 1 epoch: 29.381640195846558 secs\n",
      "\n",
      "Epoch 283 batch 0 train Loss 40.6071 test Loss 19.4863 with MSE metric 16291.3803\n",
      "Epoch 283 batch 10 train Loss 40.6021 test Loss 19.4843 with MSE metric 16290.1101\n",
      "Epoch 283 batch 20 train Loss 40.5971 test Loss 19.4823 with MSE metric 16288.8667\n",
      "Epoch 283 batch 30 train Loss 40.5921 test Loss 19.4803 with MSE metric 16287.6246\n",
      "Epoch 283 batch 40 train Loss 40.5872 test Loss 19.4783 with MSE metric 16286.3503\n",
      "Epoch 283 batch 50 train Loss 40.5822 test Loss 19.4763 with MSE metric 16285.1488\n",
      "Epoch 283 batch 60 train Loss 40.5772 test Loss 19.4743 with MSE metric 16283.9120\n",
      "Epoch 283 batch 70 train Loss 40.5722 test Loss 19.4723 with MSE metric 16282.6472\n",
      "Epoch 283 batch 80 train Loss 40.5672 test Loss 19.4703 with MSE metric 16281.3488\n",
      "Epoch 283 batch 90 train Loss 40.5623 test Loss 19.4683 with MSE metric 16280.1048\n",
      "Epoch 283 batch 100 train Loss 40.5573 test Loss 19.4663 with MSE metric 16278.8418\n",
      "Epoch 283 batch 110 train Loss 40.5523 test Loss 19.4643 with MSE metric 16277.5634\n",
      "Epoch 283 batch 120 train Loss 40.5473 test Loss 19.4623 with MSE metric 16276.3308\n",
      "Epoch 283 batch 130 train Loss 40.5424 test Loss 19.4603 with MSE metric 16275.0918\n",
      "Epoch 283 batch 140 train Loss 40.5374 test Loss 19.4583 with MSE metric 16273.8465\n",
      "Epoch 283 batch 150 train Loss 40.5324 test Loss 19.4564 with MSE metric 16272.5977\n",
      "Epoch 283 batch 160 train Loss 40.5274 test Loss 19.4544 with MSE metric 16271.3494\n",
      "Epoch 283 batch 170 train Loss 40.5225 test Loss 19.4524 with MSE metric 16270.0804\n",
      "Epoch 283 batch 180 train Loss 40.5175 test Loss 19.4504 with MSE metric 16268.8646\n",
      "Epoch 283 batch 190 train Loss 40.5126 test Loss 19.4485 with MSE metric 16267.6731\n",
      "Epoch 283 batch 200 train Loss 40.5076 test Loss 19.4465 with MSE metric 16266.4483\n",
      "Epoch 283 batch 210 train Loss 40.5026 test Loss 19.4445 with MSE metric 16265.2038\n",
      "Epoch 283 batch 220 train Loss 40.4977 test Loss 19.4425 with MSE metric 16263.9876\n",
      "Epoch 283 batch 230 train Loss 40.4927 test Loss 19.4405 with MSE metric 16262.7796\n",
      "Epoch 283 batch 240 train Loss 40.4878 test Loss 19.4386 with MSE metric 16261.5817\n",
      "Time taken for 1 epoch: 27.81438183784485 secs\n",
      "\n",
      "Epoch 284 batch 0 train Loss 40.4828 test Loss 19.4366 with MSE metric 16260.3799\n",
      "Epoch 284 batch 10 train Loss 40.4779 test Loss 19.4346 with MSE metric 16259.1284\n",
      "Epoch 284 batch 20 train Loss 40.4729 test Loss 19.4326 with MSE metric 16257.9197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284 batch 30 train Loss 40.4680 test Loss 19.4306 with MSE metric 16256.7332\n",
      "Epoch 284 batch 40 train Loss 40.4630 test Loss 19.4286 with MSE metric 16255.5236\n",
      "Epoch 284 batch 50 train Loss 40.4581 test Loss 19.4267 with MSE metric 16254.3374\n",
      "Epoch 284 batch 60 train Loss 40.4531 test Loss 19.4247 with MSE metric 16253.0733\n",
      "Epoch 284 batch 70 train Loss 40.4482 test Loss 19.4227 with MSE metric 16251.8429\n",
      "Epoch 284 batch 80 train Loss 40.4432 test Loss 19.4207 with MSE metric 16250.5696\n",
      "Epoch 284 batch 90 train Loss 40.4383 test Loss 19.4188 with MSE metric 16249.3842\n",
      "Epoch 284 batch 100 train Loss 40.4334 test Loss 19.4168 with MSE metric 16248.1697\n",
      "Epoch 284 batch 110 train Loss 40.4284 test Loss 19.4148 with MSE metric 16247.0009\n",
      "Epoch 284 batch 120 train Loss 40.4235 test Loss 19.4128 with MSE metric 16245.8194\n",
      "Epoch 284 batch 130 train Loss 40.4185 test Loss 19.4108 with MSE metric 16244.6146\n",
      "Epoch 284 batch 140 train Loss 40.4136 test Loss 19.4089 with MSE metric 16243.4043\n",
      "Epoch 284 batch 150 train Loss 40.4087 test Loss 19.4069 with MSE metric 16242.1936\n",
      "Epoch 284 batch 160 train Loss 40.4038 test Loss 19.4049 with MSE metric 16240.9938\n",
      "Epoch 284 batch 170 train Loss 40.3988 test Loss 19.4029 with MSE metric 16239.7764\n",
      "Epoch 284 batch 180 train Loss 40.3939 test Loss 19.4010 with MSE metric 16238.5102\n",
      "Epoch 284 batch 190 train Loss 40.3890 test Loss 19.3990 with MSE metric 16237.3346\n",
      "Epoch 284 batch 200 train Loss 40.3840 test Loss 19.3970 with MSE metric 16236.1223\n",
      "Epoch 284 batch 210 train Loss 40.3791 test Loss 19.3951 with MSE metric 16234.9495\n",
      "Epoch 284 batch 220 train Loss 40.3742 test Loss 19.3931 with MSE metric 16233.7346\n",
      "Epoch 284 batch 230 train Loss 40.3693 test Loss 19.3911 with MSE metric 16232.5153\n",
      "Epoch 284 batch 240 train Loss 40.3643 test Loss 19.3891 with MSE metric 16231.3019\n",
      "Time taken for 1 epoch: 30.161532163619995 secs\n",
      "\n",
      "Epoch 285 batch 0 train Loss 40.3594 test Loss 19.3872 with MSE metric 16230.0867\n",
      "Epoch 285 batch 10 train Loss 40.3545 test Loss 19.3852 with MSE metric 16228.9093\n",
      "Epoch 285 batch 20 train Loss 40.3496 test Loss 19.3832 with MSE metric 16227.6698\n",
      "Epoch 285 batch 30 train Loss 40.3447 test Loss 19.3812 with MSE metric 16226.4511\n",
      "Epoch 285 batch 40 train Loss 40.3398 test Loss 19.3793 with MSE metric 16225.2441\n",
      "Epoch 285 batch 50 train Loss 40.3349 test Loss 19.3773 with MSE metric 16224.0507\n",
      "Epoch 285 batch 60 train Loss 40.3299 test Loss 19.3754 with MSE metric 16222.8227\n",
      "Epoch 285 batch 70 train Loss 40.3250 test Loss 19.3734 with MSE metric 16221.6129\n",
      "Epoch 285 batch 80 train Loss 40.3201 test Loss 19.3714 with MSE metric 16220.3806\n",
      "Epoch 285 batch 90 train Loss 40.3152 test Loss 19.3695 with MSE metric 16219.1431\n",
      "Epoch 285 batch 100 train Loss 40.3103 test Loss 19.3675 with MSE metric 16217.8993\n",
      "Epoch 285 batch 110 train Loss 40.3054 test Loss 19.3655 with MSE metric 16216.6830\n",
      "Epoch 285 batch 120 train Loss 40.3005 test Loss 19.3636 with MSE metric 16215.4783\n",
      "Epoch 285 batch 130 train Loss 40.2956 test Loss 19.3616 with MSE metric 16214.3252\n",
      "Epoch 285 batch 140 train Loss 40.2907 test Loss 19.3597 with MSE metric 16213.1076\n",
      "Epoch 285 batch 150 train Loss 40.2858 test Loss 19.3577 with MSE metric 16211.9718\n",
      "Epoch 285 batch 160 train Loss 40.2809 test Loss 19.3558 with MSE metric 16210.7738\n",
      "Epoch 285 batch 170 train Loss 40.2760 test Loss 19.3538 with MSE metric 16209.5421\n",
      "Epoch 285 batch 180 train Loss 40.2711 test Loss 19.3518 with MSE metric 16208.3006\n",
      "Epoch 285 batch 190 train Loss 40.2662 test Loss 19.3499 with MSE metric 16207.1156\n",
      "Epoch 285 batch 200 train Loss 40.2613 test Loss 19.3479 with MSE metric 16205.8984\n",
      "Epoch 285 batch 210 train Loss 40.2564 test Loss 19.3460 with MSE metric 16204.6528\n",
      "Epoch 285 batch 220 train Loss 40.2515 test Loss 19.3440 with MSE metric 16203.4244\n",
      "Epoch 285 batch 230 train Loss 40.2467 test Loss 19.3421 with MSE metric 16202.2068\n",
      "Epoch 285 batch 240 train Loss 40.2418 test Loss 19.3401 with MSE metric 16201.0095\n",
      "Time taken for 1 epoch: 28.5151150226593 secs\n",
      "\n",
      "Epoch 286 batch 0 train Loss 40.2369 test Loss 19.3382 with MSE metric 16199.7967\n",
      "Epoch 286 batch 10 train Loss 40.2320 test Loss 19.3362 with MSE metric 16198.5902\n",
      "Epoch 286 batch 20 train Loss 40.2271 test Loss 19.3343 with MSE metric 16197.3841\n",
      "Epoch 286 batch 30 train Loss 40.2222 test Loss 19.3323 with MSE metric 16196.2101\n",
      "Epoch 286 batch 40 train Loss 40.2174 test Loss 19.3304 with MSE metric 16195.0684\n",
      "Epoch 286 batch 50 train Loss 40.2125 test Loss 19.3284 with MSE metric 16193.8774\n",
      "Epoch 286 batch 60 train Loss 40.2076 test Loss 19.3265 with MSE metric 16192.6177\n",
      "Epoch 286 batch 70 train Loss 40.2027 test Loss 19.3245 with MSE metric 16191.4305\n",
      "Epoch 286 batch 80 train Loss 40.1979 test Loss 19.3226 with MSE metric 16190.2366\n",
      "Epoch 286 batch 90 train Loss 40.1930 test Loss 19.3206 with MSE metric 16188.9826\n",
      "Epoch 286 batch 100 train Loss 40.1881 test Loss 19.3187 with MSE metric 16187.7776\n",
      "Epoch 286 batch 110 train Loss 40.1832 test Loss 19.3167 with MSE metric 16186.5282\n",
      "Epoch 286 batch 120 train Loss 40.1784 test Loss 19.3148 with MSE metric 16185.3255\n",
      "Epoch 286 batch 130 train Loss 40.1735 test Loss 19.3128 with MSE metric 16184.1087\n",
      "Epoch 286 batch 140 train Loss 40.1686 test Loss 19.3109 with MSE metric 16182.9154\n",
      "Epoch 286 batch 150 train Loss 40.1638 test Loss 19.3090 with MSE metric 16181.6953\n",
      "Epoch 286 batch 160 train Loss 40.1589 test Loss 19.3070 with MSE metric 16180.5009\n",
      "Epoch 286 batch 170 train Loss 40.1540 test Loss 19.3051 with MSE metric 16179.3350\n",
      "Epoch 286 batch 180 train Loss 40.1492 test Loss 19.3032 with MSE metric 16178.1104\n",
      "Epoch 286 batch 190 train Loss 40.1443 test Loss 19.3012 with MSE metric 16176.8715\n",
      "Epoch 286 batch 200 train Loss 40.1395 test Loss 19.2993 with MSE metric 16175.6745\n",
      "Epoch 286 batch 210 train Loss 40.1346 test Loss 19.2973 with MSE metric 16174.4884\n",
      "Epoch 286 batch 220 train Loss 40.1297 test Loss 19.2954 with MSE metric 16173.3185\n",
      "Epoch 286 batch 230 train Loss 40.1249 test Loss 19.2935 with MSE metric 16172.0988\n",
      "Epoch 286 batch 240 train Loss 40.1200 test Loss 19.2915 with MSE metric 16170.9149\n",
      "Time taken for 1 epoch: 27.456562042236328 secs\n",
      "\n",
      "Epoch 287 batch 0 train Loss 40.1152 test Loss 19.2896 with MSE metric 16169.7135\n",
      "Epoch 287 batch 10 train Loss 40.1103 test Loss 19.2877 with MSE metric 16168.5372\n",
      "Epoch 287 batch 20 train Loss 40.1055 test Loss 19.2857 with MSE metric 16167.3145\n",
      "Epoch 287 batch 30 train Loss 40.1006 test Loss 19.2838 with MSE metric 16166.1105\n",
      "Epoch 287 batch 40 train Loss 40.0958 test Loss 19.2818 with MSE metric 16164.9351\n",
      "Epoch 287 batch 50 train Loss 40.0909 test Loss 19.2799 with MSE metric 16163.7201\n",
      "Epoch 287 batch 60 train Loss 40.0861 test Loss 19.2780 with MSE metric 16162.4984\n",
      "Epoch 287 batch 70 train Loss 40.0813 test Loss 19.2760 with MSE metric 16161.3172\n",
      "Epoch 287 batch 80 train Loss 40.0764 test Loss 19.2741 with MSE metric 16160.1431\n",
      "Epoch 287 batch 90 train Loss 40.0716 test Loss 19.2721 with MSE metric 16158.9618\n",
      "Epoch 287 batch 100 train Loss 40.0667 test Loss 19.2702 with MSE metric 16157.8013\n",
      "Epoch 287 batch 110 train Loss 40.0619 test Loss 19.2683 with MSE metric 16156.5774\n",
      "Epoch 287 batch 120 train Loss 40.0571 test Loss 19.2664 with MSE metric 16155.3849\n",
      "Epoch 287 batch 130 train Loss 40.0522 test Loss 19.2644 with MSE metric 16154.2520\n",
      "Epoch 287 batch 140 train Loss 40.0474 test Loss 19.2625 with MSE metric 16153.1195\n",
      "Epoch 287 batch 150 train Loss 40.0426 test Loss 19.2605 with MSE metric 16151.9277\n",
      "Epoch 287 batch 160 train Loss 40.0378 test Loss 19.2586 with MSE metric 16150.7169\n",
      "Epoch 287 batch 170 train Loss 40.0329 test Loss 19.2567 with MSE metric 16149.5359\n",
      "Epoch 287 batch 180 train Loss 40.0281 test Loss 19.2547 with MSE metric 16148.3226\n",
      "Epoch 287 batch 190 train Loss 40.0233 test Loss 19.2528 with MSE metric 16147.1312\n",
      "Epoch 287 batch 200 train Loss 40.0185 test Loss 19.2509 with MSE metric 16145.9062\n",
      "Epoch 287 batch 210 train Loss 40.0136 test Loss 19.2490 with MSE metric 16144.7214\n",
      "Epoch 287 batch 220 train Loss 40.0088 test Loss 19.2470 with MSE metric 16143.5500\n",
      "Epoch 287 batch 230 train Loss 40.0040 test Loss 19.2451 with MSE metric 16142.3608\n",
      "Epoch 287 batch 240 train Loss 39.9992 test Loss 19.2432 with MSE metric 16141.1865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 29.817690134048462 secs\n",
      "\n",
      "Epoch 288 batch 0 train Loss 39.9944 test Loss 19.2413 with MSE metric 16139.9630\n",
      "Epoch 288 batch 10 train Loss 39.9895 test Loss 19.2393 with MSE metric 16138.7735\n",
      "Epoch 288 batch 20 train Loss 39.9847 test Loss 19.2374 with MSE metric 16137.6066\n",
      "Epoch 288 batch 30 train Loss 39.9799 test Loss 19.2355 with MSE metric 16136.4242\n",
      "Epoch 288 batch 40 train Loss 39.9751 test Loss 19.2335 with MSE metric 16135.1910\n",
      "Epoch 288 batch 50 train Loss 39.9703 test Loss 19.2316 with MSE metric 16134.0427\n",
      "Epoch 288 batch 60 train Loss 39.9655 test Loss 19.2297 with MSE metric 16132.8727\n",
      "Epoch 288 batch 70 train Loss 39.9607 test Loss 19.2278 with MSE metric 16131.6864\n",
      "Epoch 288 batch 80 train Loss 39.9559 test Loss 19.2259 with MSE metric 16130.4724\n",
      "Epoch 288 batch 90 train Loss 39.9511 test Loss 19.2240 with MSE metric 16129.2792\n",
      "Epoch 288 batch 100 train Loss 39.9463 test Loss 19.2220 with MSE metric 16128.1258\n",
      "Epoch 288 batch 110 train Loss 39.9415 test Loss 19.2201 with MSE metric 16126.8785\n",
      "Epoch 288 batch 120 train Loss 39.9367 test Loss 19.2182 with MSE metric 16125.6582\n",
      "Epoch 288 batch 130 train Loss 39.9318 test Loss 19.2163 with MSE metric 16124.4668\n",
      "Epoch 288 batch 140 train Loss 39.9270 test Loss 19.2144 with MSE metric 16123.2774\n",
      "Epoch 288 batch 150 train Loss 39.9222 test Loss 19.2125 with MSE metric 16122.0792\n",
      "Epoch 288 batch 160 train Loss 39.9175 test Loss 19.2105 with MSE metric 16120.8623\n",
      "Epoch 288 batch 170 train Loss 39.9127 test Loss 19.2086 with MSE metric 16119.6833\n",
      "Epoch 288 batch 180 train Loss 39.9079 test Loss 19.2067 with MSE metric 16118.4896\n",
      "Epoch 288 batch 190 train Loss 39.9031 test Loss 19.2048 with MSE metric 16117.2833\n",
      "Epoch 288 batch 200 train Loss 39.8983 test Loss 19.2029 with MSE metric 16116.1013\n",
      "Epoch 288 batch 210 train Loss 39.8935 test Loss 19.2010 with MSE metric 16114.9243\n",
      "Epoch 288 batch 220 train Loss 39.8887 test Loss 19.1991 with MSE metric 16113.7247\n",
      "Epoch 288 batch 230 train Loss 39.8839 test Loss 19.1971 with MSE metric 16112.5603\n",
      "Epoch 288 batch 240 train Loss 39.8791 test Loss 19.1952 with MSE metric 16111.4286\n",
      "Time taken for 1 epoch: 28.111379146575928 secs\n",
      "\n",
      "Epoch 289 batch 0 train Loss 39.8744 test Loss 19.1933 with MSE metric 16110.2779\n",
      "Epoch 289 batch 10 train Loss 39.8696 test Loss 19.1914 with MSE metric 16109.0899\n",
      "Epoch 289 batch 20 train Loss 39.8648 test Loss 19.1895 with MSE metric 16107.8835\n",
      "Epoch 289 batch 30 train Loss 39.8600 test Loss 19.1876 with MSE metric 16106.6386\n",
      "Epoch 289 batch 40 train Loss 39.8552 test Loss 19.1857 with MSE metric 16105.4298\n",
      "Epoch 289 batch 50 train Loss 39.8504 test Loss 19.1838 with MSE metric 16104.2693\n",
      "Epoch 289 batch 60 train Loss 39.8457 test Loss 19.1818 with MSE metric 16103.1419\n",
      "Epoch 289 batch 70 train Loss 39.8409 test Loss 19.1799 with MSE metric 16101.9301\n",
      "Epoch 289 batch 80 train Loss 39.8361 test Loss 19.1780 with MSE metric 16100.7273\n",
      "Epoch 289 batch 90 train Loss 39.8313 test Loss 19.1761 with MSE metric 16099.5283\n",
      "Epoch 289 batch 100 train Loss 39.8266 test Loss 19.1742 with MSE metric 16098.3041\n",
      "Epoch 289 batch 110 train Loss 39.8218 test Loss 19.1723 with MSE metric 16097.1485\n",
      "Epoch 289 batch 120 train Loss 39.8170 test Loss 19.1704 with MSE metric 16095.9765\n",
      "Epoch 289 batch 130 train Loss 39.8123 test Loss 19.1685 with MSE metric 16094.7873\n",
      "Epoch 289 batch 140 train Loss 39.8075 test Loss 19.1666 with MSE metric 16093.5905\n",
      "Epoch 289 batch 150 train Loss 39.8027 test Loss 19.1647 with MSE metric 16092.3583\n",
      "Epoch 289 batch 160 train Loss 39.7980 test Loss 19.1628 with MSE metric 16091.1731\n",
      "Epoch 289 batch 170 train Loss 39.7932 test Loss 19.1608 with MSE metric 16089.9845\n",
      "Epoch 289 batch 180 train Loss 39.7884 test Loss 19.1589 with MSE metric 16088.7892\n",
      "Epoch 289 batch 190 train Loss 39.7837 test Loss 19.1571 with MSE metric 16087.6339\n",
      "Epoch 289 batch 200 train Loss 39.7789 test Loss 19.1552 with MSE metric 16086.4816\n",
      "Epoch 289 batch 210 train Loss 39.7742 test Loss 19.1532 with MSE metric 16085.2672\n",
      "Epoch 289 batch 220 train Loss 39.7694 test Loss 19.1513 with MSE metric 16084.0442\n",
      "Epoch 289 batch 230 train Loss 39.7646 test Loss 19.1494 with MSE metric 16082.8563\n",
      "Epoch 289 batch 240 train Loss 39.7599 test Loss 19.1475 with MSE metric 16081.6933\n",
      "Time taken for 1 epoch: 29.558871030807495 secs\n",
      "\n",
      "Epoch 290 batch 0 train Loss 39.7551 test Loss 19.1456 with MSE metric 16080.5336\n",
      "Epoch 290 batch 10 train Loss 39.7504 test Loss 19.1437 with MSE metric 16079.3968\n",
      "Epoch 290 batch 20 train Loss 39.7456 test Loss 19.1418 with MSE metric 16078.2143\n",
      "Epoch 290 batch 30 train Loss 39.7409 test Loss 19.1400 with MSE metric 16077.0358\n",
      "Epoch 290 batch 40 train Loss 39.7361 test Loss 19.1381 with MSE metric 16075.8402\n",
      "Epoch 290 batch 50 train Loss 39.7314 test Loss 19.1362 with MSE metric 16074.6483\n",
      "Epoch 290 batch 60 train Loss 39.7267 test Loss 19.1343 with MSE metric 16073.5027\n",
      "Epoch 290 batch 70 train Loss 39.7219 test Loss 19.1324 with MSE metric 16072.3503\n",
      "Epoch 290 batch 80 train Loss 39.7172 test Loss 19.1305 with MSE metric 16071.1152\n",
      "Epoch 290 batch 90 train Loss 39.7124 test Loss 19.1286 with MSE metric 16069.9673\n",
      "Epoch 290 batch 100 train Loss 39.7077 test Loss 19.1267 with MSE metric 16068.7886\n",
      "Epoch 290 batch 110 train Loss 39.7030 test Loss 19.1248 with MSE metric 16067.5791\n",
      "Epoch 290 batch 120 train Loss 39.6982 test Loss 19.1229 with MSE metric 16066.4262\n",
      "Epoch 290 batch 130 train Loss 39.6935 test Loss 19.1210 with MSE metric 16065.2351\n",
      "Epoch 290 batch 140 train Loss 39.6888 test Loss 19.1191 with MSE metric 16064.0627\n",
      "Epoch 290 batch 150 train Loss 39.6840 test Loss 19.1172 with MSE metric 16062.8865\n",
      "Epoch 290 batch 160 train Loss 39.6793 test Loss 19.1154 with MSE metric 16061.7580\n",
      "Epoch 290 batch 170 train Loss 39.6746 test Loss 19.1135 with MSE metric 16060.5945\n",
      "Epoch 290 batch 180 train Loss 39.6698 test Loss 19.1116 with MSE metric 16059.3962\n",
      "Epoch 290 batch 190 train Loss 39.6651 test Loss 19.1097 with MSE metric 16058.2308\n",
      "Epoch 290 batch 200 train Loss 39.6604 test Loss 19.1078 with MSE metric 16057.0197\n",
      "Epoch 290 batch 210 train Loss 39.6557 test Loss 19.1059 with MSE metric 16055.9030\n",
      "Epoch 290 batch 220 train Loss 39.6509 test Loss 19.1041 with MSE metric 16054.7057\n",
      "Epoch 290 batch 230 train Loss 39.6462 test Loss 19.1022 with MSE metric 16053.5535\n",
      "Epoch 290 batch 240 train Loss 39.6415 test Loss 19.1003 with MSE metric 16052.3791\n",
      "Time taken for 1 epoch: 27.91474199295044 secs\n",
      "\n",
      "Epoch 291 batch 0 train Loss 39.6368 test Loss 19.0984 with MSE metric 16051.1739\n",
      "Epoch 291 batch 10 train Loss 39.6321 test Loss 19.0965 with MSE metric 16050.0310\n",
      "Epoch 291 batch 20 train Loss 39.6273 test Loss 19.0946 with MSE metric 16048.8519\n",
      "Epoch 291 batch 30 train Loss 39.6226 test Loss 19.0927 with MSE metric 16047.7093\n",
      "Epoch 291 batch 40 train Loss 39.6179 test Loss 19.0908 with MSE metric 16046.5709\n",
      "Epoch 291 batch 50 train Loss 39.6132 test Loss 19.0889 with MSE metric 16045.4341\n",
      "Epoch 291 batch 60 train Loss 39.6085 test Loss 19.0871 with MSE metric 16044.2663\n",
      "Epoch 291 batch 70 train Loss 39.6038 test Loss 19.0852 with MSE metric 16043.1493\n",
      "Epoch 291 batch 80 train Loss 39.5991 test Loss 19.0833 with MSE metric 16041.9838\n",
      "Epoch 291 batch 90 train Loss 39.5944 test Loss 19.0814 with MSE metric 16040.8195\n",
      "Epoch 291 batch 100 train Loss 39.5897 test Loss 19.0795 with MSE metric 16039.6507\n",
      "Epoch 291 batch 110 train Loss 39.5850 test Loss 19.0777 with MSE metric 16038.5263\n",
      "Epoch 291 batch 120 train Loss 39.5803 test Loss 19.0758 with MSE metric 16037.3681\n",
      "Epoch 291 batch 130 train Loss 39.5756 test Loss 19.0739 with MSE metric 16036.2284\n",
      "Epoch 291 batch 140 train Loss 39.5709 test Loss 19.0720 with MSE metric 16035.0320\n",
      "Epoch 291 batch 150 train Loss 39.5662 test Loss 19.0702 with MSE metric 16033.8844\n",
      "Epoch 291 batch 160 train Loss 39.5615 test Loss 19.0683 with MSE metric 16032.7495\n",
      "Epoch 291 batch 170 train Loss 39.5568 test Loss 19.0664 with MSE metric 16031.6351\n",
      "Epoch 291 batch 180 train Loss 39.5521 test Loss 19.0645 with MSE metric 16030.5329\n",
      "Epoch 291 batch 190 train Loss 39.5474 test Loss 19.0627 with MSE metric 16029.3703\n",
      "Epoch 291 batch 200 train Loss 39.5427 test Loss 19.0608 with MSE metric 16028.2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291 batch 210 train Loss 39.5380 test Loss 19.0589 with MSE metric 16027.0114\n",
      "Epoch 291 batch 220 train Loss 39.5333 test Loss 19.0570 with MSE metric 16025.8490\n",
      "Epoch 291 batch 230 train Loss 39.5286 test Loss 19.0552 with MSE metric 16024.7154\n",
      "Epoch 291 batch 240 train Loss 39.5239 test Loss 19.0533 with MSE metric 16023.5897\n",
      "Time taken for 1 epoch: 28.239630937576294 secs\n",
      "\n",
      "Epoch 292 batch 0 train Loss 39.5192 test Loss 19.0514 with MSE metric 16022.3602\n",
      "Epoch 292 batch 10 train Loss 39.5145 test Loss 19.0495 with MSE metric 16021.2098\n",
      "Epoch 292 batch 20 train Loss 39.5099 test Loss 19.0477 with MSE metric 16020.0729\n",
      "Epoch 292 batch 30 train Loss 39.5052 test Loss 19.0458 with MSE metric 16018.9242\n",
      "Epoch 292 batch 40 train Loss 39.5005 test Loss 19.0439 with MSE metric 16017.7899\n",
      "Epoch 292 batch 50 train Loss 39.4958 test Loss 19.0421 with MSE metric 16016.6463\n",
      "Epoch 292 batch 60 train Loss 39.4911 test Loss 19.0402 with MSE metric 16015.4868\n",
      "Epoch 292 batch 70 train Loss 39.4865 test Loss 19.0383 with MSE metric 16014.3885\n",
      "Epoch 292 batch 80 train Loss 39.4818 test Loss 19.0365 with MSE metric 16013.2084\n",
      "Epoch 292 batch 90 train Loss 39.4771 test Loss 19.0346 with MSE metric 16012.0712\n",
      "Epoch 292 batch 100 train Loss 39.4724 test Loss 19.0327 with MSE metric 16010.9011\n",
      "Epoch 292 batch 110 train Loss 39.4678 test Loss 19.0309 with MSE metric 16009.7364\n",
      "Epoch 292 batch 120 train Loss 39.4631 test Loss 19.0290 with MSE metric 16008.5534\n",
      "Epoch 292 batch 130 train Loss 39.4584 test Loss 19.0271 with MSE metric 16007.4389\n",
      "Epoch 292 batch 140 train Loss 39.4538 test Loss 19.0253 with MSE metric 16006.3140\n",
      "Epoch 292 batch 150 train Loss 39.4491 test Loss 19.0234 with MSE metric 16005.1344\n",
      "Epoch 292 batch 160 train Loss 39.4444 test Loss 19.0216 with MSE metric 16004.0181\n",
      "Epoch 292 batch 170 train Loss 39.4398 test Loss 19.0197 with MSE metric 16002.8193\n",
      "Epoch 292 batch 180 train Loss 39.4351 test Loss 19.0178 with MSE metric 16001.6953\n",
      "Epoch 292 batch 190 train Loss 39.4304 test Loss 19.0160 with MSE metric 16000.5602\n",
      "Epoch 292 batch 200 train Loss 39.4258 test Loss 19.0141 with MSE metric 15999.3681\n",
      "Epoch 292 batch 210 train Loss 39.4211 test Loss 19.0122 with MSE metric 15998.2112\n",
      "Epoch 292 batch 220 train Loss 39.4165 test Loss 19.0104 with MSE metric 15997.0417\n",
      "Epoch 292 batch 230 train Loss 39.4118 test Loss 19.0085 with MSE metric 15995.9043\n",
      "Epoch 292 batch 240 train Loss 39.4071 test Loss 19.0066 with MSE metric 15994.7419\n",
      "Time taken for 1 epoch: 28.324235200881958 secs\n",
      "\n",
      "Epoch 293 batch 0 train Loss 39.4025 test Loss 19.0048 with MSE metric 15993.6317\n",
      "Epoch 293 batch 10 train Loss 39.3978 test Loss 19.0029 with MSE metric 15992.4682\n",
      "Epoch 293 batch 20 train Loss 39.3932 test Loss 19.0010 with MSE metric 15991.3246\n",
      "Epoch 293 batch 30 train Loss 39.3885 test Loss 18.9992 with MSE metric 15990.1857\n",
      "Epoch 293 batch 40 train Loss 39.3839 test Loss 18.9973 with MSE metric 15989.0297\n",
      "Epoch 293 batch 50 train Loss 39.3792 test Loss 18.9955 with MSE metric 15987.8772\n",
      "Epoch 293 batch 60 train Loss 39.3746 test Loss 18.9936 with MSE metric 15986.7430\n",
      "Epoch 293 batch 70 train Loss 39.3699 test Loss 18.9918 with MSE metric 15985.5596\n",
      "Epoch 293 batch 80 train Loss 39.3653 test Loss 18.9899 with MSE metric 15984.4211\n",
      "Epoch 293 batch 90 train Loss 39.3607 test Loss 18.9880 with MSE metric 15983.2680\n",
      "Epoch 293 batch 100 train Loss 39.3560 test Loss 18.9862 with MSE metric 15982.1274\n",
      "Epoch 293 batch 110 train Loss 39.3514 test Loss 18.9843 with MSE metric 15980.9639\n",
      "Epoch 293 batch 120 train Loss 39.3467 test Loss 18.9825 with MSE metric 15979.8062\n",
      "Epoch 293 batch 130 train Loss 39.3421 test Loss 18.9806 with MSE metric 15978.6329\n",
      "Epoch 293 batch 140 train Loss 39.3375 test Loss 18.9788 with MSE metric 15977.4885\n",
      "Epoch 293 batch 150 train Loss 39.3328 test Loss 18.9769 with MSE metric 15976.3362\n",
      "Epoch 293 batch 160 train Loss 39.3282 test Loss 18.9751 with MSE metric 15975.2022\n",
      "Epoch 293 batch 170 train Loss 39.3236 test Loss 18.9732 with MSE metric 15974.0636\n",
      "Epoch 293 batch 180 train Loss 39.3189 test Loss 18.9714 with MSE metric 15972.9570\n",
      "Epoch 293 batch 190 train Loss 39.3143 test Loss 18.9695 with MSE metric 15971.8402\n",
      "Epoch 293 batch 200 train Loss 39.3097 test Loss 18.9677 with MSE metric 15970.6973\n",
      "Epoch 293 batch 210 train Loss 39.3050 test Loss 18.9658 with MSE metric 15969.5484\n",
      "Epoch 293 batch 220 train Loss 39.3004 test Loss 18.9640 with MSE metric 15968.3499\n",
      "Epoch 293 batch 230 train Loss 39.2958 test Loss 18.9621 with MSE metric 15967.2190\n",
      "Epoch 293 batch 240 train Loss 39.2912 test Loss 18.9603 with MSE metric 15966.0743\n",
      "Time taken for 1 epoch: 27.531989097595215 secs\n",
      "\n",
      "Epoch 294 batch 0 train Loss 39.2865 test Loss 18.9584 with MSE metric 15964.8993\n",
      "Epoch 294 batch 10 train Loss 39.2819 test Loss 18.9566 with MSE metric 15963.7507\n",
      "Epoch 294 batch 20 train Loss 39.2773 test Loss 18.9547 with MSE metric 15962.6343\n",
      "Epoch 294 batch 30 train Loss 39.2727 test Loss 18.9529 with MSE metric 15961.4713\n",
      "Epoch 294 batch 40 train Loss 39.2681 test Loss 18.9510 with MSE metric 15960.3288\n",
      "Epoch 294 batch 50 train Loss 39.2634 test Loss 18.9492 with MSE metric 15959.1423\n",
      "Epoch 294 batch 60 train Loss 39.2588 test Loss 18.9473 with MSE metric 15958.0111\n",
      "Epoch 294 batch 70 train Loss 39.2542 test Loss 18.9455 with MSE metric 15956.8853\n",
      "Epoch 294 batch 80 train Loss 39.2496 test Loss 18.9436 with MSE metric 15955.7227\n",
      "Epoch 294 batch 90 train Loss 39.2450 test Loss 18.9418 with MSE metric 15954.5792\n",
      "Epoch 294 batch 100 train Loss 39.2404 test Loss 18.9400 with MSE metric 15953.4264\n",
      "Epoch 294 batch 110 train Loss 39.2358 test Loss 18.9381 with MSE metric 15952.3307\n",
      "Epoch 294 batch 120 train Loss 39.2312 test Loss 18.9363 with MSE metric 15951.1813\n",
      "Epoch 294 batch 130 train Loss 39.2265 test Loss 18.9345 with MSE metric 15949.9958\n",
      "Epoch 294 batch 140 train Loss 39.2219 test Loss 18.9326 with MSE metric 15948.8893\n",
      "Epoch 294 batch 150 train Loss 39.2173 test Loss 18.9307 with MSE metric 15947.6726\n",
      "Epoch 294 batch 160 train Loss 39.2127 test Loss 18.9289 with MSE metric 15946.5544\n",
      "Epoch 294 batch 170 train Loss 39.2081 test Loss 18.9271 with MSE metric 15945.3792\n",
      "Epoch 294 batch 180 train Loss 39.2035 test Loss 18.9252 with MSE metric 15944.2414\n",
      "Epoch 294 batch 190 train Loss 39.1989 test Loss 18.9234 with MSE metric 15943.1275\n",
      "Epoch 294 batch 200 train Loss 39.1943 test Loss 18.9216 with MSE metric 15942.0092\n",
      "Epoch 294 batch 210 train Loss 39.1897 test Loss 18.9197 with MSE metric 15940.9001\n",
      "Epoch 294 batch 220 train Loss 39.1851 test Loss 18.9179 with MSE metric 15939.7858\n",
      "Epoch 294 batch 230 train Loss 39.1805 test Loss 18.9160 with MSE metric 15938.6030\n",
      "Epoch 294 batch 240 train Loss 39.1760 test Loss 18.9142 with MSE metric 15937.4536\n",
      "Time taken for 1 epoch: 27.769505977630615 secs\n",
      "\n",
      "Epoch 295 batch 0 train Loss 39.1714 test Loss 18.9124 with MSE metric 15936.3517\n",
      "Epoch 295 batch 10 train Loss 39.1668 test Loss 18.9105 with MSE metric 15935.2182\n",
      "Epoch 295 batch 20 train Loss 39.1622 test Loss 18.9087 with MSE metric 15934.0669\n",
      "Epoch 295 batch 30 train Loss 39.1576 test Loss 18.9069 with MSE metric 15932.8975\n",
      "Epoch 295 batch 40 train Loss 39.1530 test Loss 18.9050 with MSE metric 15931.7513\n",
      "Epoch 295 batch 50 train Loss 39.1484 test Loss 18.9032 with MSE metric 15930.6374\n",
      "Epoch 295 batch 60 train Loss 39.1438 test Loss 18.9014 with MSE metric 15929.5068\n",
      "Epoch 295 batch 70 train Loss 39.1393 test Loss 18.8995 with MSE metric 15928.3629\n",
      "Epoch 295 batch 80 train Loss 39.1347 test Loss 18.8977 with MSE metric 15927.2187\n",
      "Epoch 295 batch 90 train Loss 39.1301 test Loss 18.8959 with MSE metric 15926.0680\n",
      "Epoch 295 batch 100 train Loss 39.1255 test Loss 18.8941 with MSE metric 15924.9823\n",
      "Epoch 295 batch 110 train Loss 39.1209 test Loss 18.8923 with MSE metric 15923.8741\n",
      "Epoch 295 batch 120 train Loss 39.1164 test Loss 18.8904 with MSE metric 15922.7585\n",
      "Epoch 295 batch 130 train Loss 39.1118 test Loss 18.8886 with MSE metric 15921.5987\n",
      "Epoch 295 batch 140 train Loss 39.1072 test Loss 18.8868 with MSE metric 15920.4811\n",
      "Epoch 295 batch 150 train Loss 39.1026 test Loss 18.8850 with MSE metric 15919.3234\n",
      "Epoch 295 batch 160 train Loss 39.0981 test Loss 18.8832 with MSE metric 15918.1729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295 batch 170 train Loss 39.0935 test Loss 18.8813 with MSE metric 15917.0291\n",
      "Epoch 295 batch 180 train Loss 39.0889 test Loss 18.8795 with MSE metric 15915.9363\n",
      "Epoch 295 batch 190 train Loss 39.0844 test Loss 18.8777 with MSE metric 15914.7647\n",
      "Epoch 295 batch 200 train Loss 39.0798 test Loss 18.8759 with MSE metric 15913.6542\n",
      "Epoch 295 batch 210 train Loss 39.0752 test Loss 18.8741 with MSE metric 15912.5253\n",
      "Epoch 295 batch 220 train Loss 39.0707 test Loss 18.8722 with MSE metric 15911.3753\n",
      "Epoch 295 batch 230 train Loss 39.0661 test Loss 18.8704 with MSE metric 15910.2709\n",
      "Epoch 295 batch 240 train Loss 39.0615 test Loss 18.8686 with MSE metric 15909.1341\n",
      "Time taken for 1 epoch: 28.21332311630249 secs\n",
      "\n",
      "Epoch 296 batch 0 train Loss 39.0570 test Loss 18.8668 with MSE metric 15907.9902\n",
      "Epoch 296 batch 10 train Loss 39.0524 test Loss 18.8650 with MSE metric 15906.8364\n",
      "Epoch 296 batch 20 train Loss 39.0479 test Loss 18.8631 with MSE metric 15905.7419\n",
      "Epoch 296 batch 30 train Loss 39.0433 test Loss 18.8613 with MSE metric 15904.6398\n",
      "Epoch 296 batch 40 train Loss 39.0387 test Loss 18.8595 with MSE metric 15903.5027\n",
      "Epoch 296 batch 50 train Loss 39.0342 test Loss 18.8577 with MSE metric 15902.3786\n",
      "Epoch 296 batch 60 train Loss 39.0296 test Loss 18.8558 with MSE metric 15901.2437\n",
      "Epoch 296 batch 70 train Loss 39.0251 test Loss 18.8540 with MSE metric 15900.0711\n",
      "Epoch 296 batch 80 train Loss 39.0205 test Loss 18.8522 with MSE metric 15898.9207\n",
      "Epoch 296 batch 90 train Loss 39.0160 test Loss 18.8504 with MSE metric 15897.8035\n",
      "Epoch 296 batch 100 train Loss 39.0114 test Loss 18.8486 with MSE metric 15896.6759\n",
      "Epoch 296 batch 110 train Loss 39.0069 test Loss 18.8468 with MSE metric 15895.5189\n",
      "Epoch 296 batch 120 train Loss 39.0023 test Loss 18.8449 with MSE metric 15894.3638\n",
      "Epoch 296 batch 130 train Loss 38.9978 test Loss 18.8431 with MSE metric 15893.2451\n",
      "Epoch 296 batch 140 train Loss 38.9932 test Loss 18.8413 with MSE metric 15892.1280\n",
      "Epoch 296 batch 150 train Loss 38.9887 test Loss 18.8395 with MSE metric 15890.9667\n",
      "Epoch 296 batch 160 train Loss 38.9842 test Loss 18.8377 with MSE metric 15889.8671\n",
      "Epoch 296 batch 170 train Loss 38.9796 test Loss 18.8359 with MSE metric 15888.7545\n",
      "Epoch 296 batch 180 train Loss 38.9751 test Loss 18.8341 with MSE metric 15887.6373\n",
      "Epoch 296 batch 190 train Loss 38.9705 test Loss 18.8322 with MSE metric 15886.5091\n",
      "Epoch 296 batch 200 train Loss 38.9660 test Loss 18.8304 with MSE metric 15885.3727\n",
      "Epoch 296 batch 210 train Loss 38.9615 test Loss 18.8286 with MSE metric 15884.2076\n",
      "Epoch 296 batch 220 train Loss 38.9569 test Loss 18.8268 with MSE metric 15883.0572\n",
      "Epoch 296 batch 230 train Loss 38.9524 test Loss 18.8250 with MSE metric 15881.9344\n",
      "Epoch 296 batch 240 train Loss 38.9479 test Loss 18.8232 with MSE metric 15880.8146\n",
      "Time taken for 1 epoch: 27.710557222366333 secs\n",
      "\n",
      "Epoch 297 batch 0 train Loss 38.9433 test Loss 18.8213 with MSE metric 15879.7110\n",
      "Epoch 297 batch 10 train Loss 38.9388 test Loss 18.8195 with MSE metric 15878.5678\n",
      "Epoch 297 batch 20 train Loss 38.9343 test Loss 18.8177 with MSE metric 15877.4453\n",
      "Epoch 297 batch 30 train Loss 38.9298 test Loss 18.8159 with MSE metric 15876.3405\n",
      "Epoch 297 batch 40 train Loss 38.9252 test Loss 18.8141 with MSE metric 15875.2041\n",
      "Epoch 297 batch 50 train Loss 38.9207 test Loss 18.8123 with MSE metric 15874.0696\n",
      "Epoch 297 batch 60 train Loss 38.9162 test Loss 18.8105 with MSE metric 15872.9292\n",
      "Epoch 297 batch 70 train Loss 38.9117 test Loss 18.8087 with MSE metric 15871.8344\n",
      "Epoch 297 batch 80 train Loss 38.9071 test Loss 18.8069 with MSE metric 15870.6773\n",
      "Epoch 297 batch 90 train Loss 38.9026 test Loss 18.8051 with MSE metric 15869.5388\n",
      "Epoch 297 batch 100 train Loss 38.8981 test Loss 18.8033 with MSE metric 15868.4561\n",
      "Epoch 297 batch 110 train Loss 38.8936 test Loss 18.8015 with MSE metric 15867.2765\n",
      "Epoch 297 batch 120 train Loss 38.8891 test Loss 18.7997 with MSE metric 15866.1612\n",
      "Epoch 297 batch 130 train Loss 38.8845 test Loss 18.7979 with MSE metric 15865.0358\n",
      "Epoch 297 batch 140 train Loss 38.8800 test Loss 18.7961 with MSE metric 15863.9173\n",
      "Epoch 297 batch 150 train Loss 38.8755 test Loss 18.7943 with MSE metric 15862.8373\n",
      "Epoch 297 batch 160 train Loss 38.8710 test Loss 18.7925 with MSE metric 15861.7285\n",
      "Epoch 297 batch 170 train Loss 38.8665 test Loss 18.7907 with MSE metric 15860.6225\n",
      "Epoch 297 batch 180 train Loss 38.8620 test Loss 18.7889 with MSE metric 15859.4938\n",
      "Epoch 297 batch 190 train Loss 38.8575 test Loss 18.7871 with MSE metric 15858.3496\n",
      "Epoch 297 batch 200 train Loss 38.8530 test Loss 18.7853 with MSE metric 15857.2527\n",
      "Epoch 297 batch 210 train Loss 38.8485 test Loss 18.7835 with MSE metric 15856.1172\n",
      "Epoch 297 batch 220 train Loss 38.8440 test Loss 18.7817 with MSE metric 15855.0227\n",
      "Epoch 297 batch 230 train Loss 38.8395 test Loss 18.7799 with MSE metric 15853.9081\n",
      "Epoch 297 batch 240 train Loss 38.8350 test Loss 18.7781 with MSE metric 15852.7626\n",
      "Time taken for 1 epoch: 27.431920051574707 secs\n",
      "\n",
      "Epoch 298 batch 0 train Loss 38.8305 test Loss 18.7763 with MSE metric 15851.6817\n",
      "Epoch 298 batch 10 train Loss 38.8260 test Loss 18.7745 with MSE metric 15850.5696\n",
      "Epoch 298 batch 20 train Loss 38.8215 test Loss 18.7727 with MSE metric 15849.4978\n",
      "Epoch 298 batch 30 train Loss 38.8170 test Loss 18.7709 with MSE metric 15848.3968\n",
      "Epoch 298 batch 40 train Loss 38.8125 test Loss 18.7691 with MSE metric 15847.2909\n",
      "Epoch 298 batch 50 train Loss 38.8080 test Loss 18.7673 with MSE metric 15846.1275\n",
      "Epoch 298 batch 60 train Loss 38.8035 test Loss 18.7655 with MSE metric 15845.0163\n",
      "Epoch 298 batch 70 train Loss 38.7990 test Loss 18.7637 with MSE metric 15843.9061\n",
      "Epoch 298 batch 80 train Loss 38.7945 test Loss 18.7619 with MSE metric 15842.8215\n",
      "Epoch 298 batch 90 train Loss 38.7900 test Loss 18.7601 with MSE metric 15841.6692\n",
      "Epoch 298 batch 100 train Loss 38.7855 test Loss 18.7583 with MSE metric 15840.5445\n",
      "Epoch 298 batch 110 train Loss 38.7811 test Loss 18.7565 with MSE metric 15839.4440\n",
      "Epoch 298 batch 120 train Loss 38.7766 test Loss 18.7547 with MSE metric 15838.3625\n",
      "Epoch 298 batch 130 train Loss 38.7721 test Loss 18.7530 with MSE metric 15837.2414\n",
      "Epoch 298 batch 140 train Loss 38.7676 test Loss 18.7512 with MSE metric 15836.1672\n",
      "Epoch 298 batch 150 train Loss 38.7631 test Loss 18.7494 with MSE metric 15835.0933\n",
      "Epoch 298 batch 160 train Loss 38.7586 test Loss 18.7476 with MSE metric 15834.0085\n",
      "Epoch 298 batch 170 train Loss 38.7542 test Loss 18.7458 with MSE metric 15832.8935\n",
      "Epoch 298 batch 180 train Loss 38.7497 test Loss 18.7440 with MSE metric 15831.7896\n",
      "Epoch 298 batch 190 train Loss 38.7452 test Loss 18.7422 with MSE metric 15830.6930\n",
      "Epoch 298 batch 200 train Loss 38.7407 test Loss 18.7404 with MSE metric 15829.6011\n",
      "Epoch 298 batch 210 train Loss 38.7363 test Loss 18.7386 with MSE metric 15828.4592\n",
      "Epoch 298 batch 220 train Loss 38.7318 test Loss 18.7368 with MSE metric 15827.3786\n",
      "Epoch 298 batch 230 train Loss 38.7273 test Loss 18.7350 with MSE metric 15826.2711\n",
      "Epoch 298 batch 240 train Loss 38.7228 test Loss 18.7332 with MSE metric 15825.1763\n",
      "Time taken for 1 epoch: 30.302313327789307 secs\n",
      "\n",
      "Epoch 299 batch 0 train Loss 38.7184 test Loss 18.7315 with MSE metric 15824.0832\n",
      "Epoch 299 batch 10 train Loss 38.7139 test Loss 18.7297 with MSE metric 15822.9399\n",
      "Epoch 299 batch 20 train Loss 38.7094 test Loss 18.7279 with MSE metric 15821.8459\n",
      "Epoch 299 batch 30 train Loss 38.7050 test Loss 18.7261 with MSE metric 15820.7162\n",
      "Epoch 299 batch 40 train Loss 38.7005 test Loss 18.7243 with MSE metric 15819.5873\n",
      "Epoch 299 batch 50 train Loss 38.6960 test Loss 18.7225 with MSE metric 15818.4467\n",
      "Epoch 299 batch 60 train Loss 38.6916 test Loss 18.7207 with MSE metric 15817.3200\n",
      "Epoch 299 batch 70 train Loss 38.6871 test Loss 18.7190 with MSE metric 15816.2469\n",
      "Epoch 299 batch 80 train Loss 38.6826 test Loss 18.7172 with MSE metric 15815.0915\n",
      "Epoch 299 batch 90 train Loss 38.6782 test Loss 18.7154 with MSE metric 15814.0611\n",
      "Epoch 299 batch 100 train Loss 38.6737 test Loss 18.7136 with MSE metric 15812.9376\n",
      "Epoch 299 batch 110 train Loss 38.6693 test Loss 18.7118 with MSE metric 15811.8160\n",
      "Epoch 299 batch 120 train Loss 38.6648 test Loss 18.7101 with MSE metric 15810.7587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 batch 130 train Loss 38.6604 test Loss 18.7083 with MSE metric 15809.6577\n",
      "Epoch 299 batch 140 train Loss 38.6559 test Loss 18.7065 with MSE metric 15808.5291\n",
      "Epoch 299 batch 150 train Loss 38.6515 test Loss 18.7047 with MSE metric 15807.4476\n",
      "Epoch 299 batch 160 train Loss 38.6470 test Loss 18.7029 with MSE metric 15806.3349\n",
      "Epoch 299 batch 170 train Loss 38.6426 test Loss 18.7012 with MSE metric 15805.2230\n",
      "Epoch 299 batch 180 train Loss 38.6381 test Loss 18.6994 with MSE metric 15804.0884\n",
      "Epoch 299 batch 190 train Loss 38.6337 test Loss 18.6976 with MSE metric 15802.9747\n",
      "Epoch 299 batch 200 train Loss 38.6292 test Loss 18.6958 with MSE metric 15801.8719\n",
      "Epoch 299 batch 210 train Loss 38.6248 test Loss 18.6940 with MSE metric 15800.7540\n",
      "Epoch 299 batch 220 train Loss 38.6203 test Loss 18.6923 with MSE metric 15799.6464\n",
      "Epoch 299 batch 230 train Loss 38.6159 test Loss 18.6905 with MSE metric 15798.5558\n",
      "Epoch 299 batch 240 train Loss 38.6114 test Loss 18.6887 with MSE metric 15797.5055\n",
      "Time taken for 1 epoch: 28.635802268981934 secs\n",
      "\n",
      "Epoch 300 batch 0 train Loss 38.6070 test Loss 18.6869 with MSE metric 15796.4367\n",
      "Epoch 300 batch 10 train Loss 38.6026 test Loss 18.6852 with MSE metric 15795.3239\n",
      "Epoch 300 batch 20 train Loss 38.5981 test Loss 18.6834 with MSE metric 15794.2472\n",
      "Epoch 300 batch 30 train Loss 38.5937 test Loss 18.6816 with MSE metric 15793.1476\n",
      "Epoch 300 batch 40 train Loss 38.5893 test Loss 18.6798 with MSE metric 15792.0836\n",
      "Epoch 300 batch 50 train Loss 38.5848 test Loss 18.6781 with MSE metric 15791.0361\n",
      "Epoch 300 batch 60 train Loss 38.5804 test Loss 18.6763 with MSE metric 15789.9745\n",
      "Epoch 300 batch 70 train Loss 38.5760 test Loss 18.6745 with MSE metric 15788.8775\n",
      "Epoch 300 batch 80 train Loss 38.5715 test Loss 18.6728 with MSE metric 15787.7979\n",
      "Epoch 300 batch 90 train Loss 38.5671 test Loss 18.6710 with MSE metric 15786.6847\n",
      "Epoch 300 batch 100 train Loss 38.5627 test Loss 18.6692 with MSE metric 15785.5966\n",
      "Epoch 300 batch 110 train Loss 38.5582 test Loss 18.6674 with MSE metric 15784.4931\n",
      "Epoch 300 batch 120 train Loss 38.5538 test Loss 18.6657 with MSE metric 15783.3801\n",
      "Epoch 300 batch 130 train Loss 38.5494 test Loss 18.6639 with MSE metric 15782.2695\n",
      "Epoch 300 batch 140 train Loss 38.5450 test Loss 18.6621 with MSE metric 15781.2334\n",
      "Epoch 300 batch 150 train Loss 38.5406 test Loss 18.6604 with MSE metric 15780.1202\n",
      "Epoch 300 batch 160 train Loss 38.5361 test Loss 18.6586 with MSE metric 15779.0444\n",
      "Epoch 300 batch 170 train Loss 38.5317 test Loss 18.6568 with MSE metric 15777.9759\n",
      "Epoch 300 batch 180 train Loss 38.5273 test Loss 18.6551 with MSE metric 15776.9067\n",
      "Epoch 300 batch 190 train Loss 38.5229 test Loss 18.6533 with MSE metric 15775.8195\n",
      "Epoch 300 batch 200 train Loss 38.5185 test Loss 18.6515 with MSE metric 15774.7339\n",
      "Epoch 300 batch 210 train Loss 38.5141 test Loss 18.6498 with MSE metric 15773.6340\n",
      "Epoch 300 batch 220 train Loss 38.5096 test Loss 18.6480 with MSE metric 15772.5600\n",
      "Epoch 300 batch 230 train Loss 38.5052 test Loss 18.6462 with MSE metric 15771.4877\n",
      "Epoch 300 batch 240 train Loss 38.5008 test Loss 18.6445 with MSE metric 15770.3704\n",
      "Time taken for 1 epoch: 28.72900700569153 secs\n",
      "\n",
      "Epoch 301 batch 0 train Loss 38.4964 test Loss 18.6427 with MSE metric 15769.2905\n",
      "Epoch 301 batch 10 train Loss 38.4920 test Loss 18.6409 with MSE metric 15768.1971\n",
      "Epoch 301 batch 20 train Loss 38.4876 test Loss 18.6392 with MSE metric 15767.0826\n",
      "Epoch 301 batch 30 train Loss 38.4832 test Loss 18.6374 with MSE metric 15765.9833\n",
      "Epoch 301 batch 40 train Loss 38.4788 test Loss 18.6357 with MSE metric 15764.8302\n",
      "Epoch 301 batch 50 train Loss 38.4744 test Loss 18.6339 with MSE metric 15763.7309\n",
      "Epoch 301 batch 60 train Loss 38.4700 test Loss 18.6321 with MSE metric 15762.6205\n",
      "Epoch 301 batch 70 train Loss 38.4656 test Loss 18.6304 with MSE metric 15761.5761\n",
      "Epoch 301 batch 80 train Loss 38.4611 test Loss 18.6286 with MSE metric 15760.4598\n",
      "Epoch 301 batch 90 train Loss 38.4567 test Loss 18.6269 with MSE metric 15759.3362\n",
      "Epoch 301 batch 100 train Loss 38.4523 test Loss 18.6251 with MSE metric 15758.2669\n",
      "Epoch 301 batch 110 train Loss 38.4480 test Loss 18.6234 with MSE metric 15757.2027\n",
      "Epoch 301 batch 120 train Loss 38.4436 test Loss 18.6216 with MSE metric 15756.1486\n",
      "Epoch 301 batch 130 train Loss 38.4392 test Loss 18.6198 with MSE metric 15755.1033\n",
      "Epoch 301 batch 140 train Loss 38.4348 test Loss 18.6181 with MSE metric 15754.0527\n",
      "Epoch 301 batch 150 train Loss 38.4304 test Loss 18.6164 with MSE metric 15752.9419\n",
      "Epoch 301 batch 160 train Loss 38.4260 test Loss 18.6146 with MSE metric 15751.8716\n",
      "Epoch 301 batch 170 train Loss 38.4216 test Loss 18.6128 with MSE metric 15750.7977\n",
      "Epoch 301 batch 180 train Loss 38.4172 test Loss 18.6111 with MSE metric 15749.7157\n",
      "Epoch 301 batch 190 train Loss 38.4128 test Loss 18.6093 with MSE metric 15748.6602\n",
      "Epoch 301 batch 200 train Loss 38.4084 test Loss 18.6076 with MSE metric 15747.5932\n",
      "Epoch 301 batch 210 train Loss 38.4040 test Loss 18.6058 with MSE metric 15746.4894\n",
      "Epoch 301 batch 220 train Loss 38.3997 test Loss 18.6041 with MSE metric 15745.4388\n",
      "Epoch 301 batch 230 train Loss 38.3953 test Loss 18.6024 with MSE metric 15744.3768\n",
      "Epoch 301 batch 240 train Loss 38.3909 test Loss 18.6006 with MSE metric 15743.3344\n",
      "Time taken for 1 epoch: 27.71855878829956 secs\n",
      "\n",
      "Epoch 302 batch 0 train Loss 38.3865 test Loss 18.5989 with MSE metric 15742.2545\n",
      "Epoch 302 batch 10 train Loss 38.3821 test Loss 18.5971 with MSE metric 15741.1805\n",
      "Epoch 302 batch 20 train Loss 38.3778 test Loss 18.5953 with MSE metric 15740.1302\n",
      "Epoch 302 batch 30 train Loss 38.3734 test Loss 18.5936 with MSE metric 15739.0724\n",
      "Epoch 302 batch 40 train Loss 38.3690 test Loss 18.5918 with MSE metric 15737.9855\n",
      "Epoch 302 batch 50 train Loss 38.3646 test Loss 18.5901 with MSE metric 15736.8757\n",
      "Epoch 302 batch 60 train Loss 38.3603 test Loss 18.5883 with MSE metric 15735.8020\n",
      "Epoch 302 batch 70 train Loss 38.3559 test Loss 18.5866 with MSE metric 15734.7166\n",
      "Epoch 302 batch 80 train Loss 38.3515 test Loss 18.5848 with MSE metric 15733.5879\n",
      "Epoch 302 batch 90 train Loss 38.3471 test Loss 18.5831 with MSE metric 15732.4867\n",
      "Epoch 302 batch 100 train Loss 38.3428 test Loss 18.5813 with MSE metric 15731.3872\n",
      "Epoch 302 batch 110 train Loss 38.3384 test Loss 18.5796 with MSE metric 15730.3271\n",
      "Epoch 302 batch 120 train Loss 38.3340 test Loss 18.5778 with MSE metric 15729.2398\n",
      "Epoch 302 batch 130 train Loss 38.3297 test Loss 18.5761 with MSE metric 15728.1391\n",
      "Epoch 302 batch 140 train Loss 38.3253 test Loss 18.5743 with MSE metric 15727.0722\n",
      "Epoch 302 batch 150 train Loss 38.3209 test Loss 18.5726 with MSE metric 15725.9835\n",
      "Epoch 302 batch 160 train Loss 38.3166 test Loss 18.5709 with MSE metric 15724.8616\n",
      "Epoch 302 batch 170 train Loss 38.3122 test Loss 18.5691 with MSE metric 15723.7692\n",
      "Epoch 302 batch 180 train Loss 38.3078 test Loss 18.5674 with MSE metric 15722.6743\n",
      "Epoch 302 batch 190 train Loss 38.3035 test Loss 18.5657 with MSE metric 15721.5764\n",
      "Epoch 302 batch 200 train Loss 38.2991 test Loss 18.5639 with MSE metric 15720.5103\n",
      "Epoch 302 batch 210 train Loss 38.2948 test Loss 18.5622 with MSE metric 15719.4169\n",
      "Epoch 302 batch 220 train Loss 38.2904 test Loss 18.5604 with MSE metric 15718.3703\n",
      "Epoch 302 batch 230 train Loss 38.2861 test Loss 18.5587 with MSE metric 15717.3306\n",
      "Epoch 302 batch 240 train Loss 38.2817 test Loss 18.5569 with MSE metric 15716.2398\n",
      "Time taken for 1 epoch: 28.965301990509033 secs\n",
      "\n",
      "Epoch 303 batch 0 train Loss 38.2774 test Loss 18.5552 with MSE metric 15715.1784\n",
      "Epoch 303 batch 10 train Loss 38.2730 test Loss 18.5535 with MSE metric 15714.1160\n",
      "Epoch 303 batch 20 train Loss 38.2687 test Loss 18.5517 with MSE metric 15713.0225\n",
      "Epoch 303 batch 30 train Loss 38.2643 test Loss 18.5500 with MSE metric 15711.9611\n",
      "Epoch 303 batch 40 train Loss 38.2600 test Loss 18.5483 with MSE metric 15710.9256\n",
      "Epoch 303 batch 50 train Loss 38.2556 test Loss 18.5465 with MSE metric 15709.8797\n",
      "Epoch 303 batch 60 train Loss 38.2513 test Loss 18.5448 with MSE metric 15708.8182\n",
      "Epoch 303 batch 70 train Loss 38.2469 test Loss 18.5430 with MSE metric 15707.7141\n",
      "Epoch 303 batch 80 train Loss 38.2426 test Loss 18.5413 with MSE metric 15706.5765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303 batch 90 train Loss 38.2382 test Loss 18.5396 with MSE metric 15705.5259\n",
      "Epoch 303 batch 100 train Loss 38.2339 test Loss 18.5378 with MSE metric 15704.4679\n",
      "Epoch 303 batch 110 train Loss 38.2296 test Loss 18.5361 with MSE metric 15703.4154\n",
      "Epoch 303 batch 120 train Loss 38.2252 test Loss 18.5344 with MSE metric 15702.4146\n",
      "Epoch 303 batch 130 train Loss 38.2209 test Loss 18.5326 with MSE metric 15701.3390\n",
      "Epoch 303 batch 140 train Loss 38.2166 test Loss 18.5309 with MSE metric 15700.2852\n",
      "Epoch 303 batch 150 train Loss 38.2122 test Loss 18.5291 with MSE metric 15699.1914\n",
      "Epoch 303 batch 160 train Loss 38.2079 test Loss 18.5274 with MSE metric 15698.1777\n",
      "Epoch 303 batch 170 train Loss 38.2036 test Loss 18.5257 with MSE metric 15697.0975\n",
      "Epoch 303 batch 180 train Loss 38.1992 test Loss 18.5239 with MSE metric 15696.0376\n",
      "Epoch 303 batch 190 train Loss 38.1949 test Loss 18.5222 with MSE metric 15694.9763\n",
      "Epoch 303 batch 200 train Loss 38.1906 test Loss 18.5204 with MSE metric 15693.9216\n",
      "Epoch 303 batch 210 train Loss 38.1862 test Loss 18.5187 with MSE metric 15692.8371\n",
      "Epoch 303 batch 220 train Loss 38.1819 test Loss 18.5170 with MSE metric 15691.7747\n",
      "Epoch 303 batch 230 train Loss 38.1776 test Loss 18.5153 with MSE metric 15690.6874\n",
      "Epoch 303 batch 240 train Loss 38.1733 test Loss 18.5135 with MSE metric 15689.6228\n",
      "Time taken for 1 epoch: 27.726426124572754 secs\n",
      "\n",
      "Epoch 304 batch 0 train Loss 38.1689 test Loss 18.5118 with MSE metric 15688.5247\n",
      "Epoch 304 batch 10 train Loss 38.1646 test Loss 18.5101 with MSE metric 15687.4349\n",
      "Epoch 304 batch 20 train Loss 38.1603 test Loss 18.5083 with MSE metric 15686.3690\n",
      "Epoch 304 batch 30 train Loss 38.1560 test Loss 18.5066 with MSE metric 15685.3553\n",
      "Epoch 304 batch 40 train Loss 38.1517 test Loss 18.5049 with MSE metric 15684.3179\n",
      "Epoch 304 batch 50 train Loss 38.1473 test Loss 18.5032 with MSE metric 15683.2748\n",
      "Epoch 304 batch 60 train Loss 38.1430 test Loss 18.5014 with MSE metric 15682.2132\n",
      "Epoch 304 batch 70 train Loss 38.1387 test Loss 18.4997 with MSE metric 15681.1392\n",
      "Epoch 304 batch 80 train Loss 38.1344 test Loss 18.4980 with MSE metric 15680.0833\n",
      "Epoch 304 batch 90 train Loss 38.1301 test Loss 18.4963 with MSE metric 15679.0344\n",
      "Epoch 304 batch 100 train Loss 38.1258 test Loss 18.4945 with MSE metric 15677.9876\n",
      "Epoch 304 batch 110 train Loss 38.1215 test Loss 18.4928 with MSE metric 15676.8999\n",
      "Epoch 304 batch 120 train Loss 38.1171 test Loss 18.4911 with MSE metric 15675.8289\n",
      "Epoch 304 batch 130 train Loss 38.1128 test Loss 18.4894 with MSE metric 15674.7977\n",
      "Epoch 304 batch 140 train Loss 38.1085 test Loss 18.4877 with MSE metric 15673.7639\n",
      "Epoch 304 batch 150 train Loss 38.1042 test Loss 18.4859 with MSE metric 15672.6502\n",
      "Epoch 304 batch 160 train Loss 38.0999 test Loss 18.4842 with MSE metric 15671.5736\n",
      "Epoch 304 batch 170 train Loss 38.0956 test Loss 18.4825 with MSE metric 15670.5282\n",
      "Epoch 304 batch 180 train Loss 38.0913 test Loss 18.4808 with MSE metric 15669.4439\n",
      "Epoch 304 batch 190 train Loss 38.0870 test Loss 18.4791 with MSE metric 15668.4215\n",
      "Epoch 304 batch 200 train Loss 38.0827 test Loss 18.4774 with MSE metric 15667.3815\n",
      "Epoch 304 batch 210 train Loss 38.0784 test Loss 18.4756 with MSE metric 15666.3055\n",
      "Epoch 304 batch 220 train Loss 38.0741 test Loss 18.4739 with MSE metric 15665.2205\n",
      "Epoch 304 batch 230 train Loss 38.0698 test Loss 18.4722 with MSE metric 15664.1557\n",
      "Epoch 304 batch 240 train Loss 38.0655 test Loss 18.4705 with MSE metric 15663.0608\n",
      "Time taken for 1 epoch: 26.624773263931274 secs\n",
      "\n",
      "Epoch 305 batch 0 train Loss 38.0612 test Loss 18.4688 with MSE metric 15661.9864\n",
      "Epoch 305 batch 10 train Loss 38.0569 test Loss 18.4670 with MSE metric 15660.9277\n",
      "Epoch 305 batch 20 train Loss 38.0526 test Loss 18.4653 with MSE metric 15659.8326\n",
      "Epoch 305 batch 30 train Loss 38.0483 test Loss 18.4636 with MSE metric 15658.7588\n",
      "Epoch 305 batch 40 train Loss 38.0440 test Loss 18.4619 with MSE metric 15657.6705\n",
      "Epoch 305 batch 50 train Loss 38.0397 test Loss 18.4602 with MSE metric 15656.5854\n",
      "Epoch 305 batch 60 train Loss 38.0354 test Loss 18.4585 with MSE metric 15655.5553\n",
      "Epoch 305 batch 70 train Loss 38.0312 test Loss 18.4568 with MSE metric 15654.5126\n",
      "Epoch 305 batch 80 train Loss 38.0269 test Loss 18.4550 with MSE metric 15653.4465\n",
      "Epoch 305 batch 90 train Loss 38.0226 test Loss 18.4533 with MSE metric 15652.3877\n",
      "Epoch 305 batch 100 train Loss 38.0183 test Loss 18.4516 with MSE metric 15651.3894\n",
      "Epoch 305 batch 110 train Loss 38.0140 test Loss 18.4499 with MSE metric 15650.3660\n",
      "Epoch 305 batch 120 train Loss 38.0097 test Loss 18.4482 with MSE metric 15649.3369\n",
      "Epoch 305 batch 130 train Loss 38.0055 test Loss 18.4465 with MSE metric 15648.3195\n",
      "Epoch 305 batch 140 train Loss 38.0012 test Loss 18.4448 with MSE metric 15647.2448\n",
      "Epoch 305 batch 150 train Loss 37.9969 test Loss 18.4431 with MSE metric 15646.1759\n",
      "Epoch 305 batch 160 train Loss 37.9926 test Loss 18.4414 with MSE metric 15645.1226\n",
      "Epoch 305 batch 170 train Loss 37.9884 test Loss 18.4397 with MSE metric 15644.0921\n",
      "Epoch 305 batch 180 train Loss 37.9841 test Loss 18.4380 with MSE metric 15643.0561\n",
      "Epoch 305 batch 190 train Loss 37.9798 test Loss 18.4363 with MSE metric 15641.9620\n",
      "Epoch 305 batch 200 train Loss 37.9755 test Loss 18.4346 with MSE metric 15640.8700\n",
      "Epoch 305 batch 210 train Loss 37.9713 test Loss 18.4329 with MSE metric 15639.8002\n",
      "Epoch 305 batch 220 train Loss 37.9670 test Loss 18.4312 with MSE metric 15638.6984\n",
      "Epoch 305 batch 230 train Loss 37.9627 test Loss 18.4295 with MSE metric 15637.6848\n",
      "Epoch 305 batch 240 train Loss 37.9585 test Loss 18.4278 with MSE metric 15636.6526\n",
      "Time taken for 1 epoch: 26.574759244918823 secs\n",
      "\n",
      "Epoch 306 batch 0 train Loss 37.9542 test Loss 18.4261 with MSE metric 15635.6383\n",
      "Epoch 306 batch 10 train Loss 37.9499 test Loss 18.4244 with MSE metric 15634.5712\n",
      "Epoch 306 batch 20 train Loss 37.9457 test Loss 18.4227 with MSE metric 15633.5396\n",
      "Epoch 306 batch 30 train Loss 37.9414 test Loss 18.4210 with MSE metric 15632.4715\n",
      "Epoch 306 batch 40 train Loss 37.9371 test Loss 18.4192 with MSE metric 15631.4151\n",
      "Epoch 306 batch 50 train Loss 37.9329 test Loss 18.4175 with MSE metric 15630.3473\n",
      "Epoch 306 batch 60 train Loss 37.9286 test Loss 18.4158 with MSE metric 15629.3094\n",
      "Epoch 306 batch 70 train Loss 37.9243 test Loss 18.4141 with MSE metric 15628.2516\n",
      "Epoch 306 batch 80 train Loss 37.9201 test Loss 18.4124 with MSE metric 15627.1774\n",
      "Epoch 306 batch 90 train Loss 37.9158 test Loss 18.4107 with MSE metric 15626.1159\n",
      "Epoch 306 batch 100 train Loss 37.9116 test Loss 18.4090 with MSE metric 15625.0387\n",
      "Epoch 306 batch 110 train Loss 37.9073 test Loss 18.4073 with MSE metric 15624.0183\n",
      "Epoch 306 batch 120 train Loss 37.9031 test Loss 18.4056 with MSE metric 15622.9638\n",
      "Epoch 306 batch 130 train Loss 37.8988 test Loss 18.4039 with MSE metric 15621.8841\n",
      "Epoch 306 batch 140 train Loss 37.8946 test Loss 18.4022 with MSE metric 15620.8587\n",
      "Epoch 306 batch 150 train Loss 37.8903 test Loss 18.4005 with MSE metric 15619.8413\n",
      "Epoch 306 batch 160 train Loss 37.8861 test Loss 18.3988 with MSE metric 15618.7798\n",
      "Epoch 306 batch 170 train Loss 37.8818 test Loss 18.3971 with MSE metric 15617.6805\n",
      "Epoch 306 batch 180 train Loss 37.8776 test Loss 18.3954 with MSE metric 15616.6142\n",
      "Epoch 306 batch 190 train Loss 37.8733 test Loss 18.3937 with MSE metric 15615.5485\n",
      "Epoch 306 batch 200 train Loss 37.8691 test Loss 18.3920 with MSE metric 15614.5027\n",
      "Epoch 306 batch 210 train Loss 37.8648 test Loss 18.3903 with MSE metric 15613.4310\n",
      "Epoch 306 batch 220 train Loss 37.8606 test Loss 18.3886 with MSE metric 15612.4187\n",
      "Epoch 306 batch 230 train Loss 37.8563 test Loss 18.3869 with MSE metric 15611.3988\n",
      "Epoch 306 batch 240 train Loss 37.8521 test Loss 18.3852 with MSE metric 15610.3603\n",
      "Time taken for 1 epoch: 26.646708965301514 secs\n",
      "\n",
      "Epoch 307 batch 0 train Loss 37.8479 test Loss 18.3835 with MSE metric 15609.2861\n",
      "Epoch 307 batch 10 train Loss 37.8436 test Loss 18.3818 with MSE metric 15608.2100\n",
      "Epoch 307 batch 20 train Loss 37.8394 test Loss 18.3801 with MSE metric 15607.1682\n",
      "Epoch 307 batch 30 train Loss 37.8351 test Loss 18.3784 with MSE metric 15606.0793\n",
      "Epoch 307 batch 40 train Loss 37.8309 test Loss 18.3767 with MSE metric 15605.0619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307 batch 50 train Loss 37.8267 test Loss 18.3750 with MSE metric 15604.0286\n",
      "Epoch 307 batch 60 train Loss 37.8224 test Loss 18.3734 with MSE metric 15602.9377\n",
      "Epoch 307 batch 70 train Loss 37.8182 test Loss 18.3717 with MSE metric 15601.8876\n",
      "Epoch 307 batch 80 train Loss 37.8140 test Loss 18.3700 with MSE metric 15600.8577\n",
      "Epoch 307 batch 90 train Loss 37.8097 test Loss 18.3683 with MSE metric 15599.8000\n",
      "Epoch 307 batch 100 train Loss 37.8055 test Loss 18.3666 with MSE metric 15598.7266\n",
      "Epoch 307 batch 110 train Loss 37.8013 test Loss 18.3649 with MSE metric 15597.6383\n",
      "Epoch 307 batch 120 train Loss 37.7971 test Loss 18.3632 with MSE metric 15596.6181\n",
      "Epoch 307 batch 130 train Loss 37.7928 test Loss 18.3615 with MSE metric 15595.6192\n",
      "Epoch 307 batch 140 train Loss 37.7886 test Loss 18.3598 with MSE metric 15594.5430\n",
      "Epoch 307 batch 150 train Loss 37.7844 test Loss 18.3581 with MSE metric 15593.5492\n",
      "Epoch 307 batch 160 train Loss 37.7802 test Loss 18.3564 with MSE metric 15592.5043\n",
      "Epoch 307 batch 170 train Loss 37.7759 test Loss 18.3548 with MSE metric 15591.4858\n",
      "Epoch 307 batch 180 train Loss 37.7717 test Loss 18.3531 with MSE metric 15590.4506\n",
      "Epoch 307 batch 190 train Loss 37.7675 test Loss 18.3514 with MSE metric 15589.4585\n",
      "Epoch 307 batch 200 train Loss 37.7633 test Loss 18.3497 with MSE metric 15588.4372\n",
      "Epoch 307 batch 210 train Loss 37.7591 test Loss 18.3480 with MSE metric 15587.3744\n",
      "Epoch 307 batch 220 train Loss 37.7549 test Loss 18.3463 with MSE metric 15586.3226\n",
      "Epoch 307 batch 230 train Loss 37.7506 test Loss 18.3446 with MSE metric 15585.2707\n",
      "Epoch 307 batch 240 train Loss 37.7464 test Loss 18.3429 with MSE metric 15584.2061\n",
      "Time taken for 1 epoch: 27.150703191757202 secs\n",
      "\n",
      "Epoch 308 batch 0 train Loss 37.7422 test Loss 18.3413 with MSE metric 15583.1697\n",
      "Epoch 308 batch 10 train Loss 37.7380 test Loss 18.3396 with MSE metric 15582.1531\n",
      "Epoch 308 batch 20 train Loss 37.7338 test Loss 18.3379 with MSE metric 15581.1189\n",
      "Epoch 308 batch 30 train Loss 37.7296 test Loss 18.3362 with MSE metric 15580.0815\n",
      "Epoch 308 batch 40 train Loss 37.7254 test Loss 18.3345 with MSE metric 15579.0741\n",
      "Epoch 308 batch 50 train Loss 37.7212 test Loss 18.3328 with MSE metric 15578.0213\n",
      "Epoch 308 batch 60 train Loss 37.7170 test Loss 18.3312 with MSE metric 15576.9771\n",
      "Epoch 308 batch 70 train Loss 37.7128 test Loss 18.3295 with MSE metric 15575.9306\n",
      "Epoch 308 batch 80 train Loss 37.7086 test Loss 18.3278 with MSE metric 15574.8863\n",
      "Epoch 308 batch 90 train Loss 37.7044 test Loss 18.3261 with MSE metric 15573.8015\n",
      "Epoch 308 batch 100 train Loss 37.7002 test Loss 18.3245 with MSE metric 15572.7704\n",
      "Epoch 308 batch 110 train Loss 37.6960 test Loss 18.3228 with MSE metric 15571.7436\n",
      "Epoch 308 batch 120 train Loss 37.6918 test Loss 18.3211 with MSE metric 15570.6622\n",
      "Epoch 308 batch 130 train Loss 37.6876 test Loss 18.3194 with MSE metric 15569.6389\n",
      "Epoch 308 batch 140 train Loss 37.6834 test Loss 18.3177 with MSE metric 15568.6893\n",
      "Epoch 308 batch 150 train Loss 37.6792 test Loss 18.3161 with MSE metric 15567.6383\n",
      "Epoch 308 batch 160 train Loss 37.6750 test Loss 18.3144 with MSE metric 15566.6059\n",
      "Epoch 308 batch 170 train Loss 37.6708 test Loss 18.3127 with MSE metric 15565.5345\n",
      "Epoch 308 batch 180 train Loss 37.6666 test Loss 18.3111 with MSE metric 15564.4913\n",
      "Epoch 308 batch 190 train Loss 37.6624 test Loss 18.3094 with MSE metric 15563.4574\n",
      "Epoch 308 batch 200 train Loss 37.6582 test Loss 18.3077 with MSE metric 15562.4289\n",
      "Epoch 308 batch 210 train Loss 37.6540 test Loss 18.3060 with MSE metric 15561.4741\n",
      "Epoch 308 batch 220 train Loss 37.6498 test Loss 18.3044 with MSE metric 15560.4096\n",
      "Epoch 308 batch 230 train Loss 37.6457 test Loss 18.3027 with MSE metric 15559.4107\n",
      "Epoch 308 batch 240 train Loss 37.6415 test Loss 18.3010 with MSE metric 15558.4566\n",
      "Time taken for 1 epoch: 27.47056794166565 secs\n",
      "\n",
      "Epoch 309 batch 0 train Loss 37.6373 test Loss 18.2994 with MSE metric 15557.4631\n",
      "Epoch 309 batch 10 train Loss 37.6331 test Loss 18.2977 with MSE metric 15556.4371\n",
      "Epoch 309 batch 20 train Loss 37.6289 test Loss 18.2960 with MSE metric 15555.4108\n",
      "Epoch 309 batch 30 train Loss 37.6247 test Loss 18.2943 with MSE metric 15554.3744\n",
      "Epoch 309 batch 40 train Loss 37.6206 test Loss 18.2927 with MSE metric 15553.3571\n",
      "Epoch 309 batch 50 train Loss 37.6164 test Loss 18.2910 with MSE metric 15552.3401\n",
      "Epoch 309 batch 60 train Loss 37.6122 test Loss 18.2893 with MSE metric 15551.2733\n",
      "Epoch 309 batch 70 train Loss 37.6080 test Loss 18.2876 with MSE metric 15550.2468\n",
      "Epoch 309 batch 80 train Loss 37.6038 test Loss 18.2860 with MSE metric 15549.1852\n",
      "Epoch 309 batch 90 train Loss 37.5997 test Loss 18.2843 with MSE metric 15548.1716\n",
      "Epoch 309 batch 100 train Loss 37.5955 test Loss 18.2826 with MSE metric 15547.1503\n",
      "Epoch 309 batch 110 train Loss 37.5913 test Loss 18.2810 with MSE metric 15546.1631\n",
      "Epoch 309 batch 120 train Loss 37.5872 test Loss 18.2793 with MSE metric 15545.1032\n",
      "Epoch 309 batch 130 train Loss 37.5830 test Loss 18.2776 with MSE metric 15544.0876\n",
      "Epoch 309 batch 140 train Loss 37.5788 test Loss 18.2759 with MSE metric 15543.0707\n",
      "Epoch 309 batch 150 train Loss 37.5747 test Loss 18.2743 with MSE metric 15542.0745\n",
      "Epoch 309 batch 160 train Loss 37.5705 test Loss 18.2726 with MSE metric 15541.0648\n",
      "Epoch 309 batch 170 train Loss 37.5663 test Loss 18.2709 with MSE metric 15540.0716\n",
      "Epoch 309 batch 180 train Loss 37.5622 test Loss 18.2693 with MSE metric 15539.0776\n",
      "Epoch 309 batch 190 train Loss 37.5580 test Loss 18.2676 with MSE metric 15538.0180\n",
      "Epoch 309 batch 200 train Loss 37.5538 test Loss 18.2659 with MSE metric 15536.9800\n",
      "Epoch 309 batch 210 train Loss 37.5497 test Loss 18.2643 with MSE metric 15535.9421\n",
      "Epoch 309 batch 220 train Loss 37.5455 test Loss 18.2626 with MSE metric 15534.8647\n",
      "Epoch 309 batch 230 train Loss 37.5413 test Loss 18.2609 with MSE metric 15533.8479\n",
      "Epoch 309 batch 240 train Loss 37.5372 test Loss 18.2593 with MSE metric 15532.8665\n",
      "Time taken for 1 epoch: 27.30558490753174 secs\n",
      "\n",
      "Epoch 310 batch 0 train Loss 37.5330 test Loss 18.2576 with MSE metric 15531.8255\n",
      "Epoch 310 batch 10 train Loss 37.5289 test Loss 18.2559 with MSE metric 15530.7522\n",
      "Epoch 310 batch 20 train Loss 37.5247 test Loss 18.2543 with MSE metric 15529.6966\n",
      "Epoch 310 batch 30 train Loss 37.5205 test Loss 18.2526 with MSE metric 15528.6274\n",
      "Epoch 310 batch 40 train Loss 37.5164 test Loss 18.2510 with MSE metric 15527.5917\n",
      "Epoch 310 batch 50 train Loss 37.5122 test Loss 18.2493 with MSE metric 15526.5718\n",
      "Epoch 310 batch 60 train Loss 37.5081 test Loss 18.2476 with MSE metric 15525.5486\n",
      "Epoch 310 batch 70 train Loss 37.5039 test Loss 18.2460 with MSE metric 15524.5475\n",
      "Epoch 310 batch 80 train Loss 37.4998 test Loss 18.2443 with MSE metric 15523.5481\n",
      "Epoch 310 batch 90 train Loss 37.4956 test Loss 18.2427 with MSE metric 15522.5668\n",
      "Epoch 310 batch 100 train Loss 37.4915 test Loss 18.2410 with MSE metric 15521.5448\n",
      "Epoch 310 batch 110 train Loss 37.4874 test Loss 18.2393 with MSE metric 15520.5024\n",
      "Epoch 310 batch 120 train Loss 37.4832 test Loss 18.2377 with MSE metric 15519.4647\n",
      "Epoch 310 batch 130 train Loss 37.4791 test Loss 18.2360 with MSE metric 15518.4058\n",
      "Epoch 310 batch 140 train Loss 37.4749 test Loss 18.2344 with MSE metric 15517.3779\n",
      "Epoch 310 batch 150 train Loss 37.4708 test Loss 18.2327 with MSE metric 15516.3419\n",
      "Epoch 310 batch 160 train Loss 37.4666 test Loss 18.2311 with MSE metric 15515.3135\n",
      "Epoch 310 batch 170 train Loss 37.4625 test Loss 18.2294 with MSE metric 15514.3062\n",
      "Epoch 310 batch 180 train Loss 37.4584 test Loss 18.2278 with MSE metric 15513.2812\n",
      "Epoch 310 batch 190 train Loss 37.4542 test Loss 18.2261 with MSE metric 15512.2271\n",
      "Epoch 310 batch 200 train Loss 37.4501 test Loss 18.2245 with MSE metric 15511.2112\n",
      "Epoch 310 batch 210 train Loss 37.4459 test Loss 18.2228 with MSE metric 15510.2200\n",
      "Epoch 310 batch 220 train Loss 37.4418 test Loss 18.2212 with MSE metric 15509.1983\n",
      "Epoch 310 batch 230 train Loss 37.4377 test Loss 18.2195 with MSE metric 15508.1634\n",
      "Epoch 310 batch 240 train Loss 37.4335 test Loss 18.2179 with MSE metric 15507.1335\n",
      "Time taken for 1 epoch: 27.76474905014038 secs\n",
      "\n",
      "Epoch 311 batch 0 train Loss 37.4294 test Loss 18.2162 with MSE metric 15506.1378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 311 batch 10 train Loss 37.4253 test Loss 18.2146 with MSE metric 15505.1046\n",
      "Epoch 311 batch 20 train Loss 37.4211 test Loss 18.2129 with MSE metric 15504.0636\n",
      "Epoch 311 batch 30 train Loss 37.4170 test Loss 18.2113 with MSE metric 15503.0996\n",
      "Epoch 311 batch 40 train Loss 37.4129 test Loss 18.2096 with MSE metric 15502.1218\n",
      "Epoch 311 batch 50 train Loss 37.4088 test Loss 18.2080 with MSE metric 15501.0331\n",
      "Epoch 311 batch 60 train Loss 37.4046 test Loss 18.2063 with MSE metric 15500.0392\n",
      "Epoch 311 batch 70 train Loss 37.4005 test Loss 18.2047 with MSE metric 15499.0269\n",
      "Epoch 311 batch 80 train Loss 37.3964 test Loss 18.2030 with MSE metric 15498.0334\n",
      "Epoch 311 batch 90 train Loss 37.3923 test Loss 18.2014 with MSE metric 15496.9534\n",
      "Epoch 311 batch 100 train Loss 37.3881 test Loss 18.1997 with MSE metric 15495.9344\n",
      "Epoch 311 batch 110 train Loss 37.3840 test Loss 18.1981 with MSE metric 15494.9129\n",
      "Epoch 311 batch 120 train Loss 37.3799 test Loss 18.1964 with MSE metric 15493.8336\n",
      "Epoch 311 batch 130 train Loss 37.3758 test Loss 18.1948 with MSE metric 15492.8567\n",
      "Epoch 311 batch 140 train Loss 37.3717 test Loss 18.1931 with MSE metric 15491.8457\n",
      "Epoch 311 batch 150 train Loss 37.3676 test Loss 18.1915 with MSE metric 15490.8667\n",
      "Epoch 311 batch 160 train Loss 37.3634 test Loss 18.1898 with MSE metric 15489.8054\n",
      "Epoch 311 batch 170 train Loss 37.3593 test Loss 18.1882 with MSE metric 15488.8089\n",
      "Epoch 311 batch 180 train Loss 37.3552 test Loss 18.1865 with MSE metric 15487.7400\n",
      "Epoch 311 batch 190 train Loss 37.3511 test Loss 18.1849 with MSE metric 15486.6951\n",
      "Epoch 311 batch 200 train Loss 37.3470 test Loss 18.1832 with MSE metric 15485.6708\n",
      "Epoch 311 batch 210 train Loss 37.3429 test Loss 18.1816 with MSE metric 15484.6813\n",
      "Epoch 311 batch 220 train Loss 37.3388 test Loss 18.1800 with MSE metric 15483.6396\n",
      "Epoch 311 batch 230 train Loss 37.3347 test Loss 18.1783 with MSE metric 15482.6016\n",
      "Epoch 311 batch 240 train Loss 37.3306 test Loss 18.1767 with MSE metric 15481.5758\n",
      "Time taken for 1 epoch: 28.668946981430054 secs\n",
      "\n",
      "Epoch 312 batch 0 train Loss 37.3264 test Loss 18.1751 with MSE metric 15480.5330\n",
      "Epoch 312 batch 10 train Loss 37.3223 test Loss 18.1734 with MSE metric 15479.4640\n",
      "Epoch 312 batch 20 train Loss 37.3182 test Loss 18.1718 with MSE metric 15478.4547\n",
      "Epoch 312 batch 30 train Loss 37.3141 test Loss 18.1701 with MSE metric 15477.4277\n",
      "Epoch 312 batch 40 train Loss 37.3100 test Loss 18.1685 with MSE metric 15476.4353\n",
      "Epoch 312 batch 50 train Loss 37.3059 test Loss 18.1668 with MSE metric 15475.3439\n",
      "Epoch 312 batch 60 train Loss 37.3018 test Loss 18.1652 with MSE metric 15474.3582\n",
      "Epoch 312 batch 70 train Loss 37.2977 test Loss 18.1635 with MSE metric 15473.3159\n",
      "Epoch 312 batch 80 train Loss 37.2936 test Loss 18.1619 with MSE metric 15472.3163\n",
      "Epoch 312 batch 90 train Loss 37.2895 test Loss 18.1603 with MSE metric 15471.2865\n",
      "Epoch 312 batch 100 train Loss 37.2855 test Loss 18.1586 with MSE metric 15470.3104\n",
      "Epoch 312 batch 110 train Loss 37.2814 test Loss 18.1570 with MSE metric 15469.2883\n",
      "Epoch 312 batch 120 train Loss 37.2773 test Loss 18.1554 with MSE metric 15468.2710\n",
      "Epoch 312 batch 130 train Loss 37.2732 test Loss 18.1537 with MSE metric 15467.2230\n",
      "Epoch 312 batch 140 train Loss 37.2691 test Loss 18.1521 with MSE metric 15466.2181\n",
      "Epoch 312 batch 150 train Loss 37.2650 test Loss 18.1504 with MSE metric 15465.2124\n",
      "Epoch 312 batch 160 train Loss 37.2609 test Loss 18.1488 with MSE metric 15464.2025\n",
      "Epoch 312 batch 170 train Loss 37.2568 test Loss 18.1472 with MSE metric 15463.1653\n",
      "Epoch 312 batch 180 train Loss 37.2527 test Loss 18.1455 with MSE metric 15462.1796\n",
      "Epoch 312 batch 190 train Loss 37.2486 test Loss 18.1439 with MSE metric 15461.1648\n",
      "Epoch 312 batch 200 train Loss 37.2446 test Loss 18.1423 with MSE metric 15460.1620\n",
      "Epoch 312 batch 210 train Loss 37.2405 test Loss 18.1406 with MSE metric 15459.1742\n",
      "Epoch 312 batch 220 train Loss 37.2364 test Loss 18.1390 with MSE metric 15458.1384\n",
      "Epoch 312 batch 230 train Loss 37.2323 test Loss 18.1374 with MSE metric 15457.1594\n",
      "Epoch 312 batch 240 train Loss 37.2282 test Loss 18.1357 with MSE metric 15456.2217\n",
      "Time taken for 1 epoch: 26.836751222610474 secs\n",
      "\n",
      "Epoch 313 batch 0 train Loss 37.2242 test Loss 18.1341 with MSE metric 15455.1972\n",
      "Epoch 313 batch 10 train Loss 37.2201 test Loss 18.1325 with MSE metric 15454.1991\n",
      "Epoch 313 batch 20 train Loss 37.2160 test Loss 18.1308 with MSE metric 15453.1441\n",
      "Epoch 313 batch 30 train Loss 37.2119 test Loss 18.1292 with MSE metric 15452.1740\n",
      "Epoch 313 batch 40 train Loss 37.2079 test Loss 18.1276 with MSE metric 15451.1457\n",
      "Epoch 313 batch 50 train Loss 37.2038 test Loss 18.1259 with MSE metric 15450.1774\n",
      "Epoch 313 batch 60 train Loss 37.1997 test Loss 18.1243 with MSE metric 15449.1415\n",
      "Epoch 313 batch 70 train Loss 37.1956 test Loss 18.1227 with MSE metric 15448.1389\n",
      "Epoch 313 batch 80 train Loss 37.1916 test Loss 18.1210 with MSE metric 15447.0991\n",
      "Epoch 313 batch 90 train Loss 37.1875 test Loss 18.1194 with MSE metric 15446.0853\n",
      "Epoch 313 batch 100 train Loss 37.1834 test Loss 18.1178 with MSE metric 15445.0908\n",
      "Epoch 313 batch 110 train Loss 37.1794 test Loss 18.1162 with MSE metric 15444.1241\n",
      "Epoch 313 batch 120 train Loss 37.1753 test Loss 18.1145 with MSE metric 15443.1311\n",
      "Epoch 313 batch 130 train Loss 37.1712 test Loss 18.1129 with MSE metric 15442.1505\n",
      "Epoch 313 batch 140 train Loss 37.1672 test Loss 18.1113 with MSE metric 15441.1742\n",
      "Epoch 313 batch 150 train Loss 37.1631 test Loss 18.1097 with MSE metric 15440.1649\n",
      "Epoch 313 batch 160 train Loss 37.1591 test Loss 18.1080 with MSE metric 15439.1466\n",
      "Epoch 313 batch 170 train Loss 37.1550 test Loss 18.1064 with MSE metric 15438.1181\n",
      "Epoch 313 batch 180 train Loss 37.1509 test Loss 18.1048 with MSE metric 15437.0656\n",
      "Epoch 313 batch 190 train Loss 37.1469 test Loss 18.1032 with MSE metric 15436.0406\n",
      "Epoch 313 batch 200 train Loss 37.1428 test Loss 18.1015 with MSE metric 15435.0483\n",
      "Epoch 313 batch 210 train Loss 37.1387 test Loss 18.0999 with MSE metric 15434.0272\n",
      "Epoch 313 batch 220 train Loss 37.1347 test Loss 18.0983 with MSE metric 15432.9978\n",
      "Epoch 313 batch 230 train Loss 37.1306 test Loss 18.0967 with MSE metric 15432.0252\n",
      "Epoch 313 batch 240 train Loss 37.1266 test Loss 18.0950 with MSE metric 15431.0464\n",
      "Time taken for 1 epoch: 27.15307903289795 secs\n",
      "\n",
      "Epoch 314 batch 0 train Loss 37.1225 test Loss 18.0934 with MSE metric 15430.0708\n",
      "Epoch 314 batch 10 train Loss 37.1185 test Loss 18.0918 with MSE metric 15429.0630\n",
      "Epoch 314 batch 20 train Loss 37.1144 test Loss 18.0902 with MSE metric 15428.0503\n",
      "Epoch 314 batch 30 train Loss 37.1104 test Loss 18.0885 with MSE metric 15427.0875\n",
      "Epoch 314 batch 40 train Loss 37.1063 test Loss 18.0869 with MSE metric 15426.0511\n",
      "Epoch 314 batch 50 train Loss 37.1023 test Loss 18.0853 with MSE metric 15425.0483\n",
      "Epoch 314 batch 60 train Loss 37.0982 test Loss 18.0837 with MSE metric 15424.0597\n",
      "Epoch 314 batch 70 train Loss 37.0942 test Loss 18.0820 with MSE metric 15423.0459\n",
      "Epoch 314 batch 80 train Loss 37.0901 test Loss 18.0804 with MSE metric 15422.0138\n",
      "Epoch 314 batch 90 train Loss 37.0861 test Loss 18.0788 with MSE metric 15420.9993\n",
      "Epoch 314 batch 100 train Loss 37.0821 test Loss 18.0772 with MSE metric 15419.9915\n",
      "Epoch 314 batch 110 train Loss 37.0780 test Loss 18.0756 with MSE metric 15418.9852\n",
      "Epoch 314 batch 120 train Loss 37.0740 test Loss 18.0740 with MSE metric 15417.9922\n",
      "Epoch 314 batch 130 train Loss 37.0699 test Loss 18.0723 with MSE metric 15417.0326\n",
      "Epoch 314 batch 140 train Loss 37.0659 test Loss 18.0707 with MSE metric 15416.0302\n",
      "Epoch 314 batch 150 train Loss 37.0619 test Loss 18.0691 with MSE metric 15415.0674\n",
      "Epoch 314 batch 160 train Loss 37.0578 test Loss 18.0675 with MSE metric 15414.0739\n",
      "Epoch 314 batch 170 train Loss 37.0538 test Loss 18.0659 with MSE metric 15413.0914\n",
      "Epoch 314 batch 180 train Loss 37.0498 test Loss 18.0643 with MSE metric 15412.1268\n",
      "Epoch 314 batch 190 train Loss 37.0457 test Loss 18.0627 with MSE metric 15411.1590\n",
      "Epoch 314 batch 200 train Loss 37.0417 test Loss 18.0611 with MSE metric 15410.1402\n",
      "Epoch 314 batch 210 train Loss 37.0377 test Loss 18.0595 with MSE metric 15409.1378\n",
      "Epoch 314 batch 220 train Loss 37.0336 test Loss 18.0579 with MSE metric 15408.1763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 314 batch 230 train Loss 37.0296 test Loss 18.0562 with MSE metric 15407.1875\n",
      "Epoch 314 batch 240 train Loss 37.0256 test Loss 18.0546 with MSE metric 15406.2148\n",
      "Time taken for 1 epoch: 28.809431791305542 secs\n",
      "\n",
      "Epoch 315 batch 0 train Loss 37.0216 test Loss 18.0530 with MSE metric 15405.1809\n",
      "Epoch 315 batch 10 train Loss 37.0175 test Loss 18.0514 with MSE metric 15404.1570\n",
      "Epoch 315 batch 20 train Loss 37.0135 test Loss 18.0498 with MSE metric 15403.1447\n",
      "Epoch 315 batch 30 train Loss 37.0095 test Loss 18.0482 with MSE metric 15402.1237\n",
      "Epoch 315 batch 40 train Loss 37.0054 test Loss 18.0466 with MSE metric 15401.1203\n",
      "Epoch 315 batch 50 train Loss 37.0014 test Loss 18.0450 with MSE metric 15400.1111\n",
      "Epoch 315 batch 60 train Loss 36.9974 test Loss 18.0434 with MSE metric 15399.1228\n",
      "Epoch 315 batch 70 train Loss 36.9934 test Loss 18.0418 with MSE metric 15398.1346\n",
      "Epoch 315 batch 80 train Loss 36.9894 test Loss 18.0402 with MSE metric 15397.1182\n",
      "Epoch 315 batch 90 train Loss 36.9853 test Loss 18.0386 with MSE metric 15396.1170\n",
      "Epoch 315 batch 100 train Loss 36.9813 test Loss 18.0370 with MSE metric 15395.1458\n",
      "Epoch 315 batch 110 train Loss 36.9773 test Loss 18.0354 with MSE metric 15394.1636\n",
      "Epoch 315 batch 120 train Loss 36.9733 test Loss 18.0338 with MSE metric 15393.1730\n",
      "Epoch 315 batch 130 train Loss 36.9693 test Loss 18.0322 with MSE metric 15392.1604\n",
      "Epoch 315 batch 140 train Loss 36.9653 test Loss 18.0306 with MSE metric 15391.1934\n",
      "Epoch 315 batch 150 train Loss 36.9613 test Loss 18.0290 with MSE metric 15390.2109\n",
      "Epoch 315 batch 160 train Loss 36.9572 test Loss 18.0274 with MSE metric 15389.1625\n",
      "Epoch 315 batch 170 train Loss 36.9532 test Loss 18.0258 with MSE metric 15388.1641\n",
      "Epoch 315 batch 180 train Loss 36.9492 test Loss 18.0242 with MSE metric 15387.1362\n",
      "Epoch 315 batch 190 train Loss 36.9452 test Loss 18.0226 with MSE metric 15386.1622\n",
      "Epoch 315 batch 200 train Loss 36.9412 test Loss 18.0210 with MSE metric 15385.2357\n",
      "Epoch 315 batch 210 train Loss 36.9372 test Loss 18.0194 with MSE metric 15384.2419\n",
      "Epoch 315 batch 220 train Loss 36.9332 test Loss 18.0178 with MSE metric 15383.2766\n",
      "Epoch 315 batch 230 train Loss 36.9292 test Loss 18.0162 with MSE metric 15382.2874\n",
      "Epoch 315 batch 240 train Loss 36.9252 test Loss 18.0146 with MSE metric 15381.2941\n",
      "Time taken for 1 epoch: 27.412585258483887 secs\n",
      "\n",
      "Epoch 316 batch 0 train Loss 36.9212 test Loss 18.0130 with MSE metric 15380.3214\n",
      "Epoch 316 batch 10 train Loss 36.9172 test Loss 18.0114 with MSE metric 15379.3363\n",
      "Epoch 316 batch 20 train Loss 36.9132 test Loss 18.0098 with MSE metric 15378.3364\n",
      "Epoch 316 batch 30 train Loss 36.9092 test Loss 18.0082 with MSE metric 15377.3765\n",
      "Epoch 316 batch 40 train Loss 36.9052 test Loss 18.0066 with MSE metric 15376.4499\n",
      "Epoch 316 batch 50 train Loss 36.9012 test Loss 18.0050 with MSE metric 15375.4841\n",
      "Epoch 316 batch 60 train Loss 36.8972 test Loss 18.0034 with MSE metric 15374.4793\n",
      "Epoch 316 batch 70 train Loss 36.8932 test Loss 18.0018 with MSE metric 15373.4767\n",
      "Epoch 316 batch 80 train Loss 36.8892 test Loss 18.0002 with MSE metric 15372.4803\n",
      "Epoch 316 batch 90 train Loss 36.8852 test Loss 17.9986 with MSE metric 15371.4830\n",
      "Epoch 316 batch 100 train Loss 36.8812 test Loss 17.9970 with MSE metric 15370.5083\n",
      "Epoch 316 batch 110 train Loss 36.8773 test Loss 17.9954 with MSE metric 15369.5213\n",
      "Epoch 316 batch 120 train Loss 36.8733 test Loss 17.9938 with MSE metric 15368.5685\n",
      "Epoch 316 batch 130 train Loss 36.8693 test Loss 17.9922 with MSE metric 15367.6741\n",
      "Epoch 316 batch 140 train Loss 36.8653 test Loss 17.9906 with MSE metric 15366.6913\n",
      "Epoch 316 batch 150 train Loss 36.8613 test Loss 17.9890 with MSE metric 15365.7322\n",
      "Epoch 316 batch 160 train Loss 36.8573 test Loss 17.9874 with MSE metric 15364.7887\n",
      "Epoch 316 batch 170 train Loss 36.8533 test Loss 17.9858 with MSE metric 15363.7997\n",
      "Epoch 316 batch 180 train Loss 36.8494 test Loss 17.9842 with MSE metric 15362.8465\n",
      "Epoch 316 batch 190 train Loss 36.8454 test Loss 17.9826 with MSE metric 15361.8338\n",
      "Epoch 316 batch 200 train Loss 36.8414 test Loss 17.9810 with MSE metric 15360.8927\n",
      "Epoch 316 batch 210 train Loss 36.8374 test Loss 17.9794 with MSE metric 15359.9112\n",
      "Epoch 316 batch 220 train Loss 36.8334 test Loss 17.9778 with MSE metric 15358.9732\n",
      "Epoch 316 batch 230 train Loss 36.8295 test Loss 17.9763 with MSE metric 15358.0359\n",
      "Epoch 316 batch 240 train Loss 36.8255 test Loss 17.9746 with MSE metric 15357.0979\n",
      "Time taken for 1 epoch: 29.176214933395386 secs\n",
      "\n",
      "Epoch 317 batch 0 train Loss 36.8215 test Loss 17.9731 with MSE metric 15356.1211\n",
      "Epoch 317 batch 10 train Loss 36.8175 test Loss 17.9715 with MSE metric 15355.1448\n",
      "Epoch 317 batch 20 train Loss 36.8136 test Loss 17.9699 with MSE metric 15354.1735\n",
      "Epoch 317 batch 30 train Loss 36.8096 test Loss 17.9683 with MSE metric 15353.1872\n",
      "Epoch 317 batch 40 train Loss 36.8056 test Loss 17.9667 with MSE metric 15352.1469\n",
      "Epoch 317 batch 50 train Loss 36.8017 test Loss 17.9651 with MSE metric 15351.1245\n",
      "Epoch 317 batch 60 train Loss 36.7977 test Loss 17.9635 with MSE metric 15350.1548\n",
      "Epoch 317 batch 70 train Loss 36.7937 test Loss 17.9619 with MSE metric 15349.1493\n",
      "Epoch 317 batch 80 train Loss 36.7897 test Loss 17.9604 with MSE metric 15348.1921\n",
      "Epoch 317 batch 90 train Loss 36.7858 test Loss 17.9588 with MSE metric 15347.2270\n",
      "Epoch 317 batch 100 train Loss 36.7818 test Loss 17.9572 with MSE metric 15346.2334\n",
      "Epoch 317 batch 110 train Loss 36.7778 test Loss 17.9556 with MSE metric 15345.2470\n",
      "Epoch 317 batch 120 train Loss 36.7739 test Loss 17.9540 with MSE metric 15344.2915\n",
      "Epoch 317 batch 130 train Loss 36.7699 test Loss 17.9524 with MSE metric 15343.3380\n",
      "Epoch 317 batch 140 train Loss 36.7660 test Loss 17.9508 with MSE metric 15342.3198\n",
      "Epoch 317 batch 150 train Loss 36.7620 test Loss 17.9493 with MSE metric 15341.2822\n",
      "Epoch 317 batch 160 train Loss 36.7580 test Loss 17.9477 with MSE metric 15340.2628\n",
      "Epoch 317 batch 170 train Loss 36.7541 test Loss 17.9461 with MSE metric 15339.3183\n",
      "Epoch 317 batch 180 train Loss 36.7501 test Loss 17.9445 with MSE metric 15338.3752\n",
      "Epoch 317 batch 190 train Loss 36.7462 test Loss 17.9429 with MSE metric 15337.4499\n",
      "Epoch 317 batch 200 train Loss 36.7422 test Loss 17.9413 with MSE metric 15336.4871\n",
      "Epoch 317 batch 210 train Loss 36.7382 test Loss 17.9398 with MSE metric 15335.5224\n",
      "Epoch 317 batch 220 train Loss 36.7343 test Loss 17.9382 with MSE metric 15334.5886\n",
      "Epoch 317 batch 230 train Loss 36.7303 test Loss 17.9366 with MSE metric 15333.5800\n",
      "Epoch 317 batch 240 train Loss 36.7264 test Loss 17.9350 with MSE metric 15332.5722\n",
      "Time taken for 1 epoch: 28.635005950927734 secs\n",
      "\n",
      "Epoch 318 batch 0 train Loss 36.7224 test Loss 17.9334 with MSE metric 15331.5528\n",
      "Epoch 318 batch 10 train Loss 36.7185 test Loss 17.9318 with MSE metric 15330.5912\n",
      "Epoch 318 batch 20 train Loss 36.7145 test Loss 17.9303 with MSE metric 15329.5648\n",
      "Epoch 318 batch 30 train Loss 36.7106 test Loss 17.9287 with MSE metric 15328.6386\n",
      "Epoch 318 batch 40 train Loss 36.7066 test Loss 17.9271 with MSE metric 15327.6044\n",
      "Epoch 318 batch 50 train Loss 36.7027 test Loss 17.9255 with MSE metric 15326.6472\n",
      "Epoch 318 batch 60 train Loss 36.6987 test Loss 17.9239 with MSE metric 15325.6814\n",
      "Epoch 318 batch 70 train Loss 36.6948 test Loss 17.9224 with MSE metric 15324.7226\n",
      "Epoch 318 batch 80 train Loss 36.6909 test Loss 17.9208 with MSE metric 15323.7319\n",
      "Epoch 318 batch 90 train Loss 36.6869 test Loss 17.9192 with MSE metric 15322.7603\n",
      "Epoch 318 batch 100 train Loss 36.6830 test Loss 17.9176 with MSE metric 15321.7828\n",
      "Epoch 318 batch 110 train Loss 36.6790 test Loss 17.9161 with MSE metric 15320.8258\n",
      "Epoch 318 batch 120 train Loss 36.6751 test Loss 17.9145 with MSE metric 15319.8791\n",
      "Epoch 318 batch 130 train Loss 36.6711 test Loss 17.9129 with MSE metric 15318.8621\n",
      "Epoch 318 batch 140 train Loss 36.6672 test Loss 17.9113 with MSE metric 15317.9039\n",
      "Epoch 318 batch 150 train Loss 36.6633 test Loss 17.9098 with MSE metric 15316.9206\n",
      "Epoch 318 batch 160 train Loss 36.6593 test Loss 17.9082 with MSE metric 15315.8901\n",
      "Epoch 318 batch 170 train Loss 36.6554 test Loss 17.9066 with MSE metric 15314.8813\n",
      "Epoch 318 batch 180 train Loss 36.6515 test Loss 17.9050 with MSE metric 15313.9181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318 batch 190 train Loss 36.6475 test Loss 17.9035 with MSE metric 15312.9210\n",
      "Epoch 318 batch 200 train Loss 36.6436 test Loss 17.9019 with MSE metric 15311.9805\n",
      "Epoch 318 batch 210 train Loss 36.6397 test Loss 17.9003 with MSE metric 15310.9924\n",
      "Epoch 318 batch 220 train Loss 36.6357 test Loss 17.8988 with MSE metric 15310.0009\n",
      "Epoch 318 batch 230 train Loss 36.6318 test Loss 17.8972 with MSE metric 15309.0095\n",
      "Epoch 318 batch 240 train Loss 36.6279 test Loss 17.8956 with MSE metric 15308.0047\n",
      "Time taken for 1 epoch: 27.977992057800293 secs\n",
      "\n",
      "Epoch 319 batch 0 train Loss 36.6240 test Loss 17.8940 with MSE metric 15307.0756\n",
      "Epoch 319 batch 10 train Loss 36.6200 test Loss 17.8925 with MSE metric 15306.1743\n",
      "Epoch 319 batch 20 train Loss 36.6161 test Loss 17.8909 with MSE metric 15305.1921\n",
      "Epoch 319 batch 30 train Loss 36.6122 test Loss 17.8893 with MSE metric 15304.1868\n",
      "Epoch 319 batch 40 train Loss 36.6083 test Loss 17.8878 with MSE metric 15303.2079\n",
      "Epoch 319 batch 50 train Loss 36.6043 test Loss 17.8862 with MSE metric 15302.2407\n",
      "Epoch 319 batch 60 train Loss 36.6004 test Loss 17.8846 with MSE metric 15301.2889\n",
      "Epoch 319 batch 70 train Loss 36.5965 test Loss 17.8831 with MSE metric 15300.3239\n",
      "Epoch 319 batch 80 train Loss 36.5926 test Loss 17.8815 with MSE metric 15299.3292\n",
      "Epoch 319 batch 90 train Loss 36.5887 test Loss 17.8800 with MSE metric 15298.4030\n",
      "Epoch 319 batch 100 train Loss 36.5847 test Loss 17.8784 with MSE metric 15297.4135\n",
      "Epoch 319 batch 110 train Loss 36.5808 test Loss 17.8768 with MSE metric 15296.4664\n",
      "Epoch 319 batch 120 train Loss 36.5769 test Loss 17.8752 with MSE metric 15295.4814\n",
      "Epoch 319 batch 130 train Loss 36.5730 test Loss 17.8737 with MSE metric 15294.5425\n",
      "Epoch 319 batch 140 train Loss 36.5691 test Loss 17.8721 with MSE metric 15293.5591\n",
      "Epoch 319 batch 150 train Loss 36.5652 test Loss 17.8706 with MSE metric 15292.5582\n",
      "Epoch 319 batch 160 train Loss 36.5613 test Loss 17.8690 with MSE metric 15291.5809\n",
      "Epoch 319 batch 170 train Loss 36.5573 test Loss 17.8675 with MSE metric 15290.6250\n",
      "Epoch 319 batch 180 train Loss 36.5534 test Loss 17.8659 with MSE metric 15289.6475\n",
      "Epoch 319 batch 190 train Loss 36.5495 test Loss 17.8643 with MSE metric 15288.7369\n",
      "Epoch 319 batch 200 train Loss 36.5456 test Loss 17.8628 with MSE metric 15287.8285\n",
      "Epoch 319 batch 210 train Loss 36.5417 test Loss 17.8612 with MSE metric 15286.8483\n",
      "Epoch 319 batch 220 train Loss 36.5378 test Loss 17.8596 with MSE metric 15285.8327\n",
      "Epoch 319 batch 230 train Loss 36.5339 test Loss 17.8581 with MSE metric 15284.8453\n",
      "Epoch 319 batch 240 train Loss 36.5300 test Loss 17.8565 with MSE metric 15283.8657\n",
      "Time taken for 1 epoch: 27.844911098480225 secs\n",
      "\n",
      "Epoch 320 batch 0 train Loss 36.5261 test Loss 17.8550 with MSE metric 15282.8980\n",
      "Epoch 320 batch 10 train Loss 36.5222 test Loss 17.8534 with MSE metric 15281.9414\n",
      "Epoch 320 batch 20 train Loss 36.5183 test Loss 17.8519 with MSE metric 15281.0115\n",
      "Epoch 320 batch 30 train Loss 36.5144 test Loss 17.8503 with MSE metric 15280.0480\n",
      "Epoch 320 batch 40 train Loss 36.5105 test Loss 17.8487 with MSE metric 15279.1077\n",
      "Epoch 320 batch 50 train Loss 36.5066 test Loss 17.8472 with MSE metric 15278.1507\n",
      "Epoch 320 batch 60 train Loss 36.5027 test Loss 17.8456 with MSE metric 15277.1981\n",
      "Epoch 320 batch 70 train Loss 36.4988 test Loss 17.8440 with MSE metric 15276.2375\n",
      "Epoch 320 batch 80 train Loss 36.4949 test Loss 17.8425 with MSE metric 15275.2565\n",
      "Epoch 320 batch 90 train Loss 36.4910 test Loss 17.8409 with MSE metric 15274.2997\n",
      "Epoch 320 batch 100 train Loss 36.4871 test Loss 17.8394 with MSE metric 15273.3183\n",
      "Epoch 320 batch 110 train Loss 36.4832 test Loss 17.8378 with MSE metric 15272.3602\n",
      "Epoch 320 batch 120 train Loss 36.4794 test Loss 17.8363 with MSE metric 15271.4416\n",
      "Epoch 320 batch 130 train Loss 36.4755 test Loss 17.8347 with MSE metric 15270.4739\n",
      "Epoch 320 batch 140 train Loss 36.4716 test Loss 17.8332 with MSE metric 15269.4875\n",
      "Epoch 320 batch 150 train Loss 36.4677 test Loss 17.8316 with MSE metric 15268.4966\n",
      "Epoch 320 batch 160 train Loss 36.4638 test Loss 17.8301 with MSE metric 15267.5409\n",
      "Epoch 320 batch 170 train Loss 36.4599 test Loss 17.8285 with MSE metric 15266.5869\n",
      "Epoch 320 batch 180 train Loss 36.4560 test Loss 17.8270 with MSE metric 15265.5970\n",
      "Epoch 320 batch 190 train Loss 36.4521 test Loss 17.8254 with MSE metric 15264.6529\n",
      "Epoch 320 batch 200 train Loss 36.4483 test Loss 17.8239 with MSE metric 15263.6749\n",
      "Epoch 320 batch 210 train Loss 36.4444 test Loss 17.8223 with MSE metric 15262.6928\n",
      "Epoch 320 batch 220 train Loss 36.4405 test Loss 17.8208 with MSE metric 15261.7569\n",
      "Epoch 320 batch 230 train Loss 36.4366 test Loss 17.8193 with MSE metric 15260.7994\n",
      "Epoch 320 batch 240 train Loss 36.4327 test Loss 17.8177 with MSE metric 15259.8621\n",
      "Time taken for 1 epoch: 26.761106967926025 secs\n",
      "\n",
      "Epoch 321 batch 0 train Loss 36.4289 test Loss 17.8161 with MSE metric 15258.8845\n",
      "Epoch 321 batch 10 train Loss 36.4250 test Loss 17.8146 with MSE metric 15257.9204\n",
      "Epoch 321 batch 20 train Loss 36.4211 test Loss 17.8130 with MSE metric 15256.9622\n",
      "Epoch 321 batch 30 train Loss 36.4172 test Loss 17.8115 with MSE metric 15255.9957\n",
      "Epoch 321 batch 40 train Loss 36.4134 test Loss 17.8099 with MSE metric 15255.0901\n",
      "Epoch 321 batch 50 train Loss 36.4095 test Loss 17.8084 with MSE metric 15254.1247\n",
      "Epoch 321 batch 60 train Loss 36.4056 test Loss 17.8068 with MSE metric 15253.1754\n",
      "Epoch 321 batch 70 train Loss 36.4017 test Loss 17.8053 with MSE metric 15252.2460\n",
      "Epoch 321 batch 80 train Loss 36.3979 test Loss 17.8038 with MSE metric 15251.2426\n",
      "Epoch 321 batch 90 train Loss 36.3940 test Loss 17.8022 with MSE metric 15250.2822\n",
      "Epoch 321 batch 100 train Loss 36.3901 test Loss 17.8007 with MSE metric 15249.2993\n",
      "Epoch 321 batch 110 train Loss 36.3863 test Loss 17.7991 with MSE metric 15248.3526\n",
      "Epoch 321 batch 120 train Loss 36.3824 test Loss 17.7976 with MSE metric 15247.4360\n",
      "Epoch 321 batch 130 train Loss 36.3785 test Loss 17.7960 with MSE metric 15246.4760\n",
      "Epoch 321 batch 140 train Loss 36.3747 test Loss 17.7945 with MSE metric 15245.4977\n",
      "Epoch 321 batch 150 train Loss 36.3708 test Loss 17.7929 with MSE metric 15244.5472\n",
      "Epoch 321 batch 160 train Loss 36.3669 test Loss 17.7914 with MSE metric 15243.6146\n",
      "Epoch 321 batch 170 train Loss 36.3631 test Loss 17.7898 with MSE metric 15242.6389\n",
      "Epoch 321 batch 180 train Loss 36.3592 test Loss 17.7883 with MSE metric 15241.7326\n",
      "Epoch 321 batch 190 train Loss 36.3554 test Loss 17.7868 with MSE metric 15240.7655\n",
      "Epoch 321 batch 200 train Loss 36.3515 test Loss 17.7852 with MSE metric 15239.8890\n",
      "Epoch 321 batch 210 train Loss 36.3477 test Loss 17.7837 with MSE metric 15238.9509\n",
      "Epoch 321 batch 220 train Loss 36.3438 test Loss 17.7821 with MSE metric 15238.0070\n",
      "Epoch 321 batch 230 train Loss 36.3399 test Loss 17.7806 with MSE metric 15237.0492\n",
      "Epoch 321 batch 240 train Loss 36.3361 test Loss 17.7791 with MSE metric 15236.1299\n",
      "Time taken for 1 epoch: 29.363161087036133 secs\n",
      "\n",
      "Epoch 322 batch 0 train Loss 36.3322 test Loss 17.7775 with MSE metric 15235.1828\n",
      "Epoch 322 batch 10 train Loss 36.3284 test Loss 17.7760 with MSE metric 15234.2193\n",
      "Epoch 322 batch 20 train Loss 36.3245 test Loss 17.7744 with MSE metric 15233.2306\n",
      "Epoch 322 batch 30 train Loss 36.3207 test Loss 17.7729 with MSE metric 15232.2858\n",
      "Epoch 322 batch 40 train Loss 36.3168 test Loss 17.7714 with MSE metric 15231.3510\n",
      "Epoch 322 batch 50 train Loss 36.3130 test Loss 17.7698 with MSE metric 15230.3902\n",
      "Epoch 322 batch 60 train Loss 36.3091 test Loss 17.7683 with MSE metric 15229.4738\n",
      "Epoch 322 batch 70 train Loss 36.3053 test Loss 17.7668 with MSE metric 15228.5112\n",
      "Epoch 322 batch 80 train Loss 36.3014 test Loss 17.7652 with MSE metric 15227.5942\n",
      "Epoch 322 batch 90 train Loss 36.2976 test Loss 17.7637 with MSE metric 15226.6496\n",
      "Epoch 322 batch 100 train Loss 36.2937 test Loss 17.7622 with MSE metric 15225.6834\n",
      "Epoch 322 batch 110 train Loss 36.2899 test Loss 17.7606 with MSE metric 15224.7694\n",
      "Epoch 322 batch 120 train Loss 36.2861 test Loss 17.7591 with MSE metric 15223.8149\n",
      "Epoch 322 batch 130 train Loss 36.2822 test Loss 17.7575 with MSE metric 15222.8802\n",
      "Epoch 322 batch 140 train Loss 36.2784 test Loss 17.7560 with MSE metric 15221.9248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322 batch 150 train Loss 36.2745 test Loss 17.7545 with MSE metric 15220.9505\n",
      "Epoch 322 batch 160 train Loss 36.2707 test Loss 17.7530 with MSE metric 15219.9726\n",
      "Epoch 322 batch 170 train Loss 36.2669 test Loss 17.7514 with MSE metric 15219.0089\n",
      "Epoch 322 batch 180 train Loss 36.2630 test Loss 17.7499 with MSE metric 15218.0420\n",
      "Epoch 322 batch 190 train Loss 36.2592 test Loss 17.7484 with MSE metric 15217.0990\n",
      "Epoch 322 batch 200 train Loss 36.2554 test Loss 17.7468 with MSE metric 15216.1746\n",
      "Epoch 322 batch 210 train Loss 36.2515 test Loss 17.7453 with MSE metric 15215.2570\n",
      "Epoch 322 batch 220 train Loss 36.2477 test Loss 17.7437 with MSE metric 15214.3167\n",
      "Epoch 322 batch 230 train Loss 36.2439 test Loss 17.7422 with MSE metric 15213.3735\n",
      "Epoch 322 batch 240 train Loss 36.2400 test Loss 17.7407 with MSE metric 15212.3825\n",
      "Time taken for 1 epoch: 28.688273906707764 secs\n",
      "\n",
      "Epoch 323 batch 0 train Loss 36.2362 test Loss 17.7391 with MSE metric 15211.4133\n",
      "Epoch 323 batch 10 train Loss 36.2324 test Loss 17.7376 with MSE metric 15210.4520\n",
      "Epoch 323 batch 20 train Loss 36.2285 test Loss 17.7361 with MSE metric 15209.4918\n",
      "Epoch 323 batch 30 train Loss 36.2247 test Loss 17.7346 with MSE metric 15208.5002\n",
      "Epoch 323 batch 40 train Loss 36.2209 test Loss 17.7330 with MSE metric 15207.5779\n",
      "Epoch 323 batch 50 train Loss 36.2171 test Loss 17.7315 with MSE metric 15206.6434\n",
      "Epoch 323 batch 60 train Loss 36.2132 test Loss 17.7300 with MSE metric 15205.6885\n",
      "Epoch 323 batch 70 train Loss 36.2094 test Loss 17.7285 with MSE metric 15204.7567\n",
      "Epoch 323 batch 80 train Loss 36.2056 test Loss 17.7269 with MSE metric 15203.8291\n",
      "Epoch 323 batch 90 train Loss 36.2018 test Loss 17.7254 with MSE metric 15202.8926\n",
      "Epoch 323 batch 100 train Loss 36.1979 test Loss 17.7239 with MSE metric 15201.9437\n",
      "Epoch 323 batch 110 train Loss 36.1941 test Loss 17.7223 with MSE metric 15201.0047\n",
      "Epoch 323 batch 120 train Loss 36.1903 test Loss 17.7208 with MSE metric 15200.0795\n",
      "Epoch 323 batch 130 train Loss 36.1865 test Loss 17.7193 with MSE metric 15199.1449\n",
      "Epoch 323 batch 140 train Loss 36.1827 test Loss 17.7177 with MSE metric 15198.2184\n",
      "Epoch 323 batch 150 train Loss 36.1789 test Loss 17.7162 with MSE metric 15197.2949\n",
      "Epoch 323 batch 160 train Loss 36.1750 test Loss 17.7147 with MSE metric 15196.3551\n",
      "Epoch 323 batch 170 train Loss 36.1712 test Loss 17.7132 with MSE metric 15195.3801\n",
      "Epoch 323 batch 180 train Loss 36.1674 test Loss 17.7117 with MSE metric 15194.4159\n",
      "Epoch 323 batch 190 train Loss 36.1636 test Loss 17.7101 with MSE metric 15193.4751\n",
      "Epoch 323 batch 200 train Loss 36.1598 test Loss 17.7086 with MSE metric 15192.5151\n",
      "Epoch 323 batch 210 train Loss 36.1560 test Loss 17.7071 with MSE metric 15191.5607\n",
      "Epoch 323 batch 220 train Loss 36.1522 test Loss 17.7056 with MSE metric 15190.5836\n",
      "Epoch 323 batch 230 train Loss 36.1484 test Loss 17.7040 with MSE metric 15189.6735\n",
      "Epoch 323 batch 240 train Loss 36.1446 test Loss 17.7025 with MSE metric 15188.7327\n",
      "Time taken for 1 epoch: 29.806167125701904 secs\n",
      "\n",
      "Epoch 324 batch 0 train Loss 36.1407 test Loss 17.7010 with MSE metric 15187.7906\n",
      "Epoch 324 batch 10 train Loss 36.1369 test Loss 17.6995 with MSE metric 15186.8444\n",
      "Epoch 324 batch 20 train Loss 36.1331 test Loss 17.6980 with MSE metric 15185.9114\n",
      "Epoch 324 batch 30 train Loss 36.1293 test Loss 17.6964 with MSE metric 15184.9453\n",
      "Epoch 324 batch 40 train Loss 36.1255 test Loss 17.6949 with MSE metric 15184.0370\n",
      "Epoch 324 batch 50 train Loss 36.1217 test Loss 17.6934 with MSE metric 15183.0749\n",
      "Epoch 324 batch 60 train Loss 36.1179 test Loss 17.6919 with MSE metric 15182.1334\n",
      "Epoch 324 batch 70 train Loss 36.1141 test Loss 17.6904 with MSE metric 15181.1868\n",
      "Epoch 324 batch 80 train Loss 36.1103 test Loss 17.6889 with MSE metric 15180.2528\n",
      "Epoch 324 batch 90 train Loss 36.1065 test Loss 17.6873 with MSE metric 15179.3685\n",
      "Epoch 324 batch 100 train Loss 36.1027 test Loss 17.6858 with MSE metric 15178.4421\n",
      "Epoch 324 batch 110 train Loss 36.0989 test Loss 17.6843 with MSE metric 15177.5171\n",
      "Epoch 324 batch 120 train Loss 36.0951 test Loss 17.6828 with MSE metric 15176.5753\n",
      "Epoch 324 batch 130 train Loss 36.0913 test Loss 17.6813 with MSE metric 15175.6150\n",
      "Epoch 324 batch 140 train Loss 36.0876 test Loss 17.6798 with MSE metric 15174.6747\n",
      "Epoch 324 batch 150 train Loss 36.0838 test Loss 17.6783 with MSE metric 15173.7082\n",
      "Epoch 324 batch 160 train Loss 36.0800 test Loss 17.6767 with MSE metric 15172.7447\n",
      "Epoch 324 batch 170 train Loss 36.0762 test Loss 17.6752 with MSE metric 15171.8199\n",
      "Epoch 324 batch 180 train Loss 36.0724 test Loss 17.6737 with MSE metric 15170.8568\n",
      "Epoch 324 batch 190 train Loss 36.0686 test Loss 17.6722 with MSE metric 15169.8886\n",
      "Epoch 324 batch 200 train Loss 36.0648 test Loss 17.6707 with MSE metric 15168.9274\n",
      "Epoch 324 batch 210 train Loss 36.0610 test Loss 17.6692 with MSE metric 15167.9284\n",
      "Epoch 324 batch 220 train Loss 36.0572 test Loss 17.6677 with MSE metric 15166.9761\n",
      "Epoch 324 batch 230 train Loss 36.0534 test Loss 17.6662 with MSE metric 15166.0427\n",
      "Epoch 324 batch 240 train Loss 36.0497 test Loss 17.6646 with MSE metric 15165.1396\n",
      "Time taken for 1 epoch: 28.08464789390564 secs\n",
      "\n",
      "Epoch 325 batch 0 train Loss 36.0459 test Loss 17.6631 with MSE metric 15164.2395\n",
      "Epoch 325 batch 10 train Loss 36.0421 test Loss 17.6616 with MSE metric 15163.3068\n",
      "Epoch 325 batch 20 train Loss 36.0383 test Loss 17.6601 with MSE metric 15162.3762\n",
      "Epoch 325 batch 30 train Loss 36.0345 test Loss 17.6586 with MSE metric 15161.4291\n",
      "Epoch 325 batch 40 train Loss 36.0307 test Loss 17.6571 with MSE metric 15160.5392\n",
      "Epoch 325 batch 50 train Loss 36.0270 test Loss 17.6556 with MSE metric 15159.6599\n",
      "Epoch 325 batch 60 train Loss 36.0232 test Loss 17.6541 with MSE metric 15158.7142\n",
      "Epoch 325 batch 70 train Loss 36.0194 test Loss 17.6526 with MSE metric 15157.7874\n",
      "Epoch 325 batch 80 train Loss 36.0156 test Loss 17.6511 with MSE metric 15156.8910\n",
      "Epoch 325 batch 90 train Loss 36.0119 test Loss 17.6496 with MSE metric 15155.9677\n",
      "Epoch 325 batch 100 train Loss 36.0081 test Loss 17.6481 with MSE metric 15154.9927\n",
      "Epoch 325 batch 110 train Loss 36.0043 test Loss 17.6466 with MSE metric 15154.0772\n",
      "Epoch 325 batch 120 train Loss 36.0006 test Loss 17.6450 with MSE metric 15153.1391\n",
      "Epoch 325 batch 130 train Loss 35.9968 test Loss 17.6435 with MSE metric 15152.2264\n",
      "Epoch 325 batch 140 train Loss 35.9930 test Loss 17.6420 with MSE metric 15151.3052\n",
      "Epoch 325 batch 150 train Loss 35.9892 test Loss 17.6405 with MSE metric 15150.3841\n",
      "Epoch 325 batch 160 train Loss 35.9855 test Loss 17.6390 with MSE metric 15149.4602\n",
      "Epoch 325 batch 170 train Loss 35.9817 test Loss 17.6375 with MSE metric 15148.5060\n",
      "Epoch 325 batch 180 train Loss 35.9779 test Loss 17.6360 with MSE metric 15147.6155\n",
      "Epoch 325 batch 190 train Loss 35.9742 test Loss 17.6345 with MSE metric 15146.6847\n",
      "Epoch 325 batch 200 train Loss 35.9704 test Loss 17.6330 with MSE metric 15145.7399\n",
      "Epoch 325 batch 210 train Loss 35.9666 test Loss 17.6315 with MSE metric 15144.7852\n",
      "Epoch 325 batch 220 train Loss 35.9629 test Loss 17.6300 with MSE metric 15143.8134\n",
      "Epoch 325 batch 230 train Loss 35.9591 test Loss 17.6285 with MSE metric 15142.8303\n",
      "Epoch 325 batch 240 train Loss 35.9554 test Loss 17.6270 with MSE metric 15141.9097\n",
      "Time taken for 1 epoch: 28.647436141967773 secs\n",
      "\n",
      "Epoch 326 batch 0 train Loss 35.9516 test Loss 17.6255 with MSE metric 15140.9711\n",
      "Epoch 326 batch 10 train Loss 35.9478 test Loss 17.6240 with MSE metric 15140.0633\n",
      "Epoch 326 batch 20 train Loss 35.9441 test Loss 17.6225 with MSE metric 15139.1606\n",
      "Epoch 326 batch 30 train Loss 35.9403 test Loss 17.6210 with MSE metric 15138.2545\n",
      "Epoch 326 batch 40 train Loss 35.9366 test Loss 17.6195 with MSE metric 15137.3295\n",
      "Epoch 326 batch 50 train Loss 35.9328 test Loss 17.6179 with MSE metric 15136.4255\n",
      "Epoch 326 batch 60 train Loss 35.9291 test Loss 17.6164 with MSE metric 15135.5071\n",
      "Epoch 326 batch 70 train Loss 35.9253 test Loss 17.6149 with MSE metric 15134.5822\n",
      "Epoch 326 batch 80 train Loss 35.9216 test Loss 17.6134 with MSE metric 15133.7104\n",
      "Epoch 326 batch 90 train Loss 35.9178 test Loss 17.6119 with MSE metric 15132.7647\n",
      "Epoch 326 batch 100 train Loss 35.9141 test Loss 17.6105 with MSE metric 15131.7878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 326 batch 110 train Loss 35.9103 test Loss 17.6090 with MSE metric 15130.8539\n",
      "Epoch 326 batch 120 train Loss 35.9066 test Loss 17.6075 with MSE metric 15129.9212\n",
      "Epoch 326 batch 130 train Loss 35.9028 test Loss 17.6060 with MSE metric 15129.0325\n",
      "Epoch 326 batch 140 train Loss 35.8991 test Loss 17.6045 with MSE metric 15128.0586\n",
      "Epoch 326 batch 150 train Loss 35.8953 test Loss 17.6030 with MSE metric 15127.1165\n",
      "Epoch 326 batch 160 train Loss 35.8916 test Loss 17.6015 with MSE metric 15126.2143\n",
      "Epoch 326 batch 170 train Loss 35.8878 test Loss 17.6000 with MSE metric 15125.2908\n",
      "Epoch 326 batch 180 train Loss 35.8841 test Loss 17.5985 with MSE metric 15124.3615\n",
      "Epoch 326 batch 190 train Loss 35.8803 test Loss 17.5970 with MSE metric 15123.4345\n",
      "Epoch 326 batch 200 train Loss 35.8766 test Loss 17.5955 with MSE metric 15122.4663\n",
      "Epoch 326 batch 210 train Loss 35.8729 test Loss 17.5940 with MSE metric 15121.5407\n",
      "Epoch 326 batch 220 train Loss 35.8691 test Loss 17.5925 with MSE metric 15120.6192\n",
      "Epoch 326 batch 230 train Loss 35.8654 test Loss 17.5910 with MSE metric 15119.7155\n",
      "Epoch 326 batch 240 train Loss 35.8616 test Loss 17.5895 with MSE metric 15118.8004\n",
      "Time taken for 1 epoch: 27.76603603363037 secs\n",
      "\n",
      "Epoch 327 batch 0 train Loss 35.8579 test Loss 17.5880 with MSE metric 15117.8832\n",
      "Epoch 327 batch 10 train Loss 35.8542 test Loss 17.5865 with MSE metric 15117.0111\n",
      "Epoch 327 batch 20 train Loss 35.8504 test Loss 17.5850 with MSE metric 15116.0128\n",
      "Epoch 327 batch 30 train Loss 35.8467 test Loss 17.5835 with MSE metric 15115.1118\n",
      "Epoch 327 batch 40 train Loss 35.8430 test Loss 17.5820 with MSE metric 15114.1892\n",
      "Epoch 327 batch 50 train Loss 35.8392 test Loss 17.5805 with MSE metric 15113.2417\n",
      "Epoch 327 batch 60 train Loss 35.8355 test Loss 17.5790 with MSE metric 15112.2733\n",
      "Epoch 327 batch 70 train Loss 35.8318 test Loss 17.5776 with MSE metric 15111.3260\n",
      "Epoch 327 batch 80 train Loss 35.8280 test Loss 17.5761 with MSE metric 15110.3596\n",
      "Epoch 327 batch 90 train Loss 35.8243 test Loss 17.5746 with MSE metric 15109.3812\n",
      "Epoch 327 batch 100 train Loss 35.8206 test Loss 17.5731 with MSE metric 15108.4747\n",
      "Epoch 327 batch 110 train Loss 35.8168 test Loss 17.5716 with MSE metric 15107.5516\n",
      "Epoch 327 batch 120 train Loss 35.8131 test Loss 17.5701 with MSE metric 15106.6141\n",
      "Epoch 327 batch 130 train Loss 35.8094 test Loss 17.5686 with MSE metric 15105.6861\n",
      "Epoch 327 batch 140 train Loss 35.8057 test Loss 17.5671 with MSE metric 15104.8036\n",
      "Epoch 327 batch 150 train Loss 35.8019 test Loss 17.5656 with MSE metric 15103.8553\n",
      "Epoch 327 batch 160 train Loss 35.7982 test Loss 17.5642 with MSE metric 15102.9748\n",
      "Epoch 327 batch 170 train Loss 35.7945 test Loss 17.5627 with MSE metric 15102.0185\n",
      "Epoch 327 batch 180 train Loss 35.7908 test Loss 17.5612 with MSE metric 15101.0981\n",
      "Epoch 327 batch 190 train Loss 35.7871 test Loss 17.5597 with MSE metric 15100.1768\n",
      "Epoch 327 batch 200 train Loss 35.7833 test Loss 17.5582 with MSE metric 15099.2572\n",
      "Epoch 327 batch 210 train Loss 35.7796 test Loss 17.5567 with MSE metric 15098.3537\n",
      "Epoch 327 batch 220 train Loss 35.7759 test Loss 17.5552 with MSE metric 15097.4473\n",
      "Epoch 327 batch 230 train Loss 35.7722 test Loss 17.5537 with MSE metric 15096.4934\n",
      "Epoch 327 batch 240 train Loss 35.7685 test Loss 17.5522 with MSE metric 15095.5523\n",
      "Time taken for 1 epoch: 29.333538055419922 secs\n",
      "\n",
      "Epoch 328 batch 0 train Loss 35.7648 test Loss 17.5508 with MSE metric 15094.6384\n",
      "Epoch 328 batch 10 train Loss 35.7610 test Loss 17.5493 with MSE metric 15093.7031\n",
      "Epoch 328 batch 20 train Loss 35.7573 test Loss 17.5478 with MSE metric 15092.8071\n",
      "Epoch 328 batch 30 train Loss 35.7536 test Loss 17.5463 with MSE metric 15091.8914\n",
      "Epoch 328 batch 40 train Loss 35.7499 test Loss 17.5449 with MSE metric 15090.9830\n",
      "Epoch 328 batch 50 train Loss 35.7462 test Loss 17.5434 with MSE metric 15090.0213\n",
      "Epoch 328 batch 60 train Loss 35.7425 test Loss 17.5419 with MSE metric 15089.1128\n",
      "Epoch 328 batch 70 train Loss 35.7388 test Loss 17.5404 with MSE metric 15088.2036\n",
      "Epoch 328 batch 80 train Loss 35.7351 test Loss 17.5389 with MSE metric 15087.2845\n",
      "Epoch 328 batch 90 train Loss 35.7314 test Loss 17.5375 with MSE metric 15086.3866\n",
      "Epoch 328 batch 100 train Loss 35.7277 test Loss 17.5360 with MSE metric 15085.4555\n",
      "Epoch 328 batch 110 train Loss 35.7240 test Loss 17.5345 with MSE metric 15084.5094\n",
      "Epoch 328 batch 120 train Loss 35.7202 test Loss 17.5330 with MSE metric 15083.5568\n",
      "Epoch 328 batch 130 train Loss 35.7165 test Loss 17.5315 with MSE metric 15082.6433\n",
      "Epoch 328 batch 140 train Loss 35.7128 test Loss 17.5300 with MSE metric 15081.7511\n",
      "Epoch 328 batch 150 train Loss 35.7091 test Loss 17.5286 with MSE metric 15080.8669\n",
      "Epoch 328 batch 160 train Loss 35.7054 test Loss 17.5271 with MSE metric 15079.9470\n",
      "Epoch 328 batch 170 train Loss 35.7018 test Loss 17.5256 with MSE metric 15079.0469\n",
      "Epoch 328 batch 180 train Loss 35.6981 test Loss 17.5241 with MSE metric 15078.1534\n",
      "Epoch 328 batch 190 train Loss 35.6944 test Loss 17.5226 with MSE metric 15077.2659\n",
      "Epoch 328 batch 200 train Loss 35.6907 test Loss 17.5212 with MSE metric 15076.3042\n",
      "Epoch 328 batch 210 train Loss 35.6870 test Loss 17.5197 with MSE metric 15075.3719\n",
      "Epoch 328 batch 220 train Loss 35.6833 test Loss 17.5182 with MSE metric 15074.5188\n",
      "Epoch 328 batch 230 train Loss 35.6796 test Loss 17.5167 with MSE metric 15073.6358\n",
      "Epoch 328 batch 240 train Loss 35.6759 test Loss 17.5153 with MSE metric 15072.7339\n",
      "Time taken for 1 epoch: 27.401861906051636 secs\n",
      "\n",
      "Epoch 329 batch 0 train Loss 35.6722 test Loss 17.5138 with MSE metric 15071.8190\n",
      "Epoch 329 batch 10 train Loss 35.6685 test Loss 17.5123 with MSE metric 15070.9321\n",
      "Epoch 329 batch 20 train Loss 35.6648 test Loss 17.5108 with MSE metric 15070.0173\n",
      "Epoch 329 batch 30 train Loss 35.6611 test Loss 17.5093 with MSE metric 15069.1044\n",
      "Epoch 329 batch 40 train Loss 35.6574 test Loss 17.5079 with MSE metric 15068.1856\n",
      "Epoch 329 batch 50 train Loss 35.6538 test Loss 17.5064 with MSE metric 15067.2769\n",
      "Epoch 329 batch 60 train Loss 35.6501 test Loss 17.5049 with MSE metric 15066.4171\n",
      "Epoch 329 batch 70 train Loss 35.6464 test Loss 17.5035 with MSE metric 15065.5092\n",
      "Epoch 329 batch 80 train Loss 35.6427 test Loss 17.5020 with MSE metric 15064.5689\n",
      "Epoch 329 batch 90 train Loss 35.6390 test Loss 17.5005 with MSE metric 15063.6485\n",
      "Epoch 329 batch 100 train Loss 35.6353 test Loss 17.4990 with MSE metric 15062.7607\n",
      "Epoch 329 batch 110 train Loss 35.6316 test Loss 17.4976 with MSE metric 15061.7871\n",
      "Epoch 329 batch 120 train Loss 35.6280 test Loss 17.4961 with MSE metric 15060.8915\n",
      "Epoch 329 batch 130 train Loss 35.6243 test Loss 17.4946 with MSE metric 15059.9811\n",
      "Epoch 329 batch 140 train Loss 35.6206 test Loss 17.4932 with MSE metric 15059.0227\n",
      "Epoch 329 batch 150 train Loss 35.6169 test Loss 17.4917 with MSE metric 15058.0830\n",
      "Epoch 329 batch 160 train Loss 35.6132 test Loss 17.4902 with MSE metric 15057.1163\n",
      "Epoch 329 batch 170 train Loss 35.6096 test Loss 17.4888 with MSE metric 15056.1973\n",
      "Epoch 329 batch 180 train Loss 35.6059 test Loss 17.4873 with MSE metric 15055.2941\n",
      "Epoch 329 batch 190 train Loss 35.6022 test Loss 17.4858 with MSE metric 15054.3361\n",
      "Epoch 329 batch 200 train Loss 35.5985 test Loss 17.4843 with MSE metric 15053.4562\n",
      "Epoch 329 batch 210 train Loss 35.5949 test Loss 17.4829 with MSE metric 15052.5420\n",
      "Epoch 329 batch 220 train Loss 35.5912 test Loss 17.4814 with MSE metric 15051.6451\n",
      "Epoch 329 batch 230 train Loss 35.5875 test Loss 17.4799 with MSE metric 15050.7853\n",
      "Epoch 329 batch 240 train Loss 35.5839 test Loss 17.4785 with MSE metric 15049.8521\n",
      "Time taken for 1 epoch: 29.40654492378235 secs\n",
      "\n",
      "Epoch 330 batch 0 train Loss 35.5802 test Loss 17.4770 with MSE metric 15048.9233\n",
      "Epoch 330 batch 10 train Loss 35.5765 test Loss 17.4756 with MSE metric 15048.0692\n",
      "Epoch 330 batch 20 train Loss 35.5729 test Loss 17.4741 with MSE metric 15047.1363\n",
      "Epoch 330 batch 30 train Loss 35.5692 test Loss 17.4726 with MSE metric 15046.2531\n",
      "Epoch 330 batch 40 train Loss 35.5655 test Loss 17.4712 with MSE metric 15045.2942\n",
      "Epoch 330 batch 50 train Loss 35.5619 test Loss 17.4697 with MSE metric 15044.4391\n",
      "Epoch 330 batch 60 train Loss 35.5582 test Loss 17.4682 with MSE metric 15043.5403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330 batch 70 train Loss 35.5545 test Loss 17.4668 with MSE metric 15042.6563\n",
      "Epoch 330 batch 80 train Loss 35.5509 test Loss 17.4653 with MSE metric 15041.7436\n",
      "Epoch 330 batch 90 train Loss 35.5472 test Loss 17.4638 with MSE metric 15040.8507\n",
      "Epoch 330 batch 100 train Loss 35.5435 test Loss 17.4624 with MSE metric 15039.9688\n",
      "Epoch 330 batch 110 train Loss 35.5399 test Loss 17.4609 with MSE metric 15039.0431\n",
      "Epoch 330 batch 120 train Loss 35.5362 test Loss 17.4594 with MSE metric 15038.1282\n",
      "Epoch 330 batch 130 train Loss 35.5326 test Loss 17.4580 with MSE metric 15037.2186\n",
      "Epoch 330 batch 140 train Loss 35.5289 test Loss 17.4565 with MSE metric 15036.2669\n",
      "Epoch 330 batch 150 train Loss 35.5253 test Loss 17.4551 with MSE metric 15035.3573\n",
      "Epoch 330 batch 160 train Loss 35.5216 test Loss 17.4536 with MSE metric 15034.4719\n",
      "Epoch 330 batch 170 train Loss 35.5179 test Loss 17.4521 with MSE metric 15033.5500\n",
      "Epoch 330 batch 180 train Loss 35.5143 test Loss 17.4507 with MSE metric 15032.6326\n",
      "Epoch 330 batch 190 train Loss 35.5106 test Loss 17.4492 with MSE metric 15031.7437\n",
      "Epoch 330 batch 200 train Loss 35.5070 test Loss 17.4478 with MSE metric 15030.8126\n",
      "Epoch 330 batch 210 train Loss 35.5033 test Loss 17.4463 with MSE metric 15029.8993\n",
      "Epoch 330 batch 220 train Loss 35.4997 test Loss 17.4448 with MSE metric 15028.9909\n",
      "Epoch 330 batch 230 train Loss 35.4960 test Loss 17.4434 with MSE metric 15028.0859\n",
      "Epoch 330 batch 240 train Loss 35.4924 test Loss 17.4419 with MSE metric 15027.1940\n",
      "Time taken for 1 epoch: 27.177284717559814 secs\n",
      "\n",
      "Epoch 331 batch 0 train Loss 35.4887 test Loss 17.4405 with MSE metric 15026.2725\n",
      "Epoch 331 batch 10 train Loss 35.4851 test Loss 17.4390 with MSE metric 15025.3571\n",
      "Epoch 331 batch 20 train Loss 35.4814 test Loss 17.4376 with MSE metric 15024.4418\n",
      "Epoch 331 batch 30 train Loss 35.4778 test Loss 17.4361 with MSE metric 15023.5832\n",
      "Epoch 331 batch 40 train Loss 35.4742 test Loss 17.4346 with MSE metric 15022.7057\n",
      "Epoch 331 batch 50 train Loss 35.4705 test Loss 17.4332 with MSE metric 15021.7634\n",
      "Epoch 331 batch 60 train Loss 35.4669 test Loss 17.4317 with MSE metric 15020.9289\n",
      "Epoch 331 batch 70 train Loss 35.4632 test Loss 17.4303 with MSE metric 15020.0127\n",
      "Epoch 331 batch 80 train Loss 35.4596 test Loss 17.4288 with MSE metric 15019.1197\n",
      "Epoch 331 batch 90 train Loss 35.4560 test Loss 17.4274 with MSE metric 15018.1926\n",
      "Epoch 331 batch 100 train Loss 35.4523 test Loss 17.4259 with MSE metric 15017.2471\n",
      "Epoch 331 batch 110 train Loss 35.4487 test Loss 17.4245 with MSE metric 15016.3903\n",
      "Epoch 331 batch 120 train Loss 35.4450 test Loss 17.4230 with MSE metric 15015.5079\n",
      "Epoch 331 batch 130 train Loss 35.4414 test Loss 17.4215 with MSE metric 15014.5928\n",
      "Epoch 331 batch 140 train Loss 35.4378 test Loss 17.4201 with MSE metric 15013.6667\n",
      "Epoch 331 batch 150 train Loss 35.4341 test Loss 17.4186 with MSE metric 15012.8193\n",
      "Epoch 331 batch 160 train Loss 35.4305 test Loss 17.4172 with MSE metric 15011.9165\n",
      "Epoch 331 batch 170 train Loss 35.4269 test Loss 17.4157 with MSE metric 15011.0181\n",
      "Epoch 331 batch 180 train Loss 35.4232 test Loss 17.4143 with MSE metric 15010.1313\n",
      "Epoch 331 batch 190 train Loss 35.4196 test Loss 17.4128 with MSE metric 15009.2184\n",
      "Epoch 331 batch 200 train Loss 35.4160 test Loss 17.4114 with MSE metric 15008.2660\n",
      "Epoch 331 batch 210 train Loss 35.4123 test Loss 17.4099 with MSE metric 15007.3714\n",
      "Epoch 331 batch 220 train Loss 35.4087 test Loss 17.4085 with MSE metric 15006.4818\n",
      "Epoch 331 batch 230 train Loss 35.4051 test Loss 17.4070 with MSE metric 15005.6331\n",
      "Epoch 331 batch 240 train Loss 35.4015 test Loss 17.4056 with MSE metric 15004.7476\n",
      "Time taken for 1 epoch: 26.58459782600403 secs\n",
      "\n",
      "Epoch 332 batch 0 train Loss 35.3978 test Loss 17.4041 with MSE metric 15003.8351\n",
      "Epoch 332 batch 10 train Loss 35.3942 test Loss 17.4027 with MSE metric 15002.8919\n",
      "Epoch 332 batch 20 train Loss 35.3906 test Loss 17.4013 with MSE metric 15001.9917\n",
      "Epoch 332 batch 30 train Loss 35.3870 test Loss 17.3998 with MSE metric 15001.1210\n",
      "Epoch 332 batch 40 train Loss 35.3834 test Loss 17.3984 with MSE metric 15000.2255\n",
      "Epoch 332 batch 50 train Loss 35.3797 test Loss 17.3969 with MSE metric 14999.3578\n",
      "Epoch 332 batch 60 train Loss 35.3761 test Loss 17.3955 with MSE metric 14998.4495\n",
      "Epoch 332 batch 70 train Loss 35.3725 test Loss 17.3940 with MSE metric 14997.5798\n",
      "Epoch 332 batch 80 train Loss 35.3689 test Loss 17.3926 with MSE metric 14996.6786\n",
      "Epoch 332 batch 90 train Loss 35.3653 test Loss 17.3911 with MSE metric 14995.8086\n",
      "Epoch 332 batch 100 train Loss 35.3616 test Loss 17.3897 with MSE metric 14994.9386\n",
      "Epoch 332 batch 110 train Loss 35.3580 test Loss 17.3882 with MSE metric 14994.0521\n",
      "Epoch 332 batch 120 train Loss 35.3544 test Loss 17.3868 with MSE metric 14993.1220\n",
      "Epoch 332 batch 130 train Loss 35.3508 test Loss 17.3854 with MSE metric 14992.2191\n",
      "Epoch 332 batch 140 train Loss 35.3472 test Loss 17.3839 with MSE metric 14991.3541\n",
      "Epoch 332 batch 150 train Loss 35.3436 test Loss 17.3825 with MSE metric 14990.4483\n",
      "Epoch 332 batch 160 train Loss 35.3400 test Loss 17.3810 with MSE metric 14989.5431\n",
      "Epoch 332 batch 170 train Loss 35.3364 test Loss 17.3796 with MSE metric 14988.6649\n",
      "Epoch 332 batch 180 train Loss 35.3327 test Loss 17.3782 with MSE metric 14987.7364\n",
      "Epoch 332 batch 190 train Loss 35.3291 test Loss 17.3767 with MSE metric 14986.8434\n",
      "Epoch 332 batch 200 train Loss 35.3255 test Loss 17.3753 with MSE metric 14985.9377\n",
      "Epoch 332 batch 210 train Loss 35.3219 test Loss 17.3738 with MSE metric 14985.0517\n",
      "Epoch 332 batch 220 train Loss 35.3183 test Loss 17.3724 with MSE metric 14984.1744\n",
      "Epoch 332 batch 230 train Loss 35.3147 test Loss 17.3710 with MSE metric 14983.2750\n",
      "Epoch 332 batch 240 train Loss 35.3111 test Loss 17.3695 with MSE metric 14982.3889\n",
      "Time taken for 1 epoch: 26.569172859191895 secs\n",
      "\n",
      "Epoch 333 batch 0 train Loss 35.3075 test Loss 17.3681 with MSE metric 14981.5041\n",
      "Epoch 333 batch 10 train Loss 35.3039 test Loss 17.3666 with MSE metric 14980.6481\n",
      "Epoch 333 batch 20 train Loss 35.3003 test Loss 17.3652 with MSE metric 14979.7522\n",
      "Epoch 333 batch 30 train Loss 35.2967 test Loss 17.3638 with MSE metric 14978.8186\n",
      "Epoch 333 batch 40 train Loss 35.2931 test Loss 17.3623 with MSE metric 14977.9584\n",
      "Epoch 333 batch 50 train Loss 35.2895 test Loss 17.3609 with MSE metric 14977.0634\n",
      "Epoch 333 batch 60 train Loss 35.2859 test Loss 17.3595 with MSE metric 14976.1335\n",
      "Epoch 333 batch 70 train Loss 35.2823 test Loss 17.3580 with MSE metric 14975.2359\n",
      "Epoch 333 batch 80 train Loss 35.2787 test Loss 17.3566 with MSE metric 14974.3604\n",
      "Epoch 333 batch 90 train Loss 35.2751 test Loss 17.3551 with MSE metric 14973.4782\n",
      "Epoch 333 batch 100 train Loss 35.2715 test Loss 17.3537 with MSE metric 14972.5954\n",
      "Epoch 333 batch 110 train Loss 35.2679 test Loss 17.3523 with MSE metric 14971.7225\n",
      "Epoch 333 batch 120 train Loss 35.2643 test Loss 17.3509 with MSE metric 14970.8123\n",
      "Epoch 333 batch 130 train Loss 35.2607 test Loss 17.3494 with MSE metric 14969.9245\n",
      "Epoch 333 batch 140 train Loss 35.2571 test Loss 17.3480 with MSE metric 14969.0050\n",
      "Epoch 333 batch 150 train Loss 35.2535 test Loss 17.3465 with MSE metric 14968.0963\n",
      "Epoch 333 batch 160 train Loss 35.2500 test Loss 17.3451 with MSE metric 14967.2013\n",
      "Epoch 333 batch 170 train Loss 35.2464 test Loss 17.3437 with MSE metric 14966.3300\n",
      "Epoch 333 batch 180 train Loss 35.2428 test Loss 17.3422 with MSE metric 14965.4446\n",
      "Epoch 333 batch 190 train Loss 35.2392 test Loss 17.3408 with MSE metric 14964.5588\n",
      "Epoch 333 batch 200 train Loss 35.2356 test Loss 17.3394 with MSE metric 14963.6990\n",
      "Epoch 333 batch 210 train Loss 35.2320 test Loss 17.3379 with MSE metric 14962.7919\n",
      "Epoch 333 batch 220 train Loss 35.2284 test Loss 17.3365 with MSE metric 14961.8855\n",
      "Epoch 333 batch 230 train Loss 35.2249 test Loss 17.3350 with MSE metric 14961.0040\n",
      "Epoch 333 batch 240 train Loss 35.2213 test Loss 17.3336 with MSE metric 14960.0787\n",
      "Time taken for 1 epoch: 26.877761125564575 secs\n",
      "\n",
      "Epoch 334 batch 0 train Loss 35.2177 test Loss 17.3322 with MSE metric 14959.1574\n",
      "Epoch 334 batch 10 train Loss 35.2141 test Loss 17.3308 with MSE metric 14958.3067\n",
      "Epoch 334 batch 20 train Loss 35.2105 test Loss 17.3293 with MSE metric 14957.4029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334 batch 30 train Loss 35.2069 test Loss 17.3279 with MSE metric 14956.5306\n",
      "Epoch 334 batch 40 train Loss 35.2034 test Loss 17.3265 with MSE metric 14955.6247\n",
      "Epoch 334 batch 50 train Loss 35.1998 test Loss 17.3251 with MSE metric 14954.7167\n",
      "Epoch 334 batch 60 train Loss 35.1962 test Loss 17.3236 with MSE metric 14953.7733\n",
      "Epoch 334 batch 70 train Loss 35.1926 test Loss 17.3222 with MSE metric 14952.9170\n",
      "Epoch 334 batch 80 train Loss 35.1891 test Loss 17.3208 with MSE metric 14952.0508\n",
      "Epoch 334 batch 90 train Loss 35.1855 test Loss 17.3194 with MSE metric 14951.1496\n",
      "Epoch 334 batch 100 train Loss 35.1819 test Loss 17.3179 with MSE metric 14950.2142\n",
      "Epoch 334 batch 110 train Loss 35.1783 test Loss 17.3165 with MSE metric 14949.3568\n",
      "Epoch 334 batch 120 train Loss 35.1748 test Loss 17.3151 with MSE metric 14948.5067\n",
      "Epoch 334 batch 130 train Loss 35.1712 test Loss 17.3136 with MSE metric 14947.6199\n",
      "Epoch 334 batch 140 train Loss 35.1676 test Loss 17.3122 with MSE metric 14946.7448\n",
      "Epoch 334 batch 150 train Loss 35.1641 test Loss 17.3108 with MSE metric 14945.8821\n",
      "Epoch 334 batch 160 train Loss 35.1605 test Loss 17.3094 with MSE metric 14944.9856\n",
      "Epoch 334 batch 170 train Loss 35.1569 test Loss 17.3079 with MSE metric 14944.1067\n",
      "Epoch 334 batch 180 train Loss 35.1534 test Loss 17.3065 with MSE metric 14943.1854\n",
      "Epoch 334 batch 190 train Loss 35.1498 test Loss 17.3051 with MSE metric 14942.3283\n",
      "Epoch 334 batch 200 train Loss 35.1462 test Loss 17.3037 with MSE metric 14941.4657\n",
      "Epoch 334 batch 210 train Loss 35.1427 test Loss 17.3022 with MSE metric 14940.5471\n",
      "Epoch 334 batch 220 train Loss 35.1391 test Loss 17.3008 with MSE metric 14939.6511\n",
      "Epoch 334 batch 230 train Loss 35.1355 test Loss 17.2994 with MSE metric 14938.7853\n",
      "Epoch 334 batch 240 train Loss 35.1320 test Loss 17.2980 with MSE metric 14937.9378\n",
      "Time taken for 1 epoch: 27.632249116897583 secs\n",
      "\n",
      "Epoch 335 batch 0 train Loss 35.1284 test Loss 17.2966 with MSE metric 14937.0684\n",
      "Epoch 335 batch 10 train Loss 35.1249 test Loss 17.2951 with MSE metric 14936.1689\n",
      "Epoch 335 batch 20 train Loss 35.1213 test Loss 17.2937 with MSE metric 14935.2656\n",
      "Epoch 335 batch 30 train Loss 35.1177 test Loss 17.2923 with MSE metric 14934.3756\n",
      "Epoch 335 batch 40 train Loss 35.1142 test Loss 17.2909 with MSE metric 14933.5047\n",
      "Epoch 335 batch 50 train Loss 35.1106 test Loss 17.2894 with MSE metric 14932.6201\n",
      "Epoch 335 batch 60 train Loss 35.1071 test Loss 17.2880 with MSE metric 14931.7004\n",
      "Epoch 335 batch 70 train Loss 35.1035 test Loss 17.2866 with MSE metric 14930.8289\n",
      "Epoch 335 batch 80 train Loss 35.1000 test Loss 17.2852 with MSE metric 14929.8967\n",
      "Epoch 335 batch 90 train Loss 35.0964 test Loss 17.2838 with MSE metric 14929.0099\n",
      "Epoch 335 batch 100 train Loss 35.0928 test Loss 17.2824 with MSE metric 14928.1615\n",
      "Epoch 335 batch 110 train Loss 35.0893 test Loss 17.2810 with MSE metric 14927.3254\n",
      "Epoch 335 batch 120 train Loss 35.0857 test Loss 17.2796 with MSE metric 14926.4150\n",
      "Epoch 335 batch 130 train Loss 35.0822 test Loss 17.2781 with MSE metric 14925.5192\n",
      "Epoch 335 batch 140 train Loss 35.0786 test Loss 17.2767 with MSE metric 14924.6081\n",
      "Epoch 335 batch 150 train Loss 35.0751 test Loss 17.2753 with MSE metric 14923.7456\n",
      "Epoch 335 batch 160 train Loss 35.0715 test Loss 17.2739 with MSE metric 14922.8517\n",
      "Epoch 335 batch 170 train Loss 35.0680 test Loss 17.2725 with MSE metric 14921.9614\n",
      "Epoch 335 batch 180 train Loss 35.0645 test Loss 17.2711 with MSE metric 14921.0893\n",
      "Epoch 335 batch 190 train Loss 35.0609 test Loss 17.2696 with MSE metric 14920.1998\n",
      "Epoch 335 batch 200 train Loss 35.0574 test Loss 17.2682 with MSE metric 14919.3309\n",
      "Epoch 335 batch 210 train Loss 35.0538 test Loss 17.2668 with MSE metric 14918.4463\n",
      "Epoch 335 batch 220 train Loss 35.0503 test Loss 17.2654 with MSE metric 14917.6167\n",
      "Epoch 335 batch 230 train Loss 35.0467 test Loss 17.2640 with MSE metric 14916.7486\n",
      "Epoch 335 batch 240 train Loss 35.0432 test Loss 17.2626 with MSE metric 14915.8512\n",
      "Time taken for 1 epoch: 26.674312114715576 secs\n",
      "\n",
      "Epoch 336 batch 0 train Loss 35.0397 test Loss 17.2612 with MSE metric 14914.9801\n",
      "Epoch 336 batch 10 train Loss 35.0361 test Loss 17.2597 with MSE metric 14914.0666\n",
      "Epoch 336 batch 20 train Loss 35.0326 test Loss 17.2583 with MSE metric 14913.2068\n",
      "Epoch 336 batch 30 train Loss 35.0290 test Loss 17.2569 with MSE metric 14912.3350\n",
      "Epoch 336 batch 40 train Loss 35.0255 test Loss 17.2555 with MSE metric 14911.5169\n",
      "Epoch 336 batch 50 train Loss 35.0220 test Loss 17.2541 with MSE metric 14910.6579\n",
      "Epoch 336 batch 60 train Loss 35.0184 test Loss 17.2527 with MSE metric 14909.7754\n",
      "Epoch 336 batch 70 train Loss 35.0149 test Loss 17.2513 with MSE metric 14908.9235\n",
      "Epoch 336 batch 80 train Loss 35.0114 test Loss 17.2499 with MSE metric 14908.0327\n",
      "Epoch 336 batch 90 train Loss 35.0078 test Loss 17.2485 with MSE metric 14907.1372\n",
      "Epoch 336 batch 100 train Loss 35.0043 test Loss 17.2470 with MSE metric 14906.2360\n",
      "Epoch 336 batch 110 train Loss 35.0008 test Loss 17.2456 with MSE metric 14905.3907\n",
      "Epoch 336 batch 120 train Loss 34.9973 test Loss 17.2442 with MSE metric 14904.5289\n",
      "Epoch 336 batch 130 train Loss 34.9937 test Loss 17.2428 with MSE metric 14903.6556\n",
      "Epoch 336 batch 140 train Loss 34.9902 test Loss 17.2414 with MSE metric 14902.7913\n",
      "Epoch 336 batch 150 train Loss 34.9867 test Loss 17.2400 with MSE metric 14901.9442\n",
      "Epoch 336 batch 160 train Loss 34.9832 test Loss 17.2386 with MSE metric 14901.1073\n",
      "Epoch 336 batch 170 train Loss 34.9796 test Loss 17.2372 with MSE metric 14900.2405\n",
      "Epoch 336 batch 180 train Loss 34.9761 test Loss 17.2357 with MSE metric 14899.4218\n",
      "Epoch 336 batch 190 train Loss 34.9726 test Loss 17.2343 with MSE metric 14898.6080\n",
      "Epoch 336 batch 200 train Loss 34.9691 test Loss 17.2329 with MSE metric 14897.6957\n",
      "Epoch 336 batch 210 train Loss 34.9655 test Loss 17.2315 with MSE metric 14896.7930\n",
      "Epoch 336 batch 220 train Loss 34.9620 test Loss 17.2301 with MSE metric 14895.8690\n",
      "Epoch 336 batch 230 train Loss 34.9585 test Loss 17.2287 with MSE metric 14894.9834\n",
      "Epoch 336 batch 240 train Loss 34.9550 test Loss 17.2273 with MSE metric 14894.1329\n",
      "Time taken for 1 epoch: 27.008230686187744 secs\n",
      "\n",
      "Epoch 337 batch 0 train Loss 34.9515 test Loss 17.2259 with MSE metric 14893.2677\n",
      "Epoch 337 batch 10 train Loss 34.9479 test Loss 17.2245 with MSE metric 14892.3777\n",
      "Epoch 337 batch 20 train Loss 34.9444 test Loss 17.2231 with MSE metric 14891.5443\n",
      "Epoch 337 batch 30 train Loss 34.9409 test Loss 17.2217 with MSE metric 14890.7133\n",
      "Epoch 337 batch 40 train Loss 34.9374 test Loss 17.2203 with MSE metric 14889.7864\n",
      "Epoch 337 batch 50 train Loss 34.9339 test Loss 17.2189 with MSE metric 14888.9117\n",
      "Epoch 337 batch 60 train Loss 34.9304 test Loss 17.2175 with MSE metric 14888.0535\n",
      "Epoch 337 batch 70 train Loss 34.9269 test Loss 17.2161 with MSE metric 14887.1186\n",
      "Epoch 337 batch 80 train Loss 34.9233 test Loss 17.2147 with MSE metric 14886.2092\n",
      "Epoch 337 batch 90 train Loss 34.9198 test Loss 17.2133 with MSE metric 14885.3341\n",
      "Epoch 337 batch 100 train Loss 34.9163 test Loss 17.2119 with MSE metric 14884.4662\n",
      "Epoch 337 batch 110 train Loss 34.9128 test Loss 17.2105 with MSE metric 14883.5617\n",
      "Epoch 337 batch 120 train Loss 34.9093 test Loss 17.2091 with MSE metric 14882.6788\n",
      "Epoch 337 batch 130 train Loss 34.9058 test Loss 17.2077 with MSE metric 14881.8251\n",
      "Epoch 337 batch 140 train Loss 34.9023 test Loss 17.2063 with MSE metric 14880.9290\n",
      "Epoch 337 batch 150 train Loss 34.8988 test Loss 17.2049 with MSE metric 14880.0594\n",
      "Epoch 337 batch 160 train Loss 34.8953 test Loss 17.2035 with MSE metric 14879.1855\n",
      "Epoch 337 batch 170 train Loss 34.8918 test Loss 17.2021 with MSE metric 14878.3227\n",
      "Epoch 337 batch 180 train Loss 34.8883 test Loss 17.2007 with MSE metric 14877.4607\n",
      "Epoch 337 batch 190 train Loss 34.8848 test Loss 17.1993 with MSE metric 14876.6073\n",
      "Epoch 337 batch 200 train Loss 34.8813 test Loss 17.1979 with MSE metric 14875.7509\n",
      "Epoch 337 batch 210 train Loss 34.8777 test Loss 17.1965 with MSE metric 14874.8618\n",
      "Epoch 337 batch 220 train Loss 34.8742 test Loss 17.1951 with MSE metric 14873.9617\n",
      "Epoch 337 batch 230 train Loss 34.8707 test Loss 17.1937 with MSE metric 14873.1124\n",
      "Epoch 337 batch 240 train Loss 34.8673 test Loss 17.1923 with MSE metric 14872.2390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 26.87293291091919 secs\n",
      "\n",
      "Epoch 338 batch 0 train Loss 34.8638 test Loss 17.1909 with MSE metric 14871.3764\n",
      "Epoch 338 batch 10 train Loss 34.8603 test Loss 17.1895 with MSE metric 14870.5226\n",
      "Epoch 338 batch 20 train Loss 34.8568 test Loss 17.1881 with MSE metric 14869.6787\n",
      "Epoch 338 batch 30 train Loss 34.8533 test Loss 17.1867 with MSE metric 14868.7577\n",
      "Epoch 338 batch 40 train Loss 34.8498 test Loss 17.1853 with MSE metric 14867.8861\n",
      "Epoch 338 batch 50 train Loss 34.8463 test Loss 17.1839 with MSE metric 14867.0124\n",
      "Epoch 338 batch 60 train Loss 34.8428 test Loss 17.1825 with MSE metric 14866.1718\n",
      "Epoch 338 batch 70 train Loss 34.8393 test Loss 17.1811 with MSE metric 14865.2830\n",
      "Epoch 338 batch 80 train Loss 34.8358 test Loss 17.1797 with MSE metric 14864.4321\n",
      "Epoch 338 batch 90 train Loss 34.8323 test Loss 17.1783 with MSE metric 14863.5711\n",
      "Epoch 338 batch 100 train Loss 34.8288 test Loss 17.1769 with MSE metric 14862.6958\n",
      "Epoch 338 batch 110 train Loss 34.8253 test Loss 17.1755 with MSE metric 14861.8060\n",
      "Epoch 338 batch 120 train Loss 34.8218 test Loss 17.1741 with MSE metric 14860.9513\n",
      "Epoch 338 batch 130 train Loss 34.8184 test Loss 17.1727 with MSE metric 14860.0873\n",
      "Epoch 338 batch 140 train Loss 34.8149 test Loss 17.1713 with MSE metric 14859.2105\n",
      "Epoch 338 batch 150 train Loss 34.8114 test Loss 17.1699 with MSE metric 14858.3318\n",
      "Epoch 338 batch 160 train Loss 34.8079 test Loss 17.1685 with MSE metric 14857.4529\n",
      "Epoch 338 batch 170 train Loss 34.8044 test Loss 17.1672 with MSE metric 14856.5987\n",
      "Epoch 338 batch 180 train Loss 34.8009 test Loss 17.1658 with MSE metric 14855.7004\n",
      "Epoch 338 batch 190 train Loss 34.7974 test Loss 17.1644 with MSE metric 14854.8369\n",
      "Epoch 338 batch 200 train Loss 34.7940 test Loss 17.1630 with MSE metric 14853.9814\n",
      "Epoch 338 batch 210 train Loss 34.7905 test Loss 17.1616 with MSE metric 14853.1301\n",
      "Epoch 338 batch 220 train Loss 34.7870 test Loss 17.1602 with MSE metric 14852.2944\n",
      "Epoch 338 batch 230 train Loss 34.7835 test Loss 17.1588 with MSE metric 14851.4272\n",
      "Epoch 338 batch 240 train Loss 34.7800 test Loss 17.1574 with MSE metric 14850.5651\n",
      "Time taken for 1 epoch: 27.001073837280273 secs\n",
      "\n",
      "Epoch 339 batch 0 train Loss 34.7766 test Loss 17.1561 with MSE metric 14849.6752\n",
      "Epoch 339 batch 10 train Loss 34.7731 test Loss 17.1547 with MSE metric 14848.8281\n",
      "Epoch 339 batch 20 train Loss 34.7696 test Loss 17.1533 with MSE metric 14847.9492\n",
      "Epoch 339 batch 30 train Loss 34.7661 test Loss 17.1519 with MSE metric 14847.0625\n",
      "Epoch 339 batch 40 train Loss 34.7627 test Loss 17.1505 with MSE metric 14846.2566\n",
      "Epoch 339 batch 50 train Loss 34.7592 test Loss 17.1491 with MSE metric 14845.4018\n",
      "Epoch 339 batch 60 train Loss 34.7557 test Loss 17.1478 with MSE metric 14844.5573\n",
      "Epoch 339 batch 70 train Loss 34.7522 test Loss 17.1464 with MSE metric 14843.6978\n",
      "Epoch 339 batch 80 train Loss 34.7488 test Loss 17.1450 with MSE metric 14842.8610\n",
      "Epoch 339 batch 90 train Loss 34.7453 test Loss 17.1436 with MSE metric 14842.0131\n",
      "Epoch 339 batch 100 train Loss 34.7418 test Loss 17.1422 with MSE metric 14841.1489\n",
      "Epoch 339 batch 110 train Loss 34.7384 test Loss 17.1408 with MSE metric 14840.3313\n",
      "Epoch 339 batch 120 train Loss 34.7349 test Loss 17.1394 with MSE metric 14839.4965\n",
      "Epoch 339 batch 130 train Loss 34.7314 test Loss 17.1380 with MSE metric 14838.6383\n",
      "Epoch 339 batch 140 train Loss 34.7280 test Loss 17.1367 with MSE metric 14837.8117\n",
      "Epoch 339 batch 150 train Loss 34.7245 test Loss 17.1353 with MSE metric 14836.9549\n",
      "Epoch 339 batch 160 train Loss 34.7211 test Loss 17.1339 with MSE metric 14836.0569\n",
      "Epoch 339 batch 170 train Loss 34.7176 test Loss 17.1325 with MSE metric 14835.1911\n",
      "Epoch 339 batch 180 train Loss 34.7141 test Loss 17.1311 with MSE metric 14834.3510\n",
      "Epoch 339 batch 190 train Loss 34.7107 test Loss 17.1297 with MSE metric 14833.4931\n",
      "Epoch 339 batch 200 train Loss 34.7072 test Loss 17.1283 with MSE metric 14832.6798\n",
      "Epoch 339 batch 210 train Loss 34.7037 test Loss 17.1270 with MSE metric 14831.8321\n",
      "Epoch 339 batch 220 train Loss 34.7003 test Loss 17.1256 with MSE metric 14831.0298\n",
      "Epoch 339 batch 230 train Loss 34.6968 test Loss 17.1242 with MSE metric 14830.1988\n",
      "Epoch 339 batch 240 train Loss 34.6934 test Loss 17.1228 with MSE metric 14829.3250\n",
      "Time taken for 1 epoch: 30.815207958221436 secs\n",
      "\n",
      "Epoch 340 batch 0 train Loss 34.6899 test Loss 17.1215 with MSE metric 14828.4553\n",
      "Epoch 340 batch 10 train Loss 34.6865 test Loss 17.1201 with MSE metric 14827.6114\n",
      "Epoch 340 batch 20 train Loss 34.6830 test Loss 17.1187 with MSE metric 14826.7756\n",
      "Epoch 340 batch 30 train Loss 34.6796 test Loss 17.1173 with MSE metric 14825.9397\n",
      "Epoch 340 batch 40 train Loss 34.6761 test Loss 17.1159 with MSE metric 14825.0803\n",
      "Epoch 340 batch 50 train Loss 34.6727 test Loss 17.1145 with MSE metric 14824.2225\n",
      "Epoch 340 batch 60 train Loss 34.6692 test Loss 17.1132 with MSE metric 14823.4118\n",
      "Epoch 340 batch 70 train Loss 34.6658 test Loss 17.1118 with MSE metric 14822.6146\n",
      "Epoch 340 batch 80 train Loss 34.6623 test Loss 17.1104 with MSE metric 14821.7517\n",
      "Epoch 340 batch 90 train Loss 34.6589 test Loss 17.1090 with MSE metric 14820.9093\n",
      "Epoch 340 batch 100 train Loss 34.6554 test Loss 17.1077 with MSE metric 14820.0485\n",
      "Epoch 340 batch 110 train Loss 34.6520 test Loss 17.1063 with MSE metric 14819.2006\n",
      "Epoch 340 batch 120 train Loss 34.6485 test Loss 17.1049 with MSE metric 14818.3619\n",
      "Epoch 340 batch 130 train Loss 34.6451 test Loss 17.1035 with MSE metric 14817.4933\n",
      "Epoch 340 batch 140 train Loss 34.6416 test Loss 17.1022 with MSE metric 14816.6511\n",
      "Epoch 340 batch 150 train Loss 34.6382 test Loss 17.1008 with MSE metric 14815.8229\n",
      "Epoch 340 batch 160 train Loss 34.6347 test Loss 17.0994 with MSE metric 14814.9866\n",
      "Epoch 340 batch 170 train Loss 34.6313 test Loss 17.0980 with MSE metric 14814.1385\n",
      "Epoch 340 batch 180 train Loss 34.6279 test Loss 17.0967 with MSE metric 14813.2609\n",
      "Epoch 340 batch 190 train Loss 34.6244 test Loss 17.0953 with MSE metric 14812.4176\n",
      "Epoch 340 batch 200 train Loss 34.6210 test Loss 17.0939 with MSE metric 14811.5617\n",
      "Epoch 340 batch 210 train Loss 34.6175 test Loss 17.0926 with MSE metric 14810.7180\n",
      "Epoch 340 batch 220 train Loss 34.6141 test Loss 17.0912 with MSE metric 14809.8833\n",
      "Epoch 340 batch 230 train Loss 34.6106 test Loss 17.0898 with MSE metric 14808.9752\n",
      "Epoch 340 batch 240 train Loss 34.6072 test Loss 17.0884 with MSE metric 14808.1396\n",
      "Time taken for 1 epoch: 28.619327068328857 secs\n",
      "\n",
      "Epoch 341 batch 0 train Loss 34.6038 test Loss 17.0871 with MSE metric 14807.2443\n",
      "Epoch 341 batch 10 train Loss 34.6003 test Loss 17.0857 with MSE metric 14806.3931\n",
      "Epoch 341 batch 20 train Loss 34.5969 test Loss 17.0843 with MSE metric 14805.5822\n",
      "Epoch 341 batch 30 train Loss 34.5935 test Loss 17.0830 with MSE metric 14804.7528\n",
      "Epoch 341 batch 40 train Loss 34.5900 test Loss 17.0816 with MSE metric 14803.8860\n",
      "Epoch 341 batch 50 train Loss 34.5866 test Loss 17.0802 with MSE metric 14803.0407\n",
      "Epoch 341 batch 60 train Loss 34.5832 test Loss 17.0788 with MSE metric 14802.1856\n",
      "Epoch 341 batch 70 train Loss 34.5797 test Loss 17.0774 with MSE metric 14801.3851\n",
      "Epoch 341 batch 80 train Loss 34.5763 test Loss 17.0761 with MSE metric 14800.5357\n",
      "Epoch 341 batch 90 train Loss 34.5729 test Loss 17.0747 with MSE metric 14799.7312\n",
      "Epoch 341 batch 100 train Loss 34.5695 test Loss 17.0733 with MSE metric 14798.8639\n",
      "Epoch 341 batch 110 train Loss 34.5660 test Loss 17.0720 with MSE metric 14798.0511\n",
      "Epoch 341 batch 120 train Loss 34.5626 test Loss 17.0706 with MSE metric 14797.1781\n",
      "Epoch 341 batch 130 train Loss 34.5592 test Loss 17.0692 with MSE metric 14796.3411\n",
      "Epoch 341 batch 140 train Loss 34.5558 test Loss 17.0678 with MSE metric 14795.4985\n",
      "Epoch 341 batch 150 train Loss 34.5523 test Loss 17.0665 with MSE metric 14794.6886\n",
      "Epoch 341 batch 160 train Loss 34.5489 test Loss 17.0651 with MSE metric 14793.8485\n",
      "Epoch 341 batch 170 train Loss 34.5455 test Loss 17.0637 with MSE metric 14792.9464\n",
      "Epoch 341 batch 180 train Loss 34.5421 test Loss 17.0624 with MSE metric 14792.0531\n",
      "Epoch 341 batch 190 train Loss 34.5386 test Loss 17.0610 with MSE metric 14791.1467\n",
      "Epoch 341 batch 200 train Loss 34.5352 test Loss 17.0597 with MSE metric 14790.2880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341 batch 210 train Loss 34.5318 test Loss 17.0583 with MSE metric 14789.4147\n",
      "Epoch 341 batch 220 train Loss 34.5284 test Loss 17.0569 with MSE metric 14788.5324\n",
      "Epoch 341 batch 230 train Loss 34.5250 test Loss 17.0556 with MSE metric 14787.6965\n",
      "Epoch 341 batch 240 train Loss 34.5215 test Loss 17.0542 with MSE metric 14786.8465\n",
      "Time taken for 1 epoch: 28.083299160003662 secs\n",
      "\n",
      "Epoch 342 batch 0 train Loss 34.5181 test Loss 17.0528 with MSE metric 14786.0183\n",
      "Epoch 342 batch 10 train Loss 34.5147 test Loss 17.0515 with MSE metric 14785.2129\n",
      "Epoch 342 batch 20 train Loss 34.5113 test Loss 17.0501 with MSE metric 14784.3785\n",
      "Epoch 342 batch 30 train Loss 34.5079 test Loss 17.0487 with MSE metric 14783.5092\n",
      "Epoch 342 batch 40 train Loss 34.5045 test Loss 17.0474 with MSE metric 14782.6761\n",
      "Epoch 342 batch 50 train Loss 34.5010 test Loss 17.0460 with MSE metric 14781.8278\n",
      "Epoch 342 batch 60 train Loss 34.4976 test Loss 17.0446 with MSE metric 14780.9611\n",
      "Epoch 342 batch 70 train Loss 34.4942 test Loss 17.0433 with MSE metric 14780.1040\n",
      "Epoch 342 batch 80 train Loss 34.4908 test Loss 17.0419 with MSE metric 14779.2498\n",
      "Epoch 342 batch 90 train Loss 34.4874 test Loss 17.0405 with MSE metric 14778.3715\n",
      "Epoch 342 batch 100 train Loss 34.4840 test Loss 17.0392 with MSE metric 14777.5384\n",
      "Epoch 342 batch 110 train Loss 34.4806 test Loss 17.0378 with MSE metric 14776.6483\n",
      "Epoch 342 batch 120 train Loss 34.4772 test Loss 17.0364 with MSE metric 14775.7879\n",
      "Epoch 342 batch 130 train Loss 34.4738 test Loss 17.0351 with MSE metric 14774.9596\n",
      "Epoch 342 batch 140 train Loss 34.4704 test Loss 17.0337 with MSE metric 14774.1467\n",
      "Epoch 342 batch 150 train Loss 34.4670 test Loss 17.0324 with MSE metric 14773.3090\n",
      "Epoch 342 batch 160 train Loss 34.4636 test Loss 17.0310 with MSE metric 14772.4735\n",
      "Epoch 342 batch 170 train Loss 34.4602 test Loss 17.0296 with MSE metric 14771.6254\n",
      "Epoch 342 batch 180 train Loss 34.4568 test Loss 17.0283 with MSE metric 14770.7842\n",
      "Epoch 342 batch 190 train Loss 34.4533 test Loss 17.0269 with MSE metric 14769.9282\n",
      "Epoch 342 batch 200 train Loss 34.4499 test Loss 17.0255 with MSE metric 14769.0997\n",
      "Epoch 342 batch 210 train Loss 34.4465 test Loss 17.0242 with MSE metric 14768.2240\n",
      "Epoch 342 batch 220 train Loss 34.4432 test Loss 17.0228 with MSE metric 14767.4269\n",
      "Epoch 342 batch 230 train Loss 34.4398 test Loss 17.0215 with MSE metric 14766.5801\n",
      "Epoch 342 batch 240 train Loss 34.4364 test Loss 17.0201 with MSE metric 14765.7204\n",
      "Time taken for 1 epoch: 28.723366022109985 secs\n",
      "\n",
      "Epoch 343 batch 0 train Loss 34.4330 test Loss 17.0187 with MSE metric 14764.9093\n",
      "Epoch 343 batch 10 train Loss 34.4296 test Loss 17.0174 with MSE metric 14764.0662\n",
      "Epoch 343 batch 20 train Loss 34.4262 test Loss 17.0160 with MSE metric 14763.2478\n",
      "Epoch 343 batch 30 train Loss 34.4228 test Loss 17.0147 with MSE metric 14762.3962\n",
      "Epoch 343 batch 40 train Loss 34.4194 test Loss 17.0133 with MSE metric 14761.5819\n",
      "Epoch 343 batch 50 train Loss 34.4160 test Loss 17.0120 with MSE metric 14760.7124\n",
      "Epoch 343 batch 60 train Loss 34.4126 test Loss 17.0106 with MSE metric 14759.8436\n",
      "Epoch 343 batch 70 train Loss 34.4092 test Loss 17.0092 with MSE metric 14758.9952\n",
      "Epoch 343 batch 80 train Loss 34.4058 test Loss 17.0079 with MSE metric 14758.1904\n",
      "Epoch 343 batch 90 train Loss 34.4024 test Loss 17.0065 with MSE metric 14757.3391\n",
      "Epoch 343 batch 100 train Loss 34.3990 test Loss 17.0052 with MSE metric 14756.5084\n",
      "Epoch 343 batch 110 train Loss 34.3956 test Loss 17.0038 with MSE metric 14755.6465\n",
      "Epoch 343 batch 120 train Loss 34.3923 test Loss 17.0025 with MSE metric 14754.8191\n",
      "Epoch 343 batch 130 train Loss 34.3889 test Loss 17.0011 with MSE metric 14754.0048\n",
      "Epoch 343 batch 140 train Loss 34.3855 test Loss 16.9998 with MSE metric 14753.1717\n",
      "Epoch 343 batch 150 train Loss 34.3821 test Loss 16.9984 with MSE metric 14752.2797\n",
      "Epoch 343 batch 160 train Loss 34.3787 test Loss 16.9971 with MSE metric 14751.4123\n",
      "Epoch 343 batch 170 train Loss 34.3753 test Loss 16.9957 with MSE metric 14750.5819\n",
      "Epoch 343 batch 180 train Loss 34.3719 test Loss 16.9944 with MSE metric 14749.7272\n",
      "Epoch 343 batch 190 train Loss 34.3686 test Loss 16.9930 with MSE metric 14748.9255\n",
      "Epoch 343 batch 200 train Loss 34.3652 test Loss 16.9917 with MSE metric 14748.1154\n",
      "Epoch 343 batch 210 train Loss 34.3618 test Loss 16.9903 with MSE metric 14747.2651\n",
      "Epoch 343 batch 220 train Loss 34.3584 test Loss 16.9890 with MSE metric 14746.3998\n",
      "Epoch 343 batch 230 train Loss 34.3551 test Loss 16.9876 with MSE metric 14745.5672\n",
      "Epoch 343 batch 240 train Loss 34.3517 test Loss 16.9863 with MSE metric 14744.7639\n",
      "Time taken for 1 epoch: 27.433329820632935 secs\n",
      "\n",
      "Epoch 344 batch 0 train Loss 34.3483 test Loss 16.9849 with MSE metric 14743.9647\n",
      "Epoch 344 batch 10 train Loss 34.3449 test Loss 16.9835 with MSE metric 14743.1327\n",
      "Epoch 344 batch 20 train Loss 34.3416 test Loss 16.9822 with MSE metric 14742.3397\n",
      "Epoch 344 batch 30 train Loss 34.3382 test Loss 16.9808 with MSE metric 14741.5246\n",
      "Epoch 344 batch 40 train Loss 34.3348 test Loss 16.9795 with MSE metric 14740.6928\n",
      "Epoch 344 batch 50 train Loss 34.3314 test Loss 16.9781 with MSE metric 14739.8021\n",
      "Epoch 344 batch 60 train Loss 34.3281 test Loss 16.9768 with MSE metric 14738.9702\n",
      "Epoch 344 batch 70 train Loss 34.3247 test Loss 16.9754 with MSE metric 14738.1346\n",
      "Epoch 344 batch 80 train Loss 34.3213 test Loss 16.9741 with MSE metric 14737.3719\n",
      "Epoch 344 batch 90 train Loss 34.3179 test Loss 16.9727 with MSE metric 14736.5073\n",
      "Epoch 344 batch 100 train Loss 34.3146 test Loss 16.9714 with MSE metric 14735.6851\n",
      "Epoch 344 batch 110 train Loss 34.3112 test Loss 16.9700 with MSE metric 14734.8456\n",
      "Epoch 344 batch 120 train Loss 34.3078 test Loss 16.9687 with MSE metric 14733.9667\n",
      "Epoch 344 batch 130 train Loss 34.3045 test Loss 16.9673 with MSE metric 14733.1813\n",
      "Epoch 344 batch 140 train Loss 34.3011 test Loss 16.9660 with MSE metric 14732.3103\n",
      "Epoch 344 batch 150 train Loss 34.2977 test Loss 16.9647 with MSE metric 14731.4935\n",
      "Epoch 344 batch 160 train Loss 34.2944 test Loss 16.9633 with MSE metric 14730.6296\n",
      "Epoch 344 batch 170 train Loss 34.2910 test Loss 16.9620 with MSE metric 14729.7723\n",
      "Epoch 344 batch 180 train Loss 34.2877 test Loss 16.9606 with MSE metric 14728.9141\n",
      "Epoch 344 batch 190 train Loss 34.2843 test Loss 16.9593 with MSE metric 14728.0769\n",
      "Epoch 344 batch 200 train Loss 34.2809 test Loss 16.9579 with MSE metric 14727.2078\n",
      "Epoch 344 batch 210 train Loss 34.2776 test Loss 16.9566 with MSE metric 14726.4018\n",
      "Epoch 344 batch 220 train Loss 34.2742 test Loss 16.9552 with MSE metric 14725.5727\n",
      "Epoch 344 batch 230 train Loss 34.2708 test Loss 16.9539 with MSE metric 14724.7740\n",
      "Epoch 344 batch 240 train Loss 34.2675 test Loss 16.9526 with MSE metric 14723.9395\n",
      "Time taken for 1 epoch: 27.353561878204346 secs\n",
      "\n",
      "Epoch 345 batch 0 train Loss 34.2641 test Loss 16.9512 with MSE metric 14723.1165\n",
      "Epoch 345 batch 10 train Loss 34.2608 test Loss 16.9499 with MSE metric 14722.2655\n",
      "Epoch 345 batch 20 train Loss 34.2574 test Loss 16.9485 with MSE metric 14721.4417\n",
      "Epoch 345 batch 30 train Loss 34.2541 test Loss 16.9472 with MSE metric 14720.6453\n",
      "Epoch 345 batch 40 train Loss 34.2507 test Loss 16.9458 with MSE metric 14719.7869\n",
      "Epoch 345 batch 50 train Loss 34.2474 test Loss 16.9445 with MSE metric 14718.9969\n",
      "Epoch 345 batch 60 train Loss 34.2440 test Loss 16.9432 with MSE metric 14718.1832\n",
      "Epoch 345 batch 70 train Loss 34.2407 test Loss 16.9418 with MSE metric 14717.3279\n",
      "Epoch 345 batch 80 train Loss 34.2373 test Loss 16.9405 with MSE metric 14716.4908\n",
      "Epoch 345 batch 90 train Loss 34.2340 test Loss 16.9392 with MSE metric 14715.6701\n",
      "Epoch 345 batch 100 train Loss 34.2306 test Loss 16.9378 with MSE metric 14714.8674\n",
      "Epoch 345 batch 110 train Loss 34.2273 test Loss 16.9365 with MSE metric 14714.0601\n",
      "Epoch 345 batch 120 train Loss 34.2239 test Loss 16.9351 with MSE metric 14713.2094\n",
      "Epoch 345 batch 130 train Loss 34.2206 test Loss 16.9338 with MSE metric 14712.4040\n",
      "Epoch 345 batch 140 train Loss 34.2172 test Loss 16.9325 with MSE metric 14711.6096\n",
      "Epoch 345 batch 150 train Loss 34.2139 test Loss 16.9311 with MSE metric 14710.7497\n",
      "Epoch 345 batch 160 train Loss 34.2105 test Loss 16.9298 with MSE metric 14709.9458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345 batch 170 train Loss 34.2072 test Loss 16.9285 with MSE metric 14709.0948\n",
      "Epoch 345 batch 180 train Loss 34.2038 test Loss 16.9271 with MSE metric 14708.2545\n",
      "Epoch 345 batch 190 train Loss 34.2005 test Loss 16.9258 with MSE metric 14707.3952\n",
      "Epoch 345 batch 200 train Loss 34.1972 test Loss 16.9244 with MSE metric 14706.5609\n",
      "Epoch 345 batch 210 train Loss 34.1938 test Loss 16.9231 with MSE metric 14705.6931\n",
      "Epoch 345 batch 220 train Loss 34.1905 test Loss 16.9218 with MSE metric 14704.8773\n",
      "Epoch 345 batch 230 train Loss 34.1871 test Loss 16.9204 with MSE metric 14704.0583\n",
      "Epoch 345 batch 240 train Loss 34.1838 test Loss 16.9191 with MSE metric 14703.1809\n",
      "Time taken for 1 epoch: 27.079162120819092 secs\n",
      "\n",
      "Epoch 346 batch 0 train Loss 34.1805 test Loss 16.9178 with MSE metric 14702.3731\n",
      "Epoch 346 batch 10 train Loss 34.1771 test Loss 16.9164 with MSE metric 14701.5464\n",
      "Epoch 346 batch 20 train Loss 34.1738 test Loss 16.9151 with MSE metric 14700.7362\n",
      "Epoch 346 batch 30 train Loss 34.1704 test Loss 16.9138 with MSE metric 14699.8663\n",
      "Epoch 346 batch 40 train Loss 34.1671 test Loss 16.9124 with MSE metric 14699.0720\n",
      "Epoch 346 batch 50 train Loss 34.1638 test Loss 16.9111 with MSE metric 14698.2652\n",
      "Epoch 346 batch 60 train Loss 34.1604 test Loss 16.9098 with MSE metric 14697.4149\n",
      "Epoch 346 batch 70 train Loss 34.1571 test Loss 16.9085 with MSE metric 14696.5836\n",
      "Epoch 346 batch 80 train Loss 34.1538 test Loss 16.9071 with MSE metric 14695.7640\n",
      "Epoch 346 batch 90 train Loss 34.1505 test Loss 16.9058 with MSE metric 14694.9522\n",
      "Epoch 346 batch 100 train Loss 34.1471 test Loss 16.9045 with MSE metric 14694.1451\n",
      "Epoch 346 batch 110 train Loss 34.1438 test Loss 16.9031 with MSE metric 14693.3023\n",
      "Epoch 346 batch 120 train Loss 34.1405 test Loss 16.9018 with MSE metric 14692.4745\n",
      "Epoch 346 batch 130 train Loss 34.1371 test Loss 16.9005 with MSE metric 14691.6738\n",
      "Epoch 346 batch 140 train Loss 34.1338 test Loss 16.8991 with MSE metric 14690.8605\n",
      "Epoch 346 batch 150 train Loss 34.1305 test Loss 16.8978 with MSE metric 14689.9989\n",
      "Epoch 346 batch 160 train Loss 34.1272 test Loss 16.8965 with MSE metric 14689.1814\n",
      "Epoch 346 batch 170 train Loss 34.1238 test Loss 16.8951 with MSE metric 14688.4013\n",
      "Epoch 346 batch 180 train Loss 34.1205 test Loss 16.8938 with MSE metric 14687.5774\n",
      "Epoch 346 batch 190 train Loss 34.1172 test Loss 16.8925 with MSE metric 14686.7571\n",
      "Epoch 346 batch 200 train Loss 34.1139 test Loss 16.8911 with MSE metric 14685.9497\n",
      "Epoch 346 batch 210 train Loss 34.1105 test Loss 16.8898 with MSE metric 14685.1941\n",
      "Epoch 346 batch 220 train Loss 34.1072 test Loss 16.8885 with MSE metric 14684.3485\n",
      "Epoch 346 batch 230 train Loss 34.1039 test Loss 16.8871 with MSE metric 14683.5214\n",
      "Epoch 346 batch 240 train Loss 34.1006 test Loss 16.8858 with MSE metric 14682.7098\n",
      "Time taken for 1 epoch: 27.27241086959839 secs\n",
      "\n",
      "Epoch 347 batch 0 train Loss 34.0973 test Loss 16.8845 with MSE metric 14681.9013\n",
      "Epoch 347 batch 10 train Loss 34.0940 test Loss 16.8832 with MSE metric 14681.0924\n",
      "Epoch 347 batch 20 train Loss 34.0906 test Loss 16.8818 with MSE metric 14680.2683\n",
      "Epoch 347 batch 30 train Loss 34.0873 test Loss 16.8805 with MSE metric 14679.3936\n",
      "Epoch 347 batch 40 train Loss 34.0840 test Loss 16.8792 with MSE metric 14678.5302\n",
      "Epoch 347 batch 50 train Loss 34.0807 test Loss 16.8779 with MSE metric 14677.6676\n",
      "Epoch 347 batch 60 train Loss 34.0774 test Loss 16.8766 with MSE metric 14676.8615\n",
      "Epoch 347 batch 70 train Loss 34.0740 test Loss 16.8752 with MSE metric 14676.0259\n",
      "Epoch 347 batch 80 train Loss 34.0707 test Loss 16.8739 with MSE metric 14675.1561\n",
      "Epoch 347 batch 90 train Loss 34.0674 test Loss 16.8726 with MSE metric 14674.3665\n",
      "Epoch 347 batch 100 train Loss 34.0641 test Loss 16.8712 with MSE metric 14673.5302\n",
      "Epoch 347 batch 110 train Loss 34.0608 test Loss 16.8699 with MSE metric 14672.6914\n",
      "Epoch 347 batch 120 train Loss 34.0575 test Loss 16.8686 with MSE metric 14671.8447\n",
      "Epoch 347 batch 130 train Loss 34.0542 test Loss 16.8673 with MSE metric 14671.0028\n",
      "Epoch 347 batch 140 train Loss 34.0509 test Loss 16.8660 with MSE metric 14670.2193\n",
      "Epoch 347 batch 150 train Loss 34.0476 test Loss 16.8647 with MSE metric 14669.4482\n",
      "Epoch 347 batch 160 train Loss 34.0443 test Loss 16.8633 with MSE metric 14668.6578\n",
      "Epoch 347 batch 170 train Loss 34.0409 test Loss 16.8620 with MSE metric 14667.7877\n",
      "Epoch 347 batch 180 train Loss 34.0376 test Loss 16.8607 with MSE metric 14666.9551\n",
      "Epoch 347 batch 190 train Loss 34.0343 test Loss 16.8594 with MSE metric 14666.1457\n",
      "Epoch 347 batch 200 train Loss 34.0310 test Loss 16.8581 with MSE metric 14665.3210\n",
      "Epoch 347 batch 210 train Loss 34.0277 test Loss 16.8567 with MSE metric 14664.5117\n",
      "Epoch 347 batch 220 train Loss 34.0244 test Loss 16.8554 with MSE metric 14663.6879\n",
      "Epoch 347 batch 230 train Loss 34.0211 test Loss 16.8541 with MSE metric 14662.9057\n",
      "Epoch 347 batch 240 train Loss 34.0178 test Loss 16.8528 with MSE metric 14662.0967\n",
      "Time taken for 1 epoch: 27.521589994430542 secs\n",
      "\n",
      "Epoch 348 batch 0 train Loss 34.0145 test Loss 16.8515 with MSE metric 14661.2984\n",
      "Epoch 348 batch 10 train Loss 34.0112 test Loss 16.8502 with MSE metric 14660.4905\n",
      "Epoch 348 batch 20 train Loss 34.0079 test Loss 16.8488 with MSE metric 14659.6849\n",
      "Epoch 348 batch 30 train Loss 34.0046 test Loss 16.8475 with MSE metric 14658.8733\n",
      "Epoch 348 batch 40 train Loss 34.0013 test Loss 16.8462 with MSE metric 14658.0550\n",
      "Epoch 348 batch 50 train Loss 33.9980 test Loss 16.8449 with MSE metric 14657.2489\n",
      "Epoch 348 batch 60 train Loss 33.9948 test Loss 16.8436 with MSE metric 14656.4342\n",
      "Epoch 348 batch 70 train Loss 33.9915 test Loss 16.8423 with MSE metric 14655.6363\n",
      "Epoch 348 batch 80 train Loss 33.9882 test Loss 16.8410 with MSE metric 14654.8062\n",
      "Epoch 348 batch 90 train Loss 33.9849 test Loss 16.8396 with MSE metric 14653.9990\n",
      "Epoch 348 batch 100 train Loss 33.9816 test Loss 16.8383 with MSE metric 14653.1961\n",
      "Epoch 348 batch 110 train Loss 33.9783 test Loss 16.8370 with MSE metric 14652.3990\n",
      "Epoch 348 batch 120 train Loss 33.9750 test Loss 16.8357 with MSE metric 14651.5851\n",
      "Epoch 348 batch 130 train Loss 33.9717 test Loss 16.8344 with MSE metric 14650.7485\n",
      "Epoch 348 batch 140 train Loss 33.9684 test Loss 16.8331 with MSE metric 14649.9539\n",
      "Epoch 348 batch 150 train Loss 33.9651 test Loss 16.8318 with MSE metric 14649.1340\n",
      "Epoch 348 batch 160 train Loss 33.9618 test Loss 16.8305 with MSE metric 14648.3347\n",
      "Epoch 348 batch 170 train Loss 33.9586 test Loss 16.8291 with MSE metric 14647.4566\n",
      "Epoch 348 batch 180 train Loss 33.9553 test Loss 16.8278 with MSE metric 14646.6056\n",
      "Epoch 348 batch 190 train Loss 33.9520 test Loss 16.8265 with MSE metric 14645.7898\n",
      "Epoch 348 batch 200 train Loss 33.9487 test Loss 16.8252 with MSE metric 14645.0064\n",
      "Epoch 348 batch 210 train Loss 33.9454 test Loss 16.8239 with MSE metric 14644.1771\n",
      "Epoch 348 batch 220 train Loss 33.9421 test Loss 16.8226 with MSE metric 14643.3850\n",
      "Epoch 348 batch 230 train Loss 33.9389 test Loss 16.8212 with MSE metric 14642.5890\n",
      "Epoch 348 batch 240 train Loss 33.9356 test Loss 16.8199 with MSE metric 14641.7906\n",
      "Time taken for 1 epoch: 27.5267813205719 secs\n",
      "\n",
      "Epoch 349 batch 0 train Loss 33.9323 test Loss 16.8186 with MSE metric 14641.0246\n",
      "Epoch 349 batch 10 train Loss 33.9290 test Loss 16.8173 with MSE metric 14640.2068\n",
      "Epoch 349 batch 20 train Loss 33.9257 test Loss 16.8160 with MSE metric 14639.4465\n",
      "Epoch 349 batch 30 train Loss 33.9225 test Loss 16.8147 with MSE metric 14638.6471\n",
      "Epoch 349 batch 40 train Loss 33.9192 test Loss 16.8134 with MSE metric 14637.8034\n",
      "Epoch 349 batch 50 train Loss 33.9159 test Loss 16.8121 with MSE metric 14637.0059\n",
      "Epoch 349 batch 60 train Loss 33.9126 test Loss 16.8107 with MSE metric 14636.2122\n",
      "Epoch 349 batch 70 train Loss 33.9094 test Loss 16.8094 with MSE metric 14635.4263\n",
      "Epoch 349 batch 80 train Loss 33.9061 test Loss 16.8081 with MSE metric 14634.6354\n",
      "Epoch 349 batch 90 train Loss 33.9028 test Loss 16.8068 with MSE metric 14633.7791\n",
      "Epoch 349 batch 100 train Loss 33.8995 test Loss 16.8055 with MSE metric 14632.9653\n",
      "Epoch 349 batch 110 train Loss 33.8963 test Loss 16.8042 with MSE metric 14632.1873\n",
      "Epoch 349 batch 120 train Loss 33.8930 test Loss 16.8029 with MSE metric 14631.3539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349 batch 130 train Loss 33.8897 test Loss 16.8016 with MSE metric 14630.5372\n",
      "Epoch 349 batch 140 train Loss 33.8864 test Loss 16.8003 with MSE metric 14629.7353\n",
      "Epoch 349 batch 150 train Loss 33.8832 test Loss 16.7990 with MSE metric 14628.8661\n",
      "Epoch 349 batch 160 train Loss 33.8799 test Loss 16.7977 with MSE metric 14628.1159\n",
      "Epoch 349 batch 170 train Loss 33.8766 test Loss 16.7964 with MSE metric 14627.3211\n",
      "Epoch 349 batch 180 train Loss 33.8734 test Loss 16.7951 with MSE metric 14626.5298\n",
      "Epoch 349 batch 190 train Loss 33.8701 test Loss 16.7938 with MSE metric 14625.7407\n",
      "Epoch 349 batch 200 train Loss 33.8668 test Loss 16.7925 with MSE metric 14624.9139\n",
      "Epoch 349 batch 210 train Loss 33.8636 test Loss 16.7911 with MSE metric 14624.1064\n",
      "Epoch 349 batch 220 train Loss 33.8603 test Loss 16.7899 with MSE metric 14623.2880\n",
      "Epoch 349 batch 230 train Loss 33.8570 test Loss 16.7886 with MSE metric 14622.4811\n",
      "Epoch 349 batch 240 train Loss 33.8538 test Loss 16.7872 with MSE metric 14621.6997\n",
      "Time taken for 1 epoch: 27.322175979614258 secs\n",
      "\n",
      "Epoch 350 batch 0 train Loss 33.8505 test Loss 16.7859 with MSE metric 14620.8922\n",
      "Epoch 350 batch 10 train Loss 33.8473 test Loss 16.7846 with MSE metric 14620.0842\n",
      "Epoch 350 batch 20 train Loss 33.8440 test Loss 16.7833 with MSE metric 14619.2804\n",
      "Epoch 350 batch 30 train Loss 33.8407 test Loss 16.7820 with MSE metric 14618.4770\n",
      "Epoch 350 batch 40 train Loss 33.8375 test Loss 16.7807 with MSE metric 14617.6756\n",
      "Epoch 350 batch 50 train Loss 33.8342 test Loss 16.7794 with MSE metric 14616.8709\n",
      "Epoch 350 batch 60 train Loss 33.8310 test Loss 16.7781 with MSE metric 14616.0517\n",
      "Epoch 350 batch 70 train Loss 33.8277 test Loss 16.7768 with MSE metric 14615.2068\n",
      "Epoch 350 batch 80 train Loss 33.8244 test Loss 16.7755 with MSE metric 14614.3872\n",
      "Epoch 350 batch 90 train Loss 33.8212 test Loss 16.7742 with MSE metric 14613.6217\n",
      "Epoch 350 batch 100 train Loss 33.8179 test Loss 16.7729 with MSE metric 14612.8061\n",
      "Epoch 350 batch 110 train Loss 33.8147 test Loss 16.7716 with MSE metric 14611.9822\n",
      "Epoch 350 batch 120 train Loss 33.8114 test Loss 16.7703 with MSE metric 14611.1992\n",
      "Epoch 350 batch 130 train Loss 33.8082 test Loss 16.7690 with MSE metric 14610.3938\n",
      "Epoch 350 batch 140 train Loss 33.8049 test Loss 16.7677 with MSE metric 14609.6276\n",
      "Epoch 350 batch 150 train Loss 33.8017 test Loss 16.7664 with MSE metric 14608.8355\n",
      "Epoch 350 batch 160 train Loss 33.7984 test Loss 16.7651 with MSE metric 14608.0590\n",
      "Epoch 350 batch 170 train Loss 33.7952 test Loss 16.7638 with MSE metric 14607.2781\n",
      "Epoch 350 batch 180 train Loss 33.7919 test Loss 16.7625 with MSE metric 14606.4803\n",
      "Epoch 350 batch 190 train Loss 33.7887 test Loss 16.7612 with MSE metric 14605.6673\n",
      "Epoch 350 batch 200 train Loss 33.7854 test Loss 16.7599 with MSE metric 14604.8315\n",
      "Epoch 350 batch 210 train Loss 33.7822 test Loss 16.7586 with MSE metric 14604.0371\n",
      "Epoch 350 batch 220 train Loss 33.7790 test Loss 16.7573 with MSE metric 14603.2150\n",
      "Epoch 350 batch 230 train Loss 33.7757 test Loss 16.7560 with MSE metric 14602.4147\n",
      "Epoch 350 batch 240 train Loss 33.7725 test Loss 16.7547 with MSE metric 14601.6129\n",
      "Time taken for 1 epoch: 26.589170932769775 secs\n",
      "\n",
      "Epoch 351 batch 0 train Loss 33.7692 test Loss 16.7535 with MSE metric 14600.8275\n",
      "Epoch 351 batch 10 train Loss 33.7660 test Loss 16.7522 with MSE metric 14600.0315\n",
      "Epoch 351 batch 20 train Loss 33.7627 test Loss 16.7509 with MSE metric 14599.2295\n",
      "Epoch 351 batch 30 train Loss 33.7595 test Loss 16.7496 with MSE metric 14598.3918\n",
      "Epoch 351 batch 40 train Loss 33.7563 test Loss 16.7483 with MSE metric 14597.5786\n",
      "Epoch 351 batch 50 train Loss 33.7530 test Loss 16.7470 with MSE metric 14596.7626\n",
      "Epoch 351 batch 60 train Loss 33.7498 test Loss 16.7457 with MSE metric 14595.9763\n",
      "Epoch 351 batch 70 train Loss 33.7465 test Loss 16.7444 with MSE metric 14595.1594\n",
      "Epoch 351 batch 80 train Loss 33.7433 test Loss 16.7431 with MSE metric 14594.3192\n",
      "Epoch 351 batch 90 train Loss 33.7401 test Loss 16.7418 with MSE metric 14593.5275\n",
      "Epoch 351 batch 100 train Loss 33.7368 test Loss 16.7405 with MSE metric 14592.7204\n",
      "Epoch 351 batch 110 train Loss 33.7336 test Loss 16.7392 with MSE metric 14591.9078\n",
      "Epoch 351 batch 120 train Loss 33.7303 test Loss 16.7379 with MSE metric 14591.0963\n",
      "Epoch 351 batch 130 train Loss 33.7271 test Loss 16.7366 with MSE metric 14590.2892\n",
      "Epoch 351 batch 140 train Loss 33.7239 test Loss 16.7353 with MSE metric 14589.4981\n",
      "Epoch 351 batch 150 train Loss 33.7207 test Loss 16.7341 with MSE metric 14588.7092\n",
      "Epoch 351 batch 160 train Loss 33.7174 test Loss 16.7327 with MSE metric 14587.9133\n",
      "Epoch 351 batch 170 train Loss 33.7142 test Loss 16.7315 with MSE metric 14587.1398\n",
      "Epoch 351 batch 180 train Loss 33.7110 test Loss 16.7302 with MSE metric 14586.3512\n",
      "Epoch 351 batch 190 train Loss 33.7077 test Loss 16.7289 with MSE metric 14585.5938\n",
      "Epoch 351 batch 200 train Loss 33.7045 test Loss 16.7276 with MSE metric 14584.7956\n",
      "Epoch 351 batch 210 train Loss 33.7013 test Loss 16.7263 with MSE metric 14583.9551\n",
      "Epoch 351 batch 220 train Loss 33.6980 test Loss 16.7250 with MSE metric 14583.1116\n",
      "Epoch 351 batch 230 train Loss 33.6948 test Loss 16.7237 with MSE metric 14582.2999\n",
      "Epoch 351 batch 240 train Loss 33.6916 test Loss 16.7224 with MSE metric 14581.4999\n",
      "Time taken for 1 epoch: 27.827835083007812 secs\n",
      "\n",
      "Epoch 352 batch 0 train Loss 33.6884 test Loss 16.7211 with MSE metric 14580.7040\n",
      "Epoch 352 batch 10 train Loss 33.6851 test Loss 16.7198 with MSE metric 14579.9124\n",
      "Epoch 352 batch 20 train Loss 33.6819 test Loss 16.7186 with MSE metric 14579.1433\n",
      "Epoch 352 batch 30 train Loss 33.6787 test Loss 16.7173 with MSE metric 14578.3651\n",
      "Epoch 352 batch 40 train Loss 33.6755 test Loss 16.7160 with MSE metric 14577.5563\n",
      "Epoch 352 batch 50 train Loss 33.6723 test Loss 16.7147 with MSE metric 14576.7391\n",
      "Epoch 352 batch 60 train Loss 33.6690 test Loss 16.7134 with MSE metric 14575.9414\n",
      "Epoch 352 batch 70 train Loss 33.6658 test Loss 16.7121 with MSE metric 14575.1622\n",
      "Epoch 352 batch 80 train Loss 33.6626 test Loss 16.7108 with MSE metric 14574.3673\n",
      "Epoch 352 batch 90 train Loss 33.6594 test Loss 16.7095 with MSE metric 14573.5223\n",
      "Epoch 352 batch 100 train Loss 33.6562 test Loss 16.7083 with MSE metric 14572.7164\n",
      "Epoch 352 batch 110 train Loss 33.6529 test Loss 16.7070 with MSE metric 14571.9178\n",
      "Epoch 352 batch 120 train Loss 33.6497 test Loss 16.7057 with MSE metric 14571.1395\n",
      "Epoch 352 batch 130 train Loss 33.6465 test Loss 16.7044 with MSE metric 14570.3699\n",
      "Epoch 352 batch 140 train Loss 33.6433 test Loss 16.7031 with MSE metric 14569.5444\n",
      "Epoch 352 batch 150 train Loss 33.6401 test Loss 16.7018 with MSE metric 14568.7261\n",
      "Epoch 352 batch 160 train Loss 33.6369 test Loss 16.7005 with MSE metric 14567.9280\n",
      "Epoch 352 batch 170 train Loss 33.6337 test Loss 16.6992 with MSE metric 14567.1277\n",
      "Epoch 352 batch 180 train Loss 33.6304 test Loss 16.6980 with MSE metric 14566.3446\n",
      "Epoch 352 batch 190 train Loss 33.6272 test Loss 16.6967 with MSE metric 14565.5349\n",
      "Epoch 352 batch 200 train Loss 33.6240 test Loss 16.6954 with MSE metric 14564.7444\n",
      "Epoch 352 batch 210 train Loss 33.6208 test Loss 16.6941 with MSE metric 14563.9578\n",
      "Epoch 352 batch 220 train Loss 33.6176 test Loss 16.6928 with MSE metric 14563.1570\n",
      "Epoch 352 batch 230 train Loss 33.6144 test Loss 16.6915 with MSE metric 14562.3671\n",
      "Epoch 352 batch 240 train Loss 33.6112 test Loss 16.6902 with MSE metric 14561.5953\n",
      "Time taken for 1 epoch: 28.242160081863403 secs\n",
      "\n",
      "Epoch 353 batch 0 train Loss 33.6080 test Loss 16.6890 with MSE metric 14560.7977\n",
      "Epoch 353 batch 10 train Loss 33.6048 test Loss 16.6877 with MSE metric 14560.0038\n",
      "Epoch 353 batch 20 train Loss 33.6016 test Loss 16.6864 with MSE metric 14559.2119\n",
      "Epoch 353 batch 30 train Loss 33.5984 test Loss 16.6851 with MSE metric 14558.4267\n",
      "Epoch 353 batch 40 train Loss 33.5952 test Loss 16.6838 with MSE metric 14557.6093\n",
      "Epoch 353 batch 50 train Loss 33.5920 test Loss 16.6826 with MSE metric 14556.8167\n",
      "Epoch 353 batch 60 train Loss 33.5888 test Loss 16.6813 with MSE metric 14556.0348\n",
      "Epoch 353 batch 70 train Loss 33.5856 test Loss 16.6800 with MSE metric 14555.2671\n",
      "Epoch 353 batch 80 train Loss 33.5824 test Loss 16.6787 with MSE metric 14554.4927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 batch 90 train Loss 33.5791 test Loss 16.6774 with MSE metric 14553.6730\n",
      "Epoch 353 batch 100 train Loss 33.5759 test Loss 16.6762 with MSE metric 14552.8841\n",
      "Epoch 353 batch 110 train Loss 33.5728 test Loss 16.6749 with MSE metric 14552.1530\n",
      "Epoch 353 batch 120 train Loss 33.5696 test Loss 16.6736 with MSE metric 14551.3874\n",
      "Epoch 353 batch 130 train Loss 33.5664 test Loss 16.6723 with MSE metric 14550.5597\n",
      "Epoch 353 batch 140 train Loss 33.5632 test Loss 16.6710 with MSE metric 14549.7212\n",
      "Epoch 353 batch 150 train Loss 33.5600 test Loss 16.6698 with MSE metric 14548.9333\n",
      "Epoch 353 batch 160 train Loss 33.5568 test Loss 16.6685 with MSE metric 14548.1223\n",
      "Epoch 353 batch 170 train Loss 33.5536 test Loss 16.6672 with MSE metric 14547.2701\n",
      "Epoch 353 batch 180 train Loss 33.5504 test Loss 16.6659 with MSE metric 14546.4494\n",
      "Epoch 353 batch 190 train Loss 33.5472 test Loss 16.6646 with MSE metric 14545.6807\n",
      "Epoch 353 batch 200 train Loss 33.5440 test Loss 16.6634 with MSE metric 14544.8955\n",
      "Epoch 353 batch 210 train Loss 33.5408 test Loss 16.6621 with MSE metric 14544.1648\n",
      "Epoch 353 batch 220 train Loss 33.5376 test Loss 16.6608 with MSE metric 14543.3452\n",
      "Epoch 353 batch 230 train Loss 33.5344 test Loss 16.6596 with MSE metric 14542.5299\n",
      "Epoch 353 batch 240 train Loss 33.5312 test Loss 16.6583 with MSE metric 14541.7423\n",
      "Time taken for 1 epoch: 28.10646891593933 secs\n",
      "\n",
      "Epoch 354 batch 0 train Loss 33.5280 test Loss 16.6570 with MSE metric 14540.9499\n",
      "Epoch 354 batch 10 train Loss 33.5248 test Loss 16.6557 with MSE metric 14540.1467\n",
      "Epoch 354 batch 20 train Loss 33.5217 test Loss 16.6545 with MSE metric 14539.3368\n",
      "Epoch 354 batch 30 train Loss 33.5185 test Loss 16.6532 with MSE metric 14538.5907\n",
      "Epoch 354 batch 40 train Loss 33.5153 test Loss 16.6519 with MSE metric 14537.8538\n",
      "Epoch 354 batch 50 train Loss 33.5121 test Loss 16.6507 with MSE metric 14537.0477\n",
      "Epoch 354 batch 60 train Loss 33.5089 test Loss 16.6494 with MSE metric 14536.2303\n",
      "Epoch 354 batch 70 train Loss 33.5057 test Loss 16.6482 with MSE metric 14535.4460\n",
      "Epoch 354 batch 80 train Loss 33.5026 test Loss 16.6469 with MSE metric 14534.6718\n",
      "Epoch 354 batch 90 train Loss 33.4994 test Loss 16.6456 with MSE metric 14533.8672\n",
      "Epoch 354 batch 100 train Loss 33.4962 test Loss 16.6443 with MSE metric 14533.0541\n",
      "Epoch 354 batch 110 train Loss 33.4930 test Loss 16.6431 with MSE metric 14532.2653\n",
      "Epoch 354 batch 120 train Loss 33.4898 test Loss 16.6418 with MSE metric 14531.4738\n",
      "Epoch 354 batch 130 train Loss 33.4866 test Loss 16.6405 with MSE metric 14530.6694\n",
      "Epoch 354 batch 140 train Loss 33.4835 test Loss 16.6393 with MSE metric 14529.8767\n",
      "Epoch 354 batch 150 train Loss 33.4803 test Loss 16.6380 with MSE metric 14529.1347\n",
      "Epoch 354 batch 160 train Loss 33.4771 test Loss 16.6367 with MSE metric 14528.3978\n",
      "Epoch 354 batch 170 train Loss 33.4739 test Loss 16.6354 with MSE metric 14527.6332\n",
      "Epoch 354 batch 180 train Loss 33.4708 test Loss 16.6341 with MSE metric 14526.8547\n",
      "Epoch 354 batch 190 train Loss 33.4676 test Loss 16.6329 with MSE metric 14526.0712\n",
      "Epoch 354 batch 200 train Loss 33.4644 test Loss 16.6316 with MSE metric 14525.2967\n",
      "Epoch 354 batch 210 train Loss 33.4612 test Loss 16.6304 with MSE metric 14524.4910\n",
      "Epoch 354 batch 220 train Loss 33.4581 test Loss 16.6291 with MSE metric 14523.7072\n",
      "Epoch 354 batch 230 train Loss 33.4549 test Loss 16.6278 with MSE metric 14522.9201\n",
      "Epoch 354 batch 240 train Loss 33.4517 test Loss 16.6265 with MSE metric 14522.1650\n",
      "Time taken for 1 epoch: 27.14918613433838 secs\n",
      "\n",
      "Epoch 355 batch 0 train Loss 33.4486 test Loss 16.6253 with MSE metric 14521.3996\n",
      "Epoch 355 batch 10 train Loss 33.4454 test Loss 16.6240 with MSE metric 14520.6040\n",
      "Epoch 355 batch 20 train Loss 33.4422 test Loss 16.6227 with MSE metric 14519.8349\n",
      "Epoch 355 batch 30 train Loss 33.4390 test Loss 16.6215 with MSE metric 14519.0373\n",
      "Epoch 355 batch 40 train Loss 33.4359 test Loss 16.6202 with MSE metric 14518.2222\n",
      "Epoch 355 batch 50 train Loss 33.4327 test Loss 16.6189 with MSE metric 14517.4353\n",
      "Epoch 355 batch 60 train Loss 33.4295 test Loss 16.6177 with MSE metric 14516.6658\n",
      "Epoch 355 batch 70 train Loss 33.4264 test Loss 16.6164 with MSE metric 14515.8784\n",
      "Epoch 355 batch 80 train Loss 33.4232 test Loss 16.6152 with MSE metric 14515.0853\n",
      "Epoch 355 batch 90 train Loss 33.4200 test Loss 16.6139 with MSE metric 14514.3019\n",
      "Epoch 355 batch 100 train Loss 33.4169 test Loss 16.6126 with MSE metric 14513.5130\n",
      "Epoch 355 batch 110 train Loss 33.4137 test Loss 16.6114 with MSE metric 14512.7721\n",
      "Epoch 355 batch 120 train Loss 33.4106 test Loss 16.6101 with MSE metric 14512.0024\n",
      "Epoch 355 batch 130 train Loss 33.4074 test Loss 16.6088 with MSE metric 14511.2225\n",
      "Epoch 355 batch 140 train Loss 33.4042 test Loss 16.6076 with MSE metric 14510.4315\n",
      "Epoch 355 batch 150 train Loss 33.4011 test Loss 16.6063 with MSE metric 14509.6490\n",
      "Epoch 355 batch 160 train Loss 33.3979 test Loss 16.6050 with MSE metric 14508.9049\n",
      "Epoch 355 batch 170 train Loss 33.3948 test Loss 16.6038 with MSE metric 14508.0727\n",
      "Epoch 355 batch 180 train Loss 33.3916 test Loss 16.6025 with MSE metric 14507.2868\n",
      "Epoch 355 batch 190 train Loss 33.3884 test Loss 16.6012 with MSE metric 14506.5248\n",
      "Epoch 355 batch 200 train Loss 33.3853 test Loss 16.6000 with MSE metric 14505.7460\n",
      "Epoch 355 batch 210 train Loss 33.3821 test Loss 16.5987 with MSE metric 14504.9434\n",
      "Epoch 355 batch 220 train Loss 33.3790 test Loss 16.5975 with MSE metric 14504.1562\n",
      "Epoch 355 batch 230 train Loss 33.3758 test Loss 16.5962 with MSE metric 14503.3703\n",
      "Epoch 355 batch 240 train Loss 33.3727 test Loss 16.5950 with MSE metric 14502.6560\n",
      "Time taken for 1 epoch: 26.984211921691895 secs\n",
      "\n",
      "Epoch 356 batch 0 train Loss 33.3695 test Loss 16.5937 with MSE metric 14501.8934\n",
      "Epoch 356 batch 10 train Loss 33.3664 test Loss 16.5924 with MSE metric 14501.1379\n",
      "Epoch 356 batch 20 train Loss 33.3632 test Loss 16.5912 with MSE metric 14500.3311\n",
      "Epoch 356 batch 30 train Loss 33.3601 test Loss 16.5899 with MSE metric 14499.5474\n",
      "Epoch 356 batch 40 train Loss 33.3569 test Loss 16.5887 with MSE metric 14498.8150\n",
      "Epoch 356 batch 50 train Loss 33.3538 test Loss 16.5874 with MSE metric 14498.0310\n",
      "Epoch 356 batch 60 train Loss 33.3506 test Loss 16.5861 with MSE metric 14497.2866\n",
      "Epoch 356 batch 70 train Loss 33.3475 test Loss 16.5849 with MSE metric 14496.4851\n",
      "Epoch 356 batch 80 train Loss 33.3443 test Loss 16.5836 with MSE metric 14495.7007\n",
      "Epoch 356 batch 90 train Loss 33.3412 test Loss 16.5824 with MSE metric 14494.9121\n",
      "Epoch 356 batch 100 train Loss 33.3380 test Loss 16.5811 with MSE metric 14494.1517\n",
      "Epoch 356 batch 110 train Loss 33.3349 test Loss 16.5799 with MSE metric 14493.4119\n",
      "Epoch 356 batch 120 train Loss 33.3318 test Loss 16.5786 with MSE metric 14492.6314\n",
      "Epoch 356 batch 130 train Loss 33.3286 test Loss 16.5773 with MSE metric 14491.8822\n",
      "Epoch 356 batch 140 train Loss 33.3255 test Loss 16.5761 with MSE metric 14491.0688\n",
      "Epoch 356 batch 150 train Loss 33.3223 test Loss 16.5748 with MSE metric 14490.2579\n",
      "Epoch 356 batch 160 train Loss 33.3192 test Loss 16.5736 with MSE metric 14489.4879\n",
      "Epoch 356 batch 170 train Loss 33.3160 test Loss 16.5723 with MSE metric 14488.7312\n",
      "Epoch 356 batch 180 train Loss 33.3129 test Loss 16.5711 with MSE metric 14487.9612\n",
      "Epoch 356 batch 190 train Loss 33.3098 test Loss 16.5698 with MSE metric 14487.2246\n",
      "Epoch 356 batch 200 train Loss 33.3066 test Loss 16.5686 with MSE metric 14486.4007\n",
      "Epoch 356 batch 210 train Loss 33.3035 test Loss 16.5673 with MSE metric 14485.5939\n",
      "Epoch 356 batch 220 train Loss 33.3003 test Loss 16.5661 with MSE metric 14484.8276\n",
      "Epoch 356 batch 230 train Loss 33.2972 test Loss 16.5648 with MSE metric 14484.0267\n",
      "Epoch 356 batch 240 train Loss 33.2941 test Loss 16.5636 with MSE metric 14483.2421\n",
      "Time taken for 1 epoch: 26.72799515724182 secs\n",
      "\n",
      "Epoch 357 batch 0 train Loss 33.2909 test Loss 16.5623 with MSE metric 14482.4551\n",
      "Epoch 357 batch 10 train Loss 33.2878 test Loss 16.5611 with MSE metric 14481.6974\n",
      "Epoch 357 batch 20 train Loss 33.2847 test Loss 16.5598 with MSE metric 14480.9395\n",
      "Epoch 357 batch 30 train Loss 33.2815 test Loss 16.5586 with MSE metric 14480.1913\n",
      "Epoch 357 batch 40 train Loss 33.2784 test Loss 16.5573 with MSE metric 14479.4354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357 batch 50 train Loss 33.2753 test Loss 16.5561 with MSE metric 14478.6362\n",
      "Epoch 357 batch 60 train Loss 33.2721 test Loss 16.5548 with MSE metric 14477.8406\n",
      "Epoch 357 batch 70 train Loss 33.2690 test Loss 16.5536 with MSE metric 14477.0785\n",
      "Epoch 357 batch 80 train Loss 33.2659 test Loss 16.5523 with MSE metric 14476.3119\n",
      "Epoch 357 batch 90 train Loss 33.2627 test Loss 16.5511 with MSE metric 14475.5244\n",
      "Epoch 357 batch 100 train Loss 33.2596 test Loss 16.5498 with MSE metric 14474.7127\n",
      "Epoch 357 batch 110 train Loss 33.2565 test Loss 16.5486 with MSE metric 14473.9264\n",
      "Epoch 357 batch 120 train Loss 33.2534 test Loss 16.5473 with MSE metric 14473.1412\n",
      "Epoch 357 batch 130 train Loss 33.2502 test Loss 16.5461 with MSE metric 14472.4022\n",
      "Epoch 357 batch 140 train Loss 33.2471 test Loss 16.5448 with MSE metric 14471.6033\n",
      "Epoch 357 batch 150 train Loss 33.2440 test Loss 16.5436 with MSE metric 14470.8276\n",
      "Epoch 357 batch 160 train Loss 33.2409 test Loss 16.5423 with MSE metric 14470.0552\n",
      "Epoch 357 batch 170 train Loss 33.2377 test Loss 16.5411 with MSE metric 14469.2928\n",
      "Epoch 357 batch 180 train Loss 33.2346 test Loss 16.5398 with MSE metric 14468.5247\n",
      "Epoch 357 batch 190 train Loss 33.2315 test Loss 16.5386 with MSE metric 14467.7271\n",
      "Epoch 357 batch 200 train Loss 33.2284 test Loss 16.5373 with MSE metric 14466.9395\n",
      "Epoch 357 batch 210 train Loss 33.2252 test Loss 16.5361 with MSE metric 14466.1193\n",
      "Epoch 357 batch 220 train Loss 33.2221 test Loss 16.5348 with MSE metric 14465.3338\n",
      "Epoch 357 batch 230 train Loss 33.2190 test Loss 16.5336 with MSE metric 14464.5473\n",
      "Epoch 357 batch 240 train Loss 33.2159 test Loss 16.5323 with MSE metric 14463.7583\n",
      "Time taken for 1 epoch: 27.26916193962097 secs\n",
      "\n",
      "Epoch 358 batch 0 train Loss 33.2128 test Loss 16.5311 with MSE metric 14462.9908\n",
      "Epoch 358 batch 10 train Loss 33.2097 test Loss 16.5298 with MSE metric 14462.3002\n",
      "Epoch 358 batch 20 train Loss 33.2065 test Loss 16.5286 with MSE metric 14461.4837\n",
      "Epoch 358 batch 30 train Loss 33.2034 test Loss 16.5273 with MSE metric 14460.6928\n",
      "Epoch 358 batch 40 train Loss 33.2003 test Loss 16.5261 with MSE metric 14459.9467\n",
      "Epoch 358 batch 50 train Loss 33.1972 test Loss 16.5249 with MSE metric 14459.1405\n",
      "Epoch 358 batch 60 train Loss 33.1941 test Loss 16.5236 with MSE metric 14458.3805\n",
      "Epoch 358 batch 70 train Loss 33.1910 test Loss 16.5224 with MSE metric 14457.5843\n",
      "Epoch 358 batch 80 train Loss 33.1878 test Loss 16.5211 with MSE metric 14456.8376\n",
      "Epoch 358 batch 90 train Loss 33.1847 test Loss 16.5199 with MSE metric 14456.0680\n",
      "Epoch 358 batch 100 train Loss 33.1816 test Loss 16.5187 with MSE metric 14455.3388\n",
      "Epoch 358 batch 110 train Loss 33.1785 test Loss 16.5174 with MSE metric 14454.5351\n",
      "Epoch 358 batch 120 train Loss 33.1754 test Loss 16.5162 with MSE metric 14453.7572\n",
      "Epoch 358 batch 130 train Loss 33.1723 test Loss 16.5149 with MSE metric 14453.0202\n",
      "Epoch 358 batch 140 train Loss 33.1692 test Loss 16.5137 with MSE metric 14452.2527\n",
      "Epoch 358 batch 150 train Loss 33.1661 test Loss 16.5124 with MSE metric 14451.4413\n",
      "Epoch 358 batch 160 train Loss 33.1630 test Loss 16.5112 with MSE metric 14450.6792\n",
      "Epoch 358 batch 170 train Loss 33.1599 test Loss 16.5099 with MSE metric 14449.9029\n",
      "Epoch 358 batch 180 train Loss 33.1568 test Loss 16.5087 with MSE metric 14449.1275\n",
      "Epoch 358 batch 190 train Loss 33.1537 test Loss 16.5075 with MSE metric 14448.3496\n",
      "Epoch 358 batch 200 train Loss 33.1506 test Loss 16.5062 with MSE metric 14447.5982\n",
      "Epoch 358 batch 210 train Loss 33.1474 test Loss 16.5050 with MSE metric 14446.8093\n",
      "Epoch 358 batch 220 train Loss 33.1443 test Loss 16.5037 with MSE metric 14446.0140\n",
      "Epoch 358 batch 230 train Loss 33.1412 test Loss 16.5025 with MSE metric 14445.2436\n",
      "Epoch 358 batch 240 train Loss 33.1381 test Loss 16.5013 with MSE metric 14444.4865\n",
      "Time taken for 1 epoch: 27.290764093399048 secs\n",
      "\n",
      "Epoch 359 batch 0 train Loss 33.1350 test Loss 16.5000 with MSE metric 14443.7040\n",
      "Epoch 359 batch 10 train Loss 33.1319 test Loss 16.4988 with MSE metric 14442.9728\n",
      "Epoch 359 batch 20 train Loss 33.1288 test Loss 16.4976 with MSE metric 14442.2377\n",
      "Epoch 359 batch 30 train Loss 33.1257 test Loss 16.4963 with MSE metric 14441.4441\n",
      "Epoch 359 batch 40 train Loss 33.1226 test Loss 16.4951 with MSE metric 14440.6684\n",
      "Epoch 359 batch 50 train Loss 33.1195 test Loss 16.4938 with MSE metric 14439.9125\n",
      "Epoch 359 batch 60 train Loss 33.1165 test Loss 16.4926 with MSE metric 14439.1525\n",
      "Epoch 359 batch 70 train Loss 33.1134 test Loss 16.4914 with MSE metric 14438.3927\n",
      "Epoch 359 batch 80 train Loss 33.1103 test Loss 16.4901 with MSE metric 14437.6016\n",
      "Epoch 359 batch 90 train Loss 33.1072 test Loss 16.4889 with MSE metric 14436.8011\n",
      "Epoch 359 batch 100 train Loss 33.1041 test Loss 16.4877 with MSE metric 14436.0367\n",
      "Epoch 359 batch 110 train Loss 33.1010 test Loss 16.4864 with MSE metric 14435.2546\n",
      "Epoch 359 batch 120 train Loss 33.0979 test Loss 16.4852 with MSE metric 14434.4896\n",
      "Epoch 359 batch 130 train Loss 33.0948 test Loss 16.4840 with MSE metric 14433.7554\n",
      "Epoch 359 batch 140 train Loss 33.0917 test Loss 16.4827 with MSE metric 14432.9900\n",
      "Epoch 359 batch 150 train Loss 33.0886 test Loss 16.4815 with MSE metric 14432.2441\n",
      "Epoch 359 batch 160 train Loss 33.0855 test Loss 16.4803 with MSE metric 14431.4840\n",
      "Epoch 359 batch 170 train Loss 33.0824 test Loss 16.4790 with MSE metric 14430.6981\n",
      "Epoch 359 batch 180 train Loss 33.0793 test Loss 16.4778 with MSE metric 14429.8977\n",
      "Epoch 359 batch 190 train Loss 33.0763 test Loss 16.4766 with MSE metric 14429.1578\n",
      "Epoch 359 batch 200 train Loss 33.0732 test Loss 16.4753 with MSE metric 14428.3995\n",
      "Epoch 359 batch 210 train Loss 33.0701 test Loss 16.4741 with MSE metric 14427.6388\n",
      "Epoch 359 batch 220 train Loss 33.0670 test Loss 16.4729 with MSE metric 14426.8758\n",
      "Epoch 359 batch 230 train Loss 33.0639 test Loss 16.4716 with MSE metric 14426.0839\n",
      "Epoch 359 batch 240 train Loss 33.0608 test Loss 16.4704 with MSE metric 14425.3442\n",
      "Time taken for 1 epoch: 27.18777322769165 secs\n",
      "\n",
      "Epoch 360 batch 0 train Loss 33.0577 test Loss 16.4692 with MSE metric 14424.5399\n",
      "Epoch 360 batch 10 train Loss 33.0547 test Loss 16.4679 with MSE metric 14423.7665\n",
      "Epoch 360 batch 20 train Loss 33.0516 test Loss 16.4667 with MSE metric 14423.0201\n",
      "Epoch 360 batch 30 train Loss 33.0485 test Loss 16.4655 with MSE metric 14422.2503\n",
      "Epoch 360 batch 40 train Loss 33.0454 test Loss 16.4642 with MSE metric 14421.4821\n",
      "Epoch 360 batch 50 train Loss 33.0423 test Loss 16.4630 with MSE metric 14420.7170\n",
      "Epoch 360 batch 60 train Loss 33.0393 test Loss 16.4618 with MSE metric 14419.9604\n",
      "Epoch 360 batch 70 train Loss 33.0362 test Loss 16.4605 with MSE metric 14419.2101\n",
      "Epoch 360 batch 80 train Loss 33.0331 test Loss 16.4593 with MSE metric 14418.4765\n",
      "Epoch 360 batch 90 train Loss 33.0300 test Loss 16.4581 with MSE metric 14417.7291\n",
      "Epoch 360 batch 100 train Loss 33.0270 test Loss 16.4568 with MSE metric 14416.9665\n",
      "Epoch 360 batch 110 train Loss 33.0239 test Loss 16.4556 with MSE metric 14416.2083\n",
      "Epoch 360 batch 120 train Loss 33.0208 test Loss 16.4544 with MSE metric 14415.4400\n",
      "Epoch 360 batch 130 train Loss 33.0177 test Loss 16.4532 with MSE metric 14414.6779\n",
      "Epoch 360 batch 140 train Loss 33.0147 test Loss 16.4519 with MSE metric 14413.9293\n",
      "Epoch 360 batch 150 train Loss 33.0116 test Loss 16.4507 with MSE metric 14413.2051\n",
      "Epoch 360 batch 160 train Loss 33.0085 test Loss 16.4495 with MSE metric 14412.4346\n",
      "Epoch 360 batch 170 train Loss 33.0054 test Loss 16.4483 with MSE metric 14411.6683\n",
      "Epoch 360 batch 180 train Loss 33.0024 test Loss 16.4470 with MSE metric 14410.9097\n",
      "Epoch 360 batch 190 train Loss 32.9993 test Loss 16.4458 with MSE metric 14410.1435\n",
      "Epoch 360 batch 200 train Loss 32.9962 test Loss 16.4446 with MSE metric 14409.4084\n",
      "Epoch 360 batch 210 train Loss 32.9932 test Loss 16.4433 with MSE metric 14408.6621\n",
      "Epoch 360 batch 220 train Loss 32.9901 test Loss 16.4421 with MSE metric 14407.8659\n",
      "Epoch 360 batch 230 train Loss 32.9870 test Loss 16.4409 with MSE metric 14407.0867\n",
      "Epoch 360 batch 240 train Loss 32.9840 test Loss 16.4397 with MSE metric 14406.3782\n",
      "Time taken for 1 epoch: 27.068329095840454 secs\n",
      "\n",
      "Epoch 361 batch 0 train Loss 32.9809 test Loss 16.4384 with MSE metric 14405.5883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361 batch 10 train Loss 32.9778 test Loss 16.4372 with MSE metric 14404.8488\n",
      "Epoch 361 batch 20 train Loss 32.9748 test Loss 16.4360 with MSE metric 14404.0812\n",
      "Epoch 361 batch 30 train Loss 32.9717 test Loss 16.4348 with MSE metric 14403.3096\n",
      "Epoch 361 batch 40 train Loss 32.9686 test Loss 16.4335 with MSE metric 14402.5442\n",
      "Epoch 361 batch 50 train Loss 32.9656 test Loss 16.4323 with MSE metric 14401.7851\n",
      "Epoch 361 batch 60 train Loss 32.9625 test Loss 16.4311 with MSE metric 14401.0170\n",
      "Epoch 361 batch 70 train Loss 32.9594 test Loss 16.4298 with MSE metric 14400.3009\n",
      "Epoch 361 batch 80 train Loss 32.9564 test Loss 16.4286 with MSE metric 14399.5386\n",
      "Epoch 361 batch 90 train Loss 32.9533 test Loss 16.4274 with MSE metric 14398.7368\n",
      "Epoch 361 batch 100 train Loss 32.9503 test Loss 16.4262 with MSE metric 14397.9596\n",
      "Epoch 361 batch 110 train Loss 32.9472 test Loss 16.4250 with MSE metric 14397.2266\n",
      "Epoch 361 batch 120 train Loss 32.9442 test Loss 16.4237 with MSE metric 14396.5515\n",
      "Epoch 361 batch 130 train Loss 32.9411 test Loss 16.4225 with MSE metric 14395.7848\n",
      "Epoch 361 batch 140 train Loss 32.9380 test Loss 16.4213 with MSE metric 14395.0266\n",
      "Epoch 361 batch 150 train Loss 32.9350 test Loss 16.4201 with MSE metric 14394.2714\n",
      "Epoch 361 batch 160 train Loss 32.9319 test Loss 16.4189 with MSE metric 14393.5625\n",
      "Epoch 361 batch 170 train Loss 32.9289 test Loss 16.4177 with MSE metric 14392.7789\n",
      "Epoch 361 batch 180 train Loss 32.9258 test Loss 16.4164 with MSE metric 14392.0451\n",
      "Epoch 361 batch 190 train Loss 32.9228 test Loss 16.4152 with MSE metric 14391.2519\n",
      "Epoch 361 batch 200 train Loss 32.9197 test Loss 16.4140 with MSE metric 14390.4930\n",
      "Epoch 361 batch 210 train Loss 32.9167 test Loss 16.4128 with MSE metric 14389.7078\n",
      "Epoch 361 batch 220 train Loss 32.9136 test Loss 16.4116 with MSE metric 14388.9285\n",
      "Epoch 361 batch 230 train Loss 32.9106 test Loss 16.4103 with MSE metric 14388.2072\n",
      "Epoch 361 batch 240 train Loss 32.9075 test Loss 16.4091 with MSE metric 14387.4785\n",
      "Time taken for 1 epoch: 27.58488392829895 secs\n",
      "\n",
      "Epoch 362 batch 0 train Loss 32.9045 test Loss 16.4079 with MSE metric 14386.7369\n",
      "Epoch 362 batch 10 train Loss 32.9014 test Loss 16.4067 with MSE metric 14386.0015\n",
      "Epoch 362 batch 20 train Loss 32.8984 test Loss 16.4055 with MSE metric 14385.2078\n",
      "Epoch 362 batch 30 train Loss 32.8953 test Loss 16.4043 with MSE metric 14384.4666\n",
      "Epoch 362 batch 40 train Loss 32.8923 test Loss 16.4030 with MSE metric 14383.7401\n",
      "Epoch 362 batch 50 train Loss 32.8892 test Loss 16.4018 with MSE metric 14382.9871\n",
      "Epoch 362 batch 60 train Loss 32.8862 test Loss 16.4006 with MSE metric 14382.2291\n",
      "Epoch 362 batch 70 train Loss 32.8831 test Loss 16.3994 with MSE metric 14381.4902\n",
      "Epoch 362 batch 80 train Loss 32.8801 test Loss 16.3982 with MSE metric 14380.7060\n",
      "Epoch 362 batch 90 train Loss 32.8770 test Loss 16.3970 with MSE metric 14379.9455\n",
      "Epoch 362 batch 100 train Loss 32.8740 test Loss 16.3957 with MSE metric 14379.2139\n",
      "Epoch 362 batch 110 train Loss 32.8710 test Loss 16.3945 with MSE metric 14378.4768\n",
      "Epoch 362 batch 120 train Loss 32.8679 test Loss 16.3933 with MSE metric 14377.6898\n",
      "Epoch 362 batch 130 train Loss 32.8649 test Loss 16.3921 with MSE metric 14376.9551\n",
      "Epoch 362 batch 140 train Loss 32.8618 test Loss 16.3909 with MSE metric 14376.2099\n",
      "Epoch 362 batch 150 train Loss 32.8588 test Loss 16.3897 with MSE metric 14375.4519\n",
      "Epoch 362 batch 160 train Loss 32.8558 test Loss 16.3885 with MSE metric 14374.7096\n",
      "Epoch 362 batch 170 train Loss 32.8527 test Loss 16.3873 with MSE metric 14373.9404\n",
      "Epoch 362 batch 180 train Loss 32.8497 test Loss 16.3861 with MSE metric 14373.2108\n",
      "Epoch 362 batch 190 train Loss 32.8467 test Loss 16.3849 with MSE metric 14372.4280\n",
      "Epoch 362 batch 200 train Loss 32.8436 test Loss 16.3836 with MSE metric 14371.6792\n",
      "Epoch 362 batch 210 train Loss 32.8406 test Loss 16.3824 with MSE metric 14370.9739\n",
      "Epoch 362 batch 220 train Loss 32.8376 test Loss 16.3812 with MSE metric 14370.2195\n",
      "Epoch 362 batch 230 train Loss 32.8345 test Loss 16.3800 with MSE metric 14369.4938\n",
      "Epoch 362 batch 240 train Loss 32.8315 test Loss 16.3788 with MSE metric 14368.7365\n",
      "Time taken for 1 epoch: 26.975119829177856 secs\n",
      "\n",
      "Epoch 363 batch 0 train Loss 32.8284 test Loss 16.3776 with MSE metric 14367.9586\n",
      "Epoch 363 batch 10 train Loss 32.8254 test Loss 16.3764 with MSE metric 14367.2234\n",
      "Epoch 363 batch 20 train Loss 32.8224 test Loss 16.3752 with MSE metric 14366.4551\n",
      "Epoch 363 batch 30 train Loss 32.8194 test Loss 16.3739 with MSE metric 14365.6948\n",
      "Epoch 363 batch 40 train Loss 32.8163 test Loss 16.3727 with MSE metric 14364.9329\n",
      "Epoch 363 batch 50 train Loss 32.8133 test Loss 16.3715 with MSE metric 14364.1707\n",
      "Epoch 363 batch 60 train Loss 32.8103 test Loss 16.3703 with MSE metric 14363.4172\n",
      "Epoch 363 batch 70 train Loss 32.8072 test Loss 16.3691 with MSE metric 14362.6905\n",
      "Epoch 363 batch 80 train Loss 32.8042 test Loss 16.3679 with MSE metric 14361.9536\n",
      "Epoch 363 batch 90 train Loss 32.8012 test Loss 16.3667 with MSE metric 14361.2171\n",
      "Epoch 363 batch 100 train Loss 32.7982 test Loss 16.3655 with MSE metric 14360.4613\n",
      "Epoch 363 batch 110 train Loss 32.7951 test Loss 16.3642 with MSE metric 14359.7446\n",
      "Epoch 363 batch 120 train Loss 32.7921 test Loss 16.3630 with MSE metric 14358.9865\n",
      "Epoch 363 batch 130 train Loss 32.7891 test Loss 16.3618 with MSE metric 14358.2581\n",
      "Epoch 363 batch 140 train Loss 32.7861 test Loss 16.3606 with MSE metric 14357.5475\n",
      "Epoch 363 batch 150 train Loss 32.7830 test Loss 16.3594 with MSE metric 14356.8042\n",
      "Epoch 363 batch 160 train Loss 32.7800 test Loss 16.3582 with MSE metric 14356.0297\n",
      "Epoch 363 batch 170 train Loss 32.7770 test Loss 16.3570 with MSE metric 14355.2819\n",
      "Epoch 363 batch 180 train Loss 32.7740 test Loss 16.3558 with MSE metric 14354.5430\n",
      "Epoch 363 batch 190 train Loss 32.7710 test Loss 16.3546 with MSE metric 14353.7546\n",
      "Epoch 363 batch 200 train Loss 32.7679 test Loss 16.3534 with MSE metric 14353.0288\n",
      "Epoch 363 batch 210 train Loss 32.7649 test Loss 16.3522 with MSE metric 14352.3226\n",
      "Epoch 363 batch 220 train Loss 32.7619 test Loss 16.3510 with MSE metric 14351.5612\n",
      "Epoch 363 batch 230 train Loss 32.7589 test Loss 16.3498 with MSE metric 14350.8131\n",
      "Epoch 363 batch 240 train Loss 32.7559 test Loss 16.3485 with MSE metric 14350.0788\n",
      "Time taken for 1 epoch: 27.47134494781494 secs\n",
      "\n",
      "Epoch 364 batch 0 train Loss 32.7529 test Loss 16.3473 with MSE metric 14349.3521\n",
      "Epoch 364 batch 10 train Loss 32.7498 test Loss 16.3461 with MSE metric 14348.6139\n",
      "Epoch 364 batch 20 train Loss 32.7468 test Loss 16.3449 with MSE metric 14347.8502\n",
      "Epoch 364 batch 30 train Loss 32.7438 test Loss 16.3437 with MSE metric 14347.0636\n",
      "Epoch 364 batch 40 train Loss 32.7408 test Loss 16.3425 with MSE metric 14346.3248\n",
      "Epoch 364 batch 50 train Loss 32.7378 test Loss 16.3413 with MSE metric 14345.5724\n",
      "Epoch 364 batch 60 train Loss 32.7348 test Loss 16.3401 with MSE metric 14344.7841\n",
      "Epoch 364 batch 70 train Loss 32.7318 test Loss 16.3389 with MSE metric 14344.0176\n",
      "Epoch 364 batch 80 train Loss 32.7287 test Loss 16.3377 with MSE metric 14343.2473\n",
      "Epoch 364 batch 90 train Loss 32.7257 test Loss 16.3365 with MSE metric 14342.5229\n",
      "Epoch 364 batch 100 train Loss 32.7227 test Loss 16.3353 with MSE metric 14341.7692\n",
      "Epoch 364 batch 110 train Loss 32.7197 test Loss 16.3341 with MSE metric 14341.0468\n",
      "Epoch 364 batch 120 train Loss 32.7167 test Loss 16.3329 with MSE metric 14340.2846\n",
      "Epoch 364 batch 130 train Loss 32.7137 test Loss 16.3317 with MSE metric 14339.5494\n",
      "Epoch 364 batch 140 train Loss 32.7107 test Loss 16.3305 with MSE metric 14338.8160\n",
      "Epoch 364 batch 150 train Loss 32.7077 test Loss 16.3293 with MSE metric 14338.0706\n",
      "Epoch 364 batch 160 train Loss 32.7047 test Loss 16.3281 with MSE metric 14337.3158\n",
      "Epoch 364 batch 170 train Loss 32.7017 test Loss 16.3269 with MSE metric 14336.5477\n",
      "Epoch 364 batch 180 train Loss 32.6987 test Loss 16.3257 with MSE metric 14335.7931\n",
      "Epoch 364 batch 190 train Loss 32.6957 test Loss 16.3245 with MSE metric 14335.0790\n",
      "Epoch 364 batch 200 train Loss 32.6927 test Loss 16.3233 with MSE metric 14334.3349\n",
      "Epoch 364 batch 210 train Loss 32.6897 test Loss 16.3221 with MSE metric 14333.5971\n",
      "Epoch 364 batch 220 train Loss 32.6867 test Loss 16.3210 with MSE metric 14332.8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364 batch 230 train Loss 32.6837 test Loss 16.3197 with MSE metric 14332.0958\n",
      "Epoch 364 batch 240 train Loss 32.6807 test Loss 16.3185 with MSE metric 14331.3100\n",
      "Time taken for 1 epoch: 29.8522629737854 secs\n",
      "\n",
      "Epoch 365 batch 0 train Loss 32.6777 test Loss 16.3173 with MSE metric 14330.6295\n",
      "Epoch 365 batch 10 train Loss 32.6747 test Loss 16.3161 with MSE metric 14329.9032\n",
      "Epoch 365 batch 20 train Loss 32.6717 test Loss 16.3150 with MSE metric 14329.1301\n",
      "Epoch 365 batch 30 train Loss 32.6687 test Loss 16.3138 with MSE metric 14328.3627\n",
      "Epoch 365 batch 40 train Loss 32.6657 test Loss 16.3126 with MSE metric 14327.6813\n",
      "Epoch 365 batch 50 train Loss 32.6627 test Loss 16.3114 with MSE metric 14326.9184\n",
      "Epoch 365 batch 60 train Loss 32.6597 test Loss 16.3102 with MSE metric 14326.1886\n",
      "Epoch 365 batch 70 train Loss 32.6567 test Loss 16.3090 with MSE metric 14325.4502\n",
      "Epoch 365 batch 80 train Loss 32.6537 test Loss 16.3078 with MSE metric 14324.7058\n",
      "Epoch 365 batch 90 train Loss 32.6507 test Loss 16.3066 with MSE metric 14323.9795\n",
      "Epoch 365 batch 100 train Loss 32.6477 test Loss 16.3054 with MSE metric 14323.2233\n",
      "Epoch 365 batch 110 train Loss 32.6447 test Loss 16.3042 with MSE metric 14322.5079\n",
      "Epoch 365 batch 120 train Loss 32.6417 test Loss 16.3030 with MSE metric 14321.7968\n",
      "Epoch 365 batch 130 train Loss 32.6387 test Loss 16.3018 with MSE metric 14321.0800\n",
      "Epoch 365 batch 140 train Loss 32.6358 test Loss 16.3006 with MSE metric 14320.3646\n",
      "Epoch 365 batch 150 train Loss 32.6328 test Loss 16.2994 with MSE metric 14319.6423\n",
      "Epoch 365 batch 160 train Loss 32.6298 test Loss 16.2982 with MSE metric 14318.9266\n",
      "Epoch 365 batch 170 train Loss 32.6268 test Loss 16.2970 with MSE metric 14318.1781\n",
      "Epoch 365 batch 180 train Loss 32.6238 test Loss 16.2958 with MSE metric 14317.4481\n",
      "Epoch 365 batch 190 train Loss 32.6208 test Loss 16.2946 with MSE metric 14316.7614\n",
      "Epoch 365 batch 200 train Loss 32.6178 test Loss 16.2934 with MSE metric 14316.0142\n",
      "Epoch 365 batch 210 train Loss 32.6149 test Loss 16.2922 with MSE metric 14315.2893\n",
      "Epoch 365 batch 220 train Loss 32.6119 test Loss 16.2910 with MSE metric 14314.5514\n",
      "Epoch 365 batch 230 train Loss 32.6089 test Loss 16.2898 with MSE metric 14313.8046\n",
      "Epoch 365 batch 240 train Loss 32.6059 test Loss 16.2886 with MSE metric 14313.0770\n",
      "Time taken for 1 epoch: 27.029505014419556 secs\n",
      "\n",
      "Epoch 366 batch 0 train Loss 32.6029 test Loss 16.2874 with MSE metric 14312.3084\n",
      "Epoch 366 batch 10 train Loss 32.5999 test Loss 16.2862 with MSE metric 14311.5672\n",
      "Epoch 366 batch 20 train Loss 32.5969 test Loss 16.2850 with MSE metric 14310.8358\n",
      "Epoch 366 batch 30 train Loss 32.5940 test Loss 16.2838 with MSE metric 14310.1161\n",
      "Epoch 366 batch 40 train Loss 32.5910 test Loss 16.2827 with MSE metric 14309.4279\n",
      "Epoch 366 batch 50 train Loss 32.5880 test Loss 16.2815 with MSE metric 14308.6924\n",
      "Epoch 366 batch 60 train Loss 32.5850 test Loss 16.2803 with MSE metric 14307.9379\n",
      "Epoch 366 batch 70 train Loss 32.5821 test Loss 16.2791 with MSE metric 14307.1795\n",
      "Epoch 366 batch 80 train Loss 32.5791 test Loss 16.2779 with MSE metric 14306.4051\n",
      "Epoch 366 batch 90 train Loss 32.5761 test Loss 16.2767 with MSE metric 14305.6665\n",
      "Epoch 366 batch 100 train Loss 32.5731 test Loss 16.2755 with MSE metric 14304.9117\n",
      "Epoch 366 batch 110 train Loss 32.5701 test Loss 16.2743 with MSE metric 14304.1661\n",
      "Epoch 366 batch 120 train Loss 32.5672 test Loss 16.2731 with MSE metric 14303.3604\n",
      "Epoch 366 batch 130 train Loss 32.5642 test Loss 16.2719 with MSE metric 14302.6333\n",
      "Epoch 366 batch 140 train Loss 32.5612 test Loss 16.2707 with MSE metric 14301.9281\n",
      "Epoch 366 batch 150 train Loss 32.5582 test Loss 16.2695 with MSE metric 14301.2108\n",
      "Epoch 366 batch 160 train Loss 32.5553 test Loss 16.2684 with MSE metric 14300.4603\n",
      "Epoch 366 batch 170 train Loss 32.5523 test Loss 16.2672 with MSE metric 14299.7069\n",
      "Epoch 366 batch 180 train Loss 32.5493 test Loss 16.2660 with MSE metric 14298.9889\n",
      "Epoch 366 batch 190 train Loss 32.5463 test Loss 16.2648 with MSE metric 14298.2349\n",
      "Epoch 366 batch 200 train Loss 32.5434 test Loss 16.2636 with MSE metric 14297.4837\n",
      "Epoch 366 batch 210 train Loss 32.5404 test Loss 16.2624 with MSE metric 14296.7707\n",
      "Epoch 366 batch 220 train Loss 32.5374 test Loss 16.2612 with MSE metric 14296.0645\n",
      "Epoch 366 batch 230 train Loss 32.5345 test Loss 16.2600 with MSE metric 14295.3668\n",
      "Epoch 366 batch 240 train Loss 32.5315 test Loss 16.2589 with MSE metric 14294.6317\n",
      "Time taken for 1 epoch: 26.98790216445923 secs\n",
      "\n",
      "Epoch 367 batch 0 train Loss 32.5285 test Loss 16.2577 with MSE metric 14293.9007\n",
      "Epoch 367 batch 10 train Loss 32.5256 test Loss 16.2565 with MSE metric 14293.1458\n",
      "Epoch 367 batch 20 train Loss 32.5226 test Loss 16.2553 with MSE metric 14292.3852\n",
      "Epoch 367 batch 30 train Loss 32.5196 test Loss 16.2541 with MSE metric 14291.6483\n",
      "Epoch 367 batch 40 train Loss 32.5167 test Loss 16.2529 with MSE metric 14290.9108\n",
      "Epoch 367 batch 50 train Loss 32.5137 test Loss 16.2517 with MSE metric 14290.2101\n",
      "Epoch 367 batch 60 train Loss 32.5108 test Loss 16.2506 with MSE metric 14289.4795\n",
      "Epoch 367 batch 70 train Loss 32.5078 test Loss 16.2494 with MSE metric 14288.6955\n",
      "Epoch 367 batch 80 train Loss 32.5048 test Loss 16.2482 with MSE metric 14287.9551\n",
      "Epoch 367 batch 90 train Loss 32.5019 test Loss 16.2470 with MSE metric 14287.2425\n",
      "Epoch 367 batch 100 train Loss 32.4989 test Loss 16.2458 with MSE metric 14286.5019\n",
      "Epoch 367 batch 110 train Loss 32.4959 test Loss 16.2446 with MSE metric 14285.7846\n",
      "Epoch 367 batch 120 train Loss 32.4930 test Loss 16.2434 with MSE metric 14285.0530\n",
      "Epoch 367 batch 130 train Loss 32.4900 test Loss 16.2423 with MSE metric 14284.3264\n",
      "Epoch 367 batch 140 train Loss 32.4871 test Loss 16.2411 with MSE metric 14283.6159\n",
      "Epoch 367 batch 150 train Loss 32.4841 test Loss 16.2399 with MSE metric 14282.8700\n",
      "Epoch 367 batch 160 train Loss 32.4812 test Loss 16.2387 with MSE metric 14282.1467\n",
      "Epoch 367 batch 170 train Loss 32.4782 test Loss 16.2375 with MSE metric 14281.3984\n",
      "Epoch 367 batch 180 train Loss 32.4753 test Loss 16.2363 with MSE metric 14280.6874\n",
      "Epoch 367 batch 190 train Loss 32.4723 test Loss 16.2351 with MSE metric 14279.9776\n",
      "Epoch 367 batch 200 train Loss 32.4693 test Loss 16.2340 with MSE metric 14279.2520\n",
      "Epoch 367 batch 210 train Loss 32.4664 test Loss 16.2328 with MSE metric 14278.5229\n",
      "Epoch 367 batch 220 train Loss 32.4634 test Loss 16.2316 with MSE metric 14277.8084\n",
      "Epoch 367 batch 230 train Loss 32.4605 test Loss 16.2304 with MSE metric 14277.0925\n",
      "Epoch 367 batch 240 train Loss 32.4575 test Loss 16.2293 with MSE metric 14276.3483\n",
      "Time taken for 1 epoch: 26.79177498817444 secs\n",
      "\n",
      "Epoch 368 batch 0 train Loss 32.4546 test Loss 16.2281 with MSE metric 14275.6328\n",
      "Epoch 368 batch 10 train Loss 32.4516 test Loss 16.2269 with MSE metric 14274.9146\n",
      "Epoch 368 batch 20 train Loss 32.4487 test Loss 16.2257 with MSE metric 14274.1677\n",
      "Epoch 368 batch 30 train Loss 32.4457 test Loss 16.2245 with MSE metric 14273.4320\n",
      "Epoch 368 batch 40 train Loss 32.4428 test Loss 16.2234 with MSE metric 14272.6889\n",
      "Epoch 368 batch 50 train Loss 32.4398 test Loss 16.2222 with MSE metric 14271.9405\n",
      "Epoch 368 batch 60 train Loss 32.4369 test Loss 16.2210 with MSE metric 14271.2290\n",
      "Epoch 368 batch 70 train Loss 32.4339 test Loss 16.2198 with MSE metric 14270.5250\n",
      "Epoch 368 batch 80 train Loss 32.4310 test Loss 16.2187 with MSE metric 14269.8423\n",
      "Epoch 368 batch 90 train Loss 32.4281 test Loss 16.2175 with MSE metric 14269.0740\n",
      "Epoch 368 batch 100 train Loss 32.4251 test Loss 16.2163 with MSE metric 14268.3485\n",
      "Epoch 368 batch 110 train Loss 32.4222 test Loss 16.2151 with MSE metric 14267.6504\n",
      "Epoch 368 batch 120 train Loss 32.4192 test Loss 16.2139 with MSE metric 14266.9325\n",
      "Epoch 368 batch 130 train Loss 32.4163 test Loss 16.2128 with MSE metric 14266.2243\n",
      "Epoch 368 batch 140 train Loss 32.4134 test Loss 16.2116 with MSE metric 14265.4907\n",
      "Epoch 368 batch 150 train Loss 32.4104 test Loss 16.2104 with MSE metric 14264.7516\n",
      "Epoch 368 batch 160 train Loss 32.4075 test Loss 16.2092 with MSE metric 14264.0436\n",
      "Epoch 368 batch 170 train Loss 32.4045 test Loss 16.2081 with MSE metric 14263.3090\n",
      "Epoch 368 batch 180 train Loss 32.4016 test Loss 16.2069 with MSE metric 14262.5668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368 batch 190 train Loss 32.3987 test Loss 16.2057 with MSE metric 14261.8251\n",
      "Epoch 368 batch 200 train Loss 32.3957 test Loss 16.2045 with MSE metric 14261.1026\n",
      "Epoch 368 batch 210 train Loss 32.3928 test Loss 16.2034 with MSE metric 14260.3375\n",
      "Epoch 368 batch 220 train Loss 32.3898 test Loss 16.2022 with MSE metric 14259.5849\n",
      "Epoch 368 batch 230 train Loss 32.3869 test Loss 16.2010 with MSE metric 14258.8439\n",
      "Epoch 368 batch 240 train Loss 32.3840 test Loss 16.1999 with MSE metric 14258.1083\n",
      "Time taken for 1 epoch: 27.327100038528442 secs\n",
      "\n",
      "Epoch 369 batch 0 train Loss 32.3810 test Loss 16.1987 with MSE metric 14257.4156\n",
      "Epoch 369 batch 10 train Loss 32.3781 test Loss 16.1975 with MSE metric 14256.6746\n",
      "Epoch 369 batch 20 train Loss 32.3752 test Loss 16.1964 with MSE metric 14255.9565\n",
      "Epoch 369 batch 30 train Loss 32.3722 test Loss 16.1952 with MSE metric 14255.2185\n",
      "Epoch 369 batch 40 train Loss 32.3693 test Loss 16.1940 with MSE metric 14254.4587\n",
      "Epoch 369 batch 50 train Loss 32.3664 test Loss 16.1928 with MSE metric 14253.7255\n",
      "Epoch 369 batch 60 train Loss 32.3634 test Loss 16.1917 with MSE metric 14252.9809\n",
      "Epoch 369 batch 70 train Loss 32.3605 test Loss 16.1905 with MSE metric 14252.2672\n",
      "Epoch 369 batch 80 train Loss 32.3576 test Loss 16.1893 with MSE metric 14251.5545\n",
      "Epoch 369 batch 90 train Loss 32.3546 test Loss 16.1881 with MSE metric 14250.8242\n",
      "Epoch 369 batch 100 train Loss 32.3517 test Loss 16.1870 with MSE metric 14250.0909\n",
      "Epoch 369 batch 110 train Loss 32.3488 test Loss 16.1858 with MSE metric 14249.3709\n",
      "Epoch 369 batch 120 train Loss 32.3459 test Loss 16.1846 with MSE metric 14248.6571\n",
      "Epoch 369 batch 130 train Loss 32.3429 test Loss 16.1835 with MSE metric 14247.9017\n",
      "Epoch 369 batch 140 train Loss 32.3400 test Loss 16.1823 with MSE metric 14247.1726\n",
      "Epoch 369 batch 150 train Loss 32.3371 test Loss 16.1811 with MSE metric 14246.4506\n",
      "Epoch 369 batch 160 train Loss 32.3342 test Loss 16.1800 with MSE metric 14245.7467\n",
      "Epoch 369 batch 170 train Loss 32.3312 test Loss 16.1788 with MSE metric 14245.0149\n",
      "Epoch 369 batch 180 train Loss 32.3283 test Loss 16.1776 with MSE metric 14244.3209\n",
      "Epoch 369 batch 190 train Loss 32.3254 test Loss 16.1764 with MSE metric 14243.6059\n",
      "Epoch 369 batch 200 train Loss 32.3225 test Loss 16.1753 with MSE metric 14242.8947\n",
      "Epoch 369 batch 210 train Loss 32.3196 test Loss 16.1741 with MSE metric 14242.2134\n",
      "Epoch 369 batch 220 train Loss 32.3166 test Loss 16.1729 with MSE metric 14241.4518\n",
      "Epoch 369 batch 230 train Loss 32.3137 test Loss 16.1718 with MSE metric 14240.7063\n",
      "Epoch 369 batch 240 train Loss 32.3108 test Loss 16.1706 with MSE metric 14240.0111\n",
      "Time taken for 1 epoch: 28.3174090385437 secs\n",
      "\n",
      "Epoch 370 batch 0 train Loss 32.3079 test Loss 16.1694 with MSE metric 14239.2654\n",
      "Epoch 370 batch 10 train Loss 32.3049 test Loss 16.1683 with MSE metric 14238.5348\n",
      "Epoch 370 batch 20 train Loss 32.3020 test Loss 16.1671 with MSE metric 14237.8094\n",
      "Epoch 370 batch 30 train Loss 32.2991 test Loss 16.1659 with MSE metric 14237.1096\n",
      "Epoch 370 batch 40 train Loss 32.2962 test Loss 16.1648 with MSE metric 14236.3920\n",
      "Epoch 370 batch 50 train Loss 32.2933 test Loss 16.1636 with MSE metric 14235.6758\n",
      "Epoch 370 batch 60 train Loss 32.2904 test Loss 16.1624 with MSE metric 14234.9446\n",
      "Epoch 370 batch 70 train Loss 32.2874 test Loss 16.1613 with MSE metric 14234.2024\n",
      "Epoch 370 batch 80 train Loss 32.2845 test Loss 16.1601 with MSE metric 14233.5162\n",
      "Epoch 370 batch 90 train Loss 32.2816 test Loss 16.1589 with MSE metric 14232.7802\n",
      "Epoch 370 batch 100 train Loss 32.2787 test Loss 16.1578 with MSE metric 14232.1083\n",
      "Epoch 370 batch 110 train Loss 32.2758 test Loss 16.1566 with MSE metric 14231.3636\n",
      "Epoch 370 batch 120 train Loss 32.2729 test Loss 16.1554 with MSE metric 14230.6471\n",
      "Epoch 370 batch 130 train Loss 32.2700 test Loss 16.1543 with MSE metric 14229.8861\n",
      "Epoch 370 batch 140 train Loss 32.2671 test Loss 16.1531 with MSE metric 14229.1440\n",
      "Epoch 370 batch 150 train Loss 32.2642 test Loss 16.1520 with MSE metric 14228.4401\n",
      "Epoch 370 batch 160 train Loss 32.2612 test Loss 16.1508 with MSE metric 14227.6822\n",
      "Epoch 370 batch 170 train Loss 32.2583 test Loss 16.1496 with MSE metric 14226.9510\n",
      "Epoch 370 batch 180 train Loss 32.2554 test Loss 16.1485 with MSE metric 14226.2415\n",
      "Epoch 370 batch 190 train Loss 32.2525 test Loss 16.1473 with MSE metric 14225.5235\n",
      "Epoch 370 batch 200 train Loss 32.2496 test Loss 16.1461 with MSE metric 14224.8055\n",
      "Epoch 370 batch 210 train Loss 32.2467 test Loss 16.1450 with MSE metric 14224.0939\n",
      "Epoch 370 batch 220 train Loss 32.2438 test Loss 16.1438 with MSE metric 14223.4006\n",
      "Epoch 370 batch 230 train Loss 32.2409 test Loss 16.1427 with MSE metric 14222.6949\n",
      "Epoch 370 batch 240 train Loss 32.2380 test Loss 16.1415 with MSE metric 14222.0275\n",
      "Time taken for 1 epoch: 27.485693216323853 secs\n",
      "\n",
      "Epoch 371 batch 0 train Loss 32.2351 test Loss 16.1403 with MSE metric 14221.2921\n",
      "Epoch 371 batch 10 train Loss 32.2322 test Loss 16.1392 with MSE metric 14220.5763\n",
      "Epoch 371 batch 20 train Loss 32.2293 test Loss 16.1380 with MSE metric 14219.8766\n",
      "Epoch 371 batch 30 train Loss 32.2264 test Loss 16.1368 with MSE metric 14219.1514\n",
      "Epoch 371 batch 40 train Loss 32.2235 test Loss 16.1357 with MSE metric 14218.4574\n",
      "Epoch 371 batch 50 train Loss 32.2206 test Loss 16.1345 with MSE metric 14217.7754\n",
      "Epoch 371 batch 60 train Loss 32.2177 test Loss 16.1334 with MSE metric 14217.0696\n",
      "Epoch 371 batch 70 train Loss 32.2148 test Loss 16.1322 with MSE metric 14216.3558\n",
      "Epoch 371 batch 80 train Loss 32.2119 test Loss 16.1310 with MSE metric 14215.6465\n",
      "Epoch 371 batch 90 train Loss 32.2090 test Loss 16.1299 with MSE metric 14214.9066\n",
      "Epoch 371 batch 100 train Loss 32.2061 test Loss 16.1287 with MSE metric 14214.1784\n",
      "Epoch 371 batch 110 train Loss 32.2032 test Loss 16.1276 with MSE metric 14213.4661\n",
      "Epoch 371 batch 120 train Loss 32.2003 test Loss 16.1264 with MSE metric 14212.7332\n",
      "Epoch 371 batch 130 train Loss 32.1974 test Loss 16.1252 with MSE metric 14212.0175\n",
      "Epoch 371 batch 140 train Loss 32.1945 test Loss 16.1241 with MSE metric 14211.3125\n",
      "Epoch 371 batch 150 train Loss 32.1916 test Loss 16.1229 with MSE metric 14210.5790\n",
      "Epoch 371 batch 160 train Loss 32.1887 test Loss 16.1218 with MSE metric 14209.8592\n",
      "Epoch 371 batch 170 train Loss 32.1858 test Loss 16.1206 with MSE metric 14209.1143\n",
      "Epoch 371 batch 180 train Loss 32.1829 test Loss 16.1195 with MSE metric 14208.3900\n",
      "Epoch 371 batch 190 train Loss 32.1801 test Loss 16.1183 with MSE metric 14207.6887\n",
      "Epoch 371 batch 200 train Loss 32.1772 test Loss 16.1171 with MSE metric 14206.9383\n",
      "Epoch 371 batch 210 train Loss 32.1743 test Loss 16.1160 with MSE metric 14206.2163\n",
      "Epoch 371 batch 220 train Loss 32.1714 test Loss 16.1148 with MSE metric 14205.5126\n",
      "Epoch 371 batch 230 train Loss 32.1685 test Loss 16.1137 with MSE metric 14204.7679\n",
      "Epoch 371 batch 240 train Loss 32.1656 test Loss 16.1125 with MSE metric 14204.0506\n",
      "Time taken for 1 epoch: 26.96780800819397 secs\n",
      "\n",
      "Epoch 372 batch 0 train Loss 32.1627 test Loss 16.1113 with MSE metric 14203.3678\n",
      "Epoch 372 batch 10 train Loss 32.1598 test Loss 16.1102 with MSE metric 14202.6568\n",
      "Epoch 372 batch 20 train Loss 32.1569 test Loss 16.1090 with MSE metric 14201.9327\n",
      "Epoch 372 batch 30 train Loss 32.1541 test Loss 16.1079 with MSE metric 14201.2365\n",
      "Epoch 372 batch 40 train Loss 32.1512 test Loss 16.1067 with MSE metric 14200.5126\n",
      "Epoch 372 batch 50 train Loss 32.1483 test Loss 16.1056 with MSE metric 14199.7946\n",
      "Epoch 372 batch 60 train Loss 32.1454 test Loss 16.1044 with MSE metric 14199.0586\n",
      "Epoch 372 batch 70 train Loss 32.1425 test Loss 16.1033 with MSE metric 14198.3571\n",
      "Epoch 372 batch 80 train Loss 32.1396 test Loss 16.1021 with MSE metric 14197.6249\n",
      "Epoch 372 batch 90 train Loss 32.1368 test Loss 16.1010 with MSE metric 14196.9202\n",
      "Epoch 372 batch 100 train Loss 32.1339 test Loss 16.0998 with MSE metric 14196.2150\n",
      "Epoch 372 batch 110 train Loss 32.1310 test Loss 16.0987 with MSE metric 14195.4845\n",
      "Epoch 372 batch 120 train Loss 32.1281 test Loss 16.0975 with MSE metric 14194.7609\n",
      "Epoch 372 batch 130 train Loss 32.1252 test Loss 16.0964 with MSE metric 14194.0580\n",
      "Epoch 372 batch 140 train Loss 32.1224 test Loss 16.0952 with MSE metric 14193.3569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 372 batch 150 train Loss 32.1195 test Loss 16.0940 with MSE metric 14192.6174\n",
      "Epoch 372 batch 160 train Loss 32.1166 test Loss 16.0929 with MSE metric 14191.9588\n",
      "Epoch 372 batch 170 train Loss 32.1137 test Loss 16.0917 with MSE metric 14191.2676\n",
      "Epoch 372 batch 180 train Loss 32.1108 test Loss 16.0906 with MSE metric 14190.5729\n",
      "Epoch 372 batch 190 train Loss 32.1080 test Loss 16.0894 with MSE metric 14189.8675\n",
      "Epoch 372 batch 200 train Loss 32.1051 test Loss 16.0883 with MSE metric 14189.1562\n",
      "Epoch 372 batch 210 train Loss 32.1022 test Loss 16.0871 with MSE metric 14188.4467\n",
      "Epoch 372 batch 220 train Loss 32.0993 test Loss 16.0860 with MSE metric 14187.7290\n",
      "Epoch 372 batch 230 train Loss 32.0965 test Loss 16.0848 with MSE metric 14187.0110\n",
      "Epoch 372 batch 240 train Loss 32.0936 test Loss 16.0837 with MSE metric 14186.3033\n",
      "Time taken for 1 epoch: 27.39517307281494 secs\n",
      "\n",
      "Epoch 373 batch 0 train Loss 32.0907 test Loss 16.0825 with MSE metric 14185.5998\n",
      "Epoch 373 batch 10 train Loss 32.0879 test Loss 16.0814 with MSE metric 14184.8983\n",
      "Epoch 373 batch 20 train Loss 32.0850 test Loss 16.0803 with MSE metric 14184.1970\n",
      "Epoch 373 batch 30 train Loss 32.0821 test Loss 16.0791 with MSE metric 14183.5176\n",
      "Epoch 373 batch 40 train Loss 32.0793 test Loss 16.0780 with MSE metric 14182.8028\n",
      "Epoch 373 batch 50 train Loss 32.0764 test Loss 16.0768 with MSE metric 14182.0691\n",
      "Epoch 373 batch 60 train Loss 32.0735 test Loss 16.0757 with MSE metric 14181.3567\n",
      "Epoch 373 batch 70 train Loss 32.0706 test Loss 16.0745 with MSE metric 14180.6816\n",
      "Epoch 373 batch 80 train Loss 32.0678 test Loss 16.0734 with MSE metric 14179.9708\n",
      "Epoch 373 batch 90 train Loss 32.0649 test Loss 16.0722 with MSE metric 14179.2780\n",
      "Epoch 373 batch 100 train Loss 32.0621 test Loss 16.0711 with MSE metric 14178.5954\n",
      "Epoch 373 batch 110 train Loss 32.0592 test Loss 16.0699 with MSE metric 14177.8837\n",
      "Epoch 373 batch 120 train Loss 32.0563 test Loss 16.0688 with MSE metric 14177.2020\n",
      "Epoch 373 batch 130 train Loss 32.0535 test Loss 16.0676 with MSE metric 14176.4600\n",
      "Epoch 373 batch 140 train Loss 32.0506 test Loss 16.0665 with MSE metric 14175.7364\n",
      "Epoch 373 batch 150 train Loss 32.0477 test Loss 16.0653 with MSE metric 14175.0237\n",
      "Epoch 373 batch 160 train Loss 32.0449 test Loss 16.0642 with MSE metric 14174.2707\n",
      "Epoch 373 batch 170 train Loss 32.0420 test Loss 16.0631 with MSE metric 14173.6051\n",
      "Epoch 373 batch 180 train Loss 32.0391 test Loss 16.0619 with MSE metric 14172.8716\n",
      "Epoch 373 batch 190 train Loss 32.0363 test Loss 16.0608 with MSE metric 14172.1366\n",
      "Epoch 373 batch 200 train Loss 32.0334 test Loss 16.0596 with MSE metric 14171.4806\n",
      "Epoch 373 batch 210 train Loss 32.0306 test Loss 16.0585 with MSE metric 14170.7380\n",
      "Epoch 373 batch 220 train Loss 32.0277 test Loss 16.0574 with MSE metric 14170.0005\n",
      "Epoch 373 batch 230 train Loss 32.0248 test Loss 16.0562 with MSE metric 14169.2996\n",
      "Epoch 373 batch 240 train Loss 32.0220 test Loss 16.0551 with MSE metric 14168.6365\n",
      "Time taken for 1 epoch: 27.31958508491516 secs\n",
      "\n",
      "Epoch 374 batch 0 train Loss 32.0191 test Loss 16.0540 with MSE metric 14167.8998\n",
      "Epoch 374 batch 10 train Loss 32.0163 test Loss 16.0528 with MSE metric 14167.2167\n",
      "Epoch 374 batch 20 train Loss 32.0134 test Loss 16.0517 with MSE metric 14166.5225\n",
      "Epoch 374 batch 30 train Loss 32.0106 test Loss 16.0505 with MSE metric 14165.8383\n",
      "Epoch 374 batch 40 train Loss 32.0077 test Loss 16.0494 with MSE metric 14165.1322\n",
      "Epoch 374 batch 50 train Loss 32.0049 test Loss 16.0483 with MSE metric 14164.4177\n",
      "Epoch 374 batch 60 train Loss 32.0020 test Loss 16.0471 with MSE metric 14163.7081\n",
      "Epoch 374 batch 70 train Loss 31.9992 test Loss 16.0460 with MSE metric 14163.0193\n",
      "Epoch 374 batch 80 train Loss 31.9963 test Loss 16.0448 with MSE metric 14162.3258\n",
      "Epoch 374 batch 90 train Loss 31.9935 test Loss 16.0437 with MSE metric 14161.6118\n",
      "Epoch 374 batch 100 train Loss 31.9906 test Loss 16.0425 with MSE metric 14160.9063\n",
      "Epoch 374 batch 110 train Loss 31.9878 test Loss 16.0414 with MSE metric 14160.2360\n",
      "Epoch 374 batch 120 train Loss 31.9849 test Loss 16.0403 with MSE metric 14159.5333\n",
      "Epoch 374 batch 130 train Loss 31.9821 test Loss 16.0391 with MSE metric 14158.8556\n",
      "Epoch 374 batch 140 train Loss 31.9792 test Loss 16.0380 with MSE metric 14158.1696\n",
      "Epoch 374 batch 150 train Loss 31.9764 test Loss 16.0369 with MSE metric 14157.4770\n",
      "Epoch 374 batch 160 train Loss 31.9735 test Loss 16.0357 with MSE metric 14156.7479\n",
      "Epoch 374 batch 170 train Loss 31.9707 test Loss 16.0346 with MSE metric 14156.0169\n",
      "Epoch 374 batch 180 train Loss 31.9678 test Loss 16.0335 with MSE metric 14155.3268\n",
      "Epoch 374 batch 190 train Loss 31.9650 test Loss 16.0323 with MSE metric 14154.6310\n",
      "Epoch 374 batch 200 train Loss 31.9621 test Loss 16.0312 with MSE metric 14153.9300\n",
      "Epoch 374 batch 210 train Loss 31.9593 test Loss 16.0300 with MSE metric 14153.2507\n",
      "Epoch 374 batch 220 train Loss 31.9564 test Loss 16.0289 with MSE metric 14152.5188\n",
      "Epoch 374 batch 230 train Loss 31.9536 test Loss 16.0278 with MSE metric 14151.8238\n",
      "Epoch 374 batch 240 train Loss 31.9508 test Loss 16.0266 with MSE metric 14151.1366\n",
      "Time taken for 1 epoch: 27.45556902885437 secs\n",
      "\n",
      "Epoch 375 batch 0 train Loss 31.9479 test Loss 16.0255 with MSE metric 14150.4509\n",
      "Epoch 375 batch 10 train Loss 31.9451 test Loss 16.0243 with MSE metric 14149.7237\n",
      "Epoch 375 batch 20 train Loss 31.9422 test Loss 16.0232 with MSE metric 14149.0136\n",
      "Epoch 375 batch 30 train Loss 31.9394 test Loss 16.0221 with MSE metric 14148.3187\n",
      "Epoch 375 batch 40 train Loss 31.9366 test Loss 16.0209 with MSE metric 14147.5731\n",
      "Epoch 375 batch 50 train Loss 31.9337 test Loss 16.0198 with MSE metric 14146.8493\n",
      "Epoch 375 batch 60 train Loss 31.9309 test Loss 16.0187 with MSE metric 14146.1503\n",
      "Epoch 375 batch 70 train Loss 31.9280 test Loss 16.0175 with MSE metric 14145.4668\n",
      "Epoch 375 batch 80 train Loss 31.9252 test Loss 16.0164 with MSE metric 14144.7411\n",
      "Epoch 375 batch 90 train Loss 31.9224 test Loss 16.0153 with MSE metric 14144.0514\n",
      "Epoch 375 batch 100 train Loss 31.9195 test Loss 16.0141 with MSE metric 14143.3375\n",
      "Epoch 375 batch 110 train Loss 31.9167 test Loss 16.0130 with MSE metric 14142.6289\n",
      "Epoch 375 batch 120 train Loss 31.9139 test Loss 16.0119 with MSE metric 14141.9353\n",
      "Epoch 375 batch 130 train Loss 31.9110 test Loss 16.0107 with MSE metric 14141.1954\n",
      "Epoch 375 batch 140 train Loss 31.9082 test Loss 16.0096 with MSE metric 14140.4521\n",
      "Epoch 375 batch 150 train Loss 31.9054 test Loss 16.0085 with MSE metric 14139.7375\n",
      "Epoch 375 batch 160 train Loss 31.9025 test Loss 16.0073 with MSE metric 14139.0466\n",
      "Epoch 375 batch 170 train Loss 31.8997 test Loss 16.0062 with MSE metric 14138.3495\n",
      "Epoch 375 batch 180 train Loss 31.8969 test Loss 16.0051 with MSE metric 14137.6401\n",
      "Epoch 375 batch 190 train Loss 31.8940 test Loss 16.0039 with MSE metric 14136.9456\n",
      "Epoch 375 batch 200 train Loss 31.8912 test Loss 16.0028 with MSE metric 14136.2545\n",
      "Epoch 375 batch 210 train Loss 31.8884 test Loss 16.0017 with MSE metric 14135.5499\n",
      "Epoch 375 batch 220 train Loss 31.8855 test Loss 16.0005 with MSE metric 14134.8530\n",
      "Epoch 375 batch 230 train Loss 31.8827 test Loss 15.9994 with MSE metric 14134.1729\n",
      "Epoch 375 batch 240 train Loss 31.8799 test Loss 15.9983 with MSE metric 14133.4844\n",
      "Time taken for 1 epoch: 27.778976917266846 secs\n",
      "\n",
      "Epoch 376 batch 0 train Loss 31.8771 test Loss 15.9971 with MSE metric 14132.7499\n",
      "Epoch 376 batch 10 train Loss 31.8742 test Loss 15.9960 with MSE metric 14132.0725\n",
      "Epoch 376 batch 20 train Loss 31.8714 test Loss 15.9949 with MSE metric 14131.3708\n",
      "Epoch 376 batch 30 train Loss 31.8686 test Loss 15.9937 with MSE metric 14130.6643\n",
      "Epoch 376 batch 40 train Loss 31.8658 test Loss 15.9926 with MSE metric 14129.9652\n",
      "Epoch 376 batch 50 train Loss 31.8629 test Loss 15.9915 with MSE metric 14129.3102\n",
      "Epoch 376 batch 60 train Loss 31.8601 test Loss 15.9903 with MSE metric 14128.6041\n",
      "Epoch 376 batch 70 train Loss 31.8573 test Loss 15.9892 with MSE metric 14127.9024\n",
      "Epoch 376 batch 80 train Loss 31.8545 test Loss 15.9881 with MSE metric 14127.1881\n",
      "Epoch 376 batch 90 train Loss 31.8517 test Loss 15.9870 with MSE metric 14126.4893\n",
      "Epoch 376 batch 100 train Loss 31.8488 test Loss 15.9858 with MSE metric 14125.8012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376 batch 110 train Loss 31.8460 test Loss 15.9847 with MSE metric 14125.1218\n",
      "Epoch 376 batch 120 train Loss 31.8432 test Loss 15.9836 with MSE metric 14124.4233\n",
      "Epoch 376 batch 130 train Loss 31.8404 test Loss 15.9824 with MSE metric 14123.7081\n",
      "Epoch 376 batch 140 train Loss 31.8376 test Loss 15.9813 with MSE metric 14122.9635\n",
      "Epoch 376 batch 150 train Loss 31.8347 test Loss 15.9802 with MSE metric 14122.2950\n",
      "Epoch 376 batch 160 train Loss 31.8319 test Loss 15.9791 with MSE metric 14121.6248\n",
      "Epoch 376 batch 170 train Loss 31.8291 test Loss 15.9780 with MSE metric 14120.9254\n",
      "Epoch 376 batch 180 train Loss 31.8263 test Loss 15.9768 with MSE metric 14120.1867\n",
      "Epoch 376 batch 190 train Loss 31.8235 test Loss 15.9757 with MSE metric 14119.5254\n",
      "Epoch 376 batch 200 train Loss 31.8207 test Loss 15.9746 with MSE metric 14118.8208\n",
      "Epoch 376 batch 210 train Loss 31.8178 test Loss 15.9735 with MSE metric 14118.1501\n",
      "Epoch 376 batch 220 train Loss 31.8150 test Loss 15.9723 with MSE metric 14117.4128\n",
      "Epoch 376 batch 230 train Loss 31.8122 test Loss 15.9712 with MSE metric 14116.7216\n",
      "Epoch 376 batch 240 train Loss 31.8094 test Loss 15.9701 with MSE metric 14116.0118\n",
      "Time taken for 1 epoch: 27.59782600402832 secs\n",
      "\n",
      "Epoch 377 batch 0 train Loss 31.8066 test Loss 15.9690 with MSE metric 14115.3574\n",
      "Epoch 377 batch 10 train Loss 31.8038 test Loss 15.9678 with MSE metric 14114.7134\n",
      "Epoch 377 batch 20 train Loss 31.8010 test Loss 15.9667 with MSE metric 14114.0067\n",
      "Epoch 377 batch 30 train Loss 31.7982 test Loss 15.9656 with MSE metric 14113.3301\n",
      "Epoch 377 batch 40 train Loss 31.7954 test Loss 15.9644 with MSE metric 14112.6419\n",
      "Epoch 377 batch 50 train Loss 31.7926 test Loss 15.9633 with MSE metric 14111.9304\n",
      "Epoch 377 batch 60 train Loss 31.7897 test Loss 15.9622 with MSE metric 14111.2090\n",
      "Epoch 377 batch 70 train Loss 31.7869 test Loss 15.9611 with MSE metric 14110.5421\n",
      "Epoch 377 batch 80 train Loss 31.7841 test Loss 15.9600 with MSE metric 14109.8005\n",
      "Epoch 377 batch 90 train Loss 31.7813 test Loss 15.9588 with MSE metric 14109.1007\n",
      "Epoch 377 batch 100 train Loss 31.7785 test Loss 15.9577 with MSE metric 14108.4110\n",
      "Epoch 377 batch 110 train Loss 31.7757 test Loss 15.9566 with MSE metric 14107.7403\n",
      "Epoch 377 batch 120 train Loss 31.7729 test Loss 15.9555 with MSE metric 14107.0547\n",
      "Epoch 377 batch 130 train Loss 31.7701 test Loss 15.9544 with MSE metric 14106.3883\n",
      "Epoch 377 batch 140 train Loss 31.7673 test Loss 15.9532 with MSE metric 14105.6940\n",
      "Epoch 377 batch 150 train Loss 31.7645 test Loss 15.9521 with MSE metric 14104.9935\n",
      "Epoch 377 batch 160 train Loss 31.7617 test Loss 15.9510 with MSE metric 14104.3179\n",
      "Epoch 377 batch 170 train Loss 31.7589 test Loss 15.9499 with MSE metric 14103.6225\n",
      "Epoch 377 batch 180 train Loss 31.7561 test Loss 15.9488 with MSE metric 14102.9207\n",
      "Epoch 377 batch 190 train Loss 31.7533 test Loss 15.9476 with MSE metric 14102.2041\n",
      "Epoch 377 batch 200 train Loss 31.7505 test Loss 15.9465 with MSE metric 14101.5331\n",
      "Epoch 377 batch 210 train Loss 31.7477 test Loss 15.9454 with MSE metric 14100.8406\n",
      "Epoch 377 batch 220 train Loss 31.7449 test Loss 15.9443 with MSE metric 14100.1557\n",
      "Epoch 377 batch 230 train Loss 31.7421 test Loss 15.9431 with MSE metric 14099.4789\n",
      "Epoch 377 batch 240 train Loss 31.7393 test Loss 15.9420 with MSE metric 14098.7868\n",
      "Time taken for 1 epoch: 27.579771280288696 secs\n",
      "\n",
      "Epoch 378 batch 0 train Loss 31.7365 test Loss 15.9409 with MSE metric 14098.1216\n",
      "Epoch 378 batch 10 train Loss 31.7337 test Loss 15.9398 with MSE metric 14097.4257\n",
      "Epoch 378 batch 20 train Loss 31.7309 test Loss 15.9387 with MSE metric 14096.7472\n",
      "Epoch 378 batch 30 train Loss 31.7281 test Loss 15.9376 with MSE metric 14096.0593\n",
      "Epoch 378 batch 40 train Loss 31.7253 test Loss 15.9364 with MSE metric 14095.3794\n",
      "Epoch 378 batch 50 train Loss 31.7225 test Loss 15.9353 with MSE metric 14094.7386\n",
      "Epoch 378 batch 60 train Loss 31.7198 test Loss 15.9342 with MSE metric 14094.0580\n",
      "Epoch 378 batch 70 train Loss 31.7170 test Loss 15.9331 with MSE metric 14093.3786\n",
      "Epoch 378 batch 80 train Loss 31.7142 test Loss 15.9320 with MSE metric 14092.6669\n",
      "Epoch 378 batch 90 train Loss 31.7114 test Loss 15.9309 with MSE metric 14091.9931\n",
      "Epoch 378 batch 100 train Loss 31.7086 test Loss 15.9298 with MSE metric 14091.3300\n",
      "Epoch 378 batch 110 train Loss 31.7058 test Loss 15.9286 with MSE metric 14090.6361\n",
      "Epoch 378 batch 120 train Loss 31.7030 test Loss 15.9275 with MSE metric 14089.9688\n",
      "Epoch 378 batch 130 train Loss 31.7002 test Loss 15.9264 with MSE metric 14089.2716\n",
      "Epoch 378 batch 140 train Loss 31.6974 test Loss 15.9253 with MSE metric 14088.5587\n",
      "Epoch 378 batch 150 train Loss 31.6947 test Loss 15.9242 with MSE metric 14087.8929\n",
      "Epoch 378 batch 160 train Loss 31.6919 test Loss 15.9230 with MSE metric 14087.2097\n",
      "Epoch 378 batch 170 train Loss 31.6891 test Loss 15.9219 with MSE metric 14086.5538\n",
      "Epoch 378 batch 180 train Loss 31.6863 test Loss 15.9208 with MSE metric 14085.8641\n",
      "Epoch 378 batch 190 train Loss 31.6835 test Loss 15.9197 with MSE metric 14085.1833\n",
      "Epoch 378 batch 200 train Loss 31.6807 test Loss 15.9186 with MSE metric 14084.4747\n",
      "Epoch 378 batch 210 train Loss 31.6779 test Loss 15.9175 with MSE metric 14083.8158\n",
      "Epoch 378 batch 220 train Loss 31.6752 test Loss 15.9164 with MSE metric 14083.1535\n",
      "Epoch 378 batch 230 train Loss 31.6724 test Loss 15.9153 with MSE metric 14082.4495\n",
      "Epoch 378 batch 240 train Loss 31.6696 test Loss 15.9141 with MSE metric 14081.7662\n",
      "Time taken for 1 epoch: 28.321424961090088 secs\n",
      "\n",
      "Epoch 379 batch 0 train Loss 31.6668 test Loss 15.9130 with MSE metric 14081.0860\n",
      "Epoch 379 batch 10 train Loss 31.6640 test Loss 15.9119 with MSE metric 14080.4292\n",
      "Epoch 379 batch 20 train Loss 31.6613 test Loss 15.9108 with MSE metric 14079.6992\n",
      "Epoch 379 batch 30 train Loss 31.6585 test Loss 15.9097 with MSE metric 14079.0373\n",
      "Epoch 379 batch 40 train Loss 31.6557 test Loss 15.9086 with MSE metric 14078.3177\n",
      "Epoch 379 batch 50 train Loss 31.6529 test Loss 15.9075 with MSE metric 14077.5793\n",
      "Epoch 379 batch 60 train Loss 31.6501 test Loss 15.9064 with MSE metric 14076.8569\n",
      "Epoch 379 batch 70 train Loss 31.6474 test Loss 15.9053 with MSE metric 14076.1865\n",
      "Epoch 379 batch 80 train Loss 31.6446 test Loss 15.9042 with MSE metric 14075.4962\n",
      "Epoch 379 batch 90 train Loss 31.6418 test Loss 15.9030 with MSE metric 14074.8095\n",
      "Epoch 379 batch 100 train Loss 31.6390 test Loss 15.9019 with MSE metric 14074.1292\n",
      "Epoch 379 batch 110 train Loss 31.6362 test Loss 15.9008 with MSE metric 14073.4629\n",
      "Epoch 379 batch 120 train Loss 31.6335 test Loss 15.8997 with MSE metric 14072.7830\n",
      "Epoch 379 batch 130 train Loss 31.6307 test Loss 15.8986 with MSE metric 14072.1180\n",
      "Epoch 379 batch 140 train Loss 31.6279 test Loss 15.8975 with MSE metric 14071.4307\n",
      "Epoch 379 batch 150 train Loss 31.6252 test Loss 15.8964 with MSE metric 14070.7237\n",
      "Epoch 379 batch 160 train Loss 31.6224 test Loss 15.8953 with MSE metric 14070.0381\n",
      "Epoch 379 batch 170 train Loss 31.6196 test Loss 15.8942 with MSE metric 14069.3229\n",
      "Epoch 379 batch 180 train Loss 31.6168 test Loss 15.8931 with MSE metric 14068.6419\n",
      "Epoch 379 batch 190 train Loss 31.6141 test Loss 15.8920 with MSE metric 14067.9673\n",
      "Epoch 379 batch 200 train Loss 31.6113 test Loss 15.8909 with MSE metric 14067.3071\n",
      "Epoch 379 batch 210 train Loss 31.6085 test Loss 15.8898 with MSE metric 14066.6183\n",
      "Epoch 379 batch 220 train Loss 31.6058 test Loss 15.8886 with MSE metric 14065.9480\n",
      "Epoch 379 batch 230 train Loss 31.6030 test Loss 15.8875 with MSE metric 14065.2552\n",
      "Epoch 379 batch 240 train Loss 31.6002 test Loss 15.8864 with MSE metric 14064.5587\n",
      "Time taken for 1 epoch: 29.64110016822815 secs\n",
      "\n",
      "Epoch 380 batch 0 train Loss 31.5975 test Loss 15.8853 with MSE metric 14063.8727\n",
      "Epoch 380 batch 10 train Loss 31.5947 test Loss 15.8842 with MSE metric 14063.2206\n",
      "Epoch 380 batch 20 train Loss 31.5919 test Loss 15.8831 with MSE metric 14062.5015\n",
      "Epoch 380 batch 30 train Loss 31.5892 test Loss 15.8820 with MSE metric 14061.8366\n",
      "Epoch 380 batch 40 train Loss 31.5864 test Loss 15.8809 with MSE metric 14061.1388\n",
      "Epoch 380 batch 50 train Loss 31.5836 test Loss 15.8798 with MSE metric 14060.4282\n",
      "Epoch 380 batch 60 train Loss 31.5809 test Loss 15.8787 with MSE metric 14059.7315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380 batch 70 train Loss 31.5781 test Loss 15.8776 with MSE metric 14059.0829\n",
      "Epoch 380 batch 80 train Loss 31.5753 test Loss 15.8765 with MSE metric 14058.3608\n",
      "Epoch 380 batch 90 train Loss 31.5726 test Loss 15.8754 with MSE metric 14057.6527\n",
      "Epoch 380 batch 100 train Loss 31.5698 test Loss 15.8742 with MSE metric 14056.9690\n",
      "Epoch 380 batch 110 train Loss 31.5670 test Loss 15.8731 with MSE metric 14056.2600\n",
      "Epoch 380 batch 120 train Loss 31.5643 test Loss 15.8720 with MSE metric 14055.5863\n",
      "Epoch 380 batch 130 train Loss 31.5615 test Loss 15.8709 with MSE metric 14054.8628\n",
      "Epoch 380 batch 140 train Loss 31.5588 test Loss 15.8698 with MSE metric 14054.1686\n",
      "Epoch 380 batch 150 train Loss 31.5560 test Loss 15.8687 with MSE metric 14053.4652\n",
      "Epoch 380 batch 160 train Loss 31.5532 test Loss 15.8676 with MSE metric 14052.7453\n",
      "Epoch 380 batch 170 train Loss 31.5505 test Loss 15.8665 with MSE metric 14052.0488\n",
      "Epoch 380 batch 180 train Loss 31.5477 test Loss 15.8654 with MSE metric 14051.3845\n",
      "Epoch 380 batch 190 train Loss 31.5450 test Loss 15.8643 with MSE metric 14050.7221\n",
      "Epoch 380 batch 200 train Loss 31.5422 test Loss 15.8632 with MSE metric 14050.0363\n",
      "Epoch 380 batch 210 train Loss 31.5395 test Loss 15.8621 with MSE metric 14049.3413\n",
      "Epoch 380 batch 220 train Loss 31.5367 test Loss 15.8610 with MSE metric 14048.6992\n",
      "Epoch 380 batch 230 train Loss 31.5340 test Loss 15.8599 with MSE metric 14048.0057\n",
      "Epoch 380 batch 240 train Loss 31.5312 test Loss 15.8588 with MSE metric 14047.3300\n",
      "Time taken for 1 epoch: 27.033618927001953 secs\n",
      "\n",
      "Epoch 381 batch 0 train Loss 31.5284 test Loss 15.8577 with MSE metric 14046.6175\n",
      "Epoch 381 batch 10 train Loss 31.5257 test Loss 15.8566 with MSE metric 14045.9070\n",
      "Epoch 381 batch 20 train Loss 31.5229 test Loss 15.8555 with MSE metric 14045.2218\n",
      "Epoch 381 batch 30 train Loss 31.5202 test Loss 15.8544 with MSE metric 14044.5599\n",
      "Epoch 381 batch 40 train Loss 31.5174 test Loss 15.8533 with MSE metric 14043.8859\n",
      "Epoch 381 batch 50 train Loss 31.5147 test Loss 15.8522 with MSE metric 14043.2001\n",
      "Epoch 381 batch 60 train Loss 31.5119 test Loss 15.8511 with MSE metric 14042.5083\n",
      "Epoch 381 batch 70 train Loss 31.5092 test Loss 15.8500 with MSE metric 14041.8098\n",
      "Epoch 381 batch 80 train Loss 31.5064 test Loss 15.8489 with MSE metric 14041.1360\n",
      "Epoch 381 batch 90 train Loss 31.5037 test Loss 15.8478 with MSE metric 14040.4293\n",
      "Epoch 381 batch 100 train Loss 31.5009 test Loss 15.8467 with MSE metric 14039.7504\n",
      "Epoch 381 batch 110 train Loss 31.4982 test Loss 15.8456 with MSE metric 14039.1051\n",
      "Epoch 381 batch 120 train Loss 31.4955 test Loss 15.8445 with MSE metric 14038.4106\n",
      "Epoch 381 batch 130 train Loss 31.4927 test Loss 15.8434 with MSE metric 14037.7557\n",
      "Epoch 381 batch 140 train Loss 31.4900 test Loss 15.8423 with MSE metric 14037.0565\n",
      "Epoch 381 batch 150 train Loss 31.4872 test Loss 15.8412 with MSE metric 14036.3941\n",
      "Epoch 381 batch 160 train Loss 31.4845 test Loss 15.8401 with MSE metric 14035.7210\n",
      "Epoch 381 batch 170 train Loss 31.4817 test Loss 15.8390 with MSE metric 14035.0096\n",
      "Epoch 381 batch 180 train Loss 31.4790 test Loss 15.8379 with MSE metric 14034.3054\n",
      "Epoch 381 batch 190 train Loss 31.4762 test Loss 15.8369 with MSE metric 14033.6251\n",
      "Epoch 381 batch 200 train Loss 31.4735 test Loss 15.8358 with MSE metric 14032.9288\n",
      "Epoch 381 batch 210 train Loss 31.4708 test Loss 15.8346 with MSE metric 14032.2724\n",
      "Epoch 381 batch 220 train Loss 31.4680 test Loss 15.8336 with MSE metric 14031.5197\n",
      "Epoch 381 batch 230 train Loss 31.4653 test Loss 15.8325 with MSE metric 14030.8200\n",
      "Epoch 381 batch 240 train Loss 31.4625 test Loss 15.8314 with MSE metric 14030.1295\n",
      "Time taken for 1 epoch: 28.12752604484558 secs\n",
      "\n",
      "Epoch 382 batch 0 train Loss 31.4598 test Loss 15.8303 with MSE metric 14029.4316\n",
      "Epoch 382 batch 10 train Loss 31.4571 test Loss 15.8292 with MSE metric 14028.8042\n",
      "Epoch 382 batch 20 train Loss 31.4543 test Loss 15.8281 with MSE metric 14028.1057\n",
      "Epoch 382 batch 30 train Loss 31.4516 test Loss 15.8270 with MSE metric 14027.4406\n",
      "Epoch 382 batch 40 train Loss 31.4488 test Loss 15.8259 with MSE metric 14026.7211\n",
      "Epoch 382 batch 50 train Loss 31.4461 test Loss 15.8248 with MSE metric 14026.0816\n",
      "Epoch 382 batch 60 train Loss 31.4434 test Loss 15.8237 with MSE metric 14025.3478\n",
      "Epoch 382 batch 70 train Loss 31.4406 test Loss 15.8226 with MSE metric 14024.6738\n",
      "Epoch 382 batch 80 train Loss 31.4379 test Loss 15.8215 with MSE metric 14024.0078\n",
      "Epoch 382 batch 90 train Loss 31.4352 test Loss 15.8204 with MSE metric 14023.3216\n",
      "Epoch 382 batch 100 train Loss 31.4324 test Loss 15.8193 with MSE metric 14022.6153\n",
      "Epoch 382 batch 110 train Loss 31.4297 test Loss 15.8182 with MSE metric 14021.9620\n",
      "Epoch 382 batch 120 train Loss 31.4270 test Loss 15.8172 with MSE metric 14021.2788\n",
      "Epoch 382 batch 130 train Loss 31.4242 test Loss 15.8161 with MSE metric 14020.5712\n",
      "Epoch 382 batch 140 train Loss 31.4215 test Loss 15.8150 with MSE metric 14019.9239\n",
      "Epoch 382 batch 150 train Loss 31.4188 test Loss 15.8139 with MSE metric 14019.2236\n",
      "Epoch 382 batch 160 train Loss 31.4161 test Loss 15.8128 with MSE metric 14018.5397\n",
      "Epoch 382 batch 170 train Loss 31.4133 test Loss 15.8117 with MSE metric 14017.8821\n",
      "Epoch 382 batch 180 train Loss 31.4106 test Loss 15.8106 with MSE metric 14017.2190\n",
      "Epoch 382 batch 190 train Loss 31.4079 test Loss 15.8095 with MSE metric 14016.5405\n",
      "Epoch 382 batch 200 train Loss 31.4051 test Loss 15.8085 with MSE metric 14015.8567\n",
      "Epoch 382 batch 210 train Loss 31.4024 test Loss 15.8074 with MSE metric 14015.1718\n",
      "Epoch 382 batch 220 train Loss 31.3997 test Loss 15.8063 with MSE metric 14014.4919\n",
      "Epoch 382 batch 230 train Loss 31.3970 test Loss 15.8052 with MSE metric 14013.8400\n",
      "Epoch 382 batch 240 train Loss 31.3942 test Loss 15.8041 with MSE metric 14013.1677\n",
      "Time taken for 1 epoch: 27.275739192962646 secs\n",
      "\n",
      "Epoch 383 batch 0 train Loss 31.3915 test Loss 15.8030 with MSE metric 14012.4873\n",
      "Epoch 383 batch 10 train Loss 31.3888 test Loss 15.8019 with MSE metric 14011.8288\n",
      "Epoch 383 batch 20 train Loss 31.3861 test Loss 15.8008 with MSE metric 14011.1493\n",
      "Epoch 383 batch 30 train Loss 31.3834 test Loss 15.7997 with MSE metric 14010.5273\n",
      "Epoch 383 batch 40 train Loss 31.3806 test Loss 15.7986 with MSE metric 14009.8023\n",
      "Epoch 383 batch 50 train Loss 31.3779 test Loss 15.7976 with MSE metric 14009.1072\n",
      "Epoch 383 batch 60 train Loss 31.3752 test Loss 15.7965 with MSE metric 14008.4452\n",
      "Epoch 383 batch 70 train Loss 31.3725 test Loss 15.7954 with MSE metric 14007.7634\n",
      "Epoch 383 batch 80 train Loss 31.3697 test Loss 15.7943 with MSE metric 14007.0646\n",
      "Epoch 383 batch 90 train Loss 31.3670 test Loss 15.7932 with MSE metric 14006.4351\n",
      "Epoch 383 batch 100 train Loss 31.3643 test Loss 15.7921 with MSE metric 14005.7722\n",
      "Epoch 383 batch 110 train Loss 31.3616 test Loss 15.7910 with MSE metric 14005.1041\n",
      "Epoch 383 batch 120 train Loss 31.3589 test Loss 15.7899 with MSE metric 14004.4590\n",
      "Epoch 383 batch 130 train Loss 31.3562 test Loss 15.7889 with MSE metric 14003.8068\n",
      "Epoch 383 batch 140 train Loss 31.3535 test Loss 15.7878 with MSE metric 14003.1360\n",
      "Epoch 383 batch 150 train Loss 31.3507 test Loss 15.7867 with MSE metric 14002.4989\n",
      "Epoch 383 batch 160 train Loss 31.3480 test Loss 15.7856 with MSE metric 14001.7871\n",
      "Epoch 383 batch 170 train Loss 31.3453 test Loss 15.7845 with MSE metric 14001.1167\n",
      "Epoch 383 batch 180 train Loss 31.3426 test Loss 15.7834 with MSE metric 14000.4176\n",
      "Epoch 383 batch 190 train Loss 31.3399 test Loss 15.7823 with MSE metric 13999.7794\n",
      "Epoch 383 batch 200 train Loss 31.3372 test Loss 15.7813 with MSE metric 13999.0704\n",
      "Epoch 383 batch 210 train Loss 31.3344 test Loss 15.7802 with MSE metric 13998.3858\n",
      "Epoch 383 batch 220 train Loss 31.3317 test Loss 15.7791 with MSE metric 13997.7274\n",
      "Epoch 383 batch 230 train Loss 31.3290 test Loss 15.7780 with MSE metric 13997.0623\n",
      "Epoch 383 batch 240 train Loss 31.3263 test Loss 15.7769 with MSE metric 13996.4267\n",
      "Time taken for 1 epoch: 27.393747806549072 secs\n",
      "\n",
      "Epoch 384 batch 0 train Loss 31.3236 test Loss 15.7758 with MSE metric 13995.7315\n",
      "Epoch 384 batch 10 train Loss 31.3209 test Loss 15.7748 with MSE metric 13995.0451\n",
      "Epoch 384 batch 20 train Loss 31.3182 test Loss 15.7737 with MSE metric 13994.3440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 batch 30 train Loss 31.3155 test Loss 15.7726 with MSE metric 13993.6816\n",
      "Epoch 384 batch 40 train Loss 31.3128 test Loss 15.7715 with MSE metric 13993.0067\n",
      "Epoch 384 batch 50 train Loss 31.3101 test Loss 15.7704 with MSE metric 13992.3396\n",
      "Epoch 384 batch 60 train Loss 31.3074 test Loss 15.7693 with MSE metric 13991.6857\n",
      "Epoch 384 batch 70 train Loss 31.3046 test Loss 15.7683 with MSE metric 13990.9934\n",
      "Epoch 384 batch 80 train Loss 31.3019 test Loss 15.7672 with MSE metric 13990.3279\n",
      "Epoch 384 batch 90 train Loss 31.2992 test Loss 15.7661 with MSE metric 13989.6370\n",
      "Epoch 384 batch 100 train Loss 31.2965 test Loss 15.7650 with MSE metric 13988.9620\n",
      "Epoch 384 batch 110 train Loss 31.2938 test Loss 15.7640 with MSE metric 13988.3252\n",
      "Epoch 384 batch 120 train Loss 31.2911 test Loss 15.7629 with MSE metric 13987.6658\n",
      "Epoch 384 batch 130 train Loss 31.2884 test Loss 15.7618 with MSE metric 13986.9778\n",
      "Epoch 384 batch 140 train Loss 31.2857 test Loss 15.7607 with MSE metric 13986.3466\n",
      "Epoch 384 batch 150 train Loss 31.2830 test Loss 15.7596 with MSE metric 13985.7178\n",
      "Epoch 384 batch 160 train Loss 31.2803 test Loss 15.7585 with MSE metric 13985.0650\n",
      "Epoch 384 batch 170 train Loss 31.2776 test Loss 15.7575 with MSE metric 13984.3675\n",
      "Epoch 384 batch 180 train Loss 31.2749 test Loss 15.7564 with MSE metric 13983.6783\n",
      "Epoch 384 batch 190 train Loss 31.2722 test Loss 15.7553 with MSE metric 13983.0159\n",
      "Epoch 384 batch 200 train Loss 31.2695 test Loss 15.7542 with MSE metric 13982.3335\n",
      "Epoch 384 batch 210 train Loss 31.2668 test Loss 15.7532 with MSE metric 13981.6422\n",
      "Epoch 384 batch 220 train Loss 31.2641 test Loss 15.7521 with MSE metric 13980.9751\n",
      "Epoch 384 batch 230 train Loss 31.2614 test Loss 15.7510 with MSE metric 13980.3041\n",
      "Epoch 384 batch 240 train Loss 31.2587 test Loss 15.7499 with MSE metric 13979.6056\n",
      "Time taken for 1 epoch: 27.646564722061157 secs\n",
      "\n",
      "Epoch 385 batch 0 train Loss 31.2560 test Loss 15.7489 with MSE metric 13978.9197\n",
      "Epoch 385 batch 10 train Loss 31.2533 test Loss 15.7478 with MSE metric 13978.2514\n",
      "Epoch 385 batch 20 train Loss 31.2506 test Loss 15.7467 with MSE metric 13977.5760\n",
      "Epoch 385 batch 30 train Loss 31.2479 test Loss 15.7456 with MSE metric 13976.9090\n",
      "Epoch 385 batch 40 train Loss 31.2453 test Loss 15.7445 with MSE metric 13976.2443\n",
      "Epoch 385 batch 50 train Loss 31.2426 test Loss 15.7434 with MSE metric 13975.5683\n",
      "Epoch 385 batch 60 train Loss 31.2399 test Loss 15.7424 with MSE metric 13974.9009\n",
      "Epoch 385 batch 70 train Loss 31.2372 test Loss 15.7413 with MSE metric 13974.2347\n",
      "Epoch 385 batch 80 train Loss 31.2345 test Loss 15.7402 with MSE metric 13973.5580\n",
      "Epoch 385 batch 90 train Loss 31.2318 test Loss 15.7391 with MSE metric 13972.9354\n",
      "Epoch 385 batch 100 train Loss 31.2291 test Loss 15.7381 with MSE metric 13972.2408\n",
      "Epoch 385 batch 110 train Loss 31.2264 test Loss 15.7370 with MSE metric 13971.5810\n",
      "Epoch 385 batch 120 train Loss 31.2237 test Loss 15.7359 with MSE metric 13970.8677\n",
      "Epoch 385 batch 130 train Loss 31.2210 test Loss 15.7348 with MSE metric 13970.2110\n",
      "Epoch 385 batch 140 train Loss 31.2183 test Loss 15.7338 with MSE metric 13969.5430\n",
      "Epoch 385 batch 150 train Loss 31.2157 test Loss 15.7327 with MSE metric 13968.8761\n",
      "Epoch 385 batch 160 train Loss 31.2130 test Loss 15.7316 with MSE metric 13968.2016\n",
      "Epoch 385 batch 170 train Loss 31.2103 test Loss 15.7305 with MSE metric 13967.5416\n",
      "Epoch 385 batch 180 train Loss 31.2076 test Loss 15.7295 with MSE metric 13966.8744\n",
      "Epoch 385 batch 190 train Loss 31.2049 test Loss 15.7284 with MSE metric 13966.2013\n",
      "Epoch 385 batch 200 train Loss 31.2022 test Loss 15.7273 with MSE metric 13965.5322\n",
      "Epoch 385 batch 210 train Loss 31.1995 test Loss 15.7262 with MSE metric 13964.8542\n",
      "Epoch 385 batch 220 train Loss 31.1969 test Loss 15.7252 with MSE metric 13964.1904\n",
      "Epoch 385 batch 230 train Loss 31.1942 test Loss 15.7241 with MSE metric 13963.5287\n",
      "Epoch 385 batch 240 train Loss 31.1915 test Loss 15.7230 with MSE metric 13962.8368\n",
      "Time taken for 1 epoch: 27.37742781639099 secs\n",
      "\n",
      "Epoch 386 batch 0 train Loss 31.1888 test Loss 15.7220 with MSE metric 13962.1434\n",
      "Epoch 386 batch 10 train Loss 31.1861 test Loss 15.7209 with MSE metric 13961.4962\n",
      "Epoch 386 batch 20 train Loss 31.1834 test Loss 15.7198 with MSE metric 13960.7903\n",
      "Epoch 386 batch 30 train Loss 31.1808 test Loss 15.7188 with MSE metric 13960.1031\n",
      "Epoch 386 batch 40 train Loss 31.1781 test Loss 15.7177 with MSE metric 13959.4351\n",
      "Epoch 386 batch 50 train Loss 31.1754 test Loss 15.7166 with MSE metric 13958.7503\n",
      "Epoch 386 batch 60 train Loss 31.1727 test Loss 15.7156 with MSE metric 13958.1190\n",
      "Epoch 386 batch 70 train Loss 31.1700 test Loss 15.7145 with MSE metric 13957.4637\n",
      "Epoch 386 batch 80 train Loss 31.1674 test Loss 15.7134 with MSE metric 13956.8149\n",
      "Epoch 386 batch 90 train Loss 31.1647 test Loss 15.7124 with MSE metric 13956.1634\n",
      "Epoch 386 batch 100 train Loss 31.1620 test Loss 15.7113 with MSE metric 13955.4722\n",
      "Epoch 386 batch 110 train Loss 31.1593 test Loss 15.7102 with MSE metric 13954.7927\n",
      "Epoch 386 batch 120 train Loss 31.1567 test Loss 15.7092 with MSE metric 13954.1438\n",
      "Epoch 386 batch 130 train Loss 31.1540 test Loss 15.7081 with MSE metric 13953.4414\n",
      "Epoch 386 batch 140 train Loss 31.1513 test Loss 15.7070 with MSE metric 13952.7867\n",
      "Epoch 386 batch 150 train Loss 31.1486 test Loss 15.7060 with MSE metric 13952.0975\n",
      "Epoch 386 batch 160 train Loss 31.1460 test Loss 15.7049 with MSE metric 13951.4309\n",
      "Epoch 386 batch 170 train Loss 31.1433 test Loss 15.7038 with MSE metric 13950.7725\n",
      "Epoch 386 batch 180 train Loss 31.1406 test Loss 15.7028 with MSE metric 13950.1104\n",
      "Epoch 386 batch 190 train Loss 31.1379 test Loss 15.7017 with MSE metric 13949.4546\n",
      "Epoch 386 batch 200 train Loss 31.1353 test Loss 15.7006 with MSE metric 13948.8029\n",
      "Epoch 386 batch 210 train Loss 31.1326 test Loss 15.6996 with MSE metric 13948.1220\n",
      "Epoch 386 batch 220 train Loss 31.1299 test Loss 15.6985 with MSE metric 13947.4518\n",
      "Epoch 386 batch 230 train Loss 31.1273 test Loss 15.6974 with MSE metric 13946.8025\n",
      "Epoch 386 batch 240 train Loss 31.1246 test Loss 15.6964 with MSE metric 13946.1264\n",
      "Time taken for 1 epoch: 27.58514928817749 secs\n",
      "\n",
      "Epoch 387 batch 0 train Loss 31.1219 test Loss 15.6953 with MSE metric 13945.4607\n",
      "Epoch 387 batch 10 train Loss 31.1193 test Loss 15.6942 with MSE metric 13944.8036\n",
      "Epoch 387 batch 20 train Loss 31.1166 test Loss 15.6932 with MSE metric 13944.1450\n",
      "Epoch 387 batch 30 train Loss 31.1139 test Loss 15.6921 with MSE metric 13943.4651\n",
      "Epoch 387 batch 40 train Loss 31.1113 test Loss 15.6910 with MSE metric 13942.7829\n",
      "Epoch 387 batch 50 train Loss 31.1086 test Loss 15.6900 with MSE metric 13942.1275\n",
      "Epoch 387 batch 60 train Loss 31.1059 test Loss 15.6889 with MSE metric 13941.4794\n",
      "Epoch 387 batch 70 train Loss 31.1033 test Loss 15.6878 with MSE metric 13940.7882\n",
      "Epoch 387 batch 80 train Loss 31.1006 test Loss 15.6868 with MSE metric 13940.1182\n",
      "Epoch 387 batch 90 train Loss 31.0979 test Loss 15.6857 with MSE metric 13939.4390\n",
      "Epoch 387 batch 100 train Loss 31.0953 test Loss 15.6846 with MSE metric 13938.8074\n",
      "Epoch 387 batch 110 train Loss 31.0926 test Loss 15.6836 with MSE metric 13938.1433\n",
      "Epoch 387 batch 120 train Loss 31.0899 test Loss 15.6825 with MSE metric 13937.4777\n",
      "Epoch 387 batch 130 train Loss 31.0873 test Loss 15.6814 with MSE metric 13936.7725\n",
      "Epoch 387 batch 140 train Loss 31.0846 test Loss 15.6804 with MSE metric 13936.0700\n",
      "Epoch 387 batch 150 train Loss 31.0820 test Loss 15.6793 with MSE metric 13935.4345\n",
      "Epoch 387 batch 160 train Loss 31.0793 test Loss 15.6782 with MSE metric 13934.7746\n",
      "Epoch 387 batch 170 train Loss 31.0766 test Loss 15.6772 with MSE metric 13934.0928\n",
      "Epoch 387 batch 180 train Loss 31.0740 test Loss 15.6761 with MSE metric 13933.4347\n",
      "Epoch 387 batch 190 train Loss 31.0713 test Loss 15.6750 with MSE metric 13932.7297\n",
      "Epoch 387 batch 200 train Loss 31.0687 test Loss 15.6740 with MSE metric 13932.0750\n",
      "Epoch 387 batch 210 train Loss 31.0660 test Loss 15.6729 with MSE metric 13931.3710\n",
      "Epoch 387 batch 220 train Loss 31.0634 test Loss 15.6719 with MSE metric 13930.7178\n",
      "Epoch 387 batch 230 train Loss 31.0607 test Loss 15.6708 with MSE metric 13930.0540\n",
      "Epoch 387 batch 240 train Loss 31.0580 test Loss 15.6697 with MSE metric 13929.3843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 27.764389753341675 secs\n",
      "\n",
      "Epoch 388 batch 0 train Loss 31.0554 test Loss 15.6687 with MSE metric 13928.7157\n",
      "Epoch 388 batch 10 train Loss 31.0527 test Loss 15.6676 with MSE metric 13928.0045\n",
      "Epoch 388 batch 20 train Loss 31.0501 test Loss 15.6665 with MSE metric 13927.3636\n",
      "Epoch 388 batch 30 train Loss 31.0474 test Loss 15.6655 with MSE metric 13926.7334\n",
      "Epoch 388 batch 40 train Loss 31.0448 test Loss 15.6644 with MSE metric 13926.0975\n",
      "Epoch 388 batch 50 train Loss 31.0421 test Loss 15.6634 with MSE metric 13925.4275\n",
      "Epoch 388 batch 60 train Loss 31.0395 test Loss 15.6623 with MSE metric 13924.8021\n",
      "Epoch 388 batch 70 train Loss 31.0368 test Loss 15.6613 with MSE metric 13924.1574\n",
      "Epoch 388 batch 80 train Loss 31.0342 test Loss 15.6602 with MSE metric 13923.4693\n",
      "Epoch 388 batch 90 train Loss 31.0315 test Loss 15.6591 with MSE metric 13922.7668\n",
      "Epoch 388 batch 100 train Loss 31.0289 test Loss 15.6581 with MSE metric 13922.1512\n",
      "Epoch 388 batch 110 train Loss 31.0262 test Loss 15.6570 with MSE metric 13921.4939\n",
      "Epoch 388 batch 120 train Loss 31.0236 test Loss 15.6560 with MSE metric 13920.8322\n",
      "Epoch 388 batch 130 train Loss 31.0209 test Loss 15.6549 with MSE metric 13920.1667\n",
      "Epoch 388 batch 140 train Loss 31.0183 test Loss 15.6539 with MSE metric 13919.4906\n",
      "Epoch 388 batch 150 train Loss 31.0156 test Loss 15.6528 with MSE metric 13918.8416\n",
      "Epoch 388 batch 160 train Loss 31.0130 test Loss 15.6517 with MSE metric 13918.2150\n",
      "Epoch 388 batch 170 train Loss 31.0103 test Loss 15.6507 with MSE metric 13917.5319\n",
      "Epoch 388 batch 180 train Loss 31.0077 test Loss 15.6496 with MSE metric 13916.8331\n",
      "Epoch 388 batch 190 train Loss 31.0051 test Loss 15.6486 with MSE metric 13916.1540\n",
      "Epoch 388 batch 200 train Loss 31.0024 test Loss 15.6475 with MSE metric 13915.5052\n",
      "Epoch 388 batch 210 train Loss 30.9998 test Loss 15.6465 with MSE metric 13914.8311\n",
      "Epoch 388 batch 220 train Loss 30.9971 test Loss 15.6454 with MSE metric 13914.1962\n",
      "Epoch 388 batch 230 train Loss 30.9945 test Loss 15.6443 with MSE metric 13913.5544\n",
      "Epoch 388 batch 240 train Loss 30.9918 test Loss 15.6433 with MSE metric 13912.8835\n",
      "Time taken for 1 epoch: 27.54357671737671 secs\n",
      "\n",
      "Epoch 389 batch 0 train Loss 30.9892 test Loss 15.6422 with MSE metric 13912.2377\n",
      "Epoch 389 batch 10 train Loss 30.9866 test Loss 15.6412 with MSE metric 13911.5718\n",
      "Epoch 389 batch 20 train Loss 30.9839 test Loss 15.6401 with MSE metric 13910.8677\n",
      "Epoch 389 batch 30 train Loss 30.9813 test Loss 15.6390 with MSE metric 13910.1946\n",
      "Epoch 389 batch 40 train Loss 30.9786 test Loss 15.6380 with MSE metric 13909.5614\n",
      "Epoch 389 batch 50 train Loss 30.9760 test Loss 15.6369 with MSE metric 13908.8671\n",
      "Epoch 389 batch 60 train Loss 30.9734 test Loss 15.6359 with MSE metric 13908.2182\n",
      "Epoch 389 batch 70 train Loss 30.9707 test Loss 15.6348 with MSE metric 13907.5791\n",
      "Epoch 389 batch 80 train Loss 30.9681 test Loss 15.6337 with MSE metric 13906.9038\n",
      "Epoch 389 batch 90 train Loss 30.9654 test Loss 15.6327 with MSE metric 13906.2158\n",
      "Epoch 389 batch 100 train Loss 30.9628 test Loss 15.6316 with MSE metric 13905.5497\n",
      "Epoch 389 batch 110 train Loss 30.9602 test Loss 15.6306 with MSE metric 13904.8826\n",
      "Epoch 389 batch 120 train Loss 30.9575 test Loss 15.6295 with MSE metric 13904.2231\n",
      "Epoch 389 batch 130 train Loss 30.9549 test Loss 15.6285 with MSE metric 13903.5748\n",
      "Epoch 389 batch 140 train Loss 30.9523 test Loss 15.6274 with MSE metric 13902.8962\n",
      "Epoch 389 batch 150 train Loss 30.9496 test Loss 15.6263 with MSE metric 13902.1816\n",
      "Epoch 389 batch 160 train Loss 30.9470 test Loss 15.6253 with MSE metric 13901.5160\n",
      "Epoch 389 batch 170 train Loss 30.9444 test Loss 15.6242 with MSE metric 13900.8865\n",
      "Epoch 389 batch 180 train Loss 30.9417 test Loss 15.6232 with MSE metric 13900.2404\n",
      "Epoch 389 batch 190 train Loss 30.9391 test Loss 15.6221 with MSE metric 13899.5717\n",
      "Epoch 389 batch 200 train Loss 30.9365 test Loss 15.6211 with MSE metric 13898.8891\n",
      "Epoch 389 batch 210 train Loss 30.9338 test Loss 15.6200 with MSE metric 13898.2181\n",
      "Epoch 389 batch 220 train Loss 30.9312 test Loss 15.6190 with MSE metric 13897.5627\n",
      "Epoch 389 batch 230 train Loss 30.9286 test Loss 15.6179 with MSE metric 13896.8991\n",
      "Epoch 389 batch 240 train Loss 30.9260 test Loss 15.6169 with MSE metric 13896.2411\n",
      "Time taken for 1 epoch: 27.614864110946655 secs\n",
      "\n",
      "Epoch 390 batch 0 train Loss 30.9233 test Loss 15.6158 with MSE metric 13895.5961\n",
      "Epoch 390 batch 10 train Loss 30.9207 test Loss 15.6148 with MSE metric 13894.9627\n",
      "Epoch 390 batch 20 train Loss 30.9181 test Loss 15.6137 with MSE metric 13894.3196\n",
      "Epoch 390 batch 30 train Loss 30.9155 test Loss 15.6127 with MSE metric 13893.6862\n",
      "Epoch 390 batch 40 train Loss 30.9128 test Loss 15.6116 with MSE metric 13893.0233\n",
      "Epoch 390 batch 50 train Loss 30.9102 test Loss 15.6106 with MSE metric 13892.3975\n",
      "Epoch 390 batch 60 train Loss 30.9076 test Loss 15.6095 with MSE metric 13891.7479\n",
      "Epoch 390 batch 70 train Loss 30.9050 test Loss 15.6085 with MSE metric 13891.0892\n",
      "Epoch 390 batch 80 train Loss 30.9023 test Loss 15.6074 with MSE metric 13890.4209\n",
      "Epoch 390 batch 90 train Loss 30.8997 test Loss 15.6064 with MSE metric 13889.7443\n",
      "Epoch 390 batch 100 train Loss 30.8971 test Loss 15.6053 with MSE metric 13889.1270\n",
      "Epoch 390 batch 110 train Loss 30.8945 test Loss 15.6043 with MSE metric 13888.4750\n",
      "Epoch 390 batch 120 train Loss 30.8918 test Loss 15.6032 with MSE metric 13887.7937\n",
      "Epoch 390 batch 130 train Loss 30.8892 test Loss 15.6022 with MSE metric 13887.1083\n",
      "Epoch 390 batch 140 train Loss 30.8866 test Loss 15.6011 with MSE metric 13886.4359\n",
      "Epoch 390 batch 150 train Loss 30.8840 test Loss 15.6001 with MSE metric 13885.7619\n",
      "Epoch 390 batch 160 train Loss 30.8814 test Loss 15.5990 with MSE metric 13885.1138\n",
      "Epoch 390 batch 170 train Loss 30.8787 test Loss 15.5979 with MSE metric 13884.4289\n",
      "Epoch 390 batch 180 train Loss 30.8761 test Loss 15.5969 with MSE metric 13883.7585\n",
      "Epoch 390 batch 190 train Loss 30.8735 test Loss 15.5959 with MSE metric 13883.1223\n",
      "Epoch 390 batch 200 train Loss 30.8709 test Loss 15.5948 with MSE metric 13882.4807\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s=32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 10 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s= 32)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
