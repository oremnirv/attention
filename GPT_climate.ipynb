{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import climate_model, losses, dot_prod_attention\n",
    "from data import data_generation, data_combine, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_climate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, t, token = data_combine.climate_data_to_model_input('./data/t2m_monthly_averaged_ensemble_members_1989_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create climate train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tr = t[:8000]; temp_tr = temp[:8000]; token_tr = token[:8000]\n",
    "time_te = t[8000:]; temp_te = temp[8000:]; token_te = token[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_log_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_log_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_log_sig, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred_te, pred_log_sig_te = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred_te, pred_log_sig_te)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred_te, pred_log_sig_te, t_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Restored from /Users/omernivron/Downloads/GPT_climate/ckpt/check_0/ckpt-21073\n",
      "Epoch 0 batch 0 train Loss 5.2659 test Loss 5.3509 with MSE metric 6894.6094\n",
      "Time taken for 1 epoch: 31.018823385238647 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 5.3307 test Loss 5.3369 with MSE metric 7835.0283\n",
      "Time taken for 1 epoch: 28.722605228424072 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 5.3091 test Loss 5.3692 with MSE metric 7518.0879\n",
      "Time taken for 1 epoch: 30.544169902801514 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 5.3132 test Loss 5.3349 with MSE metric 7579.1050\n",
      "Time taken for 1 epoch: 29.89436411857605 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 5.2974 test Loss 5.3666 with MSE metric 7339.3164\n",
      "Time taken for 1 epoch: 29.931660652160645 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 5.3256 test Loss 5.3490 with MSE metric 7759.5596\n",
      "Time taken for 1 epoch: 29.423455953598022 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 5.3310 test Loss 5.3658 with MSE metric 7828.8369\n",
      "Time taken for 1 epoch: 29.46803903579712 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 5.3005 test Loss 5.3894 with MSE metric 7388.8809\n",
      "Time taken for 1 epoch: 29.936673879623413 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 5.2786 test Loss 5.3407 with MSE metric 7070.9077\n",
      "Time taken for 1 epoch: 29.608925104141235 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 5.3480 test Loss 5.3763 with MSE metric 8077.3477\n",
      "Time taken for 1 epoch: 30.719855785369873 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 5.3057 test Loss 5.3696 with MSE metric 7466.5356\n",
      "Time taken for 1 epoch: 30.29711413383484 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 5.2679 test Loss 5.3044 with MSE metric 6920.0439\n",
      "Time taken for 1 epoch: 30.205563068389893 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 5.3006 test Loss 5.3628 with MSE metric 7374.9160\n",
      "Time taken for 1 epoch: 30.472731113433838 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 5.3101 test Loss 5.3664 with MSE metric 7532.2583\n",
      "Time taken for 1 epoch: 30.209309101104736 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 5.3249 test Loss 5.4334 with MSE metric 7721.7231\n",
      "Time taken for 1 epoch: 30.574368953704834 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 5.2588 test Loss 5.3585 with MSE metric 6756.9995\n",
      "Time taken for 1 epoch: 30.368242740631104 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 5.3064 test Loss 5.3241 with MSE metric 7475.1362\n",
      "Time taken for 1 epoch: 29.62348699569702 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 5.3103 test Loss 5.3212 with MSE metric 7534.9819\n",
      "Time taken for 1 epoch: 28.93769407272339 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 5.3454 test Loss 5.3412 with MSE metric 8056.3643\n",
      "Time taken for 1 epoch: 28.15811014175415 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 5.2679 test Loss 5.3515 with MSE metric 6894.5801\n",
      "Time taken for 1 epoch: 27.420591115951538 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 5.2870 test Loss 5.3194 with MSE metric 7192.2324\n",
      "Time taken for 1 epoch: 27.39360499382019 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 5.2593 test Loss 5.3330 with MSE metric 6771.9673\n",
      "Time taken for 1 epoch: 27.336852073669434 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 5.2894 test Loss 5.3958 with MSE metric 7215.2236\n",
      "Time taken for 1 epoch: 27.37324023246765 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 5.3397 test Loss 5.3579 with MSE metric 7964.3848\n",
      "Time taken for 1 epoch: 27.319079875946045 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 5.3154 test Loss 5.3532 with MSE metric 7556.0591\n",
      "Time taken for 1 epoch: 28.71900701522827 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 5.2871 test Loss 5.3757 with MSE metric 7175.9688\n",
      "Time taken for 1 epoch: 28.33268904685974 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 5.2933 test Loss 5.3488 with MSE metric 7283.1094\n",
      "Time taken for 1 epoch: 28.719418048858643 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 5.3186 test Loss 5.3467 with MSE metric 7656.1641\n",
      "Time taken for 1 epoch: 29.081578969955444 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 5.3053 test Loss 5.3534 with MSE metric 7461.3726\n",
      "Time taken for 1 epoch: 30.03380012512207 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 5.2873 test Loss 5.3825 with MSE metric 7197.7539\n",
      "Time taken for 1 epoch: 30.86672806739807 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 5.2918 test Loss 5.3681 with MSE metric 7257.7051\n",
      "Time taken for 1 epoch: 31.027190685272217 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 5.2682 test Loss 5.3379 with MSE metric 6911.3633\n",
      "Time taken for 1 epoch: 32.645530700683594 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 5.2772 test Loss 5.3682 with MSE metric 7050.4692\n",
      "Time taken for 1 epoch: 34.42465615272522 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 5.2767 test Loss 5.3557 with MSE metric 7045.6465\n",
      "Time taken for 1 epoch: 34.648334980010986 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 5.3174 test Loss 5.3487 with MSE metric 7629.2329\n",
      "Time taken for 1 epoch: 34.31745624542236 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 5.3222 test Loss 5.4053 with MSE metric 7704.7549\n",
      "Time taken for 1 epoch: 34.05135416984558 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 5.3475 test Loss 5.3465 with MSE metric 8083.4258\n",
      "Time taken for 1 epoch: 34.65022802352905 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 5.2897 test Loss 5.3356 with MSE metric 7230.0859\n",
      "Time taken for 1 epoch: 34.330236196517944 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 5.3642 test Loss 5.3345 with MSE metric 8346.0703\n",
      "Time taken for 1 epoch: 34.591071128845215 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 5.3071 test Loss 5.3470 with MSE metric 7487.1724\n",
      "Time taken for 1 epoch: 34.43921113014221 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 5.2791 test Loss 5.3454 with MSE metric 7077.6909\n",
      "Time taken for 1 epoch: 35.042304039001465 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 5.3330 test Loss 5.3364 with MSE metric 7870.5459\n",
      "Time taken for 1 epoch: 34.68221187591553 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 5.2324 test Loss 5.3908 with MSE metric 6384.0176\n",
      "Time taken for 1 epoch: 34.19810485839844 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 5.3067 test Loss 5.4190 with MSE metric 7480.6816\n",
      "Time taken for 1 epoch: 33.587620973587036 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 5.3276 test Loss 5.3297 with MSE metric 7795.1431\n",
      "Time taken for 1 epoch: 34.580971002578735 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 5.2733 test Loss 5.3437 with MSE metric 6985.6255\n",
      "Time taken for 1 epoch: 34.527710914611816 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 5.2781 test Loss 5.3745 with MSE metric 7056.3394\n",
      "Time taken for 1 epoch: 34.49885106086731 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 5.2574 test Loss 5.3666 with MSE metric 6777.3438\n",
      "Time taken for 1 epoch: 34.278213024139404 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 5.3152 test Loss 5.3568 with MSE metric 7610.0752\n",
      "Time taken for 1 epoch: 34.51342010498047 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 5.2993 test Loss 5.3513 with MSE metric 7371.0410\n",
      "Time taken for 1 epoch: 34.55756092071533 secs\n",
      "\n",
      "Epoch 50 batch 0 train Loss 5.2792 test Loss 5.3423 with MSE metric 7059.8315\n",
      "Time taken for 1 epoch: 34.666927099227905 secs\n",
      "\n",
      "Epoch 51 batch 0 train Loss 5.3161 test Loss 5.3369 with MSE metric 7624.1982\n",
      "Time taken for 1 epoch: 34.32347273826599 secs\n",
      "\n",
      "Epoch 52 batch 0 train Loss 5.3198 test Loss 5.3776 with MSE metric 7678.8828\n",
      "Time taken for 1 epoch: 34.039977073669434 secs\n",
      "\n",
      "Epoch 53 batch 0 train Loss 5.3532 test Loss 5.3444 with MSE metric 8157.3364\n",
      "Time taken for 1 epoch: 34.33162498474121 secs\n",
      "\n",
      "Epoch 54 batch 0 train Loss 5.3087 test Loss 5.3679 with MSE metric 7512.1348\n",
      "Time taken for 1 epoch: 34.401124000549316 secs\n",
      "\n",
      "Epoch 55 batch 0 train Loss 5.3514 test Loss 5.4037 with MSE metric 8112.7451\n",
      "Time taken for 1 epoch: 34.62771987915039 secs\n",
      "\n",
      "Epoch 56 batch 0 train Loss 5.3265 test Loss 5.3913 with MSE metric 7774.5371\n",
      "Time taken for 1 epoch: 34.58033776283264 secs\n",
      "\n",
      "Epoch 57 batch 0 train Loss 5.3218 test Loss 5.4027 with MSE metric 7693.8916\n",
      "Time taken for 1 epoch: 34.12034511566162 secs\n",
      "\n",
      "Epoch 58 batch 0 train Loss 5.2994 test Loss 5.3650 with MSE metric 7373.8584\n",
      "Time taken for 1 epoch: 34.79313826560974 secs\n",
      "\n",
      "Epoch 59 batch 0 train Loss 5.3055 test Loss 5.3741 with MSE metric 7457.6299\n",
      "Time taken for 1 epoch: 34.43722105026245 secs\n",
      "\n",
      "Epoch 60 batch 0 train Loss 5.2634 test Loss 5.3670 with MSE metric 6836.3203\n",
      "Time taken for 1 epoch: 34.41808605194092 secs\n",
      "\n",
      "Epoch 61 batch 0 train Loss 5.2643 test Loss 5.3389 with MSE metric 6864.1221\n",
      "Time taken for 1 epoch: 34.42840313911438 secs\n",
      "\n",
      "Epoch 62 batch 0 train Loss 5.3279 test Loss 5.3510 with MSE metric 7737.5820\n",
      "Time taken for 1 epoch: 34.63317322731018 secs\n",
      "\n",
      "Epoch 63 batch 0 train Loss 5.3361 test Loss 5.3780 with MSE metric 7887.0693\n",
      "Time taken for 1 epoch: 34.991753816604614 secs\n",
      "\n",
      "Epoch 64 batch 0 train Loss 5.2897 test Loss 5.3420 with MSE metric 7231.0342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 33.79659104347229 secs\n",
      "\n",
      "Epoch 65 batch 0 train Loss 5.3253 test Loss 5.3724 with MSE metric 7761.4111\n",
      "Time taken for 1 epoch: 33.87038278579712 secs\n",
      "\n",
      "Epoch 66 batch 0 train Loss 5.2509 test Loss 5.3747 with MSE metric 6678.9521\n",
      "Time taken for 1 epoch: 34.280285120010376 secs\n",
      "\n",
      "Epoch 67 batch 0 train Loss 5.2947 test Loss 5.3625 with MSE metric 7303.0029\n",
      "Time taken for 1 epoch: 34.226808071136475 secs\n",
      "\n",
      "Epoch 68 batch 0 train Loss 5.3557 test Loss 5.3580 with MSE metric 8235.4824\n",
      "Time taken for 1 epoch: 34.44808387756348 secs\n",
      "\n",
      "Epoch 69 batch 0 train Loss 5.2454 test Loss 5.3364 with MSE metric 6590.9746\n",
      "Time taken for 1 epoch: 34.71144676208496 secs\n",
      "\n",
      "Epoch 70 batch 0 train Loss 5.2392 test Loss 5.3947 with MSE metric 6440.5928\n",
      "Time taken for 1 epoch: 34.59360694885254 secs\n",
      "\n",
      "Epoch 71 batch 0 train Loss 5.3005 test Loss 5.3160 with MSE metric 7389.2988\n",
      "Time taken for 1 epoch: 34.09790802001953 secs\n",
      "\n",
      "Epoch 72 batch 0 train Loss 5.3500 test Loss 5.3577 with MSE metric 8090.1045\n",
      "Time taken for 1 epoch: 33.57078671455383 secs\n",
      "\n",
      "Epoch 73 batch 0 train Loss 5.2951 test Loss 5.3636 with MSE metric 7288.7822\n",
      "Time taken for 1 epoch: 33.73908710479736 secs\n",
      "\n",
      "Epoch 74 batch 0 train Loss 5.3331 test Loss 5.3235 with MSE metric 7878.2529\n",
      "Time taken for 1 epoch: 33.14955186843872 secs\n",
      "\n",
      "Epoch 75 batch 0 train Loss 5.2942 test Loss 5.3894 with MSE metric 7297.1035\n",
      "Time taken for 1 epoch: 33.73428559303284 secs\n",
      "\n",
      "Epoch 76 batch 0 train Loss 5.2904 test Loss 5.3509 with MSE metric 7239.5889\n",
      "Time taken for 1 epoch: 32.93508195877075 secs\n",
      "\n",
      "Epoch 77 batch 0 train Loss 5.2986 test Loss 5.3361 with MSE metric 7356.6504\n",
      "Time taken for 1 epoch: 32.814266204833984 secs\n",
      "\n",
      "Epoch 78 batch 0 train Loss 5.2970 test Loss 5.3419 with MSE metric 7332.9326\n",
      "Time taken for 1 epoch: 33.5552499294281 secs\n",
      "\n",
      "Epoch 79 batch 0 train Loss 5.3183 test Loss 5.3562 with MSE metric 7634.3140\n",
      "Time taken for 1 epoch: 31.69277000427246 secs\n",
      "\n",
      "Epoch 80 batch 0 train Loss 5.2780 test Loss 5.3461 with MSE metric 7063.6016\n",
      "Time taken for 1 epoch: 30.757288217544556 secs\n",
      "\n",
      "Epoch 81 batch 0 train Loss 5.3556 test Loss 5.3907 with MSE metric 8215.8105\n",
      "Time taken for 1 epoch: 32.84911370277405 secs\n",
      "\n",
      "Epoch 82 batch 0 train Loss 5.2803 test Loss 5.3974 with MSE metric 7096.6885\n",
      "Time taken for 1 epoch: 33.41235303878784 secs\n",
      "\n",
      "Epoch 83 batch 0 train Loss 5.3517 test Loss 5.3558 with MSE metric 8133.8579\n",
      "Time taken for 1 epoch: 33.905855894088745 secs\n",
      "\n",
      "Epoch 84 batch 0 train Loss 5.3072 test Loss 5.3788 with MSE metric 7488.1758\n",
      "Time taken for 1 epoch: 33.87318825721741 secs\n",
      "\n",
      "Epoch 85 batch 0 train Loss 5.3009 test Loss 5.3548 with MSE metric 7385.1655\n",
      "Time taken for 1 epoch: 33.820616006851196 secs\n",
      "\n",
      "Epoch 86 batch 0 train Loss 5.2901 test Loss 5.3155 with MSE metric 7236.7041\n",
      "Time taken for 1 epoch: 32.69183301925659 secs\n",
      "\n",
      "Epoch 87 batch 0 train Loss 5.2946 test Loss 5.3226 with MSE metric 7297.2153\n",
      "Time taken for 1 epoch: 33.40260887145996 secs\n",
      "\n",
      "Epoch 88 batch 0 train Loss 5.2876 test Loss 5.3820 with MSE metric 7199.7256\n",
      "Time taken for 1 epoch: 33.947789907455444 secs\n",
      "\n",
      "Epoch 89 batch 0 train Loss 5.2543 test Loss 5.3488 with MSE metric 6723.5830\n",
      "Time taken for 1 epoch: 32.49965310096741 secs\n",
      "\n",
      "Epoch 90 batch 0 train Loss 5.3385 test Loss 5.3209 with MSE metric 7931.7725\n",
      "Time taken for 1 epoch: 33.22395300865173 secs\n",
      "\n",
      "Epoch 91 batch 0 train Loss 5.3438 test Loss 5.4183 with MSE metric 8015.2930\n",
      "Time taken for 1 epoch: 33.863720178604126 secs\n",
      "\n",
      "Epoch 92 batch 0 train Loss 5.3357 test Loss 5.3114 with MSE metric 7928.6611\n",
      "Time taken for 1 epoch: 31.85370111465454 secs\n",
      "\n",
      "Epoch 93 batch 0 train Loss 5.3064 test Loss 5.3458 with MSE metric 7474.3125\n",
      "Time taken for 1 epoch: 31.43705415725708 secs\n",
      "\n",
      "Epoch 94 batch 0 train Loss 5.3094 test Loss 5.3869 with MSE metric 7521.2778\n",
      "Time taken for 1 epoch: 31.776919841766357 secs\n",
      "\n",
      "Epoch 95 batch 0 train Loss 5.2751 test Loss 5.3111 with MSE metric 6976.1621\n",
      "Time taken for 1 epoch: 31.306790828704834 secs\n",
      "\n",
      "Epoch 96 batch 0 train Loss 5.2439 test Loss 5.3158 with MSE metric 6566.2090\n",
      "Time taken for 1 epoch: 31.444931030273438 secs\n",
      "\n",
      "Epoch 97 batch 0 train Loss 5.3324 test Loss 5.3512 with MSE metric 7851.4775\n",
      "Time taken for 1 epoch: 31.599756002426147 secs\n",
      "\n",
      "Epoch 98 batch 0 train Loss 5.3379 test Loss 5.3654 with MSE metric 7874.4043\n",
      "Time taken for 1 epoch: 31.44982099533081 secs\n",
      "\n",
      "Epoch 99 batch 0 train Loss 5.2705 test Loss 5.3382 with MSE metric 6941.3682\n",
      "Time taken for 1 epoch: 30.6421377658844 secs\n",
      "\n",
      "Epoch 100 batch 0 train Loss 5.3554 test Loss 5.3437 with MSE metric 8194.8496\n",
      "Time taken for 1 epoch: 30.509232997894287 secs\n",
      "\n",
      "Epoch 101 batch 0 train Loss 5.2848 test Loss 5.3553 with MSE metric 7156.2739\n",
      "Time taken for 1 epoch: 29.14996886253357 secs\n",
      "\n",
      "Epoch 102 batch 0 train Loss 5.3137 test Loss 5.3525 with MSE metric 7587.6250\n",
      "Time taken for 1 epoch: 30.411108255386353 secs\n",
      "\n",
      "Epoch 103 batch 0 train Loss 5.2837 test Loss 5.3849 with MSE metric 7138.4189\n",
      "Time taken for 1 epoch: 32.40484023094177 secs\n",
      "\n",
      "Epoch 104 batch 0 train Loss 5.3062 test Loss 5.3790 with MSE metric 7465.0811\n",
      "Time taken for 1 epoch: 33.27734684944153 secs\n",
      "\n",
      "Epoch 105 batch 0 train Loss 5.3097 test Loss 5.3616 with MSE metric 7523.0586\n",
      "Time taken for 1 epoch: 30.131932020187378 secs\n",
      "\n",
      "Epoch 106 batch 0 train Loss 5.3049 test Loss 5.3870 with MSE metric 7440.2764\n",
      "Time taken for 1 epoch: 32.29422688484192 secs\n",
      "\n",
      "Epoch 107 batch 0 train Loss 5.2446 test Loss 5.4115 with MSE metric 6545.0952\n",
      "Time taken for 1 epoch: 32.60301685333252 secs\n",
      "\n",
      "Epoch 108 batch 0 train Loss 5.3130 test Loss 5.3680 with MSE metric 7576.6997\n",
      "Time taken for 1 epoch: 32.24725890159607 secs\n",
      "\n",
      "Epoch 109 batch 0 train Loss 5.2777 test Loss 5.3692 with MSE metric 7054.6938\n",
      "Time taken for 1 epoch: 32.22423696517944 secs\n",
      "\n",
      "Epoch 110 batch 0 train Loss 5.2991 test Loss 5.3178 with MSE metric 7365.7441\n",
      "Time taken for 1 epoch: 32.40524744987488 secs\n",
      "\n",
      "Epoch 111 batch 0 train Loss 5.3419 test Loss 5.3037 with MSE metric 7989.8081\n",
      "Time taken for 1 epoch: 32.79196810722351 secs\n",
      "\n",
      "Epoch 112 batch 0 train Loss 5.3070 test Loss 5.4199 with MSE metric 7471.9160\n",
      "Time taken for 1 epoch: 32.47814083099365 secs\n",
      "\n",
      "Epoch 113 batch 0 train Loss 5.2873 test Loss 5.3686 with MSE metric 7191.4346\n",
      "Time taken for 1 epoch: 32.48205494880676 secs\n",
      "\n",
      "Epoch 114 batch 0 train Loss 5.2854 test Loss 5.3706 with MSE metric 7169.4678\n",
      "Time taken for 1 epoch: 32.39321804046631 secs\n",
      "\n",
      "Epoch 115 batch 0 train Loss 5.2976 test Loss 5.3913 with MSE metric 7332.0391\n",
      "Time taken for 1 epoch: 31.84147000312805 secs\n",
      "\n",
      "Epoch 116 batch 0 train Loss 5.3104 test Loss 5.3759 with MSE metric 7536.0117\n",
      "Time taken for 1 epoch: 31.991145133972168 secs\n",
      "\n",
      "Epoch 117 batch 0 train Loss 5.3353 test Loss 5.3584 with MSE metric 7902.1626\n",
      "Time taken for 1 epoch: 32.63607978820801 secs\n",
      "\n",
      "Epoch 118 batch 0 train Loss 5.2888 test Loss 5.3608 with MSE metric 7215.0581\n",
      "Time taken for 1 epoch: 32.27562189102173 secs\n",
      "\n",
      "Epoch 119 batch 0 train Loss 5.2803 test Loss 5.3626 with MSE metric 7094.1548\n",
      "Time taken for 1 epoch: 32.58641314506531 secs\n",
      "\n",
      "Epoch 120 batch 0 train Loss 5.2952 test Loss 5.4050 with MSE metric 7312.0757\n",
      "Time taken for 1 epoch: 32.62836408615112 secs\n",
      "\n",
      "Epoch 121 batch 0 train Loss 5.2516 test Loss 5.4123 with MSE metric 6610.8311\n",
      "Time taken for 1 epoch: 32.77025604248047 secs\n",
      "\n",
      "Epoch 122 batch 0 train Loss 5.3000 test Loss 5.3711 with MSE metric 7382.4180\n",
      "Time taken for 1 epoch: 32.75849175453186 secs\n",
      "\n",
      "Epoch 123 batch 0 train Loss 5.3173 test Loss 5.3500 with MSE metric 7640.5781\n",
      "Time taken for 1 epoch: 33.24022912979126 secs\n",
      "\n",
      "Epoch 124 batch 0 train Loss 5.2690 test Loss 5.3287 with MSE metric 6930.1357\n",
      "Time taken for 1 epoch: 33.52313494682312 secs\n",
      "\n",
      "Epoch 125 batch 0 train Loss 5.2856 test Loss 5.3109 with MSE metric 7172.6777\n",
      "Time taken for 1 epoch: 33.494723081588745 secs\n",
      "\n",
      "Epoch 126 batch 0 train Loss 5.3119 test Loss 5.3369 with MSE metric 7557.5576\n",
      "Time taken for 1 epoch: 33.48854994773865 secs\n",
      "\n",
      "Epoch 127 batch 0 train Loss 5.2831 test Loss 5.4204 with MSE metric 7116.7754\n",
      "Time taken for 1 epoch: 33.42810416221619 secs\n",
      "\n",
      "Epoch 128 batch 0 train Loss 5.3108 test Loss 5.3601 with MSE metric 7540.1108\n",
      "Time taken for 1 epoch: 33.59281611442566 secs\n",
      "\n",
      "Epoch 129 batch 0 train Loss 5.3373 test Loss 5.3597 with MSE metric 7934.1553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 33.73696303367615 secs\n",
      "\n",
      "Epoch 130 batch 0 train Loss 5.3141 test Loss 5.3837 with MSE metric 7569.5996\n",
      "Time taken for 1 epoch: 33.592840909957886 secs\n",
      "\n",
      "Epoch 131 batch 0 train Loss 5.3119 test Loss 5.3808 with MSE metric 7560.3696\n",
      "Time taken for 1 epoch: 33.674437046051025 secs\n",
      "\n",
      "Epoch 132 batch 0 train Loss 5.2930 test Loss 5.3651 with MSE metric 7276.0117\n",
      "Time taken for 1 epoch: 33.67853403091431 secs\n",
      "\n",
      "Epoch 133 batch 0 train Loss 5.3433 test Loss 5.3540 with MSE metric 8027.8589\n",
      "Time taken for 1 epoch: 33.61728262901306 secs\n",
      "\n",
      "Epoch 134 batch 0 train Loss 5.3048 test Loss 5.3455 with MSE metric 7453.8911\n",
      "Time taken for 1 epoch: 33.59117293357849 secs\n",
      "\n",
      "Epoch 135 batch 0 train Loss 5.2646 test Loss 5.3740 with MSE metric 6857.3970\n",
      "Time taken for 1 epoch: 33.04614281654358 secs\n",
      "\n",
      "Epoch 136 batch 0 train Loss 5.2385 test Loss 5.3475 with MSE metric 6471.9102\n",
      "Time taken for 1 epoch: 33.19486904144287 secs\n",
      "\n",
      "Epoch 137 batch 0 train Loss 5.2940 test Loss 5.3690 with MSE metric 7293.7866\n",
      "Time taken for 1 epoch: 32.90169382095337 secs\n",
      "\n",
      "Epoch 138 batch 0 train Loss 5.3280 test Loss 5.3454 with MSE metric 7794.9897\n",
      "Time taken for 1 epoch: 33.21355581283569 secs\n",
      "\n",
      "Epoch 139 batch 0 train Loss 5.3228 test Loss 5.3589 with MSE metric 7708.2437\n",
      "Time taken for 1 epoch: 33.01008987426758 secs\n",
      "\n",
      "Epoch 140 batch 0 train Loss 5.2921 test Loss 5.3387 with MSE metric 7265.9023\n",
      "Time taken for 1 epoch: 33.04446792602539 secs\n",
      "\n",
      "Epoch 141 batch 0 train Loss 5.2203 test Loss 5.3768 with MSE metric 6247.4409\n",
      "Time taken for 1 epoch: 33.00891995429993 secs\n",
      "\n",
      "Epoch 142 batch 0 train Loss 5.2714 test Loss 5.3515 with MSE metric 6952.6440\n",
      "Time taken for 1 epoch: 33.15283918380737 secs\n",
      "\n",
      "Epoch 143 batch 0 train Loss 5.2831 test Loss 5.3645 with MSE metric 7135.8340\n",
      "Time taken for 1 epoch: 33.01889681816101 secs\n",
      "\n",
      "Epoch 144 batch 0 train Loss 5.3176 test Loss 5.3779 with MSE metric 7643.0977\n",
      "Time taken for 1 epoch: 33.02539372444153 secs\n",
      "\n",
      "Epoch 145 batch 0 train Loss 5.2958 test Loss 5.3494 with MSE metric 7317.1807\n",
      "Time taken for 1 epoch: 32.96804881095886 secs\n",
      "\n",
      "Epoch 146 batch 0 train Loss 5.3246 test Loss 5.3703 with MSE metric 7730.5879\n",
      "Time taken for 1 epoch: 32.77817392349243 secs\n",
      "\n",
      "Epoch 147 batch 0 train Loss 5.3308 test Loss 5.3790 with MSE metric 7825.5288\n",
      "Time taken for 1 epoch: 32.94932508468628 secs\n",
      "\n",
      "Epoch 148 batch 0 train Loss 5.3305 test Loss 5.3428 with MSE metric 7834.2637\n",
      "Time taken for 1 epoch: 33.04517722129822 secs\n",
      "\n",
      "Epoch 149 batch 0 train Loss 5.3153 test Loss 5.3301 with MSE metric 7577.0107\n",
      "Time taken for 1 epoch: 33.064231872558594 secs\n",
      "\n",
      "Epoch 150 batch 0 train Loss 5.3100 test Loss 5.3938 with MSE metric 7531.7109\n",
      "Time taken for 1 epoch: 32.900386095047 secs\n",
      "\n",
      "Epoch 151 batch 0 train Loss 5.2750 test Loss 5.3398 with MSE metric 7012.8950\n",
      "Time taken for 1 epoch: 32.908886194229126 secs\n",
      "\n",
      "Epoch 152 batch 0 train Loss 5.2572 test Loss 5.3146 with MSE metric 6750.2764\n",
      "Time taken for 1 epoch: 33.00872874259949 secs\n",
      "\n",
      "Epoch 153 batch 0 train Loss 5.2859 test Loss 5.4222 with MSE metric 7159.1826\n",
      "Time taken for 1 epoch: 32.94073796272278 secs\n",
      "\n",
      "Epoch 154 batch 0 train Loss 5.3185 test Loss 5.3651 with MSE metric 7655.9419\n",
      "Time taken for 1 epoch: 32.98644185066223 secs\n",
      "\n",
      "Epoch 155 batch 0 train Loss 5.3154 test Loss 5.3607 with MSE metric 7604.1001\n",
      "Time taken for 1 epoch: 33.09265422821045 secs\n",
      "\n",
      "Epoch 156 batch 0 train Loss 5.2927 test Loss 5.3481 with MSE metric 7275.7466\n",
      "Time taken for 1 epoch: 32.91311287879944 secs\n",
      "\n",
      "Epoch 157 batch 0 train Loss 5.2948 test Loss 5.3945 with MSE metric 7297.0166\n",
      "Time taken for 1 epoch: 33.342885971069336 secs\n",
      "\n",
      "Epoch 158 batch 0 train Loss 5.3122 test Loss 5.3287 with MSE metric 7563.5811\n",
      "Time taken for 1 epoch: 33.01488733291626 secs\n",
      "\n",
      "Epoch 159 batch 0 train Loss 5.3492 test Loss 5.3503 with MSE metric 8120.3965\n",
      "Time taken for 1 epoch: 33.12264394760132 secs\n",
      "\n",
      "Epoch 160 batch 0 train Loss 5.3276 test Loss 5.3564 with MSE metric 7787.6992\n",
      "Time taken for 1 epoch: 32.93236684799194 secs\n",
      "\n",
      "Epoch 161 batch 0 train Loss 5.2713 test Loss 5.3750 with MSE metric 6950.2578\n",
      "Time taken for 1 epoch: 33.00586485862732 secs\n",
      "\n",
      "Epoch 162 batch 0 train Loss 5.3203 test Loss 5.3653 with MSE metric 7682.0894\n",
      "Time taken for 1 epoch: 33.16386103630066 secs\n",
      "\n",
      "Epoch 163 batch 0 train Loss 5.2784 test Loss 5.3478 with MSE metric 7067.4990\n",
      "Time taken for 1 epoch: 32.88025116920471 secs\n",
      "\n",
      "Epoch 164 batch 0 train Loss 5.3005 test Loss 5.3105 with MSE metric 7388.7402\n",
      "Time taken for 1 epoch: 33.02164626121521 secs\n",
      "\n",
      "Epoch 165 batch 0 train Loss 5.3441 test Loss 5.3633 with MSE metric 8055.3633\n",
      "Time taken for 1 epoch: 32.8851900100708 secs\n",
      "\n",
      "Epoch 166 batch 0 train Loss 5.2930 test Loss 5.3894 with MSE metric 7279.2539\n",
      "Time taken for 1 epoch: 32.89949893951416 secs\n",
      "\n",
      "Epoch 167 batch 0 train Loss 5.3477 test Loss 5.3417 with MSE metric 8099.9814\n",
      "Time taken for 1 epoch: 33.06881594657898 secs\n",
      "\n",
      "Epoch 168 batch 0 train Loss 5.2976 test Loss 5.3425 with MSE metric 7346.8564\n",
      "Time taken for 1 epoch: 33.19694995880127 secs\n",
      "\n",
      "Epoch 169 batch 0 train Loss 5.2842 test Loss 5.3222 with MSE metric 7150.3174\n",
      "Time taken for 1 epoch: 32.85128211975098 secs\n",
      "\n",
      "Epoch 170 batch 0 train Loss 5.2943 test Loss 5.3628 with MSE metric 7294.9424\n",
      "Time taken for 1 epoch: 33.04425406455994 secs\n",
      "\n",
      "Epoch 171 batch 0 train Loss 5.3190 test Loss 5.3859 with MSE metric 7667.3691\n",
      "Time taken for 1 epoch: 32.94288992881775 secs\n",
      "\n",
      "Epoch 172 batch 0 train Loss 5.2960 test Loss 5.3589 with MSE metric 7321.6792\n",
      "Time taken for 1 epoch: 33.01467704772949 secs\n",
      "\n",
      "Epoch 173 batch 0 train Loss 5.2956 test Loss 5.3645 with MSE metric 7316.3770\n",
      "Time taken for 1 epoch: 32.967154026031494 secs\n",
      "\n",
      "Epoch 174 batch 0 train Loss 5.3386 test Loss 5.3154 with MSE metric 7966.5098\n",
      "Time taken for 1 epoch: 32.907570123672485 secs\n",
      "\n",
      "Epoch 175 batch 0 train Loss 5.2651 test Loss 5.3575 with MSE metric 6847.2725\n",
      "Time taken for 1 epoch: 33.05440616607666 secs\n",
      "\n",
      "Epoch 176 batch 0 train Loss 5.3247 test Loss 5.3054 with MSE metric 7729.3730\n",
      "Time taken for 1 epoch: 32.941967248916626 secs\n",
      "\n",
      "Epoch 177 batch 0 train Loss 5.3267 test Loss 5.3560 with MSE metric 7770.8105\n",
      "Time taken for 1 epoch: 32.93934416770935 secs\n",
      "\n",
      "Epoch 178 batch 0 train Loss 5.3300 test Loss 5.2795 with MSE metric 7832.1782\n",
      "Time taken for 1 epoch: 33.0335590839386 secs\n",
      "\n",
      "Epoch 179 batch 0 train Loss 5.2714 test Loss 5.3186 with MSE metric 6965.4648\n",
      "Time taken for 1 epoch: 33.03525710105896 secs\n",
      "\n",
      "Epoch 180 batch 0 train Loss 5.3108 test Loss 5.3652 with MSE metric 7542.7480\n",
      "Time taken for 1 epoch: 32.898751974105835 secs\n",
      "\n",
      "Epoch 181 batch 0 train Loss 5.2538 test Loss 5.3093 with MSE metric 6702.9448\n",
      "Time taken for 1 epoch: 33.00077295303345 secs\n",
      "\n",
      "Epoch 182 batch 0 train Loss 5.2941 test Loss 5.3509 with MSE metric 7283.3970\n",
      "Time taken for 1 epoch: 32.93152618408203 secs\n",
      "\n",
      "Epoch 183 batch 0 train Loss 5.3294 test Loss 5.3524 with MSE metric 7805.9170\n",
      "Time taken for 1 epoch: 32.9579291343689 secs\n",
      "\n",
      "Epoch 184 batch 0 train Loss 5.3229 test Loss 5.3721 with MSE metric 7710.4717\n",
      "Time taken for 1 epoch: 33.009032249450684 secs\n",
      "\n",
      "Epoch 185 batch 0 train Loss 5.3168 test Loss 5.3172 with MSE metric 7631.2817\n",
      "Time taken for 1 epoch: 32.99183201789856 secs\n",
      "\n",
      "Epoch 186 batch 0 train Loss 5.2753 test Loss 5.3400 with MSE metric 7016.8301\n",
      "Time taken for 1 epoch: 33.23789167404175 secs\n",
      "\n",
      "Epoch 187 batch 0 train Loss 5.3421 test Loss 5.3729 with MSE metric 8000.4746\n",
      "Time taken for 1 epoch: 32.88105607032776 secs\n",
      "\n",
      "Epoch 188 batch 0 train Loss 5.3565 test Loss 5.3392 with MSE metric 8252.6543\n",
      "Time taken for 1 epoch: 33.17891812324524 secs\n",
      "\n",
      "Epoch 189 batch 0 train Loss 5.3093 test Loss 5.3502 with MSE metric 7518.2861\n",
      "Time taken for 1 epoch: 32.91629910469055 secs\n",
      "\n",
      "Epoch 190 batch 0 train Loss 5.2795 test Loss 5.3356 with MSE metric 7083.1299\n",
      "Time taken for 1 epoch: 32.851319789886475 secs\n",
      "\n",
      "Epoch 191 batch 0 train Loss 5.2673 test Loss 5.3763 with MSE metric 6899.6436\n",
      "Time taken for 1 epoch: 32.87841582298279 secs\n",
      "\n",
      "Epoch 192 batch 0 train Loss 5.3694 test Loss 5.3868 with MSE metric 8385.1816\n",
      "Time taken for 1 epoch: 32.89775633811951 secs\n",
      "\n",
      "Epoch 193 batch 0 train Loss 5.3260 test Loss 5.3400 with MSE metric 7760.6362\n",
      "Time taken for 1 epoch: 33.02830266952515 secs\n",
      "\n",
      "Epoch 194 batch 0 train Loss 5.2607 test Loss 5.3162 with MSE metric 6757.4473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 32.93606114387512 secs\n",
      "\n",
      "Epoch 195 batch 0 train Loss 5.3213 test Loss 5.3381 with MSE metric 7701.3604\n",
      "Time taken for 1 epoch: 32.99523591995239 secs\n",
      "\n",
      "Epoch 196 batch 0 train Loss 5.3201 test Loss 5.4093 with MSE metric 7682.8110\n",
      "Time taken for 1 epoch: 33.00985312461853 secs\n",
      "\n",
      "Epoch 197 batch 0 train Loss 5.3280 test Loss 5.3448 with MSE metric 7801.4521\n",
      "Time taken for 1 epoch: 33.20161414146423 secs\n",
      "\n",
      "Epoch 198 batch 0 train Loss 5.3055 test Loss 5.3698 with MSE metric 7451.8906\n",
      "Time taken for 1 epoch: 33.01970100402832 secs\n",
      "\n",
      "Epoch 199 batch 0 train Loss 5.2964 test Loss 5.3434 with MSE metric 7328.7271\n",
      "Time taken for 1 epoch: 33.19156217575073 secs\n",
      "\n",
      "Epoch 200 batch 0 train Loss 5.3085 test Loss 5.3917 with MSE metric 7485.5469\n",
      "Time taken for 1 epoch: 32.95593595504761 secs\n",
      "\n",
      "Epoch 201 batch 0 train Loss 5.2906 test Loss 5.3503 with MSE metric 7243.3066\n",
      "Time taken for 1 epoch: 33.09499788284302 secs\n",
      "\n",
      "Epoch 202 batch 0 train Loss 5.3319 test Loss 5.3685 with MSE metric 7823.6309\n",
      "Time taken for 1 epoch: 32.91172003746033 secs\n",
      "\n",
      "Epoch 203 batch 0 train Loss 5.3396 test Loss 5.3615 with MSE metric 7984.2632\n",
      "Time taken for 1 epoch: 33.212294816970825 secs\n",
      "\n",
      "Epoch 204 batch 0 train Loss 5.2668 test Loss 5.3591 with MSE metric 6889.0293\n",
      "Time taken for 1 epoch: 33.09466791152954 secs\n",
      "\n",
      "Epoch 205 batch 0 train Loss 5.3233 test Loss 5.3416 with MSE metric 7725.6299\n",
      "Time taken for 1 epoch: 33.019387006759644 secs\n",
      "\n",
      "Epoch 206 batch 0 train Loss 5.2833 test Loss 5.3451 with MSE metric 7136.4116\n",
      "Time taken for 1 epoch: 32.99232077598572 secs\n",
      "\n",
      "Epoch 207 batch 0 train Loss 5.3124 test Loss 5.3393 with MSE metric 7557.1289\n",
      "Time taken for 1 epoch: 33.011656284332275 secs\n",
      "\n",
      "Epoch 208 batch 0 train Loss 5.3373 test Loss 5.3067 with MSE metric 7935.6846\n",
      "Time taken for 1 epoch: 32.86958599090576 secs\n",
      "\n",
      "Epoch 209 batch 0 train Loss 5.2648 test Loss 5.3531 with MSE metric 6876.7935\n",
      "Time taken for 1 epoch: 33.068339824676514 secs\n",
      "\n",
      "Epoch 210 batch 0 train Loss 5.3483 test Loss 5.3182 with MSE metric 8079.6831\n",
      "Time taken for 1 epoch: 33.11407995223999 secs\n",
      "\n",
      "Epoch 211 batch 0 train Loss 5.3092 test Loss 5.3507 with MSE metric 7514.1201\n",
      "Time taken for 1 epoch: 33.05582904815674 secs\n",
      "\n",
      "Epoch 212 batch 0 train Loss 5.2678 test Loss 5.3764 with MSE metric 6898.1528\n",
      "Time taken for 1 epoch: 33.08127403259277 secs\n",
      "\n",
      "Epoch 213 batch 0 train Loss 5.3374 test Loss 5.3889 with MSE metric 7943.3354\n",
      "Time taken for 1 epoch: 32.92009902000427 secs\n",
      "\n",
      "Epoch 214 batch 0 train Loss 5.3697 test Loss 5.3747 with MSE metric 8395.7441\n",
      "Time taken for 1 epoch: 33.00184607505798 secs\n",
      "\n",
      "Epoch 215 batch 0 train Loss 5.3338 test Loss 5.3510 with MSE metric 7886.4604\n",
      "Time taken for 1 epoch: 33.131110191345215 secs\n",
      "\n",
      "Epoch 216 batch 0 train Loss 5.3539 test Loss 5.3755 with MSE metric 8149.5840\n",
      "Time taken for 1 epoch: 33.13178610801697 secs\n",
      "\n",
      "Epoch 217 batch 0 train Loss 5.2992 test Loss 5.3615 with MSE metric 7370.0664\n",
      "Time taken for 1 epoch: 33.20195293426514 secs\n",
      "\n",
      "Epoch 218 batch 0 train Loss 5.3146 test Loss 5.4150 with MSE metric 7556.2778\n",
      "Time taken for 1 epoch: 32.74525785446167 secs\n",
      "\n",
      "Epoch 219 batch 0 train Loss 5.2948 test Loss 5.3844 with MSE metric 7304.0059\n",
      "Time taken for 1 epoch: 33.116426944732666 secs\n",
      "\n",
      "Epoch 220 batch 0 train Loss 5.3295 test Loss 5.3717 with MSE metric 7809.5874\n",
      "Time taken for 1 epoch: 33.30918884277344 secs\n",
      "\n",
      "Epoch 221 batch 0 train Loss 5.3040 test Loss 5.3484 with MSE metric 7438.9629\n",
      "Time taken for 1 epoch: 32.9549720287323 secs\n",
      "\n",
      "Epoch 222 batch 0 train Loss 5.2980 test Loss 5.3897 with MSE metric 7352.1963\n",
      "Time taken for 1 epoch: 32.84807896614075 secs\n",
      "\n",
      "Epoch 223 batch 0 train Loss 5.2989 test Loss 5.3749 with MSE metric 7362.9033\n",
      "Time taken for 1 epoch: 32.78575778007507 secs\n",
      "\n",
      "Epoch 224 batch 0 train Loss 5.3163 test Loss 5.3440 with MSE metric 7626.9873\n",
      "Time taken for 1 epoch: 33.19725203514099 secs\n",
      "\n",
      "Epoch 225 batch 0 train Loss 5.2696 test Loss 5.3851 with MSE metric 6904.9722\n",
      "Time taken for 1 epoch: 32.86861324310303 secs\n",
      "\n",
      "Epoch 226 batch 0 train Loss 5.3131 test Loss 5.3942 with MSE metric 7577.5742\n",
      "Time taken for 1 epoch: 32.96408009529114 secs\n",
      "\n",
      "Epoch 227 batch 0 train Loss 5.3196 test Loss 5.4118 with MSE metric 7654.4453\n",
      "Time taken for 1 epoch: 31.545826196670532 secs\n",
      "\n",
      "Epoch 228 batch 0 train Loss 5.3305 test Loss 5.3920 with MSE metric 7839.2178\n",
      "Time taken for 1 epoch: 32.59186887741089 secs\n",
      "\n",
      "Epoch 229 batch 0 train Loss 5.2979 test Loss 5.2943 with MSE metric 7346.9961\n",
      "Time taken for 1 epoch: 32.96592116355896 secs\n",
      "\n",
      "Epoch 230 batch 0 train Loss 5.2840 test Loss 5.3539 with MSE metric 7138.1865\n",
      "Time taken for 1 epoch: 32.9631507396698 secs\n",
      "\n",
      "Epoch 231 batch 0 train Loss 5.3464 test Loss 5.3344 with MSE metric 8021.2012\n",
      "Time taken for 1 epoch: 33.11530613899231 secs\n",
      "\n",
      "Epoch 232 batch 0 train Loss 5.2916 test Loss 5.3595 with MSE metric 7258.5498\n",
      "Time taken for 1 epoch: 32.78036713600159 secs\n",
      "\n",
      "Epoch 233 batch 0 train Loss 5.2537 test Loss 5.3357 with MSE metric 6698.2451\n",
      "Time taken for 1 epoch: 32.827357053756714 secs\n",
      "\n",
      "Epoch 234 batch 0 train Loss 5.3293 test Loss 5.3788 with MSE metric 7815.9795\n",
      "Time taken for 1 epoch: 33.12805986404419 secs\n",
      "\n",
      "Epoch 235 batch 0 train Loss 5.2822 test Loss 5.3879 with MSE metric 7124.6562\n",
      "Time taken for 1 epoch: 33.087990045547485 secs\n",
      "\n",
      "Epoch 236 batch 0 train Loss 5.3108 test Loss 5.3582 with MSE metric 7542.2422\n",
      "Time taken for 1 epoch: 33.25943994522095 secs\n",
      "\n",
      "Epoch 237 batch 0 train Loss 5.2417 test Loss 5.2982 with MSE metric 6552.4482\n",
      "Time taken for 1 epoch: 33.287623167037964 secs\n",
      "\n",
      "Epoch 238 batch 0 train Loss 5.2737 test Loss 5.3681 with MSE metric 6999.0771\n",
      "Time taken for 1 epoch: 33.8880569934845 secs\n",
      "\n",
      "Epoch 239 batch 0 train Loss 5.3205 test Loss 5.3526 with MSE metric 7666.2026\n",
      "Time taken for 1 epoch: 33.72898268699646 secs\n",
      "\n",
      "Epoch 240 batch 0 train Loss 5.3067 test Loss 5.3198 with MSE metric 7470.2563\n",
      "Time taken for 1 epoch: 33.156440019607544 secs\n",
      "\n",
      "Epoch 241 batch 0 train Loss 5.3777 test Loss 5.3686 with MSE metric 8533.5332\n",
      "Time taken for 1 epoch: 33.02847480773926 secs\n",
      "\n",
      "Epoch 242 batch 0 train Loss 5.2926 test Loss 5.3314 with MSE metric 7273.6035\n",
      "Time taken for 1 epoch: 32.75998497009277 secs\n",
      "\n",
      "Epoch 243 batch 0 train Loss 5.3038 test Loss 5.3359 with MSE metric 7438.9946\n",
      "Time taken for 1 epoch: 33.08872699737549 secs\n",
      "\n",
      "Epoch 244 batch 0 train Loss 5.3031 test Loss 5.3661 with MSE metric 7426.1494\n",
      "Time taken for 1 epoch: 32.844155073165894 secs\n",
      "\n",
      "Epoch 245 batch 0 train Loss 5.3327 test Loss 5.4005 with MSE metric 7810.7393\n",
      "Time taken for 1 epoch: 32.85574221611023 secs\n",
      "\n",
      "Epoch 246 batch 0 train Loss 5.2915 test Loss 5.3888 with MSE metric 7242.0171\n",
      "Time taken for 1 epoch: 33.260658979415894 secs\n",
      "\n",
      "Epoch 247 batch 0 train Loss 5.2864 test Loss 5.3216 with MSE metric 7183.8687\n",
      "Time taken for 1 epoch: 33.46264410018921 secs\n",
      "\n",
      "Epoch 248 batch 0 train Loss 5.3300 test Loss 5.3943 with MSE metric 7829.2114\n",
      "Time taken for 1 epoch: 33.255805015563965 secs\n",
      "\n",
      "Epoch 249 batch 0 train Loss 5.3151 test Loss 5.4086 with MSE metric 7606.2871\n",
      "Time taken for 1 epoch: 33.06435298919678 secs\n",
      "\n",
      "Epoch 250 batch 0 train Loss 5.2520 test Loss 5.3716 with MSE metric 6655.0664\n",
      "Time taken for 1 epoch: 33.23505711555481 secs\n",
      "\n",
      "Epoch 251 batch 0 train Loss 5.2911 test Loss 5.3748 with MSE metric 7252.6484\n",
      "Time taken for 1 epoch: 33.28128099441528 secs\n",
      "\n",
      "Epoch 252 batch 0 train Loss 5.2868 test Loss 5.3565 with MSE metric 7187.6416\n",
      "Time taken for 1 epoch: 33.26344609260559 secs\n",
      "\n",
      "Epoch 253 batch 0 train Loss 5.3058 test Loss 5.3554 with MSE metric 7467.6147\n",
      "Time taken for 1 epoch: 33.18148112297058 secs\n",
      "\n",
      "Epoch 254 batch 0 train Loss 5.3210 test Loss 5.3499 with MSE metric 7673.3550\n",
      "Time taken for 1 epoch: 33.295177936553955 secs\n",
      "\n",
      "Epoch 255 batch 0 train Loss 5.2917 test Loss 5.3633 with MSE metric 7257.2573\n",
      "Time taken for 1 epoch: 33.3488929271698 secs\n",
      "\n",
      "Epoch 256 batch 0 train Loss 5.3030 test Loss 5.3014 with MSE metric 7426.6299\n",
      "Time taken for 1 epoch: 33.066752910614014 secs\n",
      "\n",
      "Epoch 257 batch 0 train Loss 5.2814 test Loss 5.3692 with MSE metric 7095.9980\n",
      "Time taken for 1 epoch: 33.105205059051514 secs\n",
      "\n",
      "Epoch 258 batch 0 train Loss 5.2700 test Loss 5.3685 with MSE metric 6938.2944\n",
      "Time taken for 1 epoch: 33.27953505516052 secs\n",
      "\n",
      "Epoch 259 batch 0 train Loss 5.2619 test Loss 5.3928 with MSE metric 6809.9912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 33.379859924316406 secs\n",
      "\n",
      "Epoch 260 batch 0 train Loss 5.3226 test Loss 5.3249 with MSE metric 7722.3721\n",
      "Time taken for 1 epoch: 33.25590991973877 secs\n",
      "\n",
      "Epoch 261 batch 0 train Loss 5.3034 test Loss 5.3670 with MSE metric 7414.1768\n",
      "Time taken for 1 epoch: 33.301154136657715 secs\n",
      "\n",
      "Epoch 262 batch 0 train Loss 5.2910 test Loss 5.3193 with MSE metric 7246.1348\n",
      "Time taken for 1 epoch: 33.03551697731018 secs\n",
      "\n",
      "Epoch 263 batch 0 train Loss 5.3414 test Loss 5.3709 with MSE metric 7983.7114\n",
      "Time taken for 1 epoch: 33.27055907249451 secs\n",
      "\n",
      "Epoch 264 batch 0 train Loss 5.3201 test Loss 5.3102 with MSE metric 7657.6895\n",
      "Time taken for 1 epoch: 32.89598894119263 secs\n",
      "\n",
      "Epoch 265 batch 0 train Loss 5.2917 test Loss 5.3371 with MSE metric 7260.5352\n",
      "Time taken for 1 epoch: 32.96501803398132 secs\n",
      "\n",
      "Epoch 266 batch 0 train Loss 5.3118 test Loss 5.3651 with MSE metric 7553.8955\n",
      "Time taken for 1 epoch: 33.28825068473816 secs\n",
      "\n",
      "Epoch 267 batch 0 train Loss 5.3050 test Loss 5.3955 with MSE metric 7456.3145\n",
      "Time taken for 1 epoch: 33.10107898712158 secs\n",
      "\n",
      "Epoch 268 batch 0 train Loss 5.3139 test Loss 5.3918 with MSE metric 7587.3223\n",
      "Time taken for 1 epoch: 33.0876579284668 secs\n",
      "\n",
      "Epoch 269 batch 0 train Loss 5.2811 test Loss 5.3164 with MSE metric 7102.1631\n",
      "Time taken for 1 epoch: 33.1236469745636 secs\n",
      "\n",
      "Epoch 270 batch 0 train Loss 5.3225 test Loss 5.4126 with MSE metric 7714.5420\n",
      "Time taken for 1 epoch: 33.285425901412964 secs\n",
      "\n",
      "Epoch 271 batch 0 train Loss 5.3059 test Loss 5.3676 with MSE metric 7468.6201\n",
      "Time taken for 1 epoch: 32.925554037094116 secs\n",
      "\n",
      "Epoch 272 batch 0 train Loss 5.2617 test Loss 5.3616 with MSE metric 6816.2954\n",
      "Time taken for 1 epoch: 33.259989976882935 secs\n",
      "\n",
      "Epoch 273 batch 0 train Loss 5.3002 test Loss 5.3750 with MSE metric 7380.5488\n",
      "Time taken for 1 epoch: 32.78796887397766 secs\n",
      "\n",
      "Epoch 274 batch 0 train Loss 5.2846 test Loss 5.3406 with MSE metric 7133.4785\n",
      "Time taken for 1 epoch: 33.218552112579346 secs\n",
      "\n",
      "Epoch 275 batch 0 train Loss 5.3284 test Loss 5.3521 with MSE metric 7799.2119\n",
      "Time taken for 1 epoch: 33.099562883377075 secs\n",
      "\n",
      "Epoch 276 batch 0 train Loss 5.3207 test Loss 5.3292 with MSE metric 7693.4131\n",
      "Time taken for 1 epoch: 33.17006325721741 secs\n",
      "\n",
      "Epoch 277 batch 0 train Loss 5.2860 test Loss 5.3457 with MSE metric 7178.6191\n",
      "Time taken for 1 epoch: 33.28485417366028 secs\n",
      "\n",
      "Epoch 278 batch 0 train Loss 5.3362 test Loss 5.3134 with MSE metric 7912.6206\n",
      "Time taken for 1 epoch: 33.19333600997925 secs\n",
      "\n",
      "Epoch 279 batch 0 train Loss 5.3330 test Loss 5.4183 with MSE metric 7853.6748\n",
      "Time taken for 1 epoch: 33.22766423225403 secs\n",
      "\n",
      "Epoch 280 batch 0 train Loss 5.2874 test Loss 5.3093 with MSE metric 7197.7334\n",
      "Time taken for 1 epoch: 33.46156311035156 secs\n",
      "\n",
      "Epoch 281 batch 0 train Loss 5.3073 test Loss 5.4106 with MSE metric 7490.2510\n",
      "Time taken for 1 epoch: 33.17736196517944 secs\n",
      "\n",
      "Epoch 282 batch 0 train Loss 5.3008 test Loss 5.3821 with MSE metric 7394.6836\n",
      "Time taken for 1 epoch: 33.10609793663025 secs\n",
      "\n",
      "Epoch 283 batch 0 train Loss 5.2952 test Loss 5.3680 with MSE metric 7311.3716\n",
      "Time taken for 1 epoch: 33.14468717575073 secs\n",
      "\n",
      "Epoch 284 batch 0 train Loss 5.2640 test Loss 5.4361 with MSE metric 6868.5347\n",
      "Time taken for 1 epoch: 33.666751861572266 secs\n",
      "\n",
      "Epoch 285 batch 0 train Loss 5.3032 test Loss 5.4152 with MSE metric 7429.9365\n",
      "Time taken for 1 epoch: 33.37641406059265 secs\n",
      "\n",
      "Epoch 286 batch 0 train Loss 5.3042 test Loss 5.3437 with MSE metric 7444.1763\n",
      "Time taken for 1 epoch: 33.37948417663574 secs\n",
      "\n",
      "Epoch 287 batch 0 train Loss 5.3107 test Loss 5.3422 with MSE metric 7538.7583\n",
      "Time taken for 1 epoch: 33.19340682029724 secs\n",
      "\n",
      "Epoch 288 batch 0 train Loss 5.3167 test Loss 5.3849 with MSE metric 7632.7227\n",
      "Time taken for 1 epoch: 33.61856007575989 secs\n",
      "\n",
      "Epoch 289 batch 0 train Loss 5.2931 test Loss 5.3445 with MSE metric 7280.6646\n",
      "Time taken for 1 epoch: 33.628785133361816 secs\n",
      "\n",
      "Epoch 290 batch 0 train Loss 5.2817 test Loss 5.3627 with MSE metric 7092.8706\n",
      "Time taken for 1 epoch: 33.22358298301697 secs\n",
      "\n",
      "Epoch 291 batch 0 train Loss 5.2747 test Loss 5.3634 with MSE metric 7016.1455\n",
      "Time taken for 1 epoch: 33.47356915473938 secs\n",
      "\n",
      "Epoch 292 batch 0 train Loss 5.3402 test Loss 5.4070 with MSE metric 7961.1670\n",
      "Time taken for 1 epoch: 33.32918095588684 secs\n",
      "\n",
      "Epoch 293 batch 0 train Loss 5.3128 test Loss 5.3741 with MSE metric 7574.2168\n",
      "Time taken for 1 epoch: 33.96160936355591 secs\n",
      "\n",
      "Epoch 294 batch 0 train Loss 5.3550 test Loss 5.4055 with MSE metric 8194.7441\n",
      "Time taken for 1 epoch: 34.269118785858154 secs\n",
      "\n",
      "Epoch 295 batch 0 train Loss 5.3060 test Loss 5.4260 with MSE metric 7463.2446\n",
      "Time taken for 1 epoch: 33.636067152023315 secs\n",
      "\n",
      "Epoch 296 batch 0 train Loss 5.3002 test Loss 5.3460 with MSE metric 7384.6079\n",
      "Time taken for 1 epoch: 33.8294460773468 secs\n",
      "\n",
      "Epoch 297 batch 0 train Loss 5.3036 test Loss 5.4178 with MSE metric 7432.3857\n",
      "Time taken for 1 epoch: 33.23758101463318 secs\n",
      "\n",
      "Epoch 298 batch 0 train Loss 5.2885 test Loss 5.3484 with MSE metric 7210.7432\n",
      "Time taken for 1 epoch: 32.43686604499817 secs\n",
      "\n",
      "Epoch 299 batch 0 train Loss 5.2778 test Loss 5.4010 with MSE metric 7052.0449\n",
      "Time taken for 1 epoch: 27.77558207511902 secs\n",
      "\n",
      "Epoch 300 batch 0 train Loss 5.3102 test Loss 5.3471 with MSE metric 7522.8584\n",
      "Time taken for 1 epoch: 27.57251000404358 secs\n",
      "\n",
      "Epoch 301 batch 0 train Loss 5.3016 test Loss 5.3832 with MSE metric 7403.4185\n",
      "Time taken for 1 epoch: 27.599951028823853 secs\n",
      "\n",
      "Epoch 302 batch 0 train Loss 5.3067 test Loss 5.3839 with MSE metric 7482.0742\n",
      "Time taken for 1 epoch: 27.653074741363525 secs\n",
      "\n",
      "Epoch 303 batch 0 train Loss 5.2789 test Loss 5.3733 with MSE metric 7068.6606\n",
      "Time taken for 1 epoch: 27.73115611076355 secs\n",
      "\n",
      "Epoch 304 batch 0 train Loss 5.2847 test Loss 5.3868 with MSE metric 7149.7061\n",
      "Time taken for 1 epoch: 27.481210947036743 secs\n",
      "\n",
      "Epoch 305 batch 0 train Loss 5.2900 test Loss 5.3308 with MSE metric 7233.6646\n",
      "Time taken for 1 epoch: 27.36011505126953 secs\n",
      "\n",
      "Epoch 306 batch 0 train Loss 5.3384 test Loss 5.3330 with MSE metric 7963.7837\n",
      "Time taken for 1 epoch: 27.489722967147827 secs\n",
      "\n",
      "Epoch 307 batch 0 train Loss 5.2989 test Loss 5.3499 with MSE metric 7365.2837\n",
      "Time taken for 1 epoch: 27.4294912815094 secs\n",
      "\n",
      "Epoch 308 batch 0 train Loss 5.3171 test Loss 5.3485 with MSE metric 7628.2529\n",
      "Time taken for 1 epoch: 27.36956286430359 secs\n",
      "\n",
      "Epoch 309 batch 0 train Loss 5.3098 test Loss 5.3281 with MSE metric 7522.8979\n",
      "Time taken for 1 epoch: 27.32905125617981 secs\n",
      "\n",
      "Epoch 310 batch 0 train Loss 5.3071 test Loss 5.3430 with MSE metric 7487.8359\n",
      "Time taken for 1 epoch: 27.44143795967102 secs\n",
      "\n",
      "Epoch 311 batch 0 train Loss 5.2991 test Loss 5.3668 with MSE metric 7369.3760\n",
      "Time taken for 1 epoch: 27.410758018493652 secs\n",
      "\n",
      "Epoch 312 batch 0 train Loss 5.3309 test Loss 5.3918 with MSE metric 7829.2329\n",
      "Time taken for 1 epoch: 27.42582392692566 secs\n",
      "\n",
      "Epoch 313 batch 0 train Loss 5.3011 test Loss 5.3208 with MSE metric 7396.4424\n",
      "Time taken for 1 epoch: 27.41406011581421 secs\n",
      "\n",
      "Epoch 314 batch 0 train Loss 5.3064 test Loss 5.3121 with MSE metric 7476.6426\n",
      "Time taken for 1 epoch: 27.358699083328247 secs\n",
      "\n",
      "Epoch 315 batch 0 train Loss 5.3187 test Loss 5.3536 with MSE metric 7662.3560\n",
      "Time taken for 1 epoch: 27.37840700149536 secs\n",
      "\n",
      "Epoch 316 batch 0 train Loss 5.2795 test Loss 5.4035 with MSE metric 7064.6899\n",
      "Time taken for 1 epoch: 27.421020984649658 secs\n",
      "\n",
      "Epoch 317 batch 0 train Loss 5.2811 test Loss 5.3493 with MSE metric 7085.9141\n",
      "Time taken for 1 epoch: 27.492276906967163 secs\n",
      "\n",
      "Epoch 318 batch 0 train Loss 5.2933 test Loss 5.3415 with MSE metric 7284.1914\n",
      "Time taken for 1 epoch: 27.47380518913269 secs\n",
      "\n",
      "Epoch 319 batch 0 train Loss 5.3181 test Loss 5.3725 with MSE metric 7646.1064\n",
      "Time taken for 1 epoch: 27.419687271118164 secs\n",
      "\n",
      "Epoch 320 batch 0 train Loss 5.3202 test Loss 5.3985 with MSE metric 7672.1406\n",
      "Time taken for 1 epoch: 27.453301191329956 secs\n",
      "\n",
      "Epoch 321 batch 0 train Loss 5.2686 test Loss 5.3660 with MSE metric 6924.1597\n",
      "Time taken for 1 epoch: 27.471229314804077 secs\n",
      "\n",
      "Epoch 322 batch 0 train Loss 5.3155 test Loss 5.3294 with MSE metric 7614.6816\n",
      "Time taken for 1 epoch: 27.411903142929077 secs\n",
      "\n",
      "Epoch 323 batch 0 train Loss 5.2942 test Loss 5.3927 with MSE metric 7288.6582\n",
      "Time taken for 1 epoch: 27.37172508239746 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 324 batch 0 train Loss 5.2766 test Loss 5.3067 with MSE metric 7039.6465\n",
      "Time taken for 1 epoch: 27.46802282333374 secs\n",
      "\n",
      "Epoch 325 batch 0 train Loss 5.2574 test Loss 5.3660 with MSE metric 6774.1328\n",
      "Time taken for 1 epoch: 27.36406111717224 secs\n",
      "\n",
      "Epoch 326 batch 0 train Loss 5.3108 test Loss 5.3470 with MSE metric 7542.7148\n",
      "Time taken for 1 epoch: 27.33210802078247 secs\n",
      "\n",
      "Epoch 327 batch 0 train Loss 5.3219 test Loss 5.3807 with MSE metric 7706.2109\n",
      "Time taken for 1 epoch: 27.385365962982178 secs\n",
      "\n",
      "Epoch 328 batch 0 train Loss 5.2749 test Loss 5.3798 with MSE metric 7014.1011\n",
      "Time taken for 1 epoch: 27.40013098716736 secs\n",
      "\n",
      "Epoch 329 batch 0 train Loss 5.3617 test Loss 5.3536 with MSE metric 8268.8066\n",
      "Time taken for 1 epoch: 27.404680013656616 secs\n",
      "\n",
      "Epoch 330 batch 0 train Loss 5.2785 test Loss 5.3822 with MSE metric 7070.4385\n",
      "Time taken for 1 epoch: 27.43275499343872 secs\n",
      "\n",
      "Epoch 331 batch 0 train Loss 5.2780 test Loss 5.3742 with MSE metric 7059.3291\n",
      "Time taken for 1 epoch: 27.395414113998413 secs\n",
      "\n",
      "Epoch 332 batch 0 train Loss 5.3498 test Loss 5.3745 with MSE metric 8128.1948\n",
      "Time taken for 1 epoch: 27.50210213661194 secs\n",
      "\n",
      "Epoch 333 batch 0 train Loss 5.3089 test Loss 5.3679 with MSE metric 7513.5654\n",
      "Time taken for 1 epoch: 27.39422917366028 secs\n",
      "\n",
      "Epoch 334 batch 0 train Loss 5.2862 test Loss 5.3796 with MSE metric 7179.6934\n",
      "Time taken for 1 epoch: 27.502545833587646 secs\n",
      "\n",
      "Epoch 335 batch 0 train Loss 5.2459 test Loss 5.4203 with MSE metric 6568.9365\n",
      "Time taken for 1 epoch: 27.462175846099854 secs\n",
      "\n",
      "Epoch 336 batch 0 train Loss 5.2822 test Loss 5.3541 with MSE metric 7071.5815\n",
      "Time taken for 1 epoch: 27.52070188522339 secs\n",
      "\n",
      "Epoch 337 batch 0 train Loss 5.2923 test Loss 5.3433 with MSE metric 7269.5508\n",
      "Time taken for 1 epoch: 27.334156036376953 secs\n",
      "\n",
      "Epoch 338 batch 0 train Loss 5.3407 test Loss 5.3744 with MSE metric 7968.3623\n",
      "Time taken for 1 epoch: 27.418111085891724 secs\n",
      "\n",
      "Epoch 339 batch 0 train Loss 5.3122 test Loss 5.3144 with MSE metric 7549.8521\n",
      "Time taken for 1 epoch: 27.428234100341797 secs\n",
      "\n",
      "Epoch 340 batch 0 train Loss 5.2869 test Loss 5.3792 with MSE metric 7192.0801\n",
      "Time taken for 1 epoch: 27.439782857894897 secs\n",
      "\n",
      "Epoch 341 batch 0 train Loss 5.3114 test Loss 5.3151 with MSE metric 7551.4512\n",
      "Time taken for 1 epoch: 27.313854932785034 secs\n",
      "\n",
      "Epoch 342 batch 0 train Loss 5.3434 test Loss 5.3796 with MSE metric 8025.3716\n",
      "Time taken for 1 epoch: 27.37410283088684 secs\n",
      "\n",
      "Epoch 343 batch 0 train Loss 5.3352 test Loss 5.3755 with MSE metric 7884.4312\n",
      "Time taken for 1 epoch: 27.3447003364563 secs\n",
      "\n",
      "Epoch 344 batch 0 train Loss 5.3400 test Loss 5.3672 with MSE metric 7972.2939\n",
      "Time taken for 1 epoch: 27.42634391784668 secs\n",
      "\n",
      "Epoch 345 batch 0 train Loss 5.3276 test Loss 5.3585 with MSE metric 7792.6543\n",
      "Time taken for 1 epoch: 27.34280490875244 secs\n",
      "\n",
      "Epoch 346 batch 0 train Loss 5.3216 test Loss 5.3072 with MSE metric 7704.1240\n",
      "Time taken for 1 epoch: 27.396493911743164 secs\n",
      "\n",
      "Epoch 347 batch 0 train Loss 5.2530 test Loss 5.3582 with MSE metric 6694.5933\n",
      "Time taken for 1 epoch: 27.27896285057068 secs\n",
      "\n",
      "Epoch 348 batch 0 train Loss 5.2969 test Loss 5.2832 with MSE metric 7330.1636\n",
      "Time taken for 1 epoch: 27.367544174194336 secs\n",
      "\n",
      "Epoch 349 batch 0 train Loss 5.3500 test Loss 5.3596 with MSE metric 8112.5137\n",
      "Time taken for 1 epoch: 27.48012614250183 secs\n",
      "\n",
      "Epoch 350 batch 0 train Loss 5.2986 test Loss 5.3347 with MSE metric 7361.3096\n",
      "Time taken for 1 epoch: 27.41635298728943 secs\n",
      "\n",
      "Epoch 351 batch 0 train Loss 5.3470 test Loss 5.3385 with MSE metric 8075.4717\n",
      "Time taken for 1 epoch: 27.362998962402344 secs\n",
      "\n",
      "Epoch 352 batch 0 train Loss 5.3217 test Loss 5.3404 with MSE metric 7709.4590\n",
      "Time taken for 1 epoch: 27.523478031158447 secs\n",
      "\n",
      "Epoch 353 batch 0 train Loss 5.2769 test Loss 5.3557 with MSE metric 7047.9697\n",
      "Time taken for 1 epoch: 27.452193021774292 secs\n",
      "\n",
      "Epoch 354 batch 0 train Loss 5.3254 test Loss 5.3326 with MSE metric 7752.2207\n",
      "Time taken for 1 epoch: 27.42832589149475 secs\n",
      "\n",
      "Epoch 355 batch 0 train Loss 5.3256 test Loss 5.3301 with MSE metric 7769.7227\n",
      "Time taken for 1 epoch: 27.462295055389404 secs\n",
      "\n",
      "Epoch 356 batch 0 train Loss 5.2662 test Loss 5.3820 with MSE metric 6893.9619\n",
      "Time taken for 1 epoch: 27.548768997192383 secs\n",
      "\n",
      "Epoch 357 batch 0 train Loss 5.3198 test Loss 5.3358 with MSE metric 7667.9746\n",
      "Time taken for 1 epoch: 27.55343508720398 secs\n",
      "\n",
      "Epoch 358 batch 0 train Loss 5.3201 test Loss 5.3431 with MSE metric 7682.6475\n",
      "Time taken for 1 epoch: 27.368708848953247 secs\n",
      "\n",
      "Epoch 359 batch 0 train Loss 5.3055 test Loss 5.3672 with MSE metric 7462.5664\n",
      "Time taken for 1 epoch: 27.468818187713623 secs\n",
      "\n",
      "Epoch 360 batch 0 train Loss 5.2859 test Loss 5.4391 with MSE metric 7176.9976\n",
      "Time taken for 1 epoch: 26.921249628067017 secs\n",
      "\n",
      "Epoch 361 batch 0 train Loss 5.2949 test Loss 5.3865 with MSE metric 7306.4580\n",
      "Time taken for 1 epoch: 26.82269310951233 secs\n",
      "\n",
      "Epoch 362 batch 0 train Loss 5.3344 test Loss 5.3691 with MSE metric 7888.2202\n",
      "Time taken for 1 epoch: 27.01097822189331 secs\n",
      "\n",
      "Epoch 363 batch 0 train Loss 5.3246 test Loss 5.3527 with MSE metric 7736.9199\n",
      "Time taken for 1 epoch: 27.12101697921753 secs\n",
      "\n",
      "Epoch 364 batch 0 train Loss 5.3182 test Loss 5.3299 with MSE metric 7655.2705\n",
      "Time taken for 1 epoch: 26.933472156524658 secs\n",
      "\n",
      "Epoch 365 batch 0 train Loss 5.3605 test Loss 5.3441 with MSE metric 8290.7285\n",
      "Time taken for 1 epoch: 26.9071307182312 secs\n",
      "\n",
      "Epoch 366 batch 0 train Loss 5.3695 test Loss 5.3725 with MSE metric 8408.4062\n",
      "Time taken for 1 epoch: 27.075475215911865 secs\n",
      "\n",
      "Epoch 367 batch 0 train Loss 5.2849 test Loss 5.3476 with MSE metric 7162.0674\n",
      "Time taken for 1 epoch: 26.895603895187378 secs\n",
      "\n",
      "Epoch 368 batch 0 train Loss 5.3056 test Loss 5.3910 with MSE metric 7464.9790\n",
      "Time taken for 1 epoch: 26.8814377784729 secs\n",
      "\n",
      "Epoch 369 batch 0 train Loss 5.2613 test Loss 5.3516 with MSE metric 6779.9707\n",
      "Time taken for 1 epoch: 27.009432077407837 secs\n",
      "\n",
      "Epoch 370 batch 0 train Loss 5.3278 test Loss 5.3022 with MSE metric 7771.9956\n",
      "Time taken for 1 epoch: 26.952419996261597 secs\n",
      "\n",
      "Epoch 371 batch 0 train Loss 5.3112 test Loss 5.3510 with MSE metric 7548.9551\n",
      "Time taken for 1 epoch: 26.919124364852905 secs\n",
      "\n",
      "Epoch 372 batch 0 train Loss 5.2862 test Loss 5.3437 with MSE metric 7180.9595\n",
      "Time taken for 1 epoch: 26.919243574142456 secs\n",
      "\n",
      "Epoch 373 batch 0 train Loss 5.3111 test Loss 5.3643 with MSE metric 7548.3604\n",
      "Time taken for 1 epoch: 27.004451990127563 secs\n",
      "\n",
      "Epoch 374 batch 0 train Loss 5.3167 test Loss 5.3673 with MSE metric 7632.1875\n",
      "Time taken for 1 epoch: 26.99147391319275 secs\n",
      "\n",
      "Epoch 375 batch 0 train Loss 5.2834 test Loss 5.2914 with MSE metric 7140.8081\n",
      "Time taken for 1 epoch: 27.04007315635681 secs\n",
      "\n",
      "Epoch 376 batch 0 train Loss 5.3299 test Loss 5.3376 with MSE metric 7830.3721\n",
      "Time taken for 1 epoch: 26.934746980667114 secs\n",
      "\n",
      "Epoch 377 batch 0 train Loss 5.3325 test Loss 5.3106 with MSE metric 7828.7949\n",
      "Time taken for 1 epoch: 27.002201080322266 secs\n",
      "\n",
      "Epoch 378 batch 0 train Loss 5.2973 test Loss 5.3623 with MSE metric 7340.1826\n",
      "Time taken for 1 epoch: 26.913338899612427 secs\n",
      "\n",
      "Epoch 379 batch 0 train Loss 5.3070 test Loss 5.3604 with MSE metric 7485.7637\n",
      "Time taken for 1 epoch: 26.96509575843811 secs\n",
      "\n",
      "Epoch 380 batch 0 train Loss 5.2797 test Loss 5.3126 with MSE metric 7079.2715\n",
      "Time taken for 1 epoch: 26.900421142578125 secs\n",
      "\n",
      "Epoch 381 batch 0 train Loss 5.3111 test Loss 5.3800 with MSE metric 7547.5840\n",
      "Time taken for 1 epoch: 26.958356142044067 secs\n",
      "\n",
      "Epoch 382 batch 0 train Loss 5.2855 test Loss 5.2845 with MSE metric 7155.8145\n",
      "Time taken for 1 epoch: 26.885125875473022 secs\n",
      "\n",
      "Epoch 383 batch 0 train Loss 5.3142 test Loss 5.3723 with MSE metric 7592.6440\n",
      "Time taken for 1 epoch: 26.93392515182495 secs\n",
      "\n",
      "Epoch 384 batch 0 train Loss 5.3139 test Loss 5.3542 with MSE metric 7574.3838\n",
      "Time taken for 1 epoch: 26.977437019348145 secs\n",
      "\n",
      "Epoch 385 batch 0 train Loss 5.3192 test Loss 5.3355 with MSE metric 7666.2598\n",
      "Time taken for 1 epoch: 27.05065393447876 secs\n",
      "\n",
      "Epoch 386 batch 0 train Loss 5.2530 test Loss 5.3654 with MSE metric 6710.7383\n",
      "Time taken for 1 epoch: 26.953184127807617 secs\n",
      "\n",
      "Epoch 387 batch 0 train Loss 5.2628 test Loss 5.3532 with MSE metric 6815.9229\n",
      "Time taken for 1 epoch: 27.093297958374023 secs\n",
      "\n",
      "Epoch 388 batch 0 train Loss 5.2892 test Loss 5.3944 with MSE metric 7224.5493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 27.09631609916687 secs\n",
      "\n",
      "Epoch 389 batch 0 train Loss 5.3136 test Loss 5.4178 with MSE metric 7583.7886\n",
      "Time taken for 1 epoch: 27.03241991996765 secs\n",
      "\n",
      "Epoch 390 batch 0 train Loss 5.2998 test Loss 5.3530 with MSE metric 7377.3188\n",
      "Time taken for 1 epoch: 26.982159852981567 secs\n",
      "\n",
      "Epoch 391 batch 0 train Loss 5.2987 test Loss 5.3956 with MSE metric 7361.8965\n",
      "Time taken for 1 epoch: 26.956141233444214 secs\n",
      "\n",
      "Epoch 392 batch 0 train Loss 5.3177 test Loss 5.3762 with MSE metric 7646.4727\n",
      "Time taken for 1 epoch: 27.08856439590454 secs\n",
      "\n",
      "Epoch 393 batch 0 train Loss 5.2915 test Loss 5.3664 with MSE metric 7234.6675\n",
      "Time taken for 1 epoch: 26.894054889678955 secs\n",
      "\n",
      "Epoch 394 batch 0 train Loss 5.2709 test Loss 5.3892 with MSE metric 6951.7754\n",
      "Time taken for 1 epoch: 27.004430055618286 secs\n",
      "\n",
      "Epoch 395 batch 0 train Loss 5.2352 test Loss 5.3525 with MSE metric 6370.6357\n",
      "Time taken for 1 epoch: 27.033483028411865 secs\n",
      "\n",
      "Epoch 396 batch 0 train Loss 5.3258 test Loss 5.3356 with MSE metric 7737.1133\n",
      "Time taken for 1 epoch: 27.05475902557373 secs\n",
      "\n",
      "Epoch 397 batch 0 train Loss 5.2899 test Loss 5.3202 with MSE metric 7235.2471\n",
      "Time taken for 1 epoch: 27.118016958236694 secs\n",
      "\n",
      "Epoch 398 batch 0 train Loss 5.3441 test Loss 5.3409 with MSE metric 8054.0420\n",
      "Time taken for 1 epoch: 27.02279782295227 secs\n",
      "\n",
      "Epoch 399 batch 0 train Loss 5.3162 test Loss 5.3954 with MSE metric 7624.0938\n",
      "Time taken for 1 epoch: 27.168511867523193 secs\n",
      "\n",
      "Epoch 400 batch 0 train Loss 5.3550 test Loss 5.3731 with MSE metric 8156.6665\n",
      "Time taken for 1 epoch: 27.093233823776245 secs\n",
      "\n",
      "Epoch 401 batch 0 train Loss 5.3649 test Loss 5.3125 with MSE metric 8312.0947\n",
      "Time taken for 1 epoch: 27.104785919189453 secs\n",
      "\n",
      "Epoch 402 batch 0 train Loss 5.2777 test Loss 5.3360 with MSE metric 7055.4990\n",
      "Time taken for 1 epoch: 27.266619205474854 secs\n",
      "\n",
      "Epoch 403 batch 0 train Loss 5.2846 test Loss 5.3283 with MSE metric 7151.4297\n",
      "Time taken for 1 epoch: 27.229286909103394 secs\n",
      "\n",
      "Epoch 404 batch 0 train Loss 5.2865 test Loss 5.3483 with MSE metric 7185.4219\n",
      "Time taken for 1 epoch: 27.15410614013672 secs\n",
      "\n",
      "Epoch 405 batch 0 train Loss 5.2624 test Loss 5.3607 with MSE metric 6826.9517\n",
      "Time taken for 1 epoch: 27.031597137451172 secs\n",
      "\n",
      "Epoch 406 batch 0 train Loss 5.2775 test Loss 5.3331 with MSE metric 7057.9668\n",
      "Time taken for 1 epoch: 27.03995704650879 secs\n",
      "\n",
      "Epoch 407 batch 0 train Loss 5.2810 test Loss 5.3638 with MSE metric 7084.3716\n",
      "Time taken for 1 epoch: 27.179378986358643 secs\n",
      "\n",
      "Epoch 408 batch 0 train Loss 5.3160 test Loss 5.3728 with MSE metric 7612.8604\n",
      "Time taken for 1 epoch: 27.031597137451172 secs\n",
      "\n",
      "Epoch 409 batch 0 train Loss 5.3032 test Loss 5.4403 with MSE metric 7425.4595\n",
      "Time taken for 1 epoch: 26.998456954956055 secs\n",
      "\n",
      "Epoch 410 batch 0 train Loss 5.3148 test Loss 5.3966 with MSE metric 7583.0913\n",
      "Time taken for 1 epoch: 27.092792987823486 secs\n",
      "\n",
      "Epoch 411 batch 0 train Loss 5.2550 test Loss 5.3702 with MSE metric 6739.7173\n",
      "Time taken for 1 epoch: 27.139606952667236 secs\n",
      "\n",
      "Epoch 412 batch 0 train Loss 5.3162 test Loss 5.4172 with MSE metric 7624.5962\n",
      "Time taken for 1 epoch: 27.191904067993164 secs\n",
      "\n",
      "Epoch 413 batch 0 train Loss 5.3028 test Loss 5.3290 with MSE metric 7421.8926\n",
      "Time taken for 1 epoch: 27.149726152420044 secs\n",
      "\n",
      "Epoch 414 batch 0 train Loss 5.2905 test Loss 5.3143 with MSE metric 7238.8740\n",
      "Time taken for 1 epoch: 27.04543399810791 secs\n",
      "\n",
      "Epoch 415 batch 0 train Loss 5.3311 test Loss 5.3825 with MSE metric 7827.1787\n",
      "Time taken for 1 epoch: 27.09853196144104 secs\n",
      "\n",
      "Epoch 416 batch 0 train Loss 5.2998 test Loss 5.3663 with MSE metric 7378.2939\n",
      "Time taken for 1 epoch: 27.13733720779419 secs\n",
      "\n",
      "Epoch 417 batch 0 train Loss 5.3377 test Loss 5.3801 with MSE metric 7933.3281\n",
      "Time taken for 1 epoch: 27.046991109848022 secs\n",
      "\n",
      "Epoch 418 batch 0 train Loss 5.2817 test Loss 5.3570 with MSE metric 7093.8809\n",
      "Time taken for 1 epoch: 27.042083978652954 secs\n",
      "\n",
      "Epoch 419 batch 0 train Loss 5.2859 test Loss 5.3628 with MSE metric 7172.3750\n",
      "Time taken for 1 epoch: 27.095913887023926 secs\n",
      "\n",
      "Epoch 420 batch 0 train Loss 5.3222 test Loss 5.3498 with MSE metric 7716.0527\n",
      "Time taken for 1 epoch: 27.078243255615234 secs\n",
      "\n",
      "Epoch 421 batch 0 train Loss 5.2939 test Loss 5.3808 with MSE metric 7285.4160\n",
      "Time taken for 1 epoch: 27.00596308708191 secs\n",
      "\n",
      "Epoch 422 batch 0 train Loss 5.2992 test Loss 5.3001 with MSE metric 7365.3496\n",
      "Time taken for 1 epoch: 27.090720891952515 secs\n",
      "\n",
      "Epoch 423 batch 0 train Loss 5.2895 test Loss 5.3652 with MSE metric 7228.2656\n",
      "Time taken for 1 epoch: 27.209789991378784 secs\n",
      "\n",
      "Epoch 424 batch 0 train Loss 5.2625 test Loss 5.3691 with MSE metric 6793.0815\n",
      "Time taken for 1 epoch: 27.11534285545349 secs\n",
      "\n",
      "Epoch 425 batch 0 train Loss 5.2904 test Loss 5.3205 with MSE metric 7241.0576\n",
      "Time taken for 1 epoch: 27.104327917099 secs\n",
      "\n",
      "Epoch 426 batch 0 train Loss 5.2896 test Loss 5.3648 with MSE metric 7214.1416\n",
      "Time taken for 1 epoch: 26.863646984100342 secs\n",
      "\n",
      "Epoch 427 batch 0 train Loss 5.3220 test Loss 5.3525 with MSE metric 7684.6006\n",
      "Time taken for 1 epoch: 27.20815086364746 secs\n",
      "\n",
      "Epoch 428 batch 0 train Loss 5.2970 test Loss 5.3542 with MSE metric 7332.0205\n",
      "Time taken for 1 epoch: 27.072174787521362 secs\n",
      "\n",
      "Epoch 429 batch 0 train Loss 5.3358 test Loss 5.3535 with MSE metric 7914.9414\n",
      "Time taken for 1 epoch: 27.090607166290283 secs\n",
      "\n",
      "Epoch 430 batch 0 train Loss 5.2911 test Loss 5.3406 with MSE metric 7251.8125\n",
      "Time taken for 1 epoch: 27.149861812591553 secs\n",
      "\n",
      "Epoch 431 batch 0 train Loss 5.3246 test Loss 5.3813 with MSE metric 7748.6304\n",
      "Time taken for 1 epoch: 27.073094844818115 secs\n",
      "\n",
      "Epoch 432 batch 0 train Loss 5.3023 test Loss 5.3866 with MSE metric 7397.7310\n",
      "Time taken for 1 epoch: 27.07669997215271 secs\n",
      "\n",
      "Epoch 433 batch 0 train Loss 5.3056 test Loss 5.3625 with MSE metric 7438.2144\n",
      "Time taken for 1 epoch: 27.15993070602417 secs\n",
      "\n",
      "Epoch 434 batch 0 train Loss 5.2966 test Loss 5.3450 with MSE metric 7327.5801\n",
      "Time taken for 1 epoch: 27.117053985595703 secs\n",
      "\n",
      "Epoch 435 batch 0 train Loss 5.3200 test Loss 5.3766 with MSE metric 7671.2480\n",
      "Time taken for 1 epoch: 27.19480276107788 secs\n",
      "\n",
      "Epoch 436 batch 0 train Loss 5.3364 test Loss 5.3922 with MSE metric 7923.4189\n",
      "Time taken for 1 epoch: 27.030756950378418 secs\n",
      "\n",
      "Epoch 437 batch 0 train Loss 5.3211 test Loss 5.3577 with MSE metric 7697.8892\n",
      "Time taken for 1 epoch: 27.071369886398315 secs\n",
      "\n",
      "Epoch 438 batch 0 train Loss 5.2736 test Loss 5.3143 with MSE metric 6976.1230\n",
      "Time taken for 1 epoch: 27.064339876174927 secs\n",
      "\n",
      "Epoch 439 batch 0 train Loss 5.3181 test Loss 5.3637 with MSE metric 7636.7773\n",
      "Time taken for 1 epoch: 27.124584913253784 secs\n",
      "\n",
      "Epoch 440 batch 0 train Loss 5.2827 test Loss 5.3404 with MSE metric 7123.9209\n",
      "Time taken for 1 epoch: 26.95046305656433 secs\n",
      "\n",
      "Epoch 441 batch 0 train Loss 5.3137 test Loss 5.3558 with MSE metric 7581.4521\n",
      "Time taken for 1 epoch: 27.050902128219604 secs\n",
      "\n",
      "Epoch 442 batch 0 train Loss 5.3182 test Loss 5.3520 with MSE metric 7654.9458\n",
      "Time taken for 1 epoch: 27.15706181526184 secs\n",
      "\n",
      "Epoch 443 batch 0 train Loss 5.2863 test Loss 5.3809 with MSE metric 7181.6948\n",
      "Time taken for 1 epoch: 26.993292093276978 secs\n",
      "\n",
      "Epoch 444 batch 0 train Loss 5.3008 test Loss 5.3001 with MSE metric 7392.7676\n",
      "Time taken for 1 epoch: 27.118080139160156 secs\n",
      "\n",
      "Epoch 445 batch 0 train Loss 5.3058 test Loss 5.4321 with MSE metric 7467.2715\n",
      "Time taken for 1 epoch: 27.08647084236145 secs\n",
      "\n",
      "Epoch 446 batch 0 train Loss 5.2940 test Loss 5.3157 with MSE metric 7278.5679\n",
      "Time taken for 1 epoch: 27.21676206588745 secs\n",
      "\n",
      "Epoch 447 batch 0 train Loss 5.2494 test Loss 5.4039 with MSE metric 6654.0332\n",
      "Time taken for 1 epoch: 27.06649684906006 secs\n",
      "\n",
      "Epoch 448 batch 0 train Loss 5.2647 test Loss 5.3582 with MSE metric 6857.5854\n",
      "Time taken for 1 epoch: 27.193511962890625 secs\n",
      "\n",
      "Epoch 449 batch 0 train Loss 5.2816 test Loss 5.4280 with MSE metric 7115.1313\n",
      "Time taken for 1 epoch: 27.09676504135132 secs\n",
      "\n",
      "Epoch 450 batch 0 train Loss 5.3345 test Loss 5.3538 with MSE metric 7885.8350\n",
      "Time taken for 1 epoch: 27.152236700057983 secs\n",
      "\n",
      "Epoch 451 batch 0 train Loss 5.3284 test Loss 5.4109 with MSE metric 7812.4639\n",
      "Time taken for 1 epoch: 27.12639284133911 secs\n",
      "\n",
      "Epoch 452 batch 0 train Loss 5.2483 test Loss 5.4096 with MSE metric 6638.6797\n",
      "Time taken for 1 epoch: 27.11201500892639 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453 batch 0 train Loss 5.3219 test Loss 5.3439 with MSE metric 7709.7695\n",
      "Time taken for 1 epoch: 27.216262817382812 secs\n",
      "\n",
      "Epoch 454 batch 0 train Loss 5.3025 test Loss 5.3443 with MSE metric 7418.3789\n",
      "Time taken for 1 epoch: 27.065304040908813 secs\n",
      "\n",
      "Epoch 455 batch 0 train Loss 5.3019 test Loss 5.3073 with MSE metric 7405.6260\n",
      "Time taken for 1 epoch: 27.164268016815186 secs\n",
      "\n",
      "Epoch 456 batch 0 train Loss 5.2873 test Loss 5.3817 with MSE metric 7180.5107\n",
      "Time taken for 1 epoch: 27.02651882171631 secs\n",
      "\n",
      "Epoch 457 batch 0 train Loss 5.2866 test Loss 5.3515 with MSE metric 7187.3516\n",
      "Time taken for 1 epoch: 27.192296981811523 secs\n",
      "\n",
      "Epoch 458 batch 0 train Loss 5.2974 test Loss 5.3324 with MSE metric 7338.7100\n",
      "Time taken for 1 epoch: 27.25898504257202 secs\n",
      "\n",
      "Epoch 459 batch 0 train Loss 5.2738 test Loss 5.3536 with MSE metric 6986.7490\n",
      "Time taken for 1 epoch: 27.075589895248413 secs\n",
      "\n",
      "Epoch 460 batch 0 train Loss 5.3162 test Loss 5.3490 with MSE metric 7624.3311\n",
      "Time taken for 1 epoch: 27.111890077590942 secs\n",
      "\n",
      "Epoch 461 batch 0 train Loss 5.2958 test Loss 5.3804 with MSE metric 7320.4595\n",
      "Time taken for 1 epoch: 27.094707012176514 secs\n",
      "\n",
      "Epoch 462 batch 0 train Loss 5.2718 test Loss 5.3749 with MSE metric 6966.1426\n",
      "Time taken for 1 epoch: 27.13658094406128 secs\n",
      "\n",
      "Epoch 463 batch 0 train Loss 5.2885 test Loss 5.3455 with MSE metric 7213.9058\n",
      "Time taken for 1 epoch: 27.046524047851562 secs\n",
      "\n",
      "Epoch 464 batch 0 train Loss 5.2922 test Loss 5.3808 with MSE metric 7267.3423\n",
      "Time taken for 1 epoch: 27.146683931350708 secs\n",
      "\n",
      "Epoch 465 batch 0 train Loss 5.3416 test Loss 5.3355 with MSE metric 8018.1421\n",
      "Time taken for 1 epoch: 27.098022937774658 secs\n",
      "\n",
      "Epoch 466 batch 0 train Loss 5.3010 test Loss 5.4085 with MSE metric 7395.4854\n",
      "Time taken for 1 epoch: 27.271059274673462 secs\n",
      "\n",
      "Epoch 467 batch 0 train Loss 5.2963 test Loss 5.4284 with MSE metric 7326.9062\n",
      "Time taken for 1 epoch: 27.152037143707275 secs\n",
      "\n",
      "Epoch 468 batch 0 train Loss 5.2963 test Loss 5.2893 with MSE metric 7326.8721\n",
      "Time taken for 1 epoch: 27.191701889038086 secs\n",
      "\n",
      "Epoch 469 batch 0 train Loss 5.3477 test Loss 5.3840 with MSE metric 8115.4170\n",
      "Time taken for 1 epoch: 27.078460931777954 secs\n",
      "\n",
      "Epoch 470 batch 0 train Loss 5.2716 test Loss 5.3174 with MSE metric 6959.8145\n",
      "Time taken for 1 epoch: 27.305502891540527 secs\n",
      "\n",
      "Epoch 471 batch 0 train Loss 5.2901 test Loss 5.3677 with MSE metric 7237.1582\n",
      "Time taken for 1 epoch: 27.111046075820923 secs\n",
      "\n",
      "Epoch 472 batch 0 train Loss 5.2936 test Loss 5.4135 with MSE metric 7275.5166\n",
      "Time taken for 1 epoch: 27.25288486480713 secs\n",
      "\n",
      "Epoch 473 batch 0 train Loss 5.3452 test Loss 5.3540 with MSE metric 8022.6973\n",
      "Time taken for 1 epoch: 27.105109930038452 secs\n",
      "\n",
      "Epoch 474 batch 0 train Loss 5.3470 test Loss 5.3936 with MSE metric 8051.4062\n",
      "Time taken for 1 epoch: 27.189763069152832 secs\n",
      "\n",
      "Epoch 475 batch 0 train Loss 5.2893 test Loss 5.3887 with MSE metric 7221.8906\n",
      "Time taken for 1 epoch: 27.0927472114563 secs\n",
      "\n",
      "Epoch 476 batch 0 train Loss 5.3157 test Loss 5.3959 with MSE metric 7597.0400\n",
      "Time taken for 1 epoch: 27.280750036239624 secs\n",
      "\n",
      "Epoch 477 batch 0 train Loss 5.3188 test Loss 5.3927 with MSE metric 7648.3188\n",
      "Time taken for 1 epoch: 27.122863054275513 secs\n",
      "\n",
      "Epoch 478 batch 0 train Loss 5.3119 test Loss 5.3245 with MSE metric 7560.5854\n",
      "Time taken for 1 epoch: 27.223530054092407 secs\n",
      "\n",
      "Epoch 479 batch 0 train Loss 5.3227 test Loss 5.3142 with MSE metric 7713.7930\n",
      "Time taken for 1 epoch: 27.096131801605225 secs\n",
      "\n",
      "Epoch 480 batch 0 train Loss 5.3193 test Loss 5.3836 with MSE metric 7670.9512\n",
      "Time taken for 1 epoch: 27.17732810974121 secs\n",
      "\n",
      "Epoch 481 batch 0 train Loss 5.3465 test Loss 5.3549 with MSE metric 8078.0566\n",
      "Time taken for 1 epoch: 27.144388914108276 secs\n",
      "\n",
      "Epoch 482 batch 0 train Loss 5.3232 test Loss 5.3937 with MSE metric 7730.2188\n",
      "Time taken for 1 epoch: 27.152604341506958 secs\n",
      "\n",
      "Epoch 483 batch 0 train Loss 5.2597 test Loss 5.3130 with MSE metric 6789.8652\n",
      "Time taken for 1 epoch: 27.07134985923767 secs\n",
      "\n",
      "Epoch 484 batch 0 train Loss 5.3510 test Loss 5.3691 with MSE metric 8153.0176\n",
      "Time taken for 1 epoch: 27.19134783744812 secs\n",
      "\n",
      "Epoch 485 batch 0 train Loss 5.2820 test Loss 5.3695 with MSE metric 7119.2202\n",
      "Time taken for 1 epoch: 27.118860006332397 secs\n",
      "\n",
      "Epoch 486 batch 0 train Loss 5.2865 test Loss 5.3407 with MSE metric 7183.8867\n",
      "Time taken for 1 epoch: 27.130645036697388 secs\n",
      "\n",
      "Epoch 487 batch 0 train Loss 5.3443 test Loss 5.3499 with MSE metric 8035.1240\n",
      "Time taken for 1 epoch: 27.071245193481445 secs\n",
      "\n",
      "Epoch 488 batch 0 train Loss 5.2472 test Loss 5.3396 with MSE metric 6567.3428\n",
      "Time taken for 1 epoch: 27.142788887023926 secs\n",
      "\n",
      "Epoch 489 batch 0 train Loss 5.3301 test Loss 5.3442 with MSE metric 7834.3506\n",
      "Time taken for 1 epoch: 27.123227834701538 secs\n",
      "\n",
      "Epoch 490 batch 0 train Loss 5.3035 test Loss 5.3442 with MSE metric 7432.9331\n",
      "Time taken for 1 epoch: 27.170616149902344 secs\n",
      "\n",
      "Epoch 491 batch 0 train Loss 5.3417 test Loss 5.3944 with MSE metric 8011.9165\n",
      "Time taken for 1 epoch: 27.036976099014282 secs\n",
      "\n",
      "Epoch 492 batch 0 train Loss 5.2476 test Loss 5.3569 with MSE metric 6614.0029\n",
      "Time taken for 1 epoch: 27.225064039230347 secs\n",
      "\n",
      "Epoch 493 batch 0 train Loss 5.3581 test Loss 5.3695 with MSE metric 8211.2461\n",
      "Time taken for 1 epoch: 27.171886920928955 secs\n",
      "\n",
      "Epoch 494 batch 0 train Loss 5.2855 test Loss 5.4073 with MSE metric 7154.9517\n",
      "Time taken for 1 epoch: 27.195376873016357 secs\n",
      "\n",
      "Epoch 495 batch 0 train Loss 5.3107 test Loss 5.3592 with MSE metric 7538.7334\n",
      "Time taken for 1 epoch: 27.056843042373657 secs\n",
      "\n",
      "Epoch 496 batch 0 train Loss 5.2923 test Loss 5.3685 with MSE metric 7268.9409\n",
      "Time taken for 1 epoch: 27.035356283187866 secs\n",
      "\n",
      "Epoch 497 batch 0 train Loss 5.2965 test Loss 5.3919 with MSE metric 7306.0435\n",
      "Time taken for 1 epoch: 27.105270862579346 secs\n",
      "\n",
      "Epoch 498 batch 0 train Loss 5.2995 test Loss 5.3482 with MSE metric 7367.2915\n",
      "Time taken for 1 epoch: 27.073323011398315 secs\n",
      "\n",
      "Epoch 499 batch 0 train Loss 5.2666 test Loss 5.3635 with MSE metric 6866.0146\n",
      "Time taken for 1 epoch: 27.06684970855713 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam(0.0003)\n",
    "    decoder = climate_model.Decoder(16)\n",
    "    EPOCHS = 500\n",
    "    batch_s  = 128\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(temp_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_climate/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    #https://www.tensorflow.org/guide/checkpoint\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_foxes(token_tr, time_tr, temp_tr, batch_s=128)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_tok_pos_tr)\n",
    "                tar_inp, tar_real, pred, pred_log_sig, mask = train_step(decoder, optimizer_c, batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 100 == 0:\n",
    "                    batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, _ = batch_creator.create_batch_foxes(token_te, time_te, temp_te, batch_s= 128)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_tok_pos_te)\n",
    "                    tar_real_te, pred_te, pred_log_sig_te, t_mask = test_step(decoder, batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float64, numpy=\n",
       "array([277.4391 , 277.1047 , 280.83145, 277.54877, 276.43674, 280.73575,\n",
       "       279.6379 , 278.9441 , 280.72787, 279.9052 , 276.1898 , 278.17764,\n",
       "       276.66025, 277.8398 , 279.3794 , 277.32608, 279.0153 , 276.279  ,\n",
       "       277.99994, 276.37442, 280.85577, 279.12204, 280.74197, 277.27045,\n",
       "       278.04602,   0.     ,   0.     ,   0.     ,   0.     ,   0.     ,\n",
       "         0.     ,   0.     ,   0.     ,   0.     ,   0.     ,   0.     ,\n",
       "         0.     ,   0.     ,   0.     ])>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_real_te[10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float64, numpy=\n",
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_mask[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float64, numpy=\n",
       "array([305.5369064 , 305.53690675, 305.53690678, 305.53690751,\n",
       "       305.53690744, 305.79655562, 305.79655595, 305.79655609,\n",
       "       305.84506049, 305.84506066, 305.84506075, 310.90365222,\n",
       "       310.90365221, 310.90365213, 105.75508275, 105.75508295,\n",
       "       105.75508276, 105.75508288, 105.75508255, 105.75508251,\n",
       "       105.75508224, 105.75508261, 105.75508272, 105.75508303,\n",
       "       105.75508289, 105.75508286, 105.75508286, 105.75508286,\n",
       "       105.75508286, 105.75508286, 105.75508286, 105.75508286,\n",
       "       105.75508286, 105.75508286, 105.75508286, 105.75508286,\n",
       "       105.75508286, 105.75508286, 105.75508286])>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_te[10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39,), dtype=float64, numpy=\n",
       "array([  28.0978064 ,   28.43220675,   24.70545678,   27.98813751,\n",
       "         29.10016744,   25.06080562,   26.15865595,   26.85245609,\n",
       "         25.11719049,   25.93986066,   29.65526075,   32.72601222,\n",
       "         34.24340221,   33.06385213, -173.62431725, -171.57099705,\n",
       "       -173.26021724, -170.52391712, -172.24485745, -170.61933749,\n",
       "       -175.10068776, -173.36695739, -174.98688728, -171.51536697,\n",
       "       -172.29093711,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ])>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_te[10, :] * t_mask[10, :] ) - tar_real_te[10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
