{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import fox_model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_fox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('/Users/omernivron/Downloads/fnr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = foxes\n",
    "# 1 = rabbits\n",
    "# 2 = time\n",
    "# 3 = foxes_tokens\n",
    "# 4 = rabbits_tokens (token id 3 means rabbit_ode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df[2::5]\n",
    "f = df[0::5]; r = df[1::5]\n",
    "f_token = df[3::5]; r_token = df[4::5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr = t[:80]; f_tr = f[:80]; r_tr = r[:80]; f_token_tr = f_token[:80]; r_token_tr = r_token[:80]\n",
    "pad_pos_te = t[80:]; f_te = f[80:]; r_te = r[80:]; f_token_te = f_token[80:]; r_token_te = r_token[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr = np.repeat(pad_pos_tr, 2, axis = 0)\n",
    "pad_pos_te = np.repeat(pad_pos_te, 2, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tr = np.concatenate((f_tr, r_tr), axis = 0); tar_te = np.concatenate((f_te, r_te), axis = 0)\n",
    "token_tr = np.concatenate((f_token_tr, r_token_tr), axis = 0); token_te = np.concatenate((f_token_te, r_token_te), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = masks.position_mask(pad_pos_tr)\n",
    "pp_te = masks.position_mask(pad_pos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred, pred_sig = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "#     tf.print(tf.math.reduce_max(pred_sig))\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred, pred_sig)\n",
    "    tf.print(t_loss)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred, pred_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = fox_model.Decoder(16)\n",
    "    EPOCHS = 10\n",
    "    batch_s  = 128\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(tar_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)    \n",
    "    checkpoint = tf.train.Checkpoint(optimizer = optimizer_c, model = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_fox/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr , batch_pos_mask, _ = batch_creator.create_batch_foxes(token_tr, pad_pos_tr, tar_tr, pp)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                train_step(batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "#                 if batch_n % 50 == 0:\n",
    "#                     batch_tok_pos_te, batch_tim_pos_te, batch_tar_te , batch_pos_mask_te, _ = batch_creator.create_batch_foxes(token_te, pad_pos_te, tar_te, pp_te)\n",
    "#                     tar_real_te, pred, pred_sig = test_step(batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "#                     helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "#                     helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "#                     checkpoint.save(folder + '/')\n",
    "                step += 1\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(189,), dtype=float64, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_real_te[30, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(189,), dtype=float64, numpy=\n",
       "array([-0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(189,), dtype=float64, numpy=\n",
       "array([2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sig[2, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
