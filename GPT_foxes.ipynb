{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import fox_model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT_fox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('/Users/omernivron/Downloads/fnr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = foxes\n",
    "# 1 = rabbits\n",
    "# 2 = time\n",
    "# 3 = foxes_tokens\n",
    "# 4 = rabbits_tokens (token id 3 means rabbit_ode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df[2::5]\n",
    "f = df[0::5]; r = df[1::5]\n",
    "f_token = df[3::5]; r_token = df[4::5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr = t[:80]; f_tr = f[:80]; r_tr = r[:80]; f_token_tr = f_token[:80]; r_token_tr = r_token[:80]\n",
    "pad_pos_te = t[80:]; f_te = f[80:]; r_te = r[80:]; f_token_te = f_token[80:]; r_token_te = r_token[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr = np.repeat(pad_pos_tr, 2, axis = 0)\n",
    "pad_pos_te = np.repeat(pad_pos_te, 2, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tr = np.concatenate((f_tr, r_tr), axis = 0); tar_te = np.concatenate((f_te, r_te), axis = 0)\n",
    "token_tr = np.concatenate((f_token_tr, r_token_tr), axis = 0); token_te = np.concatenate((f_token_te, r_token_te), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = masks.position_mask(pad_pos_tr)\n",
    "pp_te = masks.position_mask(pad_pos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_pos_tr, tim_pos_tr, tar_tr , pos_mask, batch_idx_tr = batch_creator.create_batch_foxes(token_tr, pad_pos_tr, tar_tr, pp, batch_s = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(token_pos, time_pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(token_pos, time_pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))\n",
    "    return tar_inp, tar_real, pred, pred_sig, combined_mask_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(token_pos_te, time_pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred, pred_sig = decoder(token_pos_te, time_pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "#     tf.print(tf.math.reduce_max(pred_sig))\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred, pred_sig)\n",
    "    tf.print(t_loss)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)\n",
    "    return tar_real_te, pred, pred_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Epoch 0 batch 0 train Loss 9778.1563 test Loss 0.0000 with MSE metric 9730.3418\n",
      "Time taken for 1 epoch: 3.241412878036499 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 7238.7994 test Loss 0.0000 with MSE metric 7495.8036\n",
      "Time taken for 1 epoch: 2.1934361457824707 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 6139.8840 test Loss 0.0000 with MSE metric 6620.6549\n",
      "Time taken for 1 epoch: 2.17579984664917 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 5615.7915 test Loss 0.0000 with MSE metric 6848.3023\n",
      "Time taken for 1 epoch: 2.213737964630127 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 5035.6222 test Loss 0.0000 with MSE metric 7232.3076\n",
      "Time taken for 1 epoch: 2.173903226852417 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 4400.4060 test Loss 0.0000 with MSE metric 7260.6828\n",
      "Time taken for 1 epoch: 2.1973352432250977 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 3818.8771 test Loss 0.0000 with MSE metric 7139.3505\n",
      "Time taken for 1 epoch: 2.173475980758667 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 3377.3323 test Loss 0.0000 with MSE metric 7320.3442\n",
      "Time taken for 1 epoch: 2.193161964416504 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 3016.0315 test Loss 0.0000 with MSE metric 7183.2540\n",
      "Time taken for 1 epoch: 2.188526153564453 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 2718.7706 test Loss 0.0000 with MSE metric 7131.7028\n",
      "Time taken for 1 epoch: 2.1595449447631836 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 2473.2044 test Loss 0.0000 with MSE metric 6785.6508\n",
      "Time taken for 1 epoch: 2.185100793838501 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 2269.1207 test Loss 0.0000 with MSE metric 6728.2597\n",
      "Time taken for 1 epoch: 2.209686040878296 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 2096.3661 test Loss 0.0000 with MSE metric 6665.9730\n",
      "Time taken for 1 epoch: 2.173264980316162 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 1948.6210 test Loss 0.0000 with MSE metric 6557.9725\n",
      "Time taken for 1 epoch: 2.175689935684204 secs\n",
      "\n",
      "Epoch 14 batch 0 train Loss 1821.1763 test Loss 0.0000 with MSE metric 6511.1752\n",
      "Time taken for 1 epoch: 2.2135379314422607 secs\n",
      "\n",
      "Epoch 15 batch 0 train Loss 1709.9681 test Loss 0.0000 with MSE metric 6397.4747\n",
      "Time taken for 1 epoch: 2.198988676071167 secs\n",
      "\n",
      "Epoch 16 batch 0 train Loss 1612.5843 test Loss 0.0000 with MSE metric 6371.8968\n",
      "Time taken for 1 epoch: 2.185091257095337 secs\n",
      "\n",
      "Epoch 17 batch 0 train Loss 1525.6567 test Loss 0.0000 with MSE metric 6243.6675\n",
      "Time taken for 1 epoch: 2.2171630859375 secs\n",
      "\n",
      "Epoch 18 batch 0 train Loss 1447.8614 test Loss 0.0000 with MSE metric 6116.4922\n",
      "Time taken for 1 epoch: 2.1821389198303223 secs\n",
      "\n",
      "Epoch 19 batch 0 train Loss 1378.7314 test Loss 0.0000 with MSE metric 6036.6100\n",
      "Time taken for 1 epoch: 2.190852165222168 secs\n",
      "\n",
      "Epoch 20 batch 0 train Loss 1316.2235 test Loss 0.0000 with MSE metric 5958.1579\n",
      "Time taken for 1 epoch: 2.1913416385650635 secs\n",
      "\n",
      "Epoch 21 batch 0 train Loss 1259.2366 test Loss 0.0000 with MSE metric 5855.5647\n",
      "Time taken for 1 epoch: 2.1716620922088623 secs\n",
      "\n",
      "Epoch 22 batch 0 train Loss 1208.5244 test Loss 0.0000 with MSE metric 5829.4521\n",
      "Time taken for 1 epoch: 2.189894914627075 secs\n",
      "\n",
      "Epoch 23 batch 0 train Loss 1161.4564 test Loss 0.0000 with MSE metric 5760.9211\n",
      "Time taken for 1 epoch: 2.1865999698638916 secs\n",
      "\n",
      "Epoch 24 batch 0 train Loss 1117.9662 test Loss 0.0000 with MSE metric 5680.5714\n",
      "Time taken for 1 epoch: 2.202049970626831 secs\n",
      "\n",
      "Epoch 25 batch 0 train Loss 1077.8242 test Loss 0.0000 with MSE metric 5606.5749\n",
      "Time taken for 1 epoch: 2.18499493598938 secs\n",
      "\n",
      "Epoch 26 batch 0 train Loss 1040.5357 test Loss 0.0000 with MSE metric 5527.0946\n",
      "Time taken for 1 epoch: 2.1735129356384277 secs\n",
      "\n",
      "Epoch 27 batch 0 train Loss 1005.9431 test Loss 0.0000 with MSE metric 5473.3301\n",
      "Time taken for 1 epoch: 2.187445640563965 secs\n",
      "\n",
      "Epoch 28 batch 0 train Loss 974.5117 test Loss 0.0000 with MSE metric 5420.7810\n",
      "Time taken for 1 epoch: 2.199190855026245 secs\n",
      "\n",
      "Epoch 29 batch 0 train Loss 945.1384 test Loss 0.0000 with MSE metric 5386.9382\n",
      "Time taken for 1 epoch: 2.1689021587371826 secs\n",
      "\n",
      "Epoch 30 batch 0 train Loss 917.9658 test Loss 0.0000 with MSE metric 5360.5651\n",
      "Time taken for 1 epoch: 2.176231861114502 secs\n",
      "\n",
      "Epoch 31 batch 0 train Loss 892.3563 test Loss 0.0000 with MSE metric 5337.1753\n",
      "Time taken for 1 epoch: 2.2145349979400635 secs\n",
      "\n",
      "Epoch 32 batch 0 train Loss 868.0492 test Loss 0.0000 with MSE metric 5302.5223\n",
      "Time taken for 1 epoch: 2.1840169429779053 secs\n",
      "\n",
      "Epoch 33 batch 0 train Loss 845.3023 test Loss 0.0000 with MSE metric 5317.8918\n",
      "Time taken for 1 epoch: 2.207641124725342 secs\n",
      "\n",
      "Epoch 34 batch 0 train Loss 822.9033 test Loss 0.0000 with MSE metric 5267.3339\n",
      "Time taken for 1 epoch: 2.192945718765259 secs\n",
      "\n",
      "Epoch 35 batch 0 train Loss 801.9199 test Loss 0.0000 with MSE metric 5224.2887\n",
      "Time taken for 1 epoch: 2.180145740509033 secs\n",
      "\n",
      "Epoch 36 batch 0 train Loss 782.6988 test Loss 0.0000 with MSE metric 5215.7835\n",
      "Time taken for 1 epoch: 2.1885199546813965 secs\n",
      "\n",
      "Epoch 37 batch 0 train Loss 763.9641 test Loss 0.0000 with MSE metric 5183.5119\n",
      "Time taken for 1 epoch: 2.221982002258301 secs\n",
      "\n",
      "Epoch 38 batch 0 train Loss 746.3824 test Loss 0.0000 with MSE metric 5165.7017\n",
      "Time taken for 1 epoch: 2.251955986022949 secs\n",
      "\n",
      "Epoch 39 batch 0 train Loss 728.9658 test Loss 0.0000 with MSE metric 5076.8300\n",
      "Time taken for 1 epoch: 2.229091167449951 secs\n",
      "\n",
      "Epoch 40 batch 0 train Loss 712.7839 test Loss 0.0000 with MSE metric 5047.2101\n",
      "Time taken for 1 epoch: 2.3212711811065674 secs\n",
      "\n",
      "Epoch 41 batch 0 train Loss 697.2365 test Loss 0.0000 with MSE metric 5023.2863\n",
      "Time taken for 1 epoch: 2.315661668777466 secs\n",
      "\n",
      "Epoch 42 batch 0 train Loss 682.5610 test Loss 0.0000 with MSE metric 5014.5719\n",
      "Time taken for 1 epoch: 2.455993175506592 secs\n",
      "\n",
      "Epoch 43 batch 0 train Loss 668.4237 test Loss 0.0000 with MSE metric 4979.6781\n",
      "Time taken for 1 epoch: 2.3091368675231934 secs\n",
      "\n",
      "Epoch 44 batch 0 train Loss 654.8139 test Loss 0.0000 with MSE metric 4957.5647\n",
      "Time taken for 1 epoch: 2.275862693786621 secs\n",
      "\n",
      "Epoch 45 batch 0 train Loss 641.7452 test Loss 0.0000 with MSE metric 4921.1148\n",
      "Time taken for 1 epoch: 2.240450143814087 secs\n",
      "\n",
      "Epoch 46 batch 0 train Loss 629.4580 test Loss 0.0000 with MSE metric 4912.2671\n",
      "Time taken for 1 epoch: 2.1758999824523926 secs\n",
      "\n",
      "Epoch 47 batch 0 train Loss 617.3705 test Loss 0.0000 with MSE metric 4881.8237\n",
      "Time taken for 1 epoch: 2.2103469371795654 secs\n",
      "\n",
      "Epoch 48 batch 0 train Loss 605.8621 test Loss 0.0000 with MSE metric 4869.3778\n",
      "Time taken for 1 epoch: 2.3315351009368896 secs\n",
      "\n",
      "Epoch 49 batch 0 train Loss 594.5689 test Loss 0.0000 with MSE metric 4826.3710\n",
      "Time taken for 1 epoch: 2.2460880279541016 secs\n",
      "\n",
      "Epoch 50 batch 0 train Loss 583.9219 test Loss 0.0000 with MSE metric 4803.9594\n",
      "Time taken for 1 epoch: 2.2639498710632324 secs\n",
      "\n",
      "Epoch 51 batch 0 train Loss 573.4628 test Loss 0.0000 with MSE metric 4786.2511\n",
      "Time taken for 1 epoch: 2.271764039993286 secs\n",
      "\n",
      "Epoch 52 batch 0 train Loss 563.5427 test Loss 0.0000 with MSE metric 4770.9325\n",
      "Time taken for 1 epoch: 2.2513222694396973 secs\n",
      "\n",
      "Epoch 53 batch 0 train Loss 553.8362 test Loss 0.0000 with MSE metric 4744.7926\n",
      "Time taken for 1 epoch: 2.2049100399017334 secs\n",
      "\n",
      "Epoch 54 batch 0 train Loss 544.4172 test Loss 0.0000 with MSE metric 4716.0572\n",
      "Time taken for 1 epoch: 2.2624170780181885 secs\n",
      "\n",
      "Epoch 55 batch 0 train Loss 535.4712 test Loss 0.0000 with MSE metric 4708.7811\n",
      "Time taken for 1 epoch: 2.235982656478882 secs\n",
      "\n",
      "Epoch 56 batch 0 train Loss 526.7667 test Loss 0.0000 with MSE metric 4698.2296\n",
      "Time taken for 1 epoch: 2.2253570556640625 secs\n",
      "\n",
      "Epoch 57 batch 0 train Loss 518.4136 test Loss 0.0000 with MSE metric 4692.9232\n",
      "Time taken for 1 epoch: 2.3116517066955566 secs\n",
      "\n",
      "Epoch 58 batch 0 train Loss 510.3926 test Loss 0.0000 with MSE metric 4716.6025\n",
      "Time taken for 1 epoch: 2.226047992706299 secs\n",
      "\n",
      "Epoch 59 batch 0 train Loss 502.4755 test Loss 0.0000 with MSE metric 4694.8545\n",
      "Time taken for 1 epoch: 2.207200050354004 secs\n",
      "\n",
      "Epoch 60 batch 0 train Loss 494.8862 test Loss 0.0000 with MSE metric 4698.0619\n",
      "Time taken for 1 epoch: 2.2182631492614746 secs\n",
      "\n",
      "Epoch 61 batch 0 train Loss 487.5192 test Loss 0.0000 with MSE metric 4710.7638\n",
      "Time taken for 1 epoch: 2.1849238872528076 secs\n",
      "\n",
      "Epoch 62 batch 0 train Loss 480.3095 test Loss 0.0000 with MSE metric 4704.9859\n",
      "Time taken for 1 epoch: 2.1728477478027344 secs\n",
      "\n",
      "Epoch 63 batch 0 train Loss 473.3731 test Loss 0.0000 with MSE metric 4714.8152\n",
      "Time taken for 1 epoch: 2.1898412704467773 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 batch 0 train Loss 466.4996 test Loss 0.0000 with MSE metric 4694.6869\n",
      "Time taken for 1 epoch: 2.1731910705566406 secs\n",
      "\n",
      "Epoch 65 batch 0 train Loss 459.9390 test Loss 0.0000 with MSE metric 4719.5069\n",
      "Time taken for 1 epoch: 2.1990132331848145 secs\n",
      "\n",
      "Epoch 66 batch 0 train Loss 453.5481 test Loss 0.0000 with MSE metric 4721.3711\n",
      "Time taken for 1 epoch: 2.1838648319244385 secs\n",
      "\n",
      "Epoch 67 batch 0 train Loss 447.2627 test Loss 0.0000 with MSE metric 4711.5311\n",
      "Time taken for 1 epoch: 2.1827070713043213 secs\n",
      "\n",
      "Epoch 68 batch 0 train Loss 441.1226 test Loss 0.0000 with MSE metric 4690.5008\n",
      "Time taken for 1 epoch: 2.195844888687134 secs\n",
      "\n",
      "Epoch 69 batch 0 train Loss 435.1559 test Loss 0.0000 with MSE metric 4703.4478\n",
      "Time taken for 1 epoch: 2.1923351287841797 secs\n",
      "\n",
      "Epoch 70 batch 0 train Loss 429.3686 test Loss 0.0000 with MSE metric 4700.4374\n",
      "Time taken for 1 epoch: 2.2219629287719727 secs\n",
      "\n",
      "Epoch 71 batch 0 train Loss 423.7252 test Loss 0.0000 with MSE metric 4704.2592\n",
      "Time taken for 1 epoch: 2.345412015914917 secs\n",
      "\n",
      "Epoch 72 batch 0 train Loss 418.1684 test Loss 0.0000 with MSE metric 4679.5749\n",
      "Time taken for 1 epoch: 2.2555830478668213 secs\n",
      "\n",
      "Epoch 73 batch 0 train Loss 412.8035 test Loss 0.0000 with MSE metric 4691.9564\n",
      "Time taken for 1 epoch: 2.279513120651245 secs\n",
      "\n",
      "Epoch 74 batch 0 train Loss 407.5669 test Loss 0.0000 with MSE metric 4689.1086\n",
      "Time taken for 1 epoch: 2.26600980758667 secs\n",
      "\n",
      "Epoch 75 batch 0 train Loss 402.4343 test Loss 0.0000 with MSE metric 4670.4585\n",
      "Time taken for 1 epoch: 2.237015724182129 secs\n",
      "\n",
      "Epoch 76 batch 0 train Loss 397.4527 test Loss 0.0000 with MSE metric 4680.2583\n",
      "Time taken for 1 epoch: 2.246438980102539 secs\n",
      "\n",
      "Epoch 77 batch 0 train Loss 392.5194 test Loss 0.0000 with MSE metric 4642.1587\n",
      "Time taken for 1 epoch: 2.3354740142822266 secs\n",
      "\n",
      "Epoch 78 batch 0 train Loss 387.7336 test Loss 0.0000 with MSE metric 4629.0810\n",
      "Time taken for 1 epoch: 2.19726300239563 secs\n",
      "\n",
      "Epoch 79 batch 0 train Loss 383.0734 test Loss 0.0000 with MSE metric 4616.7369\n",
      "Time taken for 1 epoch: 2.2589340209960938 secs\n",
      "\n",
      "Epoch 80 batch 0 train Loss 378.5071 test Loss 0.0000 with MSE metric 4600.2502\n",
      "Time taken for 1 epoch: 2.2331128120422363 secs\n",
      "\n",
      "Epoch 81 batch 0 train Loss 374.1042 test Loss 0.0000 with MSE metric 4617.0487\n",
      "Time taken for 1 epoch: 2.3030052185058594 secs\n",
      "\n",
      "Epoch 82 batch 0 train Loss 369.8045 test Loss 0.0000 with MSE metric 4653.9139\n",
      "Time taken for 1 epoch: 2.2395007610321045 secs\n",
      "\n",
      "Epoch 83 batch 0 train Loss 365.5684 test Loss 0.0000 with MSE metric 4652.5361\n",
      "Time taken for 1 epoch: 2.292722225189209 secs\n",
      "\n",
      "Epoch 84 batch 0 train Loss 361.4273 test Loss 0.0000 with MSE metric 4655.5484\n",
      "Time taken for 1 epoch: 2.2551767826080322 secs\n",
      "\n",
      "Epoch 85 batch 0 train Loss 357.3859 test Loss 0.0000 with MSE metric 4664.6660\n",
      "Time taken for 1 epoch: 2.2581052780151367 secs\n",
      "\n",
      "Epoch 86 batch 0 train Loss 353.4288 test Loss 0.0000 with MSE metric 4670.5071\n",
      "Time taken for 1 epoch: 2.2875850200653076 secs\n",
      "\n",
      "Epoch 87 batch 0 train Loss 349.5578 test Loss 0.0000 with MSE metric 4659.5039\n",
      "Time taken for 1 epoch: 2.236358880996704 secs\n",
      "\n",
      "Epoch 88 batch 0 train Loss 345.7621 test Loss 0.0000 with MSE metric 4649.6605\n",
      "Time taken for 1 epoch: 2.2816600799560547 secs\n",
      "\n",
      "Epoch 89 batch 0 train Loss 342.0804 test Loss 0.0000 with MSE metric 4652.4669\n",
      "Time taken for 1 epoch: 2.372373104095459 secs\n",
      "\n",
      "Epoch 90 batch 0 train Loss 338.4869 test Loss 0.0000 with MSE metric 4668.8449\n",
      "Time taken for 1 epoch: 2.18493390083313 secs\n",
      "\n",
      "Epoch 91 batch 0 train Loss 334.9460 test Loss 0.0000 with MSE metric 4676.3402\n",
      "Time taken for 1 epoch: 2.2080421447753906 secs\n",
      "\n",
      "Epoch 92 batch 0 train Loss 331.5094 test Loss 0.0000 with MSE metric 4705.2062\n",
      "Time taken for 1 epoch: 2.187211036682129 secs\n",
      "\n",
      "Epoch 93 batch 0 train Loss 328.1414 test Loss 0.0000 with MSE metric 4742.5662\n",
      "Time taken for 1 epoch: 2.1851909160614014 secs\n",
      "\n",
      "Epoch 94 batch 0 train Loss 324.8055 test Loss 0.0000 with MSE metric 4750.7594\n",
      "Time taken for 1 epoch: 2.1989691257476807 secs\n",
      "\n",
      "Epoch 95 batch 0 train Loss 321.5519 test Loss 0.0000 with MSE metric 4752.7651\n",
      "Time taken for 1 epoch: 2.1943109035491943 secs\n",
      "\n",
      "Epoch 96 batch 0 train Loss 318.3672 test Loss 0.0000 with MSE metric 4747.6703\n",
      "Time taken for 1 epoch: 2.1790611743927 secs\n",
      "\n",
      "Epoch 97 batch 0 train Loss 315.2322 test Loss 0.0000 with MSE metric 4737.0803\n",
      "Time taken for 1 epoch: 2.199597120285034 secs\n",
      "\n",
      "Epoch 98 batch 0 train Loss 312.1460 test Loss 0.0000 with MSE metric 4730.3582\n",
      "Time taken for 1 epoch: 2.2060041427612305 secs\n",
      "\n",
      "Epoch 99 batch 0 train Loss 309.1352 test Loss 0.0000 with MSE metric 4719.8944\n",
      "Time taken for 1 epoch: 2.1771790981292725 secs\n",
      "\n",
      "Epoch 100 batch 0 train Loss 306.1869 test Loss 0.0000 with MSE metric 4737.7996\n",
      "Time taken for 1 epoch: 2.192389965057373 secs\n",
      "\n",
      "Epoch 101 batch 0 train Loss 303.2873 test Loss 0.0000 with MSE metric 4731.4630\n",
      "Time taken for 1 epoch: 2.1915810108184814 secs\n",
      "\n",
      "Epoch 102 batch 0 train Loss 300.4515 test Loss 0.0000 with MSE metric 4732.9355\n",
      "Time taken for 1 epoch: 2.1935880184173584 secs\n",
      "\n",
      "Epoch 103 batch 0 train Loss 297.6897 test Loss 0.0000 with MSE metric 4764.7986\n",
      "Time taken for 1 epoch: 2.2104578018188477 secs\n",
      "\n",
      "Epoch 104 batch 0 train Loss 294.9445 test Loss 0.0000 with MSE metric 4749.6355\n",
      "Time taken for 1 epoch: 2.1949551105499268 secs\n",
      "\n",
      "Epoch 105 batch 0 train Loss 292.2693 test Loss 0.0000 with MSE metric 4773.8898\n",
      "Time taken for 1 epoch: 2.185493230819702 secs\n",
      "\n",
      "Epoch 106 batch 0 train Loss 289.6323 test Loss 0.0000 with MSE metric 4773.2300\n",
      "Time taken for 1 epoch: 2.1631500720977783 secs\n",
      "\n",
      "Epoch 107 batch 0 train Loss 287.0644 test Loss 0.0000 with MSE metric 4798.2175\n",
      "Time taken for 1 epoch: 2.1901450157165527 secs\n",
      "\n",
      "Epoch 108 batch 0 train Loss 284.5225 test Loss 0.0000 with MSE metric 4785.1733\n",
      "Time taken for 1 epoch: 2.204035997390747 secs\n",
      "\n",
      "Epoch 109 batch 0 train Loss 282.0470 test Loss 0.0000 with MSE metric 4806.3316\n",
      "Time taken for 1 epoch: 2.181623935699463 secs\n",
      "\n",
      "Epoch 110 batch 0 train Loss 279.6041 test Loss 0.0000 with MSE metric 4817.2351\n",
      "Time taken for 1 epoch: 2.1741440296173096 secs\n",
      "\n",
      "Epoch 111 batch 0 train Loss 277.1964 test Loss 0.0000 with MSE metric 4812.7027\n",
      "Time taken for 1 epoch: 2.1677229404449463 secs\n",
      "\n",
      "Epoch 112 batch 0 train Loss 274.8351 test Loss 0.0000 with MSE metric 4802.2161\n",
      "Time taken for 1 epoch: 2.1852827072143555 secs\n",
      "\n",
      "Epoch 113 batch 0 train Loss 272.5097 test Loss 0.0000 with MSE metric 4788.7404\n",
      "Time taken for 1 epoch: 2.1839072704315186 secs\n",
      "\n",
      "Epoch 114 batch 0 train Loss 270.2316 test Loss 0.0000 with MSE metric 4805.9870\n",
      "Time taken for 1 epoch: 2.174021005630493 secs\n",
      "\n",
      "Epoch 115 batch 0 train Loss 267.9861 test Loss 0.0000 with MSE metric 4798.1218\n",
      "Time taken for 1 epoch: 2.177680730819702 secs\n",
      "\n",
      "Epoch 116 batch 0 train Loss 265.7809 test Loss 0.0000 with MSE metric 4800.7623\n",
      "Time taken for 1 epoch: 2.1585490703582764 secs\n",
      "\n",
      "Epoch 117 batch 0 train Loss 263.6070 test Loss 0.0000 with MSE metric 4785.6528\n",
      "Time taken for 1 epoch: 2.1832211017608643 secs\n",
      "\n",
      "Epoch 118 batch 0 train Loss 261.4843 test Loss 0.0000 with MSE metric 4787.6173\n",
      "Time taken for 1 epoch: 2.1671102046966553 secs\n",
      "\n",
      "Epoch 119 batch 0 train Loss 259.3785 test Loss 0.0000 with MSE metric 4769.6381\n",
      "Time taken for 1 epoch: 2.1972970962524414 secs\n",
      "\n",
      "Epoch 120 batch 0 train Loss 257.3268 test Loss 0.0000 with MSE metric 4782.1797\n",
      "Time taken for 1 epoch: 2.193859815597534 secs\n",
      "\n",
      "Epoch 121 batch 0 train Loss 255.2973 test Loss 0.0000 with MSE metric 4770.6499\n",
      "Time taken for 1 epoch: 2.1916539669036865 secs\n",
      "\n",
      "Epoch 122 batch 0 train Loss 253.3011 test Loss 0.0000 with MSE metric 4767.2491\n",
      "Time taken for 1 epoch: 2.2069339752197266 secs\n",
      "\n",
      "Epoch 123 batch 0 train Loss 251.3417 test Loss 0.0000 with MSE metric 4782.5483\n",
      "Time taken for 1 epoch: 2.19236421585083 secs\n",
      "\n",
      "Epoch 124 batch 0 train Loss 249.4159 test Loss 0.0000 with MSE metric 4786.9517\n",
      "Time taken for 1 epoch: 2.213747024536133 secs\n",
      "\n",
      "Epoch 125 batch 0 train Loss 247.5192 test Loss 0.0000 with MSE metric 4808.9319\n",
      "Time taken for 1 epoch: 2.194040060043335 secs\n",
      "\n",
      "Epoch 126 batch 0 train Loss 245.6479 test Loss 0.0000 with MSE metric 4814.8034\n",
      "Time taken for 1 epoch: 2.1800131797790527 secs\n",
      "\n",
      "Epoch 127 batch 0 train Loss 243.8018 test Loss 0.0000 with MSE metric 4815.7469\n",
      "Time taken for 1 epoch: 2.1902718544006348 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 batch 0 train Loss 241.9888 test Loss 0.0000 with MSE metric 4813.6963\n",
      "Time taken for 1 epoch: 2.2110350131988525 secs\n",
      "\n",
      "Epoch 129 batch 0 train Loss 240.2025 test Loss 0.0000 with MSE metric 4812.0421\n",
      "Time taken for 1 epoch: 2.1772427558898926 secs\n",
      "\n",
      "Epoch 130 batch 0 train Loss 238.4435 test Loss 0.0000 with MSE metric 4810.4142\n",
      "Time taken for 1 epoch: 2.187378168106079 secs\n",
      "\n",
      "Epoch 131 batch 0 train Loss 236.7071 test Loss 0.0000 with MSE metric 4806.8122\n",
      "Time taken for 1 epoch: 2.1795690059661865 secs\n",
      "\n",
      "Epoch 132 batch 0 train Loss 234.9964 test Loss 0.0000 with MSE metric 4786.6990\n",
      "Time taken for 1 epoch: 2.1698381900787354 secs\n",
      "\n",
      "Epoch 133 batch 0 train Loss 233.3169 test Loss 0.0000 with MSE metric 4798.1238\n",
      "Time taken for 1 epoch: 2.190953016281128 secs\n",
      "\n",
      "Epoch 134 batch 0 train Loss 231.6607 test Loss 0.0000 with MSE metric 4815.2159\n",
      "Time taken for 1 epoch: 2.164705991744995 secs\n",
      "\n",
      "Epoch 135 batch 0 train Loss 230.0235 test Loss 0.0000 with MSE metric 4800.0179\n",
      "Time taken for 1 epoch: 2.1766819953918457 secs\n",
      "\n",
      "Epoch 136 batch 0 train Loss 228.4215 test Loss 0.0000 with MSE metric 4788.4764\n",
      "Time taken for 1 epoch: 2.1694650650024414 secs\n",
      "\n",
      "Epoch 137 batch 0 train Loss 226.8308 test Loss 0.0000 with MSE metric 4775.1635\n",
      "Time taken for 1 epoch: 2.176689863204956 secs\n",
      "\n",
      "Epoch 138 batch 0 train Loss 225.2720 test Loss 0.0000 with MSE metric 4770.7660\n",
      "Time taken for 1 epoch: 2.173261880874634 secs\n",
      "\n",
      "Epoch 139 batch 0 train Loss 223.7270 test Loss 0.0000 with MSE metric 4751.7167\n",
      "Time taken for 1 epoch: 2.177945852279663 secs\n",
      "\n",
      "Epoch 140 batch 0 train Loss 222.2108 test Loss 0.0000 with MSE metric 4769.2131\n",
      "Time taken for 1 epoch: 2.193542003631592 secs\n",
      "\n",
      "Epoch 141 batch 0 train Loss 220.7125 test Loss 0.0000 with MSE metric 4763.3221\n",
      "Time taken for 1 epoch: 2.1978609561920166 secs\n",
      "\n",
      "Epoch 142 batch 0 train Loss 219.2355 test Loss 0.0000 with MSE metric 4763.0125\n",
      "Time taken for 1 epoch: 2.1544482707977295 secs\n",
      "\n",
      "Epoch 143 batch 0 train Loss 217.7777 test Loss 0.0000 with MSE metric 4757.3523\n",
      "Time taken for 1 epoch: 2.177560806274414 secs\n",
      "\n",
      "Epoch 144 batch 0 train Loss 216.3378 test Loss 0.0000 with MSE metric 4763.8331\n",
      "Time taken for 1 epoch: 2.1560850143432617 secs\n",
      "\n",
      "Epoch 145 batch 0 train Loss 214.9205 test Loss 0.0000 with MSE metric 4769.5679\n",
      "Time taken for 1 epoch: 2.268765926361084 secs\n",
      "\n",
      "Epoch 146 batch 0 train Loss 213.5236 test Loss 0.0000 with MSE metric 4767.4354\n",
      "Time taken for 1 epoch: 2.2409839630126953 secs\n",
      "\n",
      "Epoch 147 batch 0 train Loss 212.1427 test Loss 0.0000 with MSE metric 4761.6285\n",
      "Time taken for 1 epoch: 2.1617772579193115 secs\n",
      "\n",
      "Epoch 148 batch 0 train Loss 210.7811 test Loss 0.0000 with MSE metric 4756.7769\n",
      "Time taken for 1 epoch: 2.178133010864258 secs\n",
      "\n",
      "Epoch 149 batch 0 train Loss 209.4365 test Loss 0.0000 with MSE metric 4762.9836\n",
      "Time taken for 1 epoch: 2.2159430980682373 secs\n",
      "\n",
      "Epoch 150 batch 0 train Loss 208.1135 test Loss 0.0000 with MSE metric 4780.0559\n",
      "Time taken for 1 epoch: 2.181030035018921 secs\n",
      "\n",
      "Epoch 151 batch 0 train Loss 206.8018 test Loss 0.0000 with MSE metric 4778.8855\n",
      "Time taken for 1 epoch: 2.2071070671081543 secs\n",
      "\n",
      "Epoch 152 batch 0 train Loss 205.5093 test Loss 0.0000 with MSE metric 4780.6605\n",
      "Time taken for 1 epoch: 2.17405104637146 secs\n",
      "\n",
      "Epoch 153 batch 0 train Loss 204.2354 test Loss 0.0000 with MSE metric 4776.1194\n",
      "Time taken for 1 epoch: 2.1937267780303955 secs\n",
      "\n",
      "Epoch 154 batch 0 train Loss 202.9741 test Loss 0.0000 with MSE metric 4770.9959\n",
      "Time taken for 1 epoch: 2.1667208671569824 secs\n",
      "\n",
      "Epoch 155 batch 0 train Loss 201.7310 test Loss 0.0000 with MSE metric 4774.1224\n",
      "Time taken for 1 epoch: 2.1809210777282715 secs\n",
      "\n",
      "Epoch 156 batch 0 train Loss 200.4976 test Loss 0.0000 with MSE metric 4760.5811\n",
      "Time taken for 1 epoch: 2.1966781616210938 secs\n",
      "\n",
      "Epoch 157 batch 0 train Loss 199.2934 test Loss 0.0000 with MSE metric 4761.4732\n",
      "Time taken for 1 epoch: 2.173225164413452 secs\n",
      "\n",
      "Epoch 158 batch 0 train Loss 198.1021 test Loss 0.0000 with MSE metric 4767.3913\n",
      "Time taken for 1 epoch: 2.1805639266967773 secs\n",
      "\n",
      "Epoch 159 batch 0 train Loss 196.9226 test Loss 0.0000 with MSE metric 4768.5664\n",
      "Time taken for 1 epoch: 2.163940906524658 secs\n",
      "\n",
      "Epoch 160 batch 0 train Loss 195.7524 test Loss 0.0000 with MSE metric 4759.8225\n",
      "Time taken for 1 epoch: 2.166264057159424 secs\n",
      "\n",
      "Epoch 161 batch 0 train Loss 194.6007 test Loss 0.0000 with MSE metric 4748.0008\n",
      "Time taken for 1 epoch: 2.198568105697632 secs\n",
      "\n",
      "Epoch 162 batch 0 train Loss 193.4607 test Loss 0.0000 with MSE metric 4743.3147\n",
      "Time taken for 1 epoch: 2.1684789657592773 secs\n",
      "\n",
      "Epoch 163 batch 0 train Loss 192.3380 test Loss 0.0000 with MSE metric 4763.2815\n",
      "Time taken for 1 epoch: 2.1874518394470215 secs\n",
      "\n",
      "Epoch 164 batch 0 train Loss 191.2265 test Loss 0.0000 with MSE metric 4758.5464\n",
      "Time taken for 1 epoch: 2.165564775466919 secs\n",
      "\n",
      "Epoch 165 batch 0 train Loss 190.1265 test Loss 0.0000 with MSE metric 4756.3562\n",
      "Time taken for 1 epoch: 2.168147087097168 secs\n",
      "\n",
      "Epoch 166 batch 0 train Loss 189.0420 test Loss 0.0000 with MSE metric 4759.5076\n",
      "Time taken for 1 epoch: 2.1358699798583984 secs\n",
      "\n",
      "Epoch 167 batch 0 train Loss 187.9729 test Loss 0.0000 with MSE metric 4770.0764\n",
      "Time taken for 1 epoch: 2.1636481285095215 secs\n",
      "\n",
      "Epoch 168 batch 0 train Loss 186.9126 test Loss 0.0000 with MSE metric 4768.0296\n",
      "Time taken for 1 epoch: 2.2380809783935547 secs\n",
      "\n",
      "Epoch 169 batch 0 train Loss 185.8611 test Loss 0.0000 with MSE metric 4759.5496\n",
      "Time taken for 1 epoch: 2.2144339084625244 secs\n",
      "\n",
      "Epoch 170 batch 0 train Loss 184.8293 test Loss 0.0000 with MSE metric 4772.8629\n",
      "Time taken for 1 epoch: 2.1471688747406006 secs\n",
      "\n",
      "Epoch 171 batch 0 train Loss 183.8069 test Loss 0.0000 with MSE metric 4767.0818\n",
      "Time taken for 1 epoch: 2.329267978668213 secs\n",
      "\n",
      "Epoch 172 batch 0 train Loss 182.7983 test Loss 0.0000 with MSE metric 4780.6677\n",
      "Time taken for 1 epoch: 2.2388718128204346 secs\n",
      "\n",
      "Epoch 173 batch 0 train Loss 181.7929 test Loss 0.0000 with MSE metric 4768.2234\n",
      "Time taken for 1 epoch: 2.3273818492889404 secs\n",
      "\n",
      "Epoch 174 batch 0 train Loss 180.8038 test Loss 0.0000 with MSE metric 4767.5342\n",
      "Time taken for 1 epoch: 2.303128957748413 secs\n",
      "\n",
      "Epoch 175 batch 0 train Loss 179.8282 test Loss 0.0000 with MSE metric 4775.8334\n",
      "Time taken for 1 epoch: 2.3257899284362793 secs\n",
      "\n",
      "Epoch 176 batch 0 train Loss 178.8639 test Loss 0.0000 with MSE metric 4780.2873\n",
      "Time taken for 1 epoch: 2.3721258640289307 secs\n",
      "\n",
      "Epoch 177 batch 0 train Loss 177.9100 test Loss 0.0000 with MSE metric 4783.9747\n",
      "Time taken for 1 epoch: 2.4490671157836914 secs\n",
      "\n",
      "Epoch 178 batch 0 train Loss 176.9659 test Loss 0.0000 with MSE metric 4780.7995\n",
      "Time taken for 1 epoch: 2.4046878814697266 secs\n",
      "\n",
      "Epoch 179 batch 0 train Loss 176.0312 test Loss 0.0000 with MSE metric 4779.3211\n",
      "Time taken for 1 epoch: 2.180803060531616 secs\n",
      "\n",
      "Epoch 180 batch 0 train Loss 175.1072 test Loss 0.0000 with MSE metric 4781.5492\n",
      "Time taken for 1 epoch: 2.207653760910034 secs\n",
      "\n",
      "Epoch 181 batch 0 train Loss 174.1934 test Loss 0.0000 with MSE metric 4791.4352\n",
      "Time taken for 1 epoch: 2.2010700702667236 secs\n",
      "\n",
      "Epoch 182 batch 0 train Loss 173.2866 test Loss 0.0000 with MSE metric 4781.6546\n",
      "Time taken for 1 epoch: 2.1520278453826904 secs\n",
      "\n",
      "Epoch 183 batch 0 train Loss 172.3907 test Loss 0.0000 with MSE metric 4775.0291\n",
      "Time taken for 1 epoch: 2.33413028717041 secs\n",
      "\n",
      "Epoch 184 batch 0 train Loss 171.5031 test Loss 0.0000 with MSE metric 4766.3130\n",
      "Time taken for 1 epoch: 2.427968978881836 secs\n",
      "\n",
      "Epoch 185 batch 0 train Loss 170.6261 test Loss 0.0000 with MSE metric 4760.4691\n",
      "Time taken for 1 epoch: 2.336284875869751 secs\n",
      "\n",
      "Epoch 186 batch 0 train Loss 169.7603 test Loss 0.0000 with MSE metric 4763.9025\n",
      "Time taken for 1 epoch: 2.221036911010742 secs\n",
      "\n",
      "Epoch 187 batch 0 train Loss 168.9044 test Loss 0.0000 with MSE metric 4760.6829\n",
      "Time taken for 1 epoch: 2.2437291145324707 secs\n",
      "\n",
      "Epoch 188 batch 0 train Loss 168.0504 test Loss 0.0000 with MSE metric 4745.9364\n",
      "Time taken for 1 epoch: 2.2511110305786133 secs\n",
      "\n",
      "Epoch 189 batch 0 train Loss 167.2107 test Loss 0.0000 with MSE metric 4743.8320\n",
      "Time taken for 1 epoch: 2.3328471183776855 secs\n",
      "\n",
      "Epoch 190 batch 0 train Loss 166.3803 test Loss 0.0000 with MSE metric 4745.6154\n",
      "Time taken for 1 epoch: 2.3467557430267334 secs\n",
      "\n",
      "Epoch 191 batch 0 train Loss 165.5555 test Loss 0.0000 with MSE metric 4736.4339\n",
      "Time taken for 1 epoch: 2.330643892288208 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 batch 0 train Loss 164.7428 test Loss 0.0000 with MSE metric 4742.2826\n",
      "Time taken for 1 epoch: 2.2510969638824463 secs\n",
      "\n",
      "Epoch 193 batch 0 train Loss 163.9398 test Loss 0.0000 with MSE metric 4752.6094\n",
      "Time taken for 1 epoch: 2.34002685546875 secs\n",
      "\n",
      "Epoch 194 batch 0 train Loss 163.1388 test Loss 0.0000 with MSE metric 4741.7842\n",
      "Time taken for 1 epoch: 2.359250783920288 secs\n",
      "\n",
      "Epoch 195 batch 0 train Loss 162.3478 test Loss 0.0000 with MSE metric 4735.2095\n",
      "Time taken for 1 epoch: 2.54773211479187 secs\n",
      "\n",
      "Epoch 196 batch 0 train Loss 161.5628 test Loss 0.0000 with MSE metric 4730.7143\n",
      "Time taken for 1 epoch: 2.5318751335144043 secs\n",
      "\n",
      "Epoch 197 batch 0 train Loss 160.7925 test Loss 0.0000 with MSE metric 4734.9205\n",
      "Time taken for 1 epoch: 2.4583370685577393 secs\n",
      "\n",
      "Epoch 198 batch 0 train Loss 160.0208 test Loss 0.0000 with MSE metric 4722.9138\n",
      "Time taken for 1 epoch: 2.4237418174743652 secs\n",
      "\n",
      "Epoch 199 batch 0 train Loss 159.2588 test Loss 0.0000 with MSE metric 4714.9137\n",
      "Time taken for 1 epoch: 2.2363522052764893 secs\n",
      "\n",
      "Epoch 200 batch 0 train Loss 158.5033 test Loss 0.0000 with MSE metric 4705.4107\n",
      "Time taken for 1 epoch: 2.4457740783691406 secs\n",
      "\n",
      "Epoch 201 batch 0 train Loss 157.7564 test Loss 0.0000 with MSE metric 4697.6002\n",
      "Time taken for 1 epoch: 2.2483489513397217 secs\n",
      "\n",
      "Epoch 202 batch 0 train Loss 157.0175 test Loss 0.0000 with MSE metric 4690.7283\n",
      "Time taken for 1 epoch: 2.2640902996063232 secs\n",
      "\n",
      "Epoch 203 batch 0 train Loss 156.2862 test Loss 0.0000 with MSE metric 4691.9188\n",
      "Time taken for 1 epoch: 2.2738327980041504 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam(0.007)\n",
    "    decoder = fox_model.Decoder(16)\n",
    "    EPOCHS = 750\n",
    "    batch_s  = 32\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(tar_tr.shape[0] * 4 / batch_s)\n",
    "#     tf.random.set_seed(1)    \n",
    "    checkpoint = tf.train.Checkpoint(optimizer = optimizer_c, model = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT_fox/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr , batch_pos_mask, _ = batch_creator.create_batch_foxes(tok_pos_tr, tim_pos_tr, tar_tr , pos_mask, batch_s= 32)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                tar_inp, tar_real, pred, pred_sig, combined_mask_tar  = train_step(batch_tok_pos_tr, batch_tim_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 50 == 0:\n",
    "#                     batch_tok_pos_te, batch_tim_pos_te, batch_tar_te , batch_pos_mask_te, _ = batch_creator.create_batch_foxes(token_te, pad_pos_te, tar_te, pp_te)\n",
    "#                     tar_real_te, pred, pred_sig = test_step(batch_tok_pos_te, batch_tim_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "#                     helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "#                     checkpoint.save(folder + '/')\n",
    "                step += 1\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_pos_tr[6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = decoder.e1\n",
    "weights = e.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01198372, -0.022828  ,  0.01366034, -0.04658762,  0.01775511,\n",
       "       -0.02220659, -0.03331034, -0.02029271])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0 ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01487688, -0.02089084, -0.00441254,  0.04802811, -0.04071961,\n",
       "        0.04035367, -0.02769675, -0.02957628])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1 ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00312794, -0.0276023 , -0.05479827, -0.01871443,  0.02734485,\n",
       "       -0.00082137,  0.00663965,  0.03001904])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02947604,  0.00421871,  0.04572925,  0.01271821, -0.01137066,\n",
       "       -0.04584723, -0.03022694, -0.04152265])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0121183 , -0.03513595, -0.01996054,  0.0015047 , -0.05293077,\n",
       "        0.04234427, -0.02420702,  0.01337569])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(148,), dtype=float64, numpy=\n",
       "array([11.44574197, 11.11333129, 10.78999914, 10.47565778, 10.17020513,\n",
       "        9.87352594,  9.58549299,  9.30596822,  9.03480393,  8.77184388,\n",
       "        8.51692443,  8.26987559,  8.03052205,  7.79868417,  7.57417887,\n",
       "        7.35682054,  7.14642179,  6.94279426,  6.74574929,  6.55509852,\n",
       "        6.37065451,  6.19223122,  6.01964452,  5.85271255,  5.69125611,\n",
       "        5.53509901,  5.38406826,  5.23799439,  5.09671158,  4.96005788,\n",
       "        4.82787529,  4.76290531,  4.69681514,  4.62935489,  4.56219694,\n",
       "        4.49323823,  4.42996997,  4.36730884,  4.30534293,  4.24414548,\n",
       "        4.18377688,  4.12428635,  4.06571341,  4.00808917,  3.95143747,\n",
       "        3.89577578,  3.84111611,  3.78746569,  3.73482768,  3.68320169,\n",
       "        3.63258426,  3.58296936,  3.53434871,  3.48671211,  3.4400478 ,\n",
       "        3.39434264,  3.34958238,  3.30575187,  3.26283517,  3.22081579,\n",
       "        3.17967674,  3.1394007 ,  3.09997013,  3.06136729,  3.0235744 ,\n",
       "        2.98657366,  2.95034732,  2.91487772,  2.88014736,  2.84613891,\n",
       "        2.81283525,  2.7802195 ,  2.74827504,  2.71698553,  2.68633492,\n",
       "        2.65630746,  2.62688774,  2.59806064,  2.56981138,  2.54212552,\n",
       "        2.51498895,  2.48838787,  2.46230884,  2.43673875,  2.4116648 ,\n",
       "        2.38707452,  2.36295578,  2.33929674,  2.31608588,  2.29331201,\n",
       "        2.2709642 ,  2.24903185,  2.22750463,  2.20637249,  2.18562567,\n",
       "        2.16525468,  2.14525027,  2.12560347,  2.10630556,  2.08734804,\n",
       "        2.06872269,  2.05042147,  2.0324366 ,  2.01476052,  1.99738586,\n",
       "        1.98030547,  1.9635124 ,  1.94699989,  1.93076138,  1.91479049,\n",
       "        1.89908101,  1.8836269 ,  1.2498571 ,  1.23935222,  1.22902537,\n",
       "        1.2188722 ,  1.20888852,  1.19907026,  1.18941346,  1.1799143 ,\n",
       "        1.17056905,  1.16137413,  1.15232602,  1.14342133,  1.13465676,\n",
       "        1.18305574,  1.18687569,  1.18178716,  1.18178716,  1.18178716,\n",
       "        1.18178716,  1.18178716,  1.18178716,  1.18178716,  1.18178716,\n",
       "        1.18178716,  1.18178716,  1.18178716,  1.18178716,  1.18178716,\n",
       "        1.18178716,  1.18178716,  1.18178716,  1.18178716,  1.18178716,\n",
       "        1.18178716,  1.18178716,  1.18178716])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(148,), dtype=float32, numpy=\n",
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mask_tar[1, :, 44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(148,), dtype=float64, numpy=\n",
       "array([183.        , 170.5377    , 158.71284099, 147.52172089,\n",
       "       136.95626314, 127.00456081, 117.65142542, 108.87892606,\n",
       "       100.66690691,  92.99347403,  85.83544469,  79.16875492,\n",
       "        72.96882289,  67.21086754,  61.87018312,  56.92237156,\n",
       "        52.34353511,  48.11043257,  44.2006024 ,  40.59245631,\n",
       "        37.26534692, 183.        ,  23.        ,   5.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ])>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_inp[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(148,), dtype=float64, numpy=\n",
       "array([170.5377    , 158.71284099, 147.52172089, 136.95626314,\n",
       "       127.00456081, 117.65142542, 108.87892606, 100.66690691,\n",
       "        92.99347403,  85.83544469,  79.16875492,  72.96882289,\n",
       "        67.21086754,  61.87018312,  56.92237156,  52.34353511,\n",
       "        48.11043257,  44.2006024 ,  40.59245631,  37.26534692,\n",
       "       183.        ,  23.        ,   5.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ])>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_real[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(148,), dtype=float64, numpy=\n",
       "array([-0.03583945, -0.03630679, -0.03658146, -0.03666347, -0.03655284,\n",
       "       -0.0362496 , -0.03575379, -0.03506542, -0.03418455, -0.0331112 ,\n",
       "       -0.03184543, -0.03038729, -0.02873682, -0.02689408, -0.02485913,\n",
       "       -0.02263204, -0.02021287, -0.01760169, -0.01479858, -0.01180361,\n",
       "       -0.01453558, -0.01682202, -0.01887398, -0.02066269, -0.02222955,\n",
       "       -0.02360825, -0.02482641, -0.02590686, -0.02686857, -0.02772741,\n",
       "       -0.02849675, -0.02918788, -0.02981041, -0.03037256, -0.03088137,\n",
       "       -0.03134291, -0.03176244, -0.03214451, -0.0324931 , -0.0328117 ,\n",
       "       -0.03310334, -0.03337072, -0.0336162 , -0.03384188, -0.03404962,\n",
       "       -0.03424107, -0.0344177 , -0.03458083, -0.03473165, -0.03487121,\n",
       "       -0.03500047, -0.03512027, -0.0352314 , -0.03533456, -0.03543038,\n",
       "       -0.03551943, -0.03560224, -0.03567929, -0.03575101, -0.03581778,\n",
       "       -0.03587999, -0.03593796, -0.03589755, -0.03584261, -0.03577452,\n",
       "       -0.03569451, -0.03560371, -0.03550316, -0.03539379, -0.03527648,\n",
       "       -0.03515199, -0.03502106, -0.03488429, -0.03474237, -0.03406015,\n",
       "       -0.00215294, -0.00192335, -0.00169419, -0.00146565, -0.0012379 ,\n",
       "       -0.00101107, -0.00078531, -0.00056072, -0.00033742, -0.0001155 ,\n",
       "        0.00010497,  0.0003239 ,  0.00054124,  0.00075692,  0.0009709 ,\n",
       "        0.00118314,  0.00139359,  0.00160224,  0.00180905,  0.00201399,\n",
       "        0.00221706,  0.00241825,  0.00261753,  0.0028149 ,  0.00301036,\n",
       "        0.0032039 ,  0.00339554,  0.00358526,  0.00377307,  0.00395899,\n",
       "        0.00414301,  0.00432515,  0.00450541,  0.00468382,  0.00486037,\n",
       "        0.00503509,  0.00520798,  0.00537907,  0.00554837,  0.00571589,\n",
       "        0.00588165,  0.00604567,  0.00620797,  0.00636855,  0.00652745,\n",
       "        0.00668468,  0.00684025,  0.00699419,  0.00714651,  0.00729723,\n",
       "        0.00744637,  0.01036474,  0.01036474,  0.01036474,  0.01036474,\n",
       "        0.01036474,  0.01036474,  0.01036474,  0.01036474,  0.01036474,\n",
       "        0.01036474,  0.01036474,  0.01036474,  0.01036474,  0.01036474,\n",
       "        0.01036474,  0.01036474,  0.01036474,  0.01036474,  0.01036474,\n",
       "        0.01036474,  0.01036474,  0.01036474])>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[8, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 148, 149, 149)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_pos_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(189,), dtype=float64, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_real_te[30, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(189,), dtype=float64, numpy=\n",
       "array([-0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568, -0.04683568,\n",
       "       -0.04683568, -0.04683568, -0.04683568, -0.04683568])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(189,), dtype=float64, numpy=\n",
       "array([2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801, 2.79753801,\n",
       "       2.79753801, 2.79753801, 2.79753801, 2.79753801])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sig[2, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
