{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import classic_model_mh, classic_model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels, gp_priors\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from inference import infer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te, _, df_te = data_generation.data_generator_for_gp_mimick_gpt(150000, ordered = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean(pad_pos_tr)\n",
    "std = np.std(pad_pos_tr)\n",
    "m_y = np.mean(pad_y_fren_tr)\n",
    "std_y = np.std(pad_y_fren_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr = (pad_pos_tr - m) / std\n",
    "pad_pos_te = (pad_pos_te - m) / std\n",
    "pad_y_fren_tr = (pad_y_fren_tr - m_y) / std_y\n",
    "pad_y_fren_te = (pad_y_fren_te - m_y) / std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(decoder, optimizer_c, train_loss, m_tr, pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "#         pred, pred_log_sig = decoder(pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "        pred = decoder(pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "\n",
    "\n",
    "# \n",
    "#         loss, mse, mask = losses.loss_function(tar_real, pred, pred_log_sig)\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred)\n",
    "\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "    # Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(decoder, test_loss, m_te, pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "#   pred = decoder(pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "\n",
    "\n",
    "#     pred, pred_log_sig = decoder(pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    pred = decoder(pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "\n",
    "\n",
    "#     t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred, pred_log_sig)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Restored from /Users/omernivron/Downloads/GPT/ckpt/check_25/ckpt-436\n",
      "Epoch 0 batch 0 train Loss 0.3070 test Loss 0.3288 with MSE metric 0.3288\n",
      "Epoch 0 batch 50 train Loss 0.2632 test Loss 0.2954 with MSE metric 0.2954\n",
      "Epoch 0 batch 100 train Loss 0.2409 test Loss 0.3051 with MSE metric 0.3051\n",
      "Epoch 0 batch 150 train Loss 0.2059 test Loss 0.2864 with MSE metric 0.2864\n",
      "Epoch 0 batch 200 train Loss 0.2653 test Loss 0.2926 with MSE metric 0.2926\n",
      "Epoch 0 batch 250 train Loss 0.2815 test Loss 0.2780 with MSE metric 0.2780\n",
      "Epoch 0 batch 300 train Loss 0.2676 test Loss 0.3237 with MSE metric 0.3236\n",
      "Epoch 0 batch 350 train Loss 0.2289 test Loss 0.3234 with MSE metric 0.3234\n",
      "Epoch 0 batch 400 train Loss 0.2114 test Loss 0.3176 with MSE metric 0.3176\n",
      "Epoch 0 batch 450 train Loss 0.2612 test Loss 0.3166 with MSE metric 0.3166\n",
      "Epoch 0 batch 500 train Loss 0.2315 test Loss 0.3008 with MSE metric 0.3008\n",
      "Epoch 0 batch 550 train Loss 0.2362 test Loss 0.2903 with MSE metric 0.2903\n",
      "Epoch 0 batch 600 train Loss 0.2551 test Loss 0.2934 with MSE metric 0.2934\n",
      "Epoch 0 batch 650 train Loss 0.2539 test Loss 0.3090 with MSE metric 0.3090\n",
      "Epoch 0 batch 700 train Loss 0.2181 test Loss 0.3089 with MSE metric 0.3089\n",
      "Epoch 0 batch 750 train Loss 0.2484 test Loss 0.2989 with MSE metric 0.2989\n",
      "Epoch 0 batch 800 train Loss 0.2403 test Loss 0.2783 with MSE metric 0.2783\n",
      "Epoch 0 batch 850 train Loss 0.2228 test Loss 0.3112 with MSE metric 0.3112\n",
      "Epoch 0 batch 900 train Loss 0.2654 test Loss 0.2857 with MSE metric 0.2857\n",
      "Epoch 0 batch 950 train Loss 0.2559 test Loss 0.3399 with MSE metric 0.3399\n",
      "Epoch 0 batch 1000 train Loss 0.2430 test Loss 0.2972 with MSE metric 0.2972\n",
      "Epoch 0 batch 1050 train Loss 0.2482 test Loss 0.2912 with MSE metric 0.2912\n",
      "Epoch 0 batch 1100 train Loss 0.2430 test Loss 0.3053 with MSE metric 0.3053\n",
      "Epoch 0 batch 1150 train Loss 0.2117 test Loss 0.2917 with MSE metric 0.2917\n",
      "Epoch 0 batch 1200 train Loss 0.2446 test Loss 0.3016 with MSE metric 0.3016\n",
      "Epoch 0 batch 1250 train Loss 0.2475 test Loss 0.2919 with MSE metric 0.2919\n",
      "Epoch 0 batch 1300 train Loss 0.2741 test Loss 0.2561 with MSE metric 0.2561\n",
      "Epoch 0 batch 1350 train Loss 0.2444 test Loss 0.2688 with MSE metric 0.2688\n",
      "Epoch 0 batch 1400 train Loss 0.2594 test Loss 0.3127 with MSE metric 0.3127\n",
      "Epoch 0 batch 1450 train Loss 0.2285 test Loss 0.3046 with MSE metric 0.3046\n",
      "Epoch 0 batch 1500 train Loss 0.2261 test Loss 0.3144 with MSE metric 0.3144\n",
      "Epoch 0 batch 1550 train Loss 0.2682 test Loss 0.2804 with MSE metric 0.2804\n",
      "Epoch 0 batch 1600 train Loss 0.2427 test Loss 0.2905 with MSE metric 0.2905\n",
      "Epoch 0 batch 1650 train Loss 0.2539 test Loss 0.2908 with MSE metric 0.2908\n",
      "Epoch 0 batch 1700 train Loss 0.2311 test Loss 0.2945 with MSE metric 0.2945\n",
      "Epoch 0 batch 1750 train Loss 0.2752 test Loss 0.2630 with MSE metric 0.2630\n",
      "Epoch 0 batch 1800 train Loss 0.2612 test Loss 0.2946 with MSE metric 0.2946\n",
      "Epoch 0 batch 1850 train Loss 0.2305 test Loss 0.3100 with MSE metric 0.3100\n",
      "Time taken for 1 epoch: 184.33406805992126 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 0.2421 test Loss 0.3009 with MSE metric 0.3009\n",
      "Epoch 1 batch 50 train Loss 0.2412 test Loss 0.3102 with MSE metric 0.3102\n",
      "Epoch 1 batch 100 train Loss 0.2194 test Loss 0.3259 with MSE metric 0.3259\n",
      "Epoch 1 batch 150 train Loss 0.2505 test Loss 0.2648 with MSE metric 0.2648\n",
      "Epoch 1 batch 200 train Loss 0.2014 test Loss 0.3130 with MSE metric 0.3130\n",
      "Epoch 1 batch 250 train Loss 0.2826 test Loss 0.3077 with MSE metric 0.3077\n",
      "Epoch 1 batch 300 train Loss 0.2472 test Loss 0.2893 with MSE metric 0.2893\n",
      "Epoch 1 batch 350 train Loss 0.2032 test Loss 0.3341 with MSE metric 0.3341\n",
      "Epoch 1 batch 400 train Loss 0.2504 test Loss 0.2974 with MSE metric 0.2973\n",
      "Epoch 1 batch 450 train Loss 0.2473 test Loss 0.2669 with MSE metric 0.2669\n",
      "Epoch 1 batch 500 train Loss 0.2624 test Loss 0.3044 with MSE metric 0.3044\n",
      "Epoch 1 batch 550 train Loss 0.2829 test Loss 0.2962 with MSE metric 0.2962\n",
      "Epoch 1 batch 600 train Loss 0.2347 test Loss 0.3345 with MSE metric 0.3345\n",
      "Epoch 1 batch 650 train Loss 0.2364 test Loss 0.3049 with MSE metric 0.3049\n",
      "Epoch 1 batch 700 train Loss 0.2669 test Loss 0.3294 with MSE metric 0.3294\n",
      "Epoch 1 batch 750 train Loss 0.2589 test Loss 0.2644 with MSE metric 0.2644\n",
      "Epoch 1 batch 800 train Loss 0.2448 test Loss 0.3076 with MSE metric 0.3076\n",
      "Epoch 1 batch 850 train Loss 0.2399 test Loss 0.2937 with MSE metric 0.2937\n",
      "Epoch 1 batch 900 train Loss 0.2541 test Loss 0.2925 with MSE metric 0.2925\n",
      "Epoch 1 batch 950 train Loss 0.2379 test Loss 0.2773 with MSE metric 0.2773\n",
      "Epoch 1 batch 1000 train Loss 0.2531 test Loss 0.2967 with MSE metric 0.2967\n",
      "Epoch 1 batch 1050 train Loss 0.2494 test Loss 0.2854 with MSE metric 0.2854\n",
      "Epoch 1 batch 1100 train Loss 0.2530 test Loss 0.3168 with MSE metric 0.3168\n",
      "Epoch 1 batch 1150 train Loss 0.2359 test Loss 0.2977 with MSE metric 0.2977\n",
      "Epoch 1 batch 1200 train Loss 0.2238 test Loss 0.2753 with MSE metric 0.2753\n",
      "Epoch 1 batch 1250 train Loss 0.2831 test Loss 0.3044 with MSE metric 0.3044\n",
      "Epoch 1 batch 1300 train Loss 0.2473 test Loss 0.3018 with MSE metric 0.3018\n",
      "Epoch 1 batch 1350 train Loss 0.2847 test Loss 0.3160 with MSE metric 0.3160\n",
      "Epoch 1 batch 1400 train Loss 0.2512 test Loss 0.2882 with MSE metric 0.2882\n",
      "Epoch 1 batch 1450 train Loss 0.2309 test Loss 0.3007 with MSE metric 0.3007\n",
      "Epoch 1 batch 1500 train Loss 0.2502 test Loss 0.2509 with MSE metric 0.2509\n",
      "Epoch 1 batch 1550 train Loss 0.2656 test Loss 0.3143 with MSE metric 0.3143\n",
      "Epoch 1 batch 1600 train Loss 0.2386 test Loss 0.2907 with MSE metric 0.2907\n",
      "Epoch 1 batch 1650 train Loss 0.2436 test Loss 0.3262 with MSE metric 0.3262\n",
      "Epoch 1 batch 1700 train Loss 0.2250 test Loss 0.2985 with MSE metric 0.2985\n",
      "Epoch 1 batch 1750 train Loss 0.2763 test Loss 0.3123 with MSE metric 0.3123\n",
      "Epoch 1 batch 1800 train Loss 0.2344 test Loss 0.3016 with MSE metric 0.3016\n",
      "Epoch 1 batch 1850 train Loss 0.2613 test Loss 0.2757 with MSE metric 0.2757\n",
      "Time taken for 1 epoch: 182.12237691879272 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 0.2636 test Loss 0.3016 with MSE metric 0.3016\n",
      "Epoch 2 batch 50 train Loss 0.2479 test Loss 0.3099 with MSE metric 0.3099\n",
      "Epoch 2 batch 100 train Loss 0.2469 test Loss 0.3193 with MSE metric 0.3193\n",
      "Epoch 2 batch 150 train Loss 0.2133 test Loss 0.3232 with MSE metric 0.3232\n",
      "Epoch 2 batch 200 train Loss 0.2349 test Loss 0.2884 with MSE metric 0.2884\n",
      "Epoch 2 batch 250 train Loss 0.2784 test Loss 0.2754 with MSE metric 0.2754\n",
      "Epoch 2 batch 300 train Loss 0.2555 test Loss 0.3156 with MSE metric 0.3156\n",
      "Epoch 2 batch 350 train Loss 0.2666 test Loss 0.2930 with MSE metric 0.2930\n",
      "Epoch 2 batch 400 train Loss 0.2446 test Loss 0.2954 with MSE metric 0.2954\n",
      "Epoch 2 batch 450 train Loss 0.2650 test Loss 0.2698 with MSE metric 0.2698\n",
      "Epoch 2 batch 500 train Loss 0.2448 test Loss 0.3025 with MSE metric 0.3025\n",
      "Epoch 2 batch 550 train Loss 0.2300 test Loss 0.2919 with MSE metric 0.2919\n",
      "Epoch 2 batch 600 train Loss 0.2626 test Loss 0.3116 with MSE metric 0.3116\n",
      "Epoch 2 batch 650 train Loss 0.2342 test Loss 0.2804 with MSE metric 0.2804\n",
      "Epoch 2 batch 700 train Loss 0.2483 test Loss 0.2674 with MSE metric 0.2674\n",
      "Epoch 2 batch 750 train Loss 0.2233 test Loss 0.2978 with MSE metric 0.2978\n",
      "Epoch 2 batch 800 train Loss 0.2397 test Loss 0.3180 with MSE metric 0.3180\n",
      "Epoch 2 batch 850 train Loss 0.2479 test Loss 0.2761 with MSE metric 0.2761\n",
      "Epoch 2 batch 900 train Loss 0.2390 test Loss 0.2798 with MSE metric 0.2798\n",
      "Epoch 2 batch 950 train Loss 0.2188 test Loss 0.3088 with MSE metric 0.3088\n",
      "Epoch 2 batch 1000 train Loss 0.2538 test Loss 0.3105 with MSE metric 0.3105\n",
      "Epoch 2 batch 1050 train Loss 0.2045 test Loss 0.3182 with MSE metric 0.3182\n",
      "Epoch 2 batch 1100 train Loss 0.2543 test Loss 0.3275 with MSE metric 0.3275\n",
      "Epoch 2 batch 1150 train Loss 0.2493 test Loss 0.2764 with MSE metric 0.2764\n",
      "Epoch 2 batch 1200 train Loss 0.2297 test Loss 0.2940 with MSE metric 0.2940\n",
      "Epoch 2 batch 1250 train Loss 0.2845 test Loss 0.2928 with MSE metric 0.2928\n",
      "Epoch 2 batch 1300 train Loss 0.2515 test Loss 0.2717 with MSE metric 0.2717\n",
      "Epoch 2 batch 1350 train Loss 0.2639 test Loss 0.3255 with MSE metric 0.3255\n",
      "Epoch 2 batch 1400 train Loss 0.2718 test Loss 0.2987 with MSE metric 0.2987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 batch 1450 train Loss 0.2347 test Loss 0.2913 with MSE metric 0.2913\n",
      "Epoch 2 batch 1500 train Loss 0.2413 test Loss 0.2806 with MSE metric 0.2806\n",
      "Epoch 2 batch 1550 train Loss 0.2543 test Loss 0.2912 with MSE metric 0.2912\n",
      "Epoch 2 batch 1600 train Loss 0.2648 test Loss 0.3080 with MSE metric 0.3080\n",
      "Epoch 2 batch 1650 train Loss 0.2538 test Loss 0.3020 with MSE metric 0.3020\n",
      "Epoch 2 batch 1700 train Loss 0.2288 test Loss 0.3104 with MSE metric 0.3104\n",
      "Epoch 2 batch 1750 train Loss 0.2530 test Loss 0.3118 with MSE metric 0.3118\n",
      "Epoch 2 batch 1800 train Loss 0.2505 test Loss 0.3055 with MSE metric 0.3055\n",
      "Epoch 2 batch 1850 train Loss 0.2572 test Loss 0.3108 with MSE metric 0.3108\n",
      "Time taken for 1 epoch: 187.7289822101593 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 0.2549 test Loss 0.3203 with MSE metric 0.3203\n",
      "Epoch 3 batch 50 train Loss 0.2404 test Loss 0.2667 with MSE metric 0.2667\n",
      "Epoch 3 batch 100 train Loss 0.2449 test Loss 0.3273 with MSE metric 0.3273\n",
      "Epoch 3 batch 150 train Loss 0.2067 test Loss 0.2949 with MSE metric 0.2949\n",
      "Epoch 3 batch 200 train Loss 0.2622 test Loss 0.3428 with MSE metric 0.3428\n",
      "Epoch 3 batch 250 train Loss 0.2579 test Loss 0.2825 with MSE metric 0.2825\n",
      "Epoch 3 batch 300 train Loss 0.2535 test Loss 0.2664 with MSE metric 0.2664\n",
      "Epoch 3 batch 350 train Loss 0.2923 test Loss 0.2920 with MSE metric 0.2920\n",
      "Epoch 3 batch 400 train Loss 0.2570 test Loss 0.3252 with MSE metric 0.3252\n",
      "Epoch 3 batch 450 train Loss 0.2268 test Loss 0.2694 with MSE metric 0.2694\n",
      "Epoch 3 batch 500 train Loss 0.2340 test Loss 0.2736 with MSE metric 0.2736\n",
      "Epoch 3 batch 550 train Loss 0.2671 test Loss 0.3054 with MSE metric 0.3054\n",
      "Epoch 3 batch 600 train Loss 0.2446 test Loss 0.3186 with MSE metric 0.3186\n",
      "Epoch 3 batch 650 train Loss 0.2317 test Loss 0.3170 with MSE metric 0.3170\n",
      "Epoch 3 batch 700 train Loss 0.2339 test Loss 0.2871 with MSE metric 0.2871\n",
      "Epoch 3 batch 750 train Loss 0.2398 test Loss 0.2865 with MSE metric 0.2865\n",
      "Epoch 3 batch 800 train Loss 0.3010 test Loss 0.3010 with MSE metric 0.3010\n",
      "Epoch 3 batch 850 train Loss 0.2623 test Loss 0.2932 with MSE metric 0.2932\n",
      "Epoch 3 batch 900 train Loss 0.2590 test Loss 0.2815 with MSE metric 0.2815\n",
      "Epoch 3 batch 950 train Loss 0.2615 test Loss 0.2915 with MSE metric 0.2915\n",
      "Epoch 3 batch 1000 train Loss 0.2173 test Loss 0.2689 with MSE metric 0.2689\n",
      "Epoch 3 batch 1050 train Loss 0.2322 test Loss 0.3276 with MSE metric 0.3276\n",
      "Epoch 3 batch 1100 train Loss 0.2657 test Loss 0.3268 with MSE metric 0.3268\n",
      "Epoch 3 batch 1150 train Loss 0.2357 test Loss 0.2701 with MSE metric 0.2701\n",
      "Epoch 3 batch 1200 train Loss 0.2086 test Loss 0.2859 with MSE metric 0.2859\n",
      "Epoch 3 batch 1250 train Loss 0.2345 test Loss 0.2883 with MSE metric 0.2883\n",
      "Epoch 3 batch 1300 train Loss 0.2655 test Loss 0.2897 with MSE metric 0.2897\n",
      "Epoch 3 batch 1350 train Loss 0.2446 test Loss 0.2718 with MSE metric 0.2718\n",
      "Epoch 3 batch 1400 train Loss 0.2392 test Loss 0.2911 with MSE metric 0.2911\n",
      "Epoch 3 batch 1450 train Loss 0.2261 test Loss 0.3156 with MSE metric 0.3156\n",
      "Epoch 3 batch 1500 train Loss 0.2267 test Loss 0.2761 with MSE metric 0.2761\n",
      "Epoch 3 batch 1550 train Loss 0.2286 test Loss 0.2587 with MSE metric 0.2587\n",
      "Epoch 3 batch 1600 train Loss 0.2525 test Loss 0.2914 with MSE metric 0.2914\n",
      "Epoch 3 batch 1650 train Loss 0.2552 test Loss 0.3040 with MSE metric 0.3040\n",
      "Epoch 3 batch 1700 train Loss 0.2342 test Loss 0.2951 with MSE metric 0.2951\n",
      "Epoch 3 batch 1750 train Loss 0.2690 test Loss 0.3047 with MSE metric 0.3047\n",
      "Epoch 3 batch 1800 train Loss 0.2393 test Loss 0.2862 with MSE metric 0.2862\n",
      "Epoch 3 batch 1850 train Loss 0.2304 test Loss 0.2808 with MSE metric 0.2808\n",
      "Time taken for 1 epoch: 184.89202904701233 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 0.2343 test Loss 0.2752 with MSE metric 0.2752\n",
      "Epoch 4 batch 50 train Loss 0.2855 test Loss 0.2971 with MSE metric 0.2971\n",
      "Epoch 4 batch 100 train Loss 0.2474 test Loss 0.2795 with MSE metric 0.2795\n",
      "Epoch 4 batch 150 train Loss 0.2246 test Loss 0.2976 with MSE metric 0.2976\n",
      "Epoch 4 batch 200 train Loss 0.2516 test Loss 0.3026 with MSE metric 0.3026\n",
      "Epoch 4 batch 250 train Loss 0.2664 test Loss 0.2818 with MSE metric 0.2818\n",
      "Epoch 4 batch 300 train Loss 0.2417 test Loss 0.2951 with MSE metric 0.2951\n",
      "Epoch 4 batch 350 train Loss 0.2718 test Loss 0.3417 with MSE metric 0.3417\n",
      "Epoch 4 batch 400 train Loss 0.2322 test Loss 0.2732 with MSE metric 0.2732\n",
      "Epoch 4 batch 450 train Loss 0.2835 test Loss 0.2758 with MSE metric 0.2758\n",
      "Epoch 4 batch 500 train Loss 0.2197 test Loss 0.3077 with MSE metric 0.3077\n",
      "Epoch 4 batch 550 train Loss 0.2568 test Loss 0.2946 with MSE metric 0.2946\n",
      "Epoch 4 batch 600 train Loss 0.2430 test Loss 0.2971 with MSE metric 0.2971\n",
      "Epoch 4 batch 650 train Loss 0.2437 test Loss 0.3100 with MSE metric 0.3100\n",
      "Epoch 4 batch 700 train Loss 0.2349 test Loss 0.2699 with MSE metric 0.2699\n",
      "Epoch 4 batch 750 train Loss 0.2546 test Loss 0.2971 with MSE metric 0.2971\n",
      "Epoch 4 batch 800 train Loss 0.2194 test Loss 0.2708 with MSE metric 0.2708\n",
      "Epoch 4 batch 850 train Loss 0.2434 test Loss 0.2890 with MSE metric 0.2890\n",
      "Epoch 4 batch 900 train Loss 0.2540 test Loss 0.2644 with MSE metric 0.2644\n",
      "Epoch 4 batch 950 train Loss 0.2339 test Loss 0.3189 with MSE metric 0.3189\n",
      "Epoch 4 batch 1000 train Loss 0.2281 test Loss 0.2943 with MSE metric 0.2943\n",
      "Epoch 4 batch 1050 train Loss 0.2711 test Loss 0.2986 with MSE metric 0.2986\n",
      "Epoch 4 batch 1100 train Loss 0.2444 test Loss 0.2811 with MSE metric 0.2811\n",
      "Epoch 4 batch 1150 train Loss 0.2438 test Loss 0.2913 with MSE metric 0.2913\n",
      "Epoch 4 batch 1200 train Loss 0.2583 test Loss 0.2978 with MSE metric 0.2978\n",
      "Epoch 4 batch 1250 train Loss 0.2470 test Loss 0.2995 with MSE metric 0.2995\n",
      "Epoch 4 batch 1300 train Loss 0.2582 test Loss 0.3084 with MSE metric 0.3084\n",
      "Epoch 4 batch 1350 train Loss 0.2669 test Loss 0.3003 with MSE metric 0.3003\n",
      "Epoch 4 batch 1400 train Loss 0.2472 test Loss 0.3090 with MSE metric 0.3091\n",
      "Epoch 4 batch 1450 train Loss 0.2356 test Loss 0.2709 with MSE metric 0.2709\n",
      "Epoch 4 batch 1500 train Loss 0.2435 test Loss 0.2878 with MSE metric 0.2878\n",
      "Epoch 4 batch 1550 train Loss 0.2766 test Loss 0.3072 with MSE metric 0.3072\n",
      "Epoch 4 batch 1600 train Loss 0.2563 test Loss 0.3017 with MSE metric 0.3017\n",
      "Epoch 4 batch 1650 train Loss 0.2652 test Loss 0.2654 with MSE metric 0.2654\n",
      "Epoch 4 batch 1700 train Loss 0.2575 test Loss 0.2862 with MSE metric 0.2862\n",
      "Epoch 4 batch 1750 train Loss 0.2317 test Loss 0.2882 with MSE metric 0.2882\n",
      "Epoch 4 batch 1800 train Loss 0.2823 test Loss 0.2918 with MSE metric 0.2918\n",
      "Epoch 4 batch 1850 train Loss 0.2256 test Loss 0.3004 with MSE metric 0.3004\n",
      "Time taken for 1 epoch: 182.23433017730713 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 0.2649 test Loss 0.2986 with MSE metric 0.2986\n",
      "Epoch 5 batch 50 train Loss 0.2540 test Loss 0.3281 with MSE metric 0.3281\n",
      "Epoch 5 batch 100 train Loss 0.2406 test Loss 0.3355 with MSE metric 0.3355\n",
      "Epoch 5 batch 150 train Loss 0.2183 test Loss 0.2862 with MSE metric 0.2862\n",
      "Epoch 5 batch 200 train Loss 0.2027 test Loss 0.3026 with MSE metric 0.3026\n",
      "Epoch 5 batch 250 train Loss 0.2488 test Loss 0.2917 with MSE metric 0.2917\n",
      "Epoch 5 batch 300 train Loss 0.2230 test Loss 0.3092 with MSE metric 0.3092\n",
      "Epoch 5 batch 350 train Loss 0.2108 test Loss 0.2936 with MSE metric 0.2936\n",
      "Epoch 5 batch 400 train Loss 0.2808 test Loss 0.2905 with MSE metric 0.2905\n",
      "Epoch 5 batch 450 train Loss 0.2488 test Loss 0.2778 with MSE metric 0.2778\n",
      "Epoch 5 batch 500 train Loss 0.2435 test Loss 0.2969 with MSE metric 0.2969\n",
      "Epoch 5 batch 550 train Loss 0.2183 test Loss 0.2905 with MSE metric 0.2905\n",
      "Epoch 5 batch 600 train Loss 0.2161 test Loss 0.3201 with MSE metric 0.3201\n",
      "Epoch 5 batch 650 train Loss 0.2553 test Loss 0.2887 with MSE metric 0.2887\n",
      "Epoch 5 batch 700 train Loss 0.2221 test Loss 0.2858 with MSE metric 0.2858\n",
      "Epoch 5 batch 750 train Loss 0.2316 test Loss 0.2839 with MSE metric 0.2839\n",
      "Epoch 5 batch 800 train Loss 0.2685 test Loss 0.3138 with MSE metric 0.3138\n",
      "Epoch 5 batch 850 train Loss 0.2575 test Loss 0.3078 with MSE metric 0.3078\n",
      "Epoch 5 batch 900 train Loss 0.2567 test Loss 0.3108 with MSE metric 0.3108\n",
      "Epoch 5 batch 950 train Loss 0.2128 test Loss 0.2855 with MSE metric 0.2855\n",
      "Epoch 5 batch 1000 train Loss 0.2437 test Loss 0.2957 with MSE metric 0.2957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 batch 1050 train Loss 0.2346 test Loss 0.3577 with MSE metric 0.3577\n",
      "Epoch 5 batch 1100 train Loss 0.2178 test Loss 0.2950 with MSE metric 0.2950\n",
      "Epoch 5 batch 1150 train Loss 0.2665 test Loss 0.3018 with MSE metric 0.3018\n",
      "Epoch 5 batch 1200 train Loss 0.2536 test Loss 0.3231 with MSE metric 0.3231\n",
      "Epoch 5 batch 1250 train Loss 0.2176 test Loss 0.2897 with MSE metric 0.2897\n",
      "Epoch 5 batch 1300 train Loss 0.2139 test Loss 0.3340 with MSE metric 0.3340\n",
      "Epoch 5 batch 1350 train Loss 0.2562 test Loss 0.3115 with MSE metric 0.3115\n",
      "Epoch 5 batch 1400 train Loss 0.2377 test Loss 0.3161 with MSE metric 0.3161\n",
      "Epoch 5 batch 1450 train Loss 0.2233 test Loss 0.3072 with MSE metric 0.3072\n",
      "Epoch 5 batch 1500 train Loss 0.2511 test Loss 0.2972 with MSE metric 0.2972\n",
      "Epoch 5 batch 1550 train Loss 0.2533 test Loss 0.3166 with MSE metric 0.3166\n",
      "Epoch 5 batch 1600 train Loss 0.2882 test Loss 0.3068 with MSE metric 0.3068\n",
      "Epoch 5 batch 1650 train Loss 0.2233 test Loss 0.2607 with MSE metric 0.2607\n",
      "Epoch 5 batch 1700 train Loss 0.2355 test Loss 0.2954 with MSE metric 0.2954\n",
      "Epoch 5 batch 1750 train Loss 0.2349 test Loss 0.2872 with MSE metric 0.2872\n",
      "Epoch 5 batch 1800 train Loss 0.2293 test Loss 0.3243 with MSE metric 0.3243\n",
      "Epoch 5 batch 1850 train Loss 0.2969 test Loss 0.2949 with MSE metric 0.2949\n",
      "Time taken for 1 epoch: 183.7219636440277 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 0.2092 test Loss 0.2465 with MSE metric 0.2465\n",
      "Epoch 6 batch 50 train Loss 0.2402 test Loss 0.2756 with MSE metric 0.2756\n",
      "Epoch 6 batch 100 train Loss 0.2359 test Loss 0.2806 with MSE metric 0.2806\n",
      "Epoch 6 batch 150 train Loss 0.2447 test Loss 0.3031 with MSE metric 0.3031\n",
      "Epoch 6 batch 200 train Loss 0.2186 test Loss 0.2845 with MSE metric 0.2845\n",
      "Epoch 6 batch 250 train Loss 0.2227 test Loss 0.2876 with MSE metric 0.2876\n",
      "Epoch 6 batch 300 train Loss 0.2424 test Loss 0.3088 with MSE metric 0.3088\n",
      "Epoch 6 batch 350 train Loss 0.2536 test Loss 0.3061 with MSE metric 0.3061\n",
      "Epoch 6 batch 400 train Loss 0.2302 test Loss 0.2917 with MSE metric 0.2917\n",
      "Epoch 6 batch 450 train Loss 0.2403 test Loss 0.2612 with MSE metric 0.2612\n",
      "Epoch 6 batch 500 train Loss 0.2114 test Loss 0.2775 with MSE metric 0.2775\n",
      "Epoch 6 batch 550 train Loss 0.2330 test Loss 0.2846 with MSE metric 0.2846\n",
      "Epoch 6 batch 600 train Loss 0.2836 test Loss 0.2839 with MSE metric 0.2839\n",
      "Epoch 6 batch 650 train Loss 0.2480 test Loss 0.3091 with MSE metric 0.3091\n",
      "Epoch 6 batch 700 train Loss 0.2257 test Loss 0.2852 with MSE metric 0.2852\n",
      "Epoch 6 batch 750 train Loss 0.2410 test Loss 0.3268 with MSE metric 0.3268\n",
      "Epoch 6 batch 800 train Loss 0.2455 test Loss 0.3562 with MSE metric 0.3562\n",
      "Epoch 6 batch 850 train Loss 0.2564 test Loss 0.3232 with MSE metric 0.3232\n",
      "Epoch 6 batch 900 train Loss 0.2312 test Loss 0.2981 with MSE metric 0.2981\n",
      "Epoch 6 batch 950 train Loss 0.2667 test Loss 0.3236 with MSE metric 0.3236\n",
      "Epoch 6 batch 1000 train Loss 0.2440 test Loss 0.2847 with MSE metric 0.2847\n",
      "Epoch 6 batch 1050 train Loss 0.2464 test Loss 0.2816 with MSE metric 0.2816\n",
      "Epoch 6 batch 1100 train Loss 0.2397 test Loss 0.3171 with MSE metric 0.3171\n",
      "Epoch 6 batch 1150 train Loss 0.2406 test Loss 0.3140 with MSE metric 0.3140\n",
      "Epoch 6 batch 1200 train Loss 0.2790 test Loss 0.3035 with MSE metric 0.3035\n",
      "Epoch 6 batch 1250 train Loss 0.2611 test Loss 0.3045 with MSE metric 0.3045\n",
      "Epoch 6 batch 1300 train Loss 0.2412 test Loss 0.3295 with MSE metric 0.3295\n",
      "Epoch 6 batch 1350 train Loss 0.2403 test Loss 0.2936 with MSE metric 0.2936\n",
      "Epoch 6 batch 1400 train Loss 0.2610 test Loss 0.2837 with MSE metric 0.2837\n",
      "Epoch 6 batch 1450 train Loss 0.2532 test Loss 0.3355 with MSE metric 0.3355\n",
      "Epoch 6 batch 1500 train Loss 0.2378 test Loss 0.3306 with MSE metric 0.3306\n",
      "Epoch 6 batch 1550 train Loss 0.2460 test Loss 0.3095 with MSE metric 0.3095\n",
      "Epoch 6 batch 1600 train Loss 0.2229 test Loss 0.3473 with MSE metric 0.3473\n",
      "Epoch 6 batch 1650 train Loss 0.2104 test Loss 0.2808 with MSE metric 0.2808\n",
      "Epoch 6 batch 1700 train Loss 0.2256 test Loss 0.2658 with MSE metric 0.2658\n",
      "Epoch 6 batch 1750 train Loss 0.2484 test Loss 0.3344 with MSE metric 0.3344\n",
      "Epoch 6 batch 1800 train Loss 0.2581 test Loss 0.2845 with MSE metric 0.2845\n",
      "Epoch 6 batch 1850 train Loss 0.2453 test Loss 0.2981 with MSE metric 0.2981\n",
      "Time taken for 1 epoch: 189.65649890899658 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 0.2318 test Loss 0.2930 with MSE metric 0.2930\n",
      "Epoch 7 batch 50 train Loss 0.2593 test Loss 0.3060 with MSE metric 0.3060\n",
      "Epoch 7 batch 100 train Loss 0.2247 test Loss 0.2793 with MSE metric 0.2793\n",
      "Epoch 7 batch 150 train Loss 0.2298 test Loss 0.3249 with MSE metric 0.3249\n",
      "Epoch 7 batch 200 train Loss 0.2325 test Loss 0.3273 with MSE metric 0.3273\n",
      "Epoch 7 batch 250 train Loss 0.2560 test Loss 0.3154 with MSE metric 0.3154\n",
      "Epoch 7 batch 300 train Loss 0.2359 test Loss 0.3066 with MSE metric 0.3066\n",
      "Epoch 7 batch 350 train Loss 0.2499 test Loss 0.2912 with MSE metric 0.2912\n",
      "Epoch 7 batch 400 train Loss 0.2172 test Loss 0.2697 with MSE metric 0.2697\n",
      "Epoch 7 batch 450 train Loss 0.2799 test Loss 0.2840 with MSE metric 0.2840\n",
      "Epoch 7 batch 500 train Loss 0.2637 test Loss 0.2539 with MSE metric 0.2539\n",
      "Epoch 7 batch 550 train Loss 0.2521 test Loss 0.2763 with MSE metric 0.2763\n",
      "Epoch 7 batch 600 train Loss 0.2405 test Loss 0.3085 with MSE metric 0.3085\n",
      "Epoch 7 batch 650 train Loss 0.2703 test Loss 0.3126 with MSE metric 0.3126\n",
      "Epoch 7 batch 700 train Loss 0.2309 test Loss 0.2914 with MSE metric 0.2914\n",
      "Epoch 7 batch 750 train Loss 0.2664 test Loss 0.2986 with MSE metric 0.2986\n",
      "Epoch 7 batch 800 train Loss 0.2388 test Loss 0.2860 with MSE metric 0.2860\n",
      "Epoch 7 batch 850 train Loss 0.2495 test Loss 0.3045 with MSE metric 0.3045\n",
      "Epoch 7 batch 900 train Loss 0.2711 test Loss 0.2848 with MSE metric 0.2848\n",
      "Epoch 7 batch 950 train Loss 0.2466 test Loss 0.3089 with MSE metric 0.3089\n",
      "Epoch 7 batch 1000 train Loss 0.2564 test Loss 0.3326 with MSE metric 0.3326\n",
      "Epoch 7 batch 1050 train Loss 0.2254 test Loss 0.3030 with MSE metric 0.3030\n",
      "Epoch 7 batch 1100 train Loss 0.2574 test Loss 0.3176 with MSE metric 0.3176\n",
      "Epoch 7 batch 1150 train Loss 0.2641 test Loss 0.2946 with MSE metric 0.2946\n",
      "Epoch 7 batch 1200 train Loss 0.2541 test Loss 0.2951 with MSE metric 0.2951\n",
      "Epoch 7 batch 1250 train Loss 0.2445 test Loss 0.2852 with MSE metric 0.2852\n",
      "Epoch 7 batch 1300 train Loss 0.2847 test Loss 0.3194 with MSE metric 0.3194\n",
      "Epoch 7 batch 1350 train Loss 0.2309 test Loss 0.2915 with MSE metric 0.2915\n",
      "Epoch 7 batch 1400 train Loss 0.2468 test Loss 0.3397 with MSE metric 0.3397\n",
      "Epoch 7 batch 1450 train Loss 0.2376 test Loss 0.3137 with MSE metric 0.3137\n",
      "Epoch 7 batch 1500 train Loss 0.2437 test Loss 0.2976 with MSE metric 0.2976\n",
      "Epoch 7 batch 1550 train Loss 0.2636 test Loss 0.2757 with MSE metric 0.2757\n",
      "Epoch 7 batch 1600 train Loss 0.2633 test Loss 0.2888 with MSE metric 0.2888\n",
      "Epoch 7 batch 1650 train Loss 0.2872 test Loss 0.2976 with MSE metric 0.2976\n",
      "Epoch 7 batch 1700 train Loss 0.2268 test Loss 0.3117 with MSE metric 0.3117\n",
      "Epoch 7 batch 1750 train Loss 0.2500 test Loss 0.2842 with MSE metric 0.2842\n",
      "Epoch 7 batch 1800 train Loss 0.2512 test Loss 0.2831 with MSE metric 0.2831\n",
      "Epoch 7 batch 1850 train Loss 0.2441 test Loss 0.3384 with MSE metric 0.3384\n",
      "Time taken for 1 epoch: 182.5462679862976 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 0.2695 test Loss 0.3144 with MSE metric 0.3144\n",
      "Epoch 8 batch 50 train Loss 0.2717 test Loss 0.2965 with MSE metric 0.2965\n",
      "Epoch 8 batch 100 train Loss 0.2594 test Loss 0.2988 with MSE metric 0.2988\n",
      "Epoch 8 batch 150 train Loss 0.2432 test Loss 0.3164 with MSE metric 0.3164\n",
      "Epoch 8 batch 200 train Loss 0.2414 test Loss 0.3216 with MSE metric 0.3216\n",
      "Epoch 8 batch 250 train Loss 0.2409 test Loss 0.2809 with MSE metric 0.2809\n",
      "Epoch 8 batch 300 train Loss 0.2602 test Loss 0.3055 with MSE metric 0.3055\n",
      "Epoch 8 batch 350 train Loss 0.2226 test Loss 0.2788 with MSE metric 0.2788\n",
      "Epoch 8 batch 400 train Loss 0.2488 test Loss 0.3088 with MSE metric 0.3088\n",
      "Epoch 8 batch 450 train Loss 0.2261 test Loss 0.2774 with MSE metric 0.2774\n",
      "Epoch 8 batch 500 train Loss 0.2568 test Loss 0.2987 with MSE metric 0.2987\n",
      "Epoch 8 batch 550 train Loss 0.2438 test Loss 0.3147 with MSE metric 0.3147\n",
      "Epoch 8 batch 600 train Loss 0.2322 test Loss 0.3387 with MSE metric 0.3387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 batch 650 train Loss 0.2449 test Loss 0.2917 with MSE metric 0.2917\n",
      "Epoch 8 batch 700 train Loss 0.2304 test Loss 0.3112 with MSE metric 0.3112\n",
      "Epoch 8 batch 750 train Loss 0.2541 test Loss 0.3145 with MSE metric 0.3145\n",
      "Epoch 8 batch 800 train Loss 0.2440 test Loss 0.2943 with MSE metric 0.2943\n",
      "Epoch 8 batch 850 train Loss 0.2225 test Loss 0.2724 with MSE metric 0.2724\n",
      "Epoch 8 batch 900 train Loss 0.2211 test Loss 0.2979 with MSE metric 0.2979\n",
      "Epoch 8 batch 950 train Loss 0.2388 test Loss 0.2717 with MSE metric 0.2717\n",
      "Epoch 8 batch 1000 train Loss 0.2498 test Loss 0.2763 with MSE metric 0.2763\n",
      "Epoch 8 batch 1050 train Loss 0.2317 test Loss 0.2809 with MSE metric 0.2809\n",
      "Epoch 8 batch 1100 train Loss 0.2382 test Loss 0.2806 with MSE metric 0.2806\n",
      "Epoch 8 batch 1150 train Loss 0.2486 test Loss 0.2879 with MSE metric 0.2879\n",
      "Epoch 8 batch 1200 train Loss 0.2504 test Loss 0.2959 with MSE metric 0.2959\n",
      "Epoch 8 batch 1250 train Loss 0.2584 test Loss 0.3014 with MSE metric 0.3014\n",
      "Epoch 8 batch 1300 train Loss 0.2643 test Loss 0.2816 with MSE metric 0.2816\n",
      "Epoch 8 batch 1350 train Loss 0.2606 test Loss 0.2668 with MSE metric 0.2668\n",
      "Epoch 8 batch 1400 train Loss 0.2411 test Loss 0.3056 with MSE metric 0.3056\n",
      "Epoch 8 batch 1450 train Loss 0.2644 test Loss 0.3042 with MSE metric 0.3042\n",
      "Epoch 8 batch 1500 train Loss 0.2121 test Loss 0.3138 with MSE metric 0.3138\n",
      "Epoch 8 batch 1550 train Loss 0.2328 test Loss 0.2791 with MSE metric 0.2791\n",
      "Epoch 8 batch 1600 train Loss 0.2579 test Loss 0.2851 with MSE metric 0.2851\n",
      "Epoch 8 batch 1650 train Loss 0.2447 test Loss 0.2940 with MSE metric 0.2940\n",
      "Epoch 8 batch 1700 train Loss 0.2490 test Loss 0.3111 with MSE metric 0.3111\n",
      "Epoch 8 batch 1750 train Loss 0.2514 test Loss 0.2979 with MSE metric 0.2979\n",
      "Epoch 8 batch 1800 train Loss 0.2927 test Loss 0.3023 with MSE metric 0.3023\n",
      "Epoch 8 batch 1850 train Loss 0.2595 test Loss 0.2915 with MSE metric 0.2915\n",
      "Time taken for 1 epoch: 184.45977926254272 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 0.2350 test Loss 0.3124 with MSE metric 0.3124\n",
      "Epoch 9 batch 50 train Loss 0.2721 test Loss 0.3143 with MSE metric 0.3143\n",
      "Epoch 9 batch 100 train Loss 0.2501 test Loss 0.3128 with MSE metric 0.3128\n",
      "Epoch 9 batch 150 train Loss 0.2051 test Loss 0.2983 with MSE metric 0.2983\n",
      "Epoch 9 batch 200 train Loss 0.2569 test Loss 0.2862 with MSE metric 0.2862\n",
      "Epoch 9 batch 250 train Loss 0.2703 test Loss 0.3039 with MSE metric 0.3039\n",
      "Epoch 9 batch 300 train Loss 0.2732 test Loss 0.3108 with MSE metric 0.3108\n",
      "Epoch 9 batch 350 train Loss 0.2332 test Loss 0.2685 with MSE metric 0.2685\n",
      "Epoch 9 batch 400 train Loss 0.2767 test Loss 0.3178 with MSE metric 0.3178\n",
      "Epoch 9 batch 450 train Loss 0.2172 test Loss 0.3413 with MSE metric 0.3413\n",
      "Epoch 9 batch 500 train Loss 0.2342 test Loss 0.3100 with MSE metric 0.3100\n",
      "Epoch 9 batch 550 train Loss 0.2560 test Loss 0.2775 with MSE metric 0.2775\n",
      "Epoch 9 batch 600 train Loss 0.2546 test Loss 0.2936 with MSE metric 0.2936\n",
      "Epoch 9 batch 650 train Loss 0.2250 test Loss 0.2902 with MSE metric 0.2902\n",
      "Epoch 9 batch 700 train Loss 0.2276 test Loss 0.2732 with MSE metric 0.2732\n",
      "Epoch 9 batch 750 train Loss 0.2566 test Loss 0.3267 with MSE metric 0.3267\n",
      "Epoch 9 batch 800 train Loss 0.2214 test Loss 0.2984 with MSE metric 0.2984\n",
      "Epoch 9 batch 850 train Loss 0.2684 test Loss 0.3145 with MSE metric 0.3145\n",
      "Epoch 9 batch 900 train Loss 0.2390 test Loss 0.2770 with MSE metric 0.2770\n",
      "Epoch 9 batch 950 train Loss 0.2390 test Loss 0.3069 with MSE metric 0.3069\n",
      "Epoch 9 batch 1000 train Loss 0.2818 test Loss 0.2922 with MSE metric 0.2922\n",
      "Epoch 9 batch 1050 train Loss 0.2374 test Loss 0.2699 with MSE metric 0.2699\n",
      "Epoch 9 batch 1100 train Loss 0.2487 test Loss 0.2879 with MSE metric 0.2879\n",
      "Epoch 9 batch 1150 train Loss 0.2364 test Loss 0.2793 with MSE metric 0.2793\n",
      "Epoch 9 batch 1200 train Loss 0.2362 test Loss 0.2920 with MSE metric 0.2920\n",
      "Epoch 9 batch 1250 train Loss 0.2377 test Loss 0.2742 with MSE metric 0.2742\n",
      "Epoch 9 batch 1300 train Loss 0.2693 test Loss 0.2918 with MSE metric 0.2918\n",
      "Epoch 9 batch 1350 train Loss 0.2481 test Loss 0.3437 with MSE metric 0.3437\n",
      "Epoch 9 batch 1400 train Loss 0.2571 test Loss 0.2962 with MSE metric 0.2962\n",
      "Epoch 9 batch 1450 train Loss 0.2297 test Loss 0.2918 with MSE metric 0.2918\n",
      "Epoch 9 batch 1500 train Loss 0.2529 test Loss 0.3160 with MSE metric 0.3160\n",
      "Epoch 9 batch 1550 train Loss 0.2613 test Loss 0.2812 with MSE metric 0.2812\n",
      "Epoch 9 batch 1600 train Loss 0.2531 test Loss 0.3304 with MSE metric 0.3304\n",
      "Epoch 9 batch 1650 train Loss 0.2322 test Loss 0.3003 with MSE metric 0.3003\n",
      "Epoch 9 batch 1700 train Loss 0.2258 test Loss 0.3205 with MSE metric 0.3205\n",
      "Epoch 9 batch 1750 train Loss 0.2513 test Loss 0.2862 with MSE metric 0.2862\n",
      "Epoch 9 batch 1800 train Loss 0.2384 test Loss 0.3259 with MSE metric 0.3259\n",
      "Epoch 9 batch 1850 train Loss 0.2580 test Loss 0.3255 with MSE metric 0.3255\n",
      "Time taken for 1 epoch: 182.91400408744812 secs\n",
      "\n",
      "Epoch 10 batch 0 train Loss 0.2467 test Loss 0.3156 with MSE metric 0.3156\n",
      "Epoch 10 batch 50 train Loss 0.2356 test Loss 0.2892 with MSE metric 0.2892\n",
      "Epoch 10 batch 100 train Loss 0.2482 test Loss 0.2783 with MSE metric 0.2783\n",
      "Epoch 10 batch 150 train Loss 0.2679 test Loss 0.2956 with MSE metric 0.2956\n",
      "Epoch 10 batch 200 train Loss 0.2197 test Loss 0.3044 with MSE metric 0.3044\n",
      "Epoch 10 batch 250 train Loss 0.2368 test Loss 0.2860 with MSE metric 0.2860\n",
      "Epoch 10 batch 300 train Loss 0.2452 test Loss 0.2747 with MSE metric 0.2747\n",
      "Epoch 10 batch 350 train Loss 0.2413 test Loss 0.3134 with MSE metric 0.3134\n",
      "Epoch 10 batch 400 train Loss 0.2301 test Loss 0.3022 with MSE metric 0.3022\n",
      "Epoch 10 batch 450 train Loss 0.2759 test Loss 0.3176 with MSE metric 0.3176\n",
      "Epoch 10 batch 500 train Loss 0.2621 test Loss 0.3211 with MSE metric 0.3211\n",
      "Epoch 10 batch 550 train Loss 0.2647 test Loss 0.2838 with MSE metric 0.2838\n",
      "Epoch 10 batch 600 train Loss 0.2469 test Loss 0.3083 with MSE metric 0.3083\n",
      "Epoch 10 batch 650 train Loss 0.2691 test Loss 0.2920 with MSE metric 0.2920\n",
      "Epoch 10 batch 700 train Loss 0.2790 test Loss 0.3105 with MSE metric 0.3105\n",
      "Epoch 10 batch 750 train Loss 0.2594 test Loss 0.2744 with MSE metric 0.2744\n",
      "Epoch 10 batch 800 train Loss 0.2499 test Loss 0.3182 with MSE metric 0.3182\n",
      "Epoch 10 batch 850 train Loss 0.2276 test Loss 0.3122 with MSE metric 0.3122\n",
      "Epoch 10 batch 900 train Loss 0.2239 test Loss 0.2740 with MSE metric 0.2740\n",
      "Epoch 10 batch 950 train Loss 0.2450 test Loss 0.2862 with MSE metric 0.2862\n",
      "Epoch 10 batch 1000 train Loss 0.2638 test Loss 0.3293 with MSE metric 0.3293\n",
      "Epoch 10 batch 1050 train Loss 0.2424 test Loss 0.2785 with MSE metric 0.2785\n",
      "Epoch 10 batch 1100 train Loss 0.2574 test Loss 0.2890 with MSE metric 0.2890\n",
      "Epoch 10 batch 1150 train Loss 0.2328 test Loss 0.2724 with MSE metric 0.2724\n",
      "Epoch 10 batch 1200 train Loss 0.2313 test Loss 0.3236 with MSE metric 0.3236\n",
      "Epoch 10 batch 1250 train Loss 0.2423 test Loss 0.2949 with MSE metric 0.2949\n",
      "Epoch 10 batch 1300 train Loss 0.2614 test Loss 0.2988 with MSE metric 0.2988\n",
      "Epoch 10 batch 1350 train Loss 0.2580 test Loss 0.3087 with MSE metric 0.3087\n",
      "Epoch 10 batch 1400 train Loss 0.2445 test Loss 0.3109 with MSE metric 0.3109\n",
      "Epoch 10 batch 1450 train Loss 0.2211 test Loss 0.2896 with MSE metric 0.2896\n",
      "Epoch 10 batch 1500 train Loss 0.2172 test Loss 0.3406 with MSE metric 0.3406\n",
      "Epoch 10 batch 1550 train Loss 0.2235 test Loss 0.3082 with MSE metric 0.3082\n",
      "Epoch 10 batch 1600 train Loss 0.2432 test Loss 0.2645 with MSE metric 0.2645\n",
      "Epoch 10 batch 1650 train Loss 0.2177 test Loss 0.3170 with MSE metric 0.3170\n",
      "Epoch 10 batch 1700 train Loss 0.2680 test Loss 0.2744 with MSE metric 0.2744\n",
      "Epoch 10 batch 1750 train Loss 0.2393 test Loss 0.3087 with MSE metric 0.3087\n",
      "Epoch 10 batch 1800 train Loss 0.2722 test Loss 0.2916 with MSE metric 0.2916\n",
      "Epoch 10 batch 1850 train Loss 0.2732 test Loss 0.2851 with MSE metric 0.2851\n",
      "Time taken for 1 epoch: 184.08893275260925 secs\n",
      "\n",
      "Epoch 11 batch 0 train Loss 0.2597 test Loss 0.2852 with MSE metric 0.2852\n",
      "Epoch 11 batch 50 train Loss 0.2373 test Loss 0.3333 with MSE metric 0.3333\n",
      "Epoch 11 batch 100 train Loss 0.2236 test Loss 0.2783 with MSE metric 0.2783\n",
      "Epoch 11 batch 150 train Loss 0.2151 test Loss 0.3276 with MSE metric 0.3276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 batch 200 train Loss 0.2528 test Loss 0.2686 with MSE metric 0.2686\n",
      "Epoch 11 batch 250 train Loss 0.2272 test Loss 0.2888 with MSE metric 0.2888\n",
      "Epoch 11 batch 300 train Loss 0.2457 test Loss 0.2905 with MSE metric 0.2905\n",
      "Epoch 11 batch 350 train Loss 0.2315 test Loss 0.3087 with MSE metric 0.3087\n",
      "Epoch 11 batch 400 train Loss 0.2309 test Loss 0.2887 with MSE metric 0.2887\n",
      "Epoch 11 batch 450 train Loss 0.2149 test Loss 0.2770 with MSE metric 0.2770\n",
      "Epoch 11 batch 500 train Loss 0.2296 test Loss 0.3172 with MSE metric 0.3172\n",
      "Epoch 11 batch 550 train Loss 0.2323 test Loss 0.3052 with MSE metric 0.3052\n",
      "Epoch 11 batch 600 train Loss 0.2604 test Loss 0.3271 with MSE metric 0.3271\n",
      "Epoch 11 batch 650 train Loss 0.2655 test Loss 0.3449 with MSE metric 0.3449\n",
      "Epoch 11 batch 700 train Loss 0.1960 test Loss 0.2982 with MSE metric 0.2982\n",
      "Epoch 11 batch 750 train Loss 0.2487 test Loss 0.3037 with MSE metric 0.3037\n",
      "Epoch 11 batch 800 train Loss 0.2322 test Loss 0.3132 with MSE metric 0.3132\n",
      "Epoch 11 batch 850 train Loss 0.2674 test Loss 0.2973 with MSE metric 0.2973\n",
      "Epoch 11 batch 900 train Loss 0.2521 test Loss 0.2976 with MSE metric 0.2976\n",
      "Epoch 11 batch 950 train Loss 0.2562 test Loss 0.2881 with MSE metric 0.2881\n",
      "Epoch 11 batch 1000 train Loss 0.2403 test Loss 0.2958 with MSE metric 0.2958\n",
      "Epoch 11 batch 1050 train Loss 0.2436 test Loss 0.3026 with MSE metric 0.3026\n",
      "Epoch 11 batch 1100 train Loss 0.2339 test Loss 0.2808 with MSE metric 0.2808\n",
      "Epoch 11 batch 1150 train Loss 0.2580 test Loss 0.2949 with MSE metric 0.2949\n",
      "Epoch 11 batch 1200 train Loss 0.2155 test Loss 0.2913 with MSE metric 0.2913\n",
      "Epoch 11 batch 1250 train Loss 0.2537 test Loss 0.2880 with MSE metric 0.2880\n",
      "Epoch 11 batch 1300 train Loss 0.2786 test Loss 0.2796 with MSE metric 0.2796\n",
      "Epoch 11 batch 1350 train Loss 0.2344 test Loss 0.2758 with MSE metric 0.2758\n",
      "Epoch 11 batch 1400 train Loss 0.2503 test Loss 0.3035 with MSE metric 0.3035\n",
      "Epoch 11 batch 1450 train Loss 0.2596 test Loss 0.2933 with MSE metric 0.2933\n",
      "Epoch 11 batch 1500 train Loss 0.2358 test Loss 0.3249 with MSE metric 0.3249\n",
      "Epoch 11 batch 1550 train Loss 0.2441 test Loss 0.2816 with MSE metric 0.2816\n",
      "Epoch 11 batch 1600 train Loss 0.2124 test Loss 0.3166 with MSE metric 0.3166\n",
      "Epoch 11 batch 1650 train Loss 0.2483 test Loss 0.3180 with MSE metric 0.3180\n",
      "Epoch 11 batch 1700 train Loss 0.2240 test Loss 0.3009 with MSE metric 0.3009\n",
      "Epoch 11 batch 1750 train Loss 0.2688 test Loss 0.2680 with MSE metric 0.2680\n",
      "Epoch 11 batch 1800 train Loss 0.2519 test Loss 0.3466 with MSE metric 0.3466\n",
      "Epoch 11 batch 1850 train Loss 0.2433 test Loss 0.2992 with MSE metric 0.2992\n",
      "Time taken for 1 epoch: 182.37497210502625 secs\n",
      "\n",
      "Epoch 12 batch 0 train Loss 0.2310 test Loss 0.2959 with MSE metric 0.2959\n",
      "Epoch 12 batch 50 train Loss 0.2740 test Loss 0.3214 with MSE metric 0.3214\n",
      "Epoch 12 batch 100 train Loss 0.2398 test Loss 0.3435 with MSE metric 0.3435\n",
      "Epoch 12 batch 150 train Loss 0.2414 test Loss 0.2950 with MSE metric 0.2950\n",
      "Epoch 12 batch 200 train Loss 0.2158 test Loss 0.2819 with MSE metric 0.2820\n",
      "Epoch 12 batch 250 train Loss 0.2407 test Loss 0.3025 with MSE metric 0.3025\n",
      "Epoch 12 batch 300 train Loss 0.2424 test Loss 0.2922 with MSE metric 0.2922\n",
      "Epoch 12 batch 350 train Loss 0.2399 test Loss 0.2994 with MSE metric 0.2994\n",
      "Epoch 12 batch 400 train Loss 0.2474 test Loss 0.3077 with MSE metric 0.3077\n",
      "Epoch 12 batch 450 train Loss 0.2468 test Loss 0.2842 with MSE metric 0.2842\n",
      "Epoch 12 batch 500 train Loss 0.2483 test Loss 0.3074 with MSE metric 0.3074\n",
      "Epoch 12 batch 550 train Loss 0.2548 test Loss 0.2840 with MSE metric 0.2840\n",
      "Epoch 12 batch 600 train Loss 0.2377 test Loss 0.2856 with MSE metric 0.2856\n",
      "Epoch 12 batch 650 train Loss 0.2291 test Loss 0.3215 with MSE metric 0.3215\n",
      "Epoch 12 batch 700 train Loss 0.2377 test Loss 0.3205 with MSE metric 0.3205\n",
      "Epoch 12 batch 750 train Loss 0.2159 test Loss 0.3013 with MSE metric 0.3013\n",
      "Epoch 12 batch 800 train Loss 0.2402 test Loss 0.3208 with MSE metric 0.3208\n",
      "Epoch 12 batch 850 train Loss 0.2387 test Loss 0.3061 with MSE metric 0.3061\n",
      "Epoch 12 batch 900 train Loss 0.2654 test Loss 0.2632 with MSE metric 0.2632\n",
      "Epoch 12 batch 950 train Loss 0.2484 test Loss 0.2974 with MSE metric 0.2974\n",
      "Epoch 12 batch 1000 train Loss 0.2658 test Loss 0.3058 with MSE metric 0.3058\n",
      "Epoch 12 batch 1050 train Loss 0.2470 test Loss 0.2572 with MSE metric 0.2572\n",
      "Epoch 12 batch 1100 train Loss 0.2237 test Loss 0.2832 with MSE metric 0.2832\n",
      "Epoch 12 batch 1150 train Loss 0.2254 test Loss 0.3216 with MSE metric 0.3216\n",
      "Epoch 12 batch 1200 train Loss 0.2229 test Loss 0.3046 with MSE metric 0.3046\n",
      "Epoch 12 batch 1250 train Loss 0.2504 test Loss 0.2832 with MSE metric 0.2832\n",
      "Epoch 12 batch 1300 train Loss 0.2574 test Loss 0.2955 with MSE metric 0.2955\n",
      "Epoch 12 batch 1350 train Loss 0.2206 test Loss 0.3007 with MSE metric 0.3007\n",
      "Epoch 12 batch 1400 train Loss 0.2164 test Loss 0.2917 with MSE metric 0.2917\n",
      "Epoch 12 batch 1450 train Loss 0.2341 test Loss 0.2976 with MSE metric 0.2976\n",
      "Epoch 12 batch 1500 train Loss 0.2565 test Loss 0.3150 with MSE metric 0.3150\n",
      "Epoch 12 batch 1550 train Loss 0.2612 test Loss 0.2841 with MSE metric 0.2841\n",
      "Epoch 12 batch 1600 train Loss 0.2516 test Loss 0.2940 with MSE metric 0.2940\n",
      "Epoch 12 batch 1650 train Loss 0.2510 test Loss 0.3410 with MSE metric 0.3410\n",
      "Epoch 12 batch 1700 train Loss 0.2598 test Loss 0.3238 with MSE metric 0.3238\n",
      "Epoch 12 batch 1750 train Loss 0.2271 test Loss 0.2871 with MSE metric 0.2871\n",
      "Epoch 12 batch 1800 train Loss 0.2693 test Loss 0.2515 with MSE metric 0.2515\n",
      "Epoch 12 batch 1850 train Loss 0.2412 test Loss 0.2942 with MSE metric 0.2942\n",
      "Time taken for 1 epoch: 183.54047322273254 secs\n",
      "\n",
      "Epoch 13 batch 0 train Loss 0.2300 test Loss 0.2744 with MSE metric 0.2744\n",
      "Epoch 13 batch 50 train Loss 0.2495 test Loss 0.3419 with MSE metric 0.3419\n",
      "Epoch 13 batch 100 train Loss 0.2484 test Loss 0.2992 with MSE metric 0.2992\n",
      "Epoch 13 batch 150 train Loss 0.2465 test Loss 0.3515 with MSE metric 0.3515\n",
      "Epoch 13 batch 200 train Loss 0.2349 test Loss 0.2726 with MSE metric 0.2726\n",
      "Epoch 13 batch 250 train Loss 0.2322 test Loss 0.2733 with MSE metric 0.2733\n",
      "Epoch 13 batch 300 train Loss 0.2384 test Loss 0.3117 with MSE metric 0.3117\n",
      "Epoch 13 batch 350 train Loss 0.2562 test Loss 0.3178 with MSE metric 0.3178\n",
      "Epoch 13 batch 400 train Loss 0.2575 test Loss 0.2851 with MSE metric 0.2851\n",
      "Epoch 13 batch 450 train Loss 0.2689 test Loss 0.3140 with MSE metric 0.3140\n",
      "Epoch 13 batch 500 train Loss 0.2518 test Loss 0.2785 with MSE metric 0.2785\n",
      "Epoch 13 batch 550 train Loss 0.2279 test Loss 0.2873 with MSE metric 0.2873\n",
      "Epoch 13 batch 600 train Loss 0.2332 test Loss 0.3305 with MSE metric 0.3305\n",
      "Epoch 13 batch 650 train Loss 0.2462 test Loss 0.2875 with MSE metric 0.2875\n",
      "Epoch 13 batch 700 train Loss 0.2205 test Loss 0.3015 with MSE metric 0.3015\n",
      "Epoch 13 batch 750 train Loss 0.2614 test Loss 0.2940 with MSE metric 0.2940\n",
      "Epoch 13 batch 800 train Loss 0.2860 test Loss 0.2610 with MSE metric 0.2610\n",
      "Epoch 13 batch 850 train Loss 0.2409 test Loss 0.2816 with MSE metric 0.2816\n",
      "Epoch 13 batch 900 train Loss 0.2365 test Loss 0.3517 with MSE metric 0.3517\n",
      "Epoch 13 batch 950 train Loss 0.2468 test Loss 0.3169 with MSE metric 0.3169\n",
      "Epoch 13 batch 1000 train Loss 0.2537 test Loss 0.3059 with MSE metric 0.3059\n",
      "Epoch 13 batch 1050 train Loss 0.2521 test Loss 0.2756 with MSE metric 0.2756\n",
      "Epoch 13 batch 1100 train Loss 0.2686 test Loss 0.2903 with MSE metric 0.2903\n",
      "Epoch 13 batch 1150 train Loss 0.2344 test Loss 0.2780 with MSE metric 0.2780\n",
      "Epoch 13 batch 1200 train Loss 0.2731 test Loss 0.3027 with MSE metric 0.3027\n",
      "Epoch 13 batch 1250 train Loss 0.2475 test Loss 0.2658 with MSE metric 0.2658\n",
      "Epoch 13 batch 1300 train Loss 0.2259 test Loss 0.2730 with MSE metric 0.2730\n",
      "Epoch 13 batch 1350 train Loss 0.2402 test Loss 0.3029 with MSE metric 0.3029\n",
      "Epoch 13 batch 1400 train Loss 0.2305 test Loss 0.3341 with MSE metric 0.3341\n",
      "Epoch 13 batch 1450 train Loss 0.2614 test Loss 0.2892 with MSE metric 0.2892\n",
      "Epoch 13 batch 1500 train Loss 0.2199 test Loss 0.2865 with MSE metric 0.2865\n",
      "Epoch 13 batch 1550 train Loss 0.2498 test Loss 0.2913 with MSE metric 0.2913\n",
      "Epoch 13 batch 1600 train Loss 0.2353 test Loss 0.2736 with MSE metric 0.2736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 batch 1650 train Loss 0.2626 test Loss 0.3177 with MSE metric 0.3177\n",
      "Epoch 13 batch 1700 train Loss 0.2425 test Loss 0.2662 with MSE metric 0.2662\n",
      "Epoch 13 batch 1750 train Loss 0.2151 test Loss 0.3050 with MSE metric 0.3050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a81c19e1c924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mbatch_pos_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_pos_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_tar_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/studies/Cambridge/Damon/attention/helpers/masks.py\u001b[0m in \u001b[0;36mposition_mask\u001b[0;34m(arr, mask_val)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mk\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mspecific\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam(0.003)\n",
    "    decoder = classic_model.Decoder(64)\n",
    "    EPOCHS = 750\n",
    "    batch_s  = 64\n",
    "    run = 25; step = 0\n",
    "    num_batches = int(pad_y_fren_tr.shape[0] / batch_s)\n",
    "#   tf.random.set_seed(1)   \n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer_c, net = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "    manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "    \n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                m_tr.reset_states(); train_loss.reset_states()\n",
    "                m_te.reset_states(); test_loss.reset_states()\n",
    "                batch_pos_tr, batch_tar_tr, _ = batch_creator.create_batch_gp_mim_2(pad_pos_tr, pad_y_fren_tr, batch_s=64)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                batch_pos_mask = masks.position_mask(batch_pos_tr)\n",
    "                train_step(decoder, optimizer_c, train_loss, m_tr, batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "                \n",
    "                if batch_n % 50 == 0:\n",
    "                    batch_pos_te, batch_tar_te, _ = batch_creator.create_batch_gp_mim_2(pad_pos_te, pad_y_fren_te, batch_s=64)\n",
    "                    batch_pos_mask_te = masks.position_mask(batch_pos_te)\n",
    "                    test_step(decoder, test_loss, m_te, batch_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_te.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    manager.save()\n",
    "                step += 1\n",
    "                ckpt.step.assign_add(1)\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrapo = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extrapo:\n",
    "    x = np.load('/Users/omernivron/Downloads/GPT_data_goldstandard/x_extra.npy')\n",
    "    y = np.load('/Users/omernivron/Downloads/GPT_data_goldstandard/y_extra.npy')\n",
    "else:\n",
    "    x = np.load('/Users/omernivron/Downloads/GPT_data_goldstandard/x_interpol.npy')\n",
    "    y = np.load('/Users/omernivron/Downloads/GPT_data_goldstandard/y_interpol.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_metric = 0; r_sq_metric = 0; kuee_metric = 0;\n",
    " = [];  = []\n",
    "m = int(x.shape[0] / 10)\n",
    "y_mean = np.mean(y[:m, :40])\n",
    "y_te = y[:m, 40]\n",
    "for j in range(0, m):\n",
    "    x_tr = x[j, :41].reshape(1, -1)\n",
    "    y_tr = y[j, :40].reshape(1, -1)\n",
    "    _te = infer.inference(decoder, x_tr, y_tr)\n",
    "#     _te, log__te = infer.inference(decoder, x_tr, y_tr, mh=True)\n",
    "\n",
    "\n",
    "    .append(_te[0][-1].numpy()); \n",
    "#     .append(log__te[-1])\n",
    "#     kuee_metric += metrics.KUEE(y_te[j], _te[-1], np.exp(log__te[-1]))\n",
    "#     if (j % 400 == 0): \n",
    "#         print('J: ', j)\n",
    "#         axes = plt.gca()\n",
    "#         axes.set_ylim([-2, 2])\n",
    "#         plt.scatter(x_tr[:, :-1], y_tr, c = 'black')\n",
    "#         plt.scatter(x_tr[:, 1:], _te, c='navy')\n",
    "#         plt.scatter(x_tr[:, -1], y_te[j], c='purple')\n",
    "#         plt.scatter(x_tr[:, -1], _te[-1], c='red')\n",
    "# #         plt.errorbar(x = x_tr[:, 40], y = (_te[-1]), yerr = 2 * np.exp(log__te[-1]), fmt='o', ecolor='g', capthick=2)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# #         plt.fill_between(x_tr[:, 1:].squeeze(), _te -2 * np.exp(log__te), _te  + 2 * np.exp(log__te), alpha=.2)\n",
    "\n",
    "#         plt.show()\n",
    "    \n",
    "mse_metric = metrics.mse(y_te, ) \n",
    "r_sq_metric = metrics.r_squared(y_te, , y_mean)  \n",
    "mse_metric *= (1 / m)\n",
    "# kuee_metric *= (1 / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5012839537371766"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_sq_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4851938633914596"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    mse_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
