{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn.gaussian_process as gp\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "import os\n",
    "from data_generation import *\n",
    "from batch_creator import *\n",
    "from gp_kernels import *\n",
    "from gp_priors import *\n",
    "from gp_plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp_prior(4, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_gp_mim_2(pos, tar, pos_mask, batch_s=128):\n",
    "    '''\n",
    "    '''\n",
    "    shape = tar.shape[0]\n",
    "    batch_idx_tr = np.random.choice(list(range(shape)), batch_s)\n",
    "    batch_tar_tr = tar[batch_idx_tr, :]\n",
    "    batch_pos_tr = pos[batch_idx_tr, :]\n",
    "    batch_pos_mask = pos_mask[batch_idx_tr, :, :, :]\n",
    "    return batch_tar_tr, batch_pos_tr, batch_pos_mask, batch_idx_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_for_gp_mimick_gpt(num_obs, kernel, tr_percent=0.8):\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    obs_per_sample = np.random.randint(20, 60, size = num_obs)\n",
    "    df = np.zeros((num_obs * 2, np.max(obs_per_sample)))\n",
    "    for i in range(0, num_obs * 2, 2):\n",
    "        x = np.random.uniform(-5, 5, size=(1, obs_per_sample[int(i / 2)]))\n",
    "        k = kernel(x)\n",
    "        f_prior = generate_priors(k, obs_per_sample[int(i / 2)], 1)\n",
    "\n",
    "        df[i, :x.shape[1]] = x\n",
    "        df[i + 1, :x.shape[1]] = f_prior\n",
    "\n",
    "    rows = df.shape[0]\n",
    "    cols = df.shape[1]\n",
    "    tr_rows = int(tr_percent * rows)\n",
    "    tr_rows = tr_rows if tr_rows % 2 == 0 else tr_rows + 1\n",
    "    df_tr = df[:tr_rows, :]\n",
    "    df_te = df[tr_rows:, :]\n",
    "\n",
    "    pad_pos_tr = df_tr[::2, :]\n",
    "    pad_pos_te = df_te[::2, :]\n",
    "    pad_y_fren_tr = df_tr[1::2, :]\n",
    "    pad_y_fren_te = df_te[1::2, :]\n",
    "\n",
    "    return pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_mask(batch):\n",
    "    '''\n",
    "    This tries to emulate the kernel matrix\n",
    "    '''\n",
    "    rows = batch.shape[0]\n",
    "    cols = batch.shape[1]\n",
    "    generic_mask = np.zeros((rows, cols, cols, cols))\n",
    "    specific = np.sum(np.equal(batch, 0), 1)\n",
    "    for i in range(2, cols + 1):\n",
    "        generic_mask[:, i - 2, :i, :i] = np.ones((i, i))\n",
    "    for j in range(rows):\n",
    "        k  = specific[j]\n",
    "        generic_mask[j, k:, :, :] = 0\n",
    "            \n",
    "    return generic_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention.\n",
    "    return seq[:, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(position, tar):\n",
    "    '''\n",
    "    -------------------\n",
    "    Parameters: \n",
    "    position :\n",
    "    tar :\n",
    "    -------------------\n",
    "    Returns: \n",
    "    combined_mask_pos :\n",
    "    combined_mask_tar :\n",
    "    '''\n",
    "    \n",
    "    pos_padding_mask = create_padding_mask(position)\n",
    "    tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    look_ahead_mask_pos = create_look_ahead_mask(tf.shape(position)[1])\n",
    "    # if max seq length is 40 -- > this will be 40X40 \n",
    "    ## this will be batch_size X 1 X 40\n",
    "    \n",
    "    ## This will also be (64, 40, 40)\n",
    "    combined_mask_pos = tf.maximum(pos_padding_mask, look_ahead_mask_pos)\n",
    "    combined_mask_tar = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    \n",
    "    return combined_mask_pos, combined_mask_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te = data_generator_for_gp_mimick_gpt(10000, rbf_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = position_mask(pad_pos_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.Loss()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the target sequences are padded, \n",
    "# it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.square(tf.cast(real, tf.float32) - pred)\n",
    "    \n",
    "#     print('loss_ :', loss_)\n",
    "#     shape= (128X58)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_prod_position(q, k, mask):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    matmul_qk = tf.matmul(q, k, transpose_a = True)\n",
    "    nl_qk = tf.nn.relu(matmul_qk) \n",
    "    nl_qk = nl_qk[:, tf.newaxis, :, :]\n",
    "#     print('nl_qk: ', nl_qk)\n",
    "#     shape=(128, 1, 59, 59)\n",
    "\n",
    "#     print('pos_mask:', mask)\n",
    "#     shape=(128, 59, 59, 59)\n",
    "    if mask is not None:\n",
    "        nl_qk *= (tf.cast(mask, tf.float32))\n",
    "    \n",
    "    return nl_qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q, k, v, mask):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # similarity\n",
    "    # q = k = v  shape := (batch_size, max_seq_len - 1, max_seq_len -1)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_a = True)\n",
    "#     print('matmul_qk: ', matmul_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "    \n",
    "    nl_qk = tf.nn.relu(matmul_qk) \n",
    "#     print('nl_qk: ', nl_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "#     nl_qk shape := (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "\n",
    "    # -1e9 will turn the softmax output in this locations to zero\n",
    "    # this is a good mask as an input for softmax -- we need also masking when \n",
    "    # want to use matmul as is \n",
    "    \n",
    "    if mask is not None:\n",
    "        nl_qk +=  (mask * -1e9)\n",
    "    \n",
    "        \n",
    "#     print('nl_qk after mask: ', nl_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "        \n",
    "     # turn simialrity to scores\n",
    "    attention_weights = tf.nn.softmax(nl_qk, axis = -1)\n",
    "    \n",
    "#     print('attention_weights: ', attention_weights)\n",
    "#     shape=(128, 58, 58)\n",
    "   \n",
    "    # weight values \n",
    "    # attention_weights shape := (batch_size, max_seq_len - 1, max_seq_len - 1), \n",
    "    # v_transpose shape := max_seq_len X batch_size\n",
    "    out_tar = tf.matmul(attention_weights, v, transpose_b = True)\n",
    "    \n",
    "#   print('out_tar: ', out_tar)\n",
    "#   shape=(128, 58, 58)\n",
    "    \n",
    "    return out_tar, attention_weights, matmul_qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(dff):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(1)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, pos_d_model, tar_d_model, rate, d_att, dff):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.tar_d_model = tar_d_model\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(pos_d_model)\n",
    "        self.wk = tf.keras.layers.Dense(pos_d_model)\n",
    "        self.wv = tf.keras.layers.Dense(pos_d_model)\n",
    "                    \n",
    "        \n",
    "        self.hq = tf.keras.layers.Dense(tar_d_model)\n",
    "        self.hk = tf.keras.layers.Dense(tar_d_model)\n",
    "        self.hv = tf.keras.layers.Dense(tar_d_model)\n",
    "        \n",
    "        self.h_att = tf.keras.layers.Dense(d_att)\n",
    "        \n",
    "#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(dff)\n",
    "\n",
    "    #a call method, the layer's forward pass\n",
    "    def call(self, tar_position, tar_inp, training, pos_mask, tar_mask):\n",
    "        \n",
    "        tar_position = tar_position[:, :, tf.newaxis]\n",
    "        \n",
    "        q_p = self.wq(tar_position) \n",
    "        k_p = self.wk(tar_position)\n",
    "#       print('q_p: ', q_p)\n",
    "#       shape=(128, 59, 59) = (batch_size X max_seq_len X max_seq_len) \n",
    "        \n",
    "        pos_attn1 = dot_prod_position(q_p, k_p, mask = pos_mask)\n",
    "#       print('pos_attn1 :', pos_attn1)\n",
    "#       shape=(128, 59, 59, 59)\n",
    "        pos_attn1 = tf.reshape(pos_attn1, shape = [tf.shape(pos_attn1)[0] ,-1])\n",
    "        # pos_attn1 is (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "    \n",
    "        tar_inp = tar_inp[:, :, tf.newaxis]\n",
    "\n",
    "        \n",
    "        q = self.hq(tar_inp) \n",
    "        k = self.hk(tar_inp)\n",
    "        v = self.hv(tar_inp)\n",
    "        \n",
    "#       print('q :', q)\n",
    "#       shape=(128, 58, 58)\n",
    "\n",
    "        tar_attn1, _, _ = dot_product_attention(q, k, v, tar_mask)\n",
    "        # tar_attn1 is (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "\n",
    "#         print('tar_attn1 :', tar_attn1)\n",
    "#         shape=(128, 58, 58)\n",
    "        \n",
    "        position = tf.reshape(self.h_att(pos_attn1), shape = [tf.shape(pos_attn1)[0], self.tar_d_model, self.tar_d_model])\n",
    "                \n",
    "        connector = tf.matmul(position, tar_attn1) \n",
    "        \n",
    "#         print('connector :', connector)\n",
    "#         shape=(128, 58, 58)\n",
    "\n",
    "        out = tf.reshape(self.ffn(connector), shape = [tf.shape(connector)[0], tf.shape(connector)[1]])\n",
    "        \n",
    "#         print('out :', out)\n",
    "#         shape=(128, 58)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(59, 58, 0.3, 58**2, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(pos, tar, pos_mask):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    _, combined_mask_tar = create_masks(pos, tar_inp)\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = decoder(pos, tar_inp, True,pos_mask, combined_mask_tar)\n",
    "\n",
    "        loss = loss_function(tar_real, pred)\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 6.6077\n",
      "Epoch 1 Batch 50 Loss 3.8566\n",
      "Time taken for 1 epoch: 1088.7277348041534 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    EPOCHS = 1\n",
    "    batch_s  = 128\n",
    "    num_batches = int(pad_y_fren_tr.shape[0] / batch_s)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        train_loss.reset_states()\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            batch_tar_tr, batch_pos_tr, batch_pos_mask, _ = create_batch_gp_mim_2(pad_pos_tr, pad_y_fren_tr, pp)\n",
    "            # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "            # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "            train_step(batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, batch, train_loss.result()))\n",
    "\n",
    "        print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt[5000, 0,  1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
