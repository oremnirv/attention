{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te, _, df_te = data_generation.data_generator_for_gp_mimick_gpt(50000, gp_kernels.rbf_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = masks.position_mask(pad_pos_tr)\n",
    "pp_te = masks.position_mask(pad_pos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "r_sq_tr = tfa.metrics.RSquare(dtype = tf.float64)\n",
    "r_sq_te = tfa.metrics.RSquare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred)\n",
    "\n",
    "        loss = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred, pred_sig = decoder(pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss = losses.loss_function(tar_real_te, pred, pred_sig)\n",
    "    test_loss(t_loss)\n",
    "#     r_sq_te.update_state(tar_real_te, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New folder /Users/omernivron/Downloads/GPT/ckpt/check_0\n",
      "Epoch 0 batch 0 train Loss 3.5894 test Loss 1.6112 with R^2 nan\n",
      "Epoch 0 batch 50 train Loss 1.2462 test Loss 0.9822 with R^2 nan\n",
      "Epoch 0 batch 100 train Loss 0.9187 test Loss 0.7395 with R^2 nan\n",
      "Epoch 0 batch 150 train Loss 0.7620 test Loss 0.6066 with R^2 nan\n",
      "Epoch 0 batch 200 train Loss 0.6627 test Loss 0.5288 with R^2 nan\n",
      "Epoch 0 batch 250 train Loss 0.5921 test Loss 0.4552 with R^2 nan\n",
      "Epoch 0 batch 300 train Loss 0.5352 test Loss 0.3995 with R^2 nan\n",
      "Time taken for 1 epoch: 458.16467475891113 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 0.5237 test Loss 0.3616 with R^2 nan\n",
      "Epoch 1 batch 50 train Loss 0.4806 test Loss 0.3317 with R^2 nan\n",
      "Epoch 1 batch 100 train Loss 0.4458 test Loss 0.3057 with R^2 nan\n",
      "Epoch 1 batch 150 train Loss 0.4171 test Loss 0.2847 with R^2 nan\n",
      "Epoch 1 batch 200 train Loss 0.3923 test Loss 0.2665 with R^2 nan\n",
      "Epoch 1 batch 250 train Loss 0.3706 test Loss 0.2526 with R^2 nan\n",
      "Epoch 1 batch 300 train Loss 0.3511 test Loss 0.2393 with R^2 nan\n",
      "Time taken for 1 epoch: 474.08354091644287 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 0.3469 test Loss 0.2297 with R^2 nan\n",
      "Epoch 2 batch 50 train Loss 0.3299 test Loss 0.2192 with R^2 nan\n",
      "Epoch 2 batch 100 train Loss 0.3146 test Loss 0.2114 with R^2 nan\n",
      "Epoch 2 batch 150 train Loss 0.3009 test Loss 0.2024 with R^2 nan\n",
      "Epoch 2 batch 200 train Loss 0.2888 test Loss 0.1951 with R^2 nan\n",
      "Epoch 2 batch 250 train Loss 0.2776 test Loss 0.1884 with R^2 nan\n",
      "Epoch 2 batch 300 train Loss 0.2673 test Loss 0.1827 with R^2 nan\n",
      "Time taken for 1 epoch: 486.4712471961975 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss 0.2649 test Loss 0.1765 with R^2 nan\n",
      "Epoch 3 batch 50 train Loss 0.2557 test Loss 0.1719 with R^2 nan\n",
      "Epoch 3 batch 100 train Loss 0.2469 test Loss 0.1668 with R^2 nan\n",
      "Epoch 3 batch 150 train Loss 0.2387 test Loss 0.1616 with R^2 nan\n",
      "Epoch 3 batch 200 train Loss 0.2312 test Loss 0.1576 with R^2 nan\n",
      "Epoch 3 batch 250 train Loss 0.2243 test Loss 0.1538 with R^2 nan\n",
      "Epoch 3 batch 300 train Loss 0.2177 test Loss 0.1499 with R^2 nan\n",
      "Time taken for 1 epoch: 489.12917709350586 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss 0.2161 test Loss 0.1468 with R^2 nan\n",
      "Epoch 4 batch 50 train Loss 0.2099 test Loss 0.1435 with R^2 nan\n",
      "Epoch 4 batch 100 train Loss 0.2040 test Loss 0.1405 with R^2 nan\n",
      "Epoch 4 batch 150 train Loss 0.1983 test Loss 0.1374 with R^2 nan\n",
      "Epoch 4 batch 200 train Loss 0.1930 test Loss 0.1344 with R^2 nan\n",
      "Epoch 4 batch 250 train Loss 0.1881 test Loss 0.1321 with R^2 nan\n",
      "Epoch 4 batch 300 train Loss 0.1835 test Loss 0.1297 with R^2 nan\n",
      "Time taken for 1 epoch: 487.9830322265625 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss 0.1824 test Loss 0.1273 with R^2 nan\n",
      "Epoch 5 batch 50 train Loss 0.1781 test Loss 0.1249 with R^2 nan\n",
      "Epoch 5 batch 100 train Loss 0.1739 test Loss 0.1228 with R^2 nan\n",
      "Epoch 5 batch 150 train Loss 0.1700 test Loss 0.1206 with R^2 nan\n",
      "Epoch 5 batch 200 train Loss 0.1662 test Loss 0.1184 with R^2 nan\n",
      "Epoch 5 batch 250 train Loss 0.1626 test Loss 0.1168 with R^2 nan\n",
      "Epoch 5 batch 300 train Loss 0.1592 test Loss 0.1151 with R^2 nan\n",
      "Time taken for 1 epoch: 486.31542587280273 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss 0.1584 test Loss 0.1131 with R^2 nan\n",
      "Epoch 6 batch 50 train Loss 0.1551 test Loss 0.1117 with R^2 nan\n",
      "Epoch 6 batch 100 train Loss 0.1519 test Loss 0.1102 with R^2 nan\n",
      "Epoch 6 batch 150 train Loss 0.1489 test Loss 0.1085 with R^2 nan\n",
      "Epoch 6 batch 200 train Loss 0.1460 test Loss 0.1070 with R^2 nan\n",
      "Epoch 6 batch 250 train Loss 0.1432 test Loss 0.1054 with R^2 nan\n",
      "Epoch 6 batch 300 train Loss 0.1406 test Loss 0.1039 with R^2 nan\n",
      "Time taken for 1 epoch: 496.43780994415283 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss 0.1399 test Loss 0.1024 with R^2 nan\n",
      "Epoch 7 batch 50 train Loss 0.1374 test Loss 0.1012 with R^2 nan\n",
      "Epoch 7 batch 100 train Loss 0.1350 test Loss 0.0999 with R^2 nan\n",
      "Epoch 7 batch 150 train Loss 0.1327 test Loss 0.0987 with R^2 nan\n",
      "Epoch 7 batch 200 train Loss 0.1304 test Loss 0.0978 with R^2 nan\n",
      "Epoch 7 batch 250 train Loss 0.1283 test Loss 0.0967 with R^2 nan\n",
      "Epoch 7 batch 300 train Loss 0.1262 test Loss 0.0954 with R^2 nan\n",
      "Time taken for 1 epoch: 500.8818337917328 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss 0.1257 test Loss 0.0942 with R^2 nan\n",
      "Epoch 8 batch 50 train Loss 0.1237 test Loss 0.0930 with R^2 nan\n",
      "Epoch 8 batch 100 train Loss 0.1218 test Loss 0.0924 with R^2 nan\n",
      "Epoch 8 batch 150 train Loss 0.1199 test Loss 0.0913 with R^2 nan\n",
      "Epoch 8 batch 200 train Loss 0.1182 test Loss 0.0902 with R^2 nan\n",
      "Epoch 8 batch 250 train Loss 0.1164 test Loss 0.0893 with R^2 nan\n",
      "Epoch 8 batch 300 train Loss 0.1148 test Loss 0.0885 with R^2 nan\n",
      "Time taken for 1 epoch: 490.8207108974457 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss 0.1144 test Loss 0.0877 with R^2 nan\n",
      "Epoch 9 batch 50 train Loss 0.1127 test Loss 0.0868 with R^2 nan\n",
      "Epoch 9 batch 100 train Loss 0.1112 test Loss 0.0859 with R^2 nan\n",
      "Epoch 9 batch 150 train Loss 0.1097 test Loss 0.0853 with R^2 nan\n",
      "Epoch 9 batch 200 train Loss 0.1082 test Loss 0.0845 with R^2 nan\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = model.Decoder(16)\n",
    "    EPOCHS = 50\n",
    "    batch_s  = 128\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(pad_y_fren_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)    \n",
    "    checkpoint = tf.train.Checkpoint(optimizer = optimizer_c, model = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                batch_pos_tr, batch_tar_tr, batch_pos_mask, _ = batch_creator.create_batch_gp_mim_2(pad_pos_tr, pad_y_fren_tr, pp)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                train_step(batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 50 == 0:\n",
    "                    batch_pos_te, batch_tar_te, batch_pos_mask_te, _ = batch_creator.create_batch_gp_mim_2(pad_pos_te, pad_y_fren_te, pp_te)\n",
    "                    test_step(batch_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), r_sq_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), r_sq_tr.result(), r_sq_te.result())\n",
    "                    checkpoint.save(folder + '/')\n",
    "                step += 1\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tar = pad_y_fren_te[:, 1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999999999833459"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23126513, -0.22783515, -0.15818167, ..., -0.12317099,\n",
       "        -0.32767493, -0.21421905],\n",
       "       [-0.70544454, -0.73005885, -0.6830323 , ..., -0.74098858,\n",
       "        -0.59824057, -0.81173125],\n",
       "       [ 0.10899795,  0.18115021,  0.22626579, ...,  0.20117891,\n",
       "         0.25371165,  0.13876939],\n",
       "       ...,\n",
       "       [-0.38531545, -0.30379112, -0.27217713, ..., -0.26784763,\n",
       "        -0.299351  , -0.39355877],\n",
       "       [ 1.2178523 ,  1.30296639,  1.24647646, ...,  1.19968792,\n",
       "         1.19953355,  1.18947239],\n",
       "       [-1.21407981, -1.19715032, -1.27335662, ..., -1.23137991,\n",
       "        -1.22176898, -1.21790715]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 58)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005712169343526802"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0170927027632388"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916270624993678"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
