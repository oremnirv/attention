{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn.gaussian_process as gp\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "import os\n",
    "from data_generation import *\n",
    "from batch_creator import *\n",
    "from gp_kernels import *\n",
    "from gp_priors import *\n",
    "from gp_plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp_prior(4, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_gp_mim_2(pos, tar, pos_mask, batch_s=128):\n",
    "    '''\n",
    "    Get a batch of positions, targets and position mask from data generated \n",
    "    by data_generator_for_gp_mimick_gpt function and from position_mask function \n",
    "    -------------------------\n",
    "    Parameters:\n",
    "    pos (2D np array): 1st/2nd output from data_generator_for_gp_mimick_gpt function \n",
    "    tar (2D np array): 3rd/4th output from data_generator_for_gp_mimick_gpt function  \n",
    "    pos_mask (4D np.array): output from position_mask function \n",
    "    batch_s (int): deafult 128\n",
    "    -------------------------\n",
    "    Returns:\n",
    "    batch_pos_tr (2D np array)\n",
    "    batch_tar_tr (2D np array)\n",
    "    batch_pos_mask (4D np array)\n",
    "    batch_idx_tr (1D np array): indices (=row numbers) chosen for current batch\n",
    "    \n",
    "    '''\n",
    "    shape = tar.shape[0]\n",
    "    batch_idx_tr = np.random.choice(list(range(shape)), batch_s)\n",
    "    batch_tar_tr = tar[batch_idx_tr, :]\n",
    "    batch_pos_tr = pos[batch_idx_tr, :]\n",
    "    batch_pos_mask = pos_mask[batch_idx_tr, :, :, :]\n",
    "    return batch_pos_tr, batch_tar_tr , batch_pos_mask, batch_idx_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_for_gp_mimick_gpt(num_obs, kernel, tr_percent=0.8):\n",
    "    '''\n",
    "    Generator for training a GPT inspired netowrk. Make sure x is drawn in a range that \n",
    "    Doesn't include 0 --> 0 is currently used for padding.\n",
    "    -----------------------\n",
    "    Parameters:\n",
    "    num_obs (int): how many observations to generate\n",
    "    kernel (function of am SKlearn kernel object): e.g. rbf_kernel which comes from gp_kernels file\n",
    "    tr_percent (float): daefult 0.8\n",
    "    -----------------------\n",
    "    Returns:\n",
    "    pad_pos_tr (np array): the first rows * tr_percent from the x generated values padded by zeros according to obs_per_sample  \n",
    "    pad_pos_te (np array): all rows of x not chosen for training \n",
    "    pad_y_fren_tr (np array): the first rows * tr_percent from the f_prior generated values padded by zeros according to obs_per_sample  \n",
    "    pad_y_fren_te (np array): all rows of f_prior not chosen for training\n",
    "    df_tr (np array): positions and targets combined (training) \n",
    "    df_te (np array): positions and targets combined (testing) \n",
    "    '''\n",
    "    df = np.zeros((num_obs * 2, 59))\n",
    "    for i in range(0, num_obs * 2, 2):\n",
    "        x = np.random.uniform(5, 15, size=(1, 59))\n",
    "        k = kernel(x)\n",
    "        f_prior = generate_priors(k, 59, 1)\n",
    "\n",
    "        df[i, :x.shape[1]] = x\n",
    "        df[i + 1, :x.shape[1]] = f_prior\n",
    "\n",
    "    rows = df.shape[0]\n",
    "    cols = df.shape[1]\n",
    "    tr_rows = int(tr_percent * rows)\n",
    "    tr_rows = tr_rows if tr_rows % 2 == 0 else tr_rows + 1\n",
    "    df_tr = df[:tr_rows, :]\n",
    "    df_te = df[tr_rows:, :]\n",
    "    \n",
    "    # get all even rows\n",
    "    pad_pos_tr = df_tr[::2, :]\n",
    "    pad_pos_te = df_te[::2, :]\n",
    "    # get all odd rows\n",
    "    pad_y_fren_tr = df_tr[1::2, :]\n",
    "    pad_y_fren_te = df_te[1::2, :]\n",
    "\n",
    "    return pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te, df_tr, df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_mask(arr):\n",
    "    '''\n",
    "    This tries to emulate the kernel matrix. \n",
    "    In the first stage we have a 2X2 matrix of zeros, next\n",
    "    3X3 matrix of zeros, etc.\n",
    "    -------------------------\n",
    "    Parameters:\n",
    "    arr (np array): the 1st/2nd output from data_generator_for_gp_mimick_gpt function\n",
    "    -------------------------\n",
    "    Returns:\n",
    "    mask (4D np array): if there are 100 rows and 50 cols in arr then this will \n",
    "    return [100, 49, 50, 50] array -- where the first dim is observation number \n",
    "    second dim is timestamp and third+fourth dim are the mask matrix.\n",
    "    '''\n",
    "    rows = arr.shape[0]\n",
    "    cols = arr.shape[1]\n",
    "    mask = np.ones((rows, cols - 1, cols, cols))\n",
    "    specific = np.sum(np.equal(arr, 0), 1)\n",
    "    for i in range(2, cols + 1):\n",
    "        mask[:, i - 2, :i, :i] = np.zeros((i, i))\n",
    "    for j in range(rows):\n",
    "        k  = specific[j]\n",
    "        mask[j, k:, :, :] = 1\n",
    "            \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    '''\n",
    "    Used to pad sequences that have zeros where there was no event.\n",
    "    Typically this will be combined with create_look_ahead_mask function.\n",
    "    This function is used inside an open session of tensorflow. \n",
    "    To try it out create a tf.constant tensor.\n",
    "    -------------------\n",
    "    Parameters:\n",
    "    seq (tensor): shape is (batch_size, seq_len)\n",
    "    \n",
    "    -------------------\n",
    "    Returns:\n",
    "    A binary tensor  (batch_size, 1, seq_len): 1 where there was no event and 0 otherwise.\n",
    "    \n",
    "    '''\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention. Extra dimension is used in create_masks function\n",
    "    return seq[:, tf.newaxis, :]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tar_mask(size):\n",
    "    '''\n",
    "    '''\n",
    "    mask = tf.linalg.diag(tf.ones(size, size))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_tar_mask(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    '''\n",
    "    Hide future outputs from a decoder style network.\n",
    "    Used typically together with create_padding_mask function\n",
    "    -----------------------\n",
    "    Parameters:\n",
    "    size (int): max sequnce length \n",
    "    \n",
    "    -----------------------\n",
    "    Returns:\n",
    "    mask (tensor): shape is (seq_len X seq_len). Example: if size is 4, returns\n",
    "    0 1 1 1\n",
    "    0 0 1 1\n",
    "    0 0 0 1\n",
    "    0 0 0 0 \n",
    "    where 1 signifies what to hide.\n",
    "    '''\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(tar):\n",
    "    '''\n",
    "    Create unified masking hiding future from current timestamps and hiding paddings. \n",
    "    -------------------\n",
    "    Parameters: \n",
    "    tar (tensor): batch of padded target sequences \n",
    "    -------------------\n",
    "    Returns: \n",
    "    combined_mask_tar  (tensor): shape is batch_size X max_seq_len X max_seq_len\n",
    "    '''\n",
    "    \n",
    "    tar_padding_mask = create_padding_mask(tar)\n",
    "    ## this will be batch_size X 1 X 40\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    # if max seq length is 40 -- > this will be 40X40 \n",
    "    \n",
    "    \n",
    "    ## This will also be (64, 40, 40)\n",
    "    combined_mask_tar = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    \n",
    "    return combined_mask_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te, _, df_te = data_generator_for_gp_mimick_gpt(50000, rbf_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = position_mask(pad_pos_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_te = position_mask(pad_pos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    '''\n",
    "    Masked MSE. Since the target sequences are padded, \n",
    "    it is important to apply a padding mask when calculating the loss.\n",
    "    ----------------\n",
    "    Parameters:\n",
    "    real (tf.tensor float64): shape batch_size X max_seq_len. True values of sequences.\n",
    "    pred (tf.tensor float64): shape batch_size X max_seq_len. Predictions from GPT network. \n",
    "    \n",
    "    ----------------\n",
    "    Returns: \n",
    "    loss value (tf.float64)\n",
    "    '''\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "#     print('loss_ :', loss_)\n",
    "#     shape= (128X58)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_prod_position(q, k, v, mask):\n",
    "    '''\n",
    "    Used to create a pseudo XX^T covariance matrix for each \n",
    "    positional sequence in the batch.\n",
    "    ------------------\n",
    "    Parameters: \n",
    "    q : shape (batch_size X max_seq_len X l). Position outptut from create_batch_gp_mim_2 function (or after another Dense layer) \n",
    "    k : shape (batch_size X max_seq_len X l). Position outptut from create_batch_gp_mim_2 function (or after another Dense layer) \n",
    "    v : shape (batch_size X max_seq_len X l). Position outptut from create_batch_gp_mim_2 function (or after another Dense layer) \n",
    "    mask: shape (batch_size X (max_seq_len - 1) X max_seq_len X max_seq_len). The positional mask created by position_mask function and selected in batch indices \n",
    "    \n",
    "    ------------------\n",
    "    Returns:\n",
    "    u2 (tf.tensor float64): shape (batch_size X (max_seq_len - 1) X l X l).\n",
    "    Each observation (1st dim) has seq_len - 1 timestamps (2nd dim) and each timestamp has an associated\n",
    "    l X l pseudo covariance matrix (3rd & 4th dims).\n",
    "    \n",
    "    '''\n",
    "    qk = tf.matmul(q, k, transpose_b = True)\n",
    "    qk = tf.cast(qk[:, tf.newaxis, :, :], tf.float64)\n",
    "#     print('qk1: ', qk)\n",
    "#     shape=(128, 1, 59, 59)\n",
    "\n",
    "#     print('pos_mask: ', mask)\n",
    "#     shape=(128, 58, 59, 59)\n",
    "    if mask is not None:\n",
    "        qk +=  ((tf.cast(mask, tf.float64)) * -1e9)\n",
    "        \n",
    "#     print('qk2: ', qk)\n",
    "# shape=(128, 58, 59, 59)\n",
    "\n",
    "    qk = tf.reshape(qk, shape = [tf.shape(mask)[0], tf.shape(mask)[1], -1])\n",
    "    \n",
    "#     print('qk3: ', qk)\n",
    "#     shape=(128, 58, 3481)\n",
    "    \n",
    "    qk = tf.reshape(tf.nn.softmax(qk, axis = -1), shape = [tf.shape(mask)[0], tf.shape(mask)[1], tf.shape(mask)[2], tf.shape(mask)[3]])\n",
    "    \n",
    "#     print('qk4: ', qk)\n",
    "    #shape=(128, 58, 59, 59)\n",
    "    \n",
    "    v = tf.cast(v[:, tf.newaxis, :, :], tf.float64)\n",
    "    \n",
    "    u = tf.transpose(tf.matmul(qk, v), perm = [0, 1, 3 ,2])\n",
    "    \n",
    "#     print('u: ', u)\n",
    "#     shape=(128, 58, 16, 59)\n",
    "    \n",
    "    u2 = tf.matmul(u, v)\n",
    "    \n",
    "    \n",
    "    return u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q, k, v, mask):\n",
    "    '''\n",
    "    Attention inspired by Transformer (but not the same). The Transformer embeds the \n",
    "    target words to q (query), k (key), v (value). So if we have a batch of 128 sequences \n",
    "    with max length 40 and embedding layer is 20, we will get shape q = shape k = shape v\n",
    "    = (128 X  max sequence length X 20). The Transformer then transposes k \n",
    "    to get after matmul (128 X max seq X max seq) matrix. We then apply relu layer (unlike in Transformer)\n",
    "    ---------------------\n",
    "    Parameters:\n",
    "    q (tf.tensor float64): shape (batch_size, max_seq_len - 1, l)\n",
    "    k (tf.tensor float64): shape (batch_size, max_seq_len - 1, l)\n",
    "    v (tf.tensor float64): shape (batch_size, max_seq_len - 1, l)\n",
    "    mask (tf.tensor float64): shape (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "    ---------------------\n",
    "    Returns:\n",
    "    out_tar: shape (batch_size, max_seq_len - 1, l). The sequences after embedding (or Dense layer) weighted by attention_weights. \n",
    "    attention_weights : shape (batch_size, max_seq_len - 1, max_seq_len - 1). Weights to assign for each sequence member at each timestamp (2nd dim).\n",
    "    matmul_qk: shape (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # similarity\n",
    "    # q = k = v  shape := (batch_size, max_seq_len - 1, l)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b = True, name = 'qk')\n",
    "#     print('matmul_qk: ', matmul_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "    \n",
    "    nl_qk = tf.cast(tf.nn.relu(matmul_qk, name = 'nl_qk'), tf.float64) \n",
    "#     print('nl_qk: ', nl_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "#     nl_qk shape := (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "\n",
    "    # -1e9 will turn the softmax output in this locations to zero\n",
    "    # this is a good mask as an input for softmax -- we need also masking when \n",
    "    # want to use matmul as is \n",
    "    \n",
    "    if mask is not None:\n",
    "        nl_qk +=  ((tf.cast(mask, tf.float64)) * -1e9)\n",
    "    \n",
    "        \n",
    "#     print('nl_qk after mask: ', nl_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "        \n",
    "     # turn simialrity to scores\n",
    "    attention_weights = tf.nn.softmax(nl_qk, axis = -1, name = 'attention_weights')\n",
    "    # Notice that for all the rows where \n",
    "    # everything is 0, the masking will turn everything to -inf\n",
    "    # and the output from the softmax would be 1/num_cols \n",
    "    # (try a = tf.constant([-1e9, -1e9, -1e9]), tf.nn.softmax(a))\n",
    "    # So we can expect an output from these rows which we want to ignore\n",
    "    # this will be enforced in the masking of the loss function \n",
    "    \n",
    "#     print('attention_weights: ', attention_weights)\n",
    "#     shape=(128, 58, 58)\n",
    "   \n",
    "    # weight values \n",
    "    # attention_weights shape := (batch_size, max_seq_len - 1, max_seq_len - 1), \n",
    "    # v shape := batch_size X (max_seq_len - 1) X l\n",
    "    out_tar = tf.matmul(attention_weights, tf.cast(v, tf.float64))\n",
    "    \n",
    "#   print('out_tar: ', out_tar)\n",
    "#   shape=(128, 58, l)\n",
    "    \n",
    "    return out_tar, attention_weights, matmul_qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, l):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.l = l\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(l, name = 'wq')\n",
    "        self.wk = tf.keras.layers.Dense(l, name = 'wk')\n",
    "        self.wv = tf.keras.layers.Dense(l, name = 'wk')                    \n",
    "        \n",
    "        self.hq = tf.keras.layers.Dense(l, name = 'hq')\n",
    "        self.hk = tf.keras.layers.Dense(l, name = 'hk')\n",
    "        self.hv = tf.keras.layers.Dense(l, name = 'hv')\n",
    "        \n",
    "        self.B = tf.keras.layers.Dense(l, name = 'B')\n",
    "        self.A = tf.keras.layers.Dense(1, name = 'A')\n",
    "\n",
    "    #a call method, the layer's forward pass\n",
    "    def call(self, tar_position, tar_inp, training, pos_mask, tar_mask):\n",
    "        \n",
    "        # Adding extra dimension to allow multiplication of \n",
    "        # a sequnce with itself. \n",
    "        tar_position = tar_position[:, :, tf.newaxis]\n",
    "        \n",
    "        q_p = self.wq(tar_position) \n",
    "        k_p = self.wk(tar_position)\n",
    "        v_p = self.wk(tar_position)\n",
    "\n",
    "\n",
    "#         print('v_p: ', v_p)\n",
    "        #shape=(128, 59, 16)\n",
    "        \n",
    "        pos_attn1 = dot_prod_position(q_p, k_p, v_p, mask = pos_mask)\n",
    "#         print('pos_attn1 :', pos_attn1)\n",
    "#       shape=(128, 58, 16, 16)\n",
    "    \n",
    "        tar_inp = tar_inp[:, :, tf.newaxis]\n",
    "\n",
    "        \n",
    "        q = self.hq(tar_inp) \n",
    "        k = self.hk(tar_inp)\n",
    "        v = self.hv(tar_inp)\n",
    "        \n",
    "#         print('q :', q)\n",
    "#       shape=(128, 58, 16)\n",
    "\n",
    "        tar_attn1, _, _ = dot_product_attention(q, k, v, tar_mask)\n",
    "        # tar_attn1 is (batch_size, max_seq_len - 1, tar_d_model)\n",
    "\n",
    "#         print('tar_attn1 :', tar_attn1)\n",
    "#       shape=(128, 58, l)\n",
    "#       shape=(128, 58, 16)\n",
    "        tar_attn1 = tar_attn1[:, :, :, tf.newaxis]\n",
    "        \n",
    "        tar1 = self.B(tar_attn1)\n",
    "        \n",
    "#         print('tar1 :', tar1)\n",
    "        # shape=(128, 58, 16, 16)\n",
    "\n",
    "        L = tf.matmul(tar1, tf.cast(pos_attn1, tf.float64))\n",
    "        \n",
    "#         print('L :', L)\n",
    "        # shape=(128, 58, 16, 16)\n",
    "        \n",
    "        L2 = self.A(tf.reshape(L, shape = [tf.shape(L)[0], tf.shape(L)[1] ,self.l ** 2])) \n",
    "        \n",
    "#         print('L2 :', L2)\n",
    "      # shape=(128, 58, 1)  \n",
    "        \n",
    "        return tf.squeeze(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred = decoder(pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred)\n",
    "\n",
    "        loss = loss_function(tar_real, pred)\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred = decoder(pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss = loss_function(tar_real_te, pred)\n",
    "    test_loss(t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 train Loss 5.2071 test Loss 2.7027\n",
      "Epoch 1 Batch 50 train Loss 0.4357 test Loss 1.3576\n",
      "Epoch 1 Batch 100 train Loss 0.2238 test Loss 0.9080\n",
      "Epoch 1 Batch 150 train Loss 0.1520 test Loss 0.6826\n",
      "Epoch 1 Batch 200 train Loss 0.1159 test Loss 0.5472\n",
      "Epoch 1 Batch 250 train Loss 0.0942 test Loss 0.4572\n",
      "Epoch 1 Batch 300 train Loss 0.0797 test Loss 0.3930\n",
      "Time taken for 1 epoch: 401.82344007492065 secs\n",
      "\n",
      "Epoch 2 Batch 0 train Loss 0.0769 test Loss 0.3446\n",
      "Epoch 2 Batch 50 train Loss 0.0673 test Loss 0.3074\n",
      "Epoch 2 Batch 100 train Loss 0.0600 test Loss 0.2780\n",
      "Epoch 2 Batch 150 train Loss 0.0542 test Loss 0.2533\n",
      "Epoch 2 Batch 200 train Loss 0.0496 test Loss 0.2330\n",
      "Epoch 2 Batch 250 train Loss 0.0458 test Loss 0.2155\n",
      "Epoch 2 Batch 300 train Loss 0.0427 test Loss 0.2005\n",
      "Time taken for 1 epoch: 400.69294691085815 secs\n",
      "\n",
      "Epoch 3 Batch 0 train Loss 0.0420 test Loss 0.1877\n",
      "Epoch 3 Batch 50 train Loss 0.0394 test Loss 0.1764\n",
      "Epoch 3 Batch 100 train Loss 0.0372 test Loss 0.1663\n",
      "Epoch 3 Batch 150 train Loss 0.0352 test Loss 0.1576\n",
      "Epoch 3 Batch 200 train Loss 0.0335 test Loss 0.1497\n",
      "Epoch 3 Batch 250 train Loss 0.0320 test Loss 0.1430\n",
      "Epoch 3 Batch 300 train Loss 0.0306 test Loss 0.1365\n",
      "Time taken for 1 epoch: 409.00484108924866 secs\n",
      "\n",
      "Epoch 4 Batch 0 train Loss 0.0303 test Loss 0.1305\n",
      "Epoch 4 Batch 50 train Loss 0.0291 test Loss 0.1251\n",
      "Epoch 4 Batch 100 train Loss 0.0280 test Loss 0.1202\n",
      "Epoch 4 Batch 150 train Loss 0.0271 test Loss 0.1156\n",
      "Epoch 4 Batch 200 train Loss 0.0262 test Loss 0.1114\n",
      "Epoch 4 Batch 250 train Loss 0.0253 test Loss 0.1075\n",
      "Epoch 4 Batch 300 train Loss 0.0246 test Loss 0.1038\n",
      "Time taken for 1 epoch: 405.18039894104004 secs\n",
      "\n",
      "Epoch 5 Batch 0 train Loss 0.0244 test Loss 0.1004\n",
      "Epoch 5 Batch 50 train Loss 0.0237 test Loss 0.0973\n",
      "Epoch 5 Batch 100 train Loss 0.0231 test Loss 0.0943\n",
      "Epoch 5 Batch 150 train Loss 0.0225 test Loss 0.0916\n",
      "Epoch 5 Batch 200 train Loss 0.0219 test Loss 0.0890\n",
      "Epoch 5 Batch 250 train Loss 0.0214 test Loss 0.0865\n",
      "Epoch 5 Batch 300 train Loss 0.0209 test Loss 0.0842\n",
      "Time taken for 1 epoch: 407.59202313423157 secs\n",
      "\n",
      "Epoch 6 Batch 0 train Loss 0.0208 test Loss 0.0821\n",
      "Epoch 6 Batch 50 train Loss 0.0204 test Loss 0.0801\n",
      "Epoch 6 Batch 100 train Loss 0.0199 test Loss 0.0781\n",
      "Epoch 6 Batch 150 train Loss 0.0195 test Loss 0.0763\n",
      "Epoch 6 Batch 200 train Loss 0.0192 test Loss 0.0746\n",
      "Epoch 6 Batch 250 train Loss 0.0188 test Loss 0.0730\n",
      "Epoch 6 Batch 300 train Loss 0.0185 test Loss 0.0714\n",
      "Time taken for 1 epoch: 401.1774871349335 secs\n",
      "\n",
      "Epoch 7 Batch 0 train Loss 0.0184 test Loss 0.0698\n",
      "Epoch 7 Batch 50 train Loss 0.0181 test Loss 0.0684\n",
      "Epoch 7 Batch 100 train Loss 0.0178 test Loss 0.0670\n",
      "Epoch 7 Batch 150 train Loss 0.0175 test Loss 0.0657\n",
      "Epoch 7 Batch 200 train Loss 0.0172 test Loss 0.0644\n",
      "Epoch 7 Batch 250 train Loss 0.0169 test Loss 0.0632\n",
      "Epoch 7 Batch 300 train Loss 0.0167 test Loss 0.0620\n",
      "Time taken for 1 epoch: 407.00119185447693 secs\n",
      "\n",
      "Epoch 8 Batch 0 train Loss 0.0166 test Loss 0.0609\n",
      "Epoch 8 Batch 50 train Loss 0.0164 test Loss 0.0598\n",
      "Epoch 8 Batch 100 train Loss 0.0162 test Loss 0.0588\n",
      "Epoch 8 Batch 150 train Loss 0.0159 test Loss 0.0578\n",
      "Epoch 8 Batch 200 train Loss 0.0157 test Loss 0.0568\n",
      "Epoch 8 Batch 250 train Loss 0.0155 test Loss 0.0559\n",
      "Epoch 8 Batch 300 train Loss 0.0153 test Loss 0.0550\n",
      "Time taken for 1 epoch: 402.56083703041077 secs\n",
      "\n",
      "Epoch 9 Batch 0 train Loss 0.0153 test Loss 0.0541\n",
      "Epoch 9 Batch 50 train Loss 0.0151 test Loss 0.0533\n",
      "Epoch 9 Batch 100 train Loss 0.0149 test Loss 0.0525\n",
      "Epoch 9 Batch 150 train Loss 0.0147 test Loss 0.0517\n",
      "Epoch 9 Batch 200 train Loss 0.0145 test Loss 0.0509\n",
      "Epoch 9 Batch 250 train Loss 0.0144 test Loss 0.0502\n",
      "Epoch 9 Batch 300 train Loss 0.0142 test Loss 0.0495\n",
      "Time taken for 1 epoch: 403.1816289424896 secs\n",
      "\n",
      "Epoch 10 Batch 0 train Loss 0.0142 test Loss 0.0488\n",
      "Epoch 10 Batch 50 train Loss 0.0140 test Loss 0.0481\n",
      "Epoch 10 Batch 100 train Loss 0.0139 test Loss 0.0475\n",
      "Epoch 10 Batch 150 train Loss 0.0137 test Loss 0.0468\n",
      "Epoch 10 Batch 200 train Loss 0.0136 test Loss 0.0462\n",
      "Epoch 10 Batch 250 train Loss 0.0134 test Loss 0.0457\n",
      "Epoch 10 Batch 300 train Loss 0.0133 test Loss 0.0451\n",
      "Time taken for 1 epoch: 399.78285121917725 secs\n",
      "\n",
      "Epoch 11 Batch 0 train Loss 0.0133 test Loss 0.0445\n",
      "Epoch 11 Batch 50 train Loss 0.0131 test Loss 0.0440\n",
      "Epoch 11 Batch 100 train Loss 0.0130 test Loss 0.0434\n",
      "Epoch 11 Batch 150 train Loss 0.0129 test Loss 0.0429\n",
      "Epoch 11 Batch 200 train Loss 0.0127 test Loss 0.0424\n",
      "Epoch 11 Batch 250 train Loss 0.0126 test Loss 0.0419\n",
      "Epoch 11 Batch 300 train Loss 0.0125 test Loss 0.0414\n",
      "Time taken for 1 epoch: 400.8214409351349 secs\n",
      "\n",
      "Epoch 12 Batch 0 train Loss 0.0125 test Loss 0.0409\n",
      "Epoch 12 Batch 50 train Loss 0.0123 test Loss 0.0405\n",
      "Epoch 12 Batch 100 train Loss 0.0122 test Loss 0.0400\n",
      "Epoch 12 Batch 150 train Loss 0.0121 test Loss 0.0396\n",
      "Epoch 12 Batch 200 train Loss 0.0120 test Loss 0.0391\n",
      "Epoch 12 Batch 250 train Loss 0.0119 test Loss 0.0387\n",
      "Epoch 12 Batch 300 train Loss 0.0118 test Loss 0.0383\n",
      "Time taken for 1 epoch: 398.66515707969666 secs\n",
      "\n",
      "Epoch 13 Batch 0 train Loss 0.0118 test Loss 0.0379\n",
      "Epoch 13 Batch 50 train Loss 0.0117 test Loss 0.0375\n",
      "Epoch 13 Batch 100 train Loss 0.0116 test Loss 0.0371\n",
      "Epoch 13 Batch 150 train Loss 0.0115 test Loss 0.0367\n",
      "Epoch 13 Batch 200 train Loss 0.0114 test Loss 0.0363\n",
      "Epoch 13 Batch 250 train Loss 0.0113 test Loss 0.0360\n",
      "Epoch 13 Batch 300 train Loss 0.0112 test Loss 0.0356\n",
      "Time taken for 1 epoch: 398.6297459602356 secs\n",
      "\n",
      "Epoch 14 Batch 0 train Loss 0.0111 test Loss 0.0352\n",
      "Epoch 14 Batch 50 train Loss 0.0110 test Loss 0.0349\n",
      "Epoch 14 Batch 100 train Loss 0.0109 test Loss 0.0346\n",
      "Epoch 14 Batch 150 train Loss 0.0108 test Loss 0.0342\n",
      "Epoch 14 Batch 200 train Loss 0.0108 test Loss 0.0339\n",
      "Epoch 14 Batch 250 train Loss 0.0107 test Loss 0.0336\n",
      "Epoch 14 Batch 300 train Loss 0.0106 test Loss 0.0333\n",
      "Time taken for 1 epoch: 405.8373520374298 secs\n",
      "\n",
      "Epoch 15 Batch 0 train Loss 0.0105 test Loss 0.0330\n",
      "Epoch 15 Batch 50 train Loss 0.0105 test Loss 0.0326\n",
      "Epoch 15 Batch 100 train Loss 0.0104 test Loss 0.0324\n",
      "Epoch 15 Batch 150 train Loss 0.0103 test Loss 0.0321\n",
      "Epoch 15 Batch 200 train Loss 0.0102 test Loss 0.0318\n",
      "Epoch 15 Batch 250 train Loss 0.0101 test Loss 0.0315\n",
      "Epoch 15 Batch 300 train Loss 0.0100 test Loss 0.0312\n",
      "Time taken for 1 epoch: 397.86759400367737 secs\n",
      "\n",
      "Epoch 16 Batch 0 train Loss 0.0100 test Loss 0.0310\n",
      "Epoch 16 Batch 50 train Loss 0.0100 test Loss 0.0307\n",
      "Epoch 16 Batch 100 train Loss 0.0099 test Loss 0.0304\n",
      "Epoch 16 Batch 150 train Loss 0.0098 test Loss 0.0302\n",
      "Epoch 16 Batch 200 train Loss 0.0097 test Loss 0.0299\n",
      "Epoch 16 Batch 250 train Loss 0.0097 test Loss 0.0297\n",
      "Epoch 16 Batch 300 train Loss 0.0096 test Loss 0.0294\n",
      "Time taken for 1 epoch: 402.9232518672943 secs\n",
      "\n",
      "Epoch 17 Batch 0 train Loss 0.0096 test Loss 0.0292\n",
      "Epoch 17 Batch 50 train Loss 0.0095 test Loss 0.0290\n",
      "Epoch 17 Batch 100 train Loss 0.0094 test Loss 0.0287\n",
      "Epoch 17 Batch 150 train Loss 0.0094 test Loss 0.0285\n",
      "Epoch 17 Batch 200 train Loss 0.0093 test Loss 0.0283\n",
      "Epoch 17 Batch 250 train Loss 0.0092 test Loss 0.0281\n",
      "Epoch 17 Batch 300 train Loss 0.0092 test Loss 0.0279\n",
      "Time taken for 1 epoch: 404.20899510383606 secs\n",
      "\n",
      "Epoch 18 Batch 0 train Loss 0.0092 test Loss 0.0277\n",
      "Epoch 18 Batch 50 train Loss 0.0091 test Loss 0.0275\n",
      "Epoch 18 Batch 100 train Loss 0.0090 test Loss 0.0272\n",
      "Epoch 18 Batch 150 train Loss 0.0090 test Loss 0.0270\n",
      "Epoch 18 Batch 200 train Loss 0.0089 test Loss 0.0269\n",
      "Epoch 18 Batch 250 train Loss 0.0089 test Loss 0.0267\n",
      "Epoch 18 Batch 300 train Loss 0.0088 test Loss 0.0265\n",
      "Time taken for 1 epoch: 402.85496401786804 secs\n",
      "\n",
      "Epoch 19 Batch 0 train Loss 0.0088 test Loss 0.0263\n",
      "Epoch 19 Batch 50 train Loss 0.0088 test Loss 0.0261\n",
      "Epoch 19 Batch 100 train Loss 0.0087 test Loss 0.0259\n",
      "Epoch 19 Batch 150 train Loss 0.0086 test Loss 0.0257\n",
      "Epoch 19 Batch 200 train Loss 0.0086 test Loss 0.0256\n",
      "Epoch 19 Batch 250 train Loss 0.0085 test Loss 0.0254\n",
      "Epoch 19 Batch 300 train Loss 0.0085 test Loss 0.0252\n",
      "Time taken for 1 epoch: 401.9011399745941 secs\n",
      "\n",
      "Epoch 20 Batch 0 train Loss 0.0085 test Loss 0.0251\n",
      "Epoch 20 Batch 50 train Loss 0.0084 test Loss 0.0249\n",
      "Epoch 20 Batch 100 train Loss 0.0084 test Loss 0.0247\n",
      "Epoch 20 Batch 150 train Loss 0.0083 test Loss 0.0246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 200 train Loss 0.0083 test Loss 0.0244\n",
      "Epoch 20 Batch 250 train Loss 0.0083 test Loss 0.0242\n",
      "Epoch 20 Batch 300 train Loss 0.0082 test Loss 0.0241\n",
      "Time taken for 1 epoch: 401.88769698143005 secs\n",
      "\n",
      "Epoch 21 Batch 0 train Loss 0.0082 test Loss 0.0239\n",
      "Epoch 21 Batch 50 train Loss 0.0082 test Loss 0.0238\n",
      "Epoch 21 Batch 100 train Loss 0.0081 test Loss 0.0236\n",
      "Epoch 21 Batch 150 train Loss 0.0081 test Loss 0.0235\n",
      "Epoch 21 Batch 200 train Loss 0.0080 test Loss 0.0234\n",
      "Epoch 21 Batch 250 train Loss 0.0080 test Loss 0.0232\n",
      "Epoch 21 Batch 300 train Loss 0.0079 test Loss 0.0231\n",
      "Time taken for 1 epoch: 401.2964279651642 secs\n",
      "\n",
      "Epoch 22 Batch 0 train Loss 0.0079 test Loss 0.0229\n",
      "Epoch 22 Batch 50 train Loss 0.0079 test Loss 0.0228\n",
      "Epoch 22 Batch 100 train Loss 0.0079 test Loss 0.0227\n",
      "Epoch 22 Batch 150 train Loss 0.0078 test Loss 0.0225\n",
      "Epoch 22 Batch 200 train Loss 0.0078 test Loss 0.0224\n",
      "Epoch 22 Batch 250 train Loss 0.0077 test Loss 0.0223\n",
      "Epoch 22 Batch 300 train Loss 0.0077 test Loss 0.0221\n",
      "Time taken for 1 epoch: 403.1242940425873 secs\n",
      "\n",
      "Epoch 23 Batch 0 train Loss 0.0077 test Loss 0.0220\n",
      "Epoch 23 Batch 50 train Loss 0.0077 test Loss 0.0219\n",
      "Epoch 23 Batch 100 train Loss 0.0076 test Loss 0.0218\n",
      "Epoch 23 Batch 150 train Loss 0.0076 test Loss 0.0217\n",
      "Epoch 23 Batch 200 train Loss 0.0076 test Loss 0.0215\n",
      "Epoch 23 Batch 250 train Loss 0.0075 test Loss 0.0214\n",
      "Epoch 23 Batch 300 train Loss 0.0075 test Loss 0.0213\n",
      "Time taken for 1 epoch: 404.5098478794098 secs\n",
      "\n",
      "Epoch 24 Batch 0 train Loss 0.0075 test Loss 0.0212\n",
      "Epoch 24 Batch 50 train Loss 0.0074 test Loss 0.0211\n",
      "Epoch 24 Batch 100 train Loss 0.0074 test Loss 0.0210\n",
      "Epoch 24 Batch 150 train Loss 0.0074 test Loss 0.0209\n",
      "Epoch 24 Batch 200 train Loss 0.0074 test Loss 0.0207\n",
      "Epoch 24 Batch 250 train Loss 0.0073 test Loss 0.0206\n",
      "Epoch 24 Batch 300 train Loss 0.0073 test Loss 0.0205\n",
      "Time taken for 1 epoch: 403.60303592681885 secs\n",
      "\n",
      "Epoch 25 Batch 0 train Loss 0.0073 test Loss 0.0204\n",
      "Epoch 25 Batch 50 train Loss 0.0073 test Loss 0.0203\n",
      "Epoch 25 Batch 100 train Loss 0.0072 test Loss 0.0202\n",
      "Epoch 25 Batch 150 train Loss 0.0072 test Loss 0.0201\n",
      "Epoch 25 Batch 200 train Loss 0.0072 test Loss 0.0200\n",
      "Epoch 25 Batch 250 train Loss 0.0071 test Loss 0.0199\n",
      "Epoch 25 Batch 300 train Loss 0.0071 test Loss 0.0198\n",
      "Time taken for 1 epoch: 400.60554599761963 secs\n",
      "\n",
      "Epoch 26 Batch 0 train Loss 0.0071 test Loss 0.0197\n",
      "Epoch 26 Batch 50 train Loss 0.0071 test Loss 0.0196\n",
      "Epoch 26 Batch 100 train Loss 0.0070 test Loss 0.0195\n",
      "Epoch 26 Batch 150 train Loss 0.0070 test Loss 0.0194\n",
      "Epoch 26 Batch 200 train Loss 0.0070 test Loss 0.0193\n",
      "Epoch 26 Batch 250 train Loss 0.0070 test Loss 0.0192\n",
      "Epoch 26 Batch 300 train Loss 0.0069 test Loss 0.0192\n",
      "Time taken for 1 epoch: 401.14755487442017 secs\n",
      "\n",
      "Epoch 27 Batch 0 train Loss 0.0069 test Loss 0.0191\n",
      "Epoch 27 Batch 50 train Loss 0.0069 test Loss 0.0190\n",
      "Epoch 27 Batch 100 train Loss 0.0069 test Loss 0.0189\n",
      "Epoch 27 Batch 150 train Loss 0.0069 test Loss 0.0188\n",
      "Epoch 27 Batch 200 train Loss 0.0068 test Loss 0.0187\n",
      "Epoch 27 Batch 250 train Loss 0.0068 test Loss 0.0186\n",
      "Epoch 27 Batch 300 train Loss 0.0068 test Loss 0.0185\n",
      "Time taken for 1 epoch: 404.1713480949402 secs\n",
      "\n",
      "Epoch 28 Batch 0 train Loss 0.0068 test Loss 0.0185\n",
      "Epoch 28 Batch 50 train Loss 0.0067 test Loss 0.0184\n",
      "Epoch 28 Batch 100 train Loss 0.0067 test Loss 0.0183\n",
      "Epoch 28 Batch 150 train Loss 0.0067 test Loss 0.0182\n",
      "Epoch 28 Batch 200 train Loss 0.0067 test Loss 0.0181\n",
      "Epoch 28 Batch 250 train Loss 0.0067 test Loss 0.0181\n",
      "Epoch 28 Batch 300 train Loss 0.0066 test Loss 0.0180\n",
      "Time taken for 1 epoch: 400.42371582984924 secs\n",
      "\n",
      "Epoch 29 Batch 0 train Loss 0.0066 test Loss 0.0179\n",
      "Epoch 29 Batch 50 train Loss 0.0066 test Loss 0.0178\n",
      "Epoch 29 Batch 100 train Loss 0.0066 test Loss 0.0178\n",
      "Epoch 29 Batch 150 train Loss 0.0066 test Loss 0.0177\n",
      "Epoch 29 Batch 200 train Loss 0.0065 test Loss 0.0176\n",
      "Epoch 29 Batch 250 train Loss 0.0065 test Loss 0.0175\n",
      "Epoch 29 Batch 300 train Loss 0.0065 test Loss 0.0175\n",
      "Time taken for 1 epoch: 401.990287065506 secs\n",
      "\n",
      "Epoch 30 Batch 0 train Loss 0.0065 test Loss 0.0174\n",
      "Epoch 30 Batch 50 train Loss 0.0065 test Loss 0.0173\n",
      "Epoch 30 Batch 100 train Loss 0.0065 test Loss 0.0172\n",
      "Epoch 30 Batch 150 train Loss 0.0064 test Loss 0.0172\n",
      "Epoch 30 Batch 200 train Loss 0.0064 test Loss 0.0171\n",
      "Epoch 30 Batch 250 train Loss 0.0064 test Loss 0.0170\n",
      "Epoch 30 Batch 300 train Loss 0.0064 test Loss 0.0170\n",
      "Time taken for 1 epoch: 398.45591592788696 secs\n",
      "\n",
      "Epoch 31 Batch 0 train Loss 0.0064 test Loss 0.0169\n",
      "Epoch 31 Batch 50 train Loss 0.0063 test Loss 0.0168\n",
      "Epoch 31 Batch 100 train Loss 0.0063 test Loss 0.0168\n",
      "Epoch 31 Batch 150 train Loss 0.0063 test Loss 0.0167\n",
      "Epoch 31 Batch 200 train Loss 0.0063 test Loss 0.0166\n",
      "Epoch 31 Batch 250 train Loss 0.0063 test Loss 0.0166\n",
      "Epoch 31 Batch 300 train Loss 0.0063 test Loss 0.0165\n",
      "Time taken for 1 epoch: 398.3436350822449 secs\n",
      "\n",
      "Epoch 32 Batch 0 train Loss 0.0063 test Loss 0.0164\n",
      "Epoch 32 Batch 50 train Loss 0.0062 test Loss 0.0164\n",
      "Epoch 32 Batch 100 train Loss 0.0062 test Loss 0.0163\n",
      "Epoch 32 Batch 150 train Loss 0.0062 test Loss 0.0163\n",
      "Epoch 32 Batch 200 train Loss 0.0062 test Loss 0.0162\n",
      "Epoch 32 Batch 250 train Loss 0.0062 test Loss 0.0161\n",
      "Epoch 32 Batch 300 train Loss 0.0061 test Loss 0.0161\n",
      "Time taken for 1 epoch: 404.6240577697754 secs\n",
      "\n",
      "Epoch 33 Batch 0 train Loss 0.0061 test Loss 0.0160\n",
      "Epoch 33 Batch 50 train Loss 0.0061 test Loss 0.0160\n",
      "Epoch 33 Batch 100 train Loss 0.0061 test Loss 0.0159\n",
      "Epoch 33 Batch 150 train Loss 0.0061 test Loss 0.0158\n",
      "Epoch 33 Batch 200 train Loss 0.0061 test Loss 0.0158\n",
      "Epoch 33 Batch 250 train Loss 0.0061 test Loss 0.0157\n",
      "Epoch 33 Batch 300 train Loss 0.0060 test Loss 0.0157\n",
      "Time taken for 1 epoch: 400.608864068985 secs\n",
      "\n",
      "Epoch 34 Batch 0 train Loss 0.0060 test Loss 0.0156\n",
      "Epoch 34 Batch 50 train Loss 0.0060 test Loss 0.0156\n",
      "Epoch 34 Batch 100 train Loss 0.0060 test Loss 0.0155\n",
      "Epoch 34 Batch 150 train Loss 0.0060 test Loss 0.0154\n",
      "Epoch 34 Batch 200 train Loss 0.0060 test Loss 0.0154\n",
      "Epoch 34 Batch 250 train Loss 0.0060 test Loss 0.0153\n",
      "Epoch 34 Batch 300 train Loss 0.0059 test Loss 0.0153\n",
      "Time taken for 1 epoch: 401.6485629081726 secs\n",
      "\n",
      "Epoch 35 Batch 0 train Loss 0.0059 test Loss 0.0152\n",
      "Epoch 35 Batch 50 train Loss 0.0059 test Loss 0.0152\n",
      "Epoch 35 Batch 100 train Loss 0.0059 test Loss 0.0151\n",
      "Epoch 35 Batch 150 train Loss 0.0059 test Loss 0.0151\n",
      "Epoch 35 Batch 200 train Loss 0.0059 test Loss 0.0150\n",
      "Epoch 35 Batch 250 train Loss 0.0059 test Loss 0.0150\n",
      "Epoch 35 Batch 300 train Loss 0.0059 test Loss 0.0149\n",
      "Time taken for 1 epoch: 402.62901186943054 secs\n",
      "\n",
      "Epoch 36 Batch 0 train Loss 0.0058 test Loss 0.0149\n",
      "Epoch 36 Batch 50 train Loss 0.0058 test Loss 0.0148\n",
      "Epoch 36 Batch 100 train Loss 0.0058 test Loss 0.0148\n",
      "Epoch 36 Batch 150 train Loss 0.0058 test Loss 0.0147\n",
      "Epoch 36 Batch 200 train Loss 0.0058 test Loss 0.0147\n",
      "Epoch 36 Batch 250 train Loss 0.0058 test Loss 0.0146\n",
      "Epoch 36 Batch 300 train Loss 0.0058 test Loss 0.0146\n",
      "Time taken for 1 epoch: 399.23629903793335 secs\n",
      "\n",
      "Epoch 37 Batch 0 train Loss 0.0058 test Loss 0.0145\n",
      "Epoch 37 Batch 50 train Loss 0.0057 test Loss 0.0145\n",
      "Epoch 37 Batch 100 train Loss 0.0057 test Loss 0.0145\n",
      "Epoch 37 Batch 150 train Loss 0.0057 test Loss 0.0144\n",
      "Epoch 37 Batch 200 train Loss 0.0057 test Loss 0.0144\n",
      "Epoch 37 Batch 250 train Loss 0.0057 test Loss 0.0143\n",
      "Epoch 37 Batch 300 train Loss 0.0057 test Loss 0.0143\n",
      "Time taken for 1 epoch: 397.98236203193665 secs\n",
      "\n",
      "Epoch 38 Batch 0 train Loss 0.0057 test Loss 0.0142\n",
      "Epoch 38 Batch 50 train Loss 0.0057 test Loss 0.0142\n",
      "Epoch 38 Batch 100 train Loss 0.0057 test Loss 0.0141\n",
      "Epoch 38 Batch 150 train Loss 0.0056 test Loss 0.0141\n",
      "Epoch 38 Batch 200 train Loss 0.0056 test Loss 0.0141\n",
      "Epoch 38 Batch 250 train Loss 0.0056 test Loss 0.0140\n",
      "Epoch 38 Batch 300 train Loss 0.0056 test Loss 0.0140\n",
      "Time taken for 1 epoch: 400.2826261520386 secs\n",
      "\n",
      "Epoch 39 Batch 0 train Loss 0.0056 test Loss 0.0139\n",
      "Epoch 39 Batch 50 train Loss 0.0056 test Loss 0.0139\n",
      "Epoch 39 Batch 100 train Loss 0.0056 test Loss 0.0138\n",
      "Epoch 39 Batch 150 train Loss 0.0056 test Loss 0.0138\n",
      "Epoch 39 Batch 200 train Loss 0.0056 test Loss 0.0138\n",
      "Epoch 39 Batch 250 train Loss 0.0055 test Loss 0.0137\n",
      "Epoch 39 Batch 300 train Loss 0.0055 test Loss 0.0137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 398.0005359649658 secs\n",
      "\n",
      "Epoch 40 Batch 0 train Loss 0.0055 test Loss 0.0136\n",
      "Epoch 40 Batch 50 train Loss 0.0055 test Loss 0.0136\n",
      "Epoch 40 Batch 100 train Loss 0.0055 test Loss 0.0136\n",
      "Epoch 40 Batch 150 train Loss 0.0055 test Loss 0.0135\n",
      "Epoch 40 Batch 200 train Loss 0.0055 test Loss 0.0135\n",
      "Epoch 40 Batch 250 train Loss 0.0055 test Loss 0.0134\n",
      "Epoch 40 Batch 300 train Loss 0.0055 test Loss 0.0134\n",
      "Time taken for 1 epoch: 396.4347040653229 secs\n",
      "\n",
      "Epoch 41 Batch 0 train Loss 0.0055 test Loss 0.0134\n",
      "Epoch 41 Batch 50 train Loss 0.0054 test Loss 0.0133\n",
      "Epoch 41 Batch 100 train Loss 0.0054 test Loss 0.0133\n",
      "Epoch 41 Batch 150 train Loss 0.0054 test Loss 0.0133\n",
      "Epoch 41 Batch 200 train Loss 0.0054 test Loss 0.0132\n",
      "Epoch 41 Batch 250 train Loss 0.0054 test Loss 0.0132\n",
      "Epoch 41 Batch 300 train Loss 0.0054 test Loss 0.0131\n",
      "Time taken for 1 epoch: 397.91670393943787 secs\n",
      "\n",
      "Epoch 42 Batch 0 train Loss 0.0054 test Loss 0.0131\n",
      "Epoch 42 Batch 50 train Loss 0.0054 test Loss 0.0131\n",
      "Epoch 42 Batch 100 train Loss 0.0054 test Loss 0.0130\n",
      "Epoch 42 Batch 150 train Loss 0.0054 test Loss 0.0130\n",
      "Epoch 42 Batch 200 train Loss 0.0053 test Loss 0.0130\n",
      "Epoch 42 Batch 250 train Loss 0.0053 test Loss 0.0129\n",
      "Epoch 42 Batch 300 train Loss 0.0053 test Loss 0.0129\n",
      "Time taken for 1 epoch: 397.3754539489746 secs\n",
      "\n",
      "Epoch 43 Batch 0 train Loss 0.0053 test Loss 0.0129\n",
      "Epoch 43 Batch 50 train Loss 0.0053 test Loss 0.0128\n",
      "Epoch 43 Batch 100 train Loss 0.0053 test Loss 0.0128\n",
      "Epoch 43 Batch 150 train Loss 0.0053 test Loss 0.0128\n",
      "Epoch 43 Batch 200 train Loss 0.0053 test Loss 0.0127\n",
      "Epoch 43 Batch 250 train Loss 0.0053 test Loss 0.0127\n",
      "Epoch 43 Batch 300 train Loss 0.0053 test Loss 0.0127\n",
      "Time taken for 1 epoch: 398.4454300403595 secs\n",
      "\n",
      "Epoch 44 Batch 0 train Loss 0.0053 test Loss 0.0126\n",
      "Epoch 44 Batch 50 train Loss 0.0053 test Loss 0.0126\n",
      "Epoch 44 Batch 100 train Loss 0.0052 test Loss 0.0126\n",
      "Epoch 44 Batch 150 train Loss 0.0052 test Loss 0.0125\n",
      "Epoch 44 Batch 200 train Loss 0.0052 test Loss 0.0125\n",
      "Epoch 44 Batch 250 train Loss 0.0052 test Loss 0.0125\n",
      "Epoch 44 Batch 300 train Loss 0.0052 test Loss 0.0124\n",
      "Time taken for 1 epoch: 397.7470259666443 secs\n",
      "\n",
      "Epoch 45 Batch 0 train Loss 0.0052 test Loss 0.0124\n",
      "Epoch 45 Batch 50 train Loss 0.0052 test Loss 0.0124\n",
      "Epoch 45 Batch 100 train Loss 0.0052 test Loss 0.0123\n",
      "Epoch 45 Batch 150 train Loss 0.0052 test Loss 0.0123\n",
      "Epoch 45 Batch 200 train Loss 0.0052 test Loss 0.0123\n",
      "Epoch 45 Batch 250 train Loss 0.0052 test Loss 0.0122\n",
      "Epoch 45 Batch 300 train Loss 0.0052 test Loss 0.0122\n",
      "Time taken for 1 epoch: 397.39622378349304 secs\n",
      "\n",
      "Epoch 46 Batch 0 train Loss 0.0052 test Loss 0.0122\n",
      "Epoch 46 Batch 50 train Loss 0.0051 test Loss 0.0122\n",
      "Epoch 46 Batch 100 train Loss 0.0051 test Loss 0.0121\n",
      "Epoch 46 Batch 150 train Loss 0.0051 test Loss 0.0121\n",
      "Epoch 46 Batch 200 train Loss 0.0051 test Loss 0.0121\n",
      "Epoch 46 Batch 250 train Loss 0.0051 test Loss 0.0120\n",
      "Epoch 46 Batch 300 train Loss 0.0051 test Loss 0.0120\n",
      "Time taken for 1 epoch: 396.9263107776642 secs\n",
      "\n",
      "Epoch 47 Batch 0 train Loss 0.0051 test Loss 0.0120\n",
      "Epoch 47 Batch 50 train Loss 0.0051 test Loss 0.0120\n",
      "Epoch 47 Batch 100 train Loss 0.0051 test Loss 0.0119\n",
      "Epoch 47 Batch 150 train Loss 0.0051 test Loss 0.0119\n",
      "Epoch 47 Batch 200 train Loss 0.0051 test Loss 0.0119\n",
      "Epoch 47 Batch 250 train Loss 0.0051 test Loss 0.0118\n",
      "Epoch 47 Batch 300 train Loss 0.0050 test Loss 0.0118\n",
      "Time taken for 1 epoch: 399.44488191604614 secs\n",
      "\n",
      "Epoch 48 Batch 0 train Loss 0.0050 test Loss 0.0118\n",
      "Epoch 48 Batch 50 train Loss 0.0050 test Loss 0.0118\n",
      "Epoch 48 Batch 100 train Loss 0.0050 test Loss 0.0117\n",
      "Epoch 48 Batch 150 train Loss 0.0050 test Loss 0.0117\n",
      "Epoch 48 Batch 200 train Loss 0.0050 test Loss 0.0117\n",
      "Epoch 48 Batch 250 train Loss 0.0050 test Loss 0.0116\n",
      "Epoch 48 Batch 300 train Loss 0.0050 test Loss 0.0116\n",
      "Time taken for 1 epoch: 398.8325071334839 secs\n",
      "\n",
      "Epoch 49 Batch 0 train Loss 0.0050 test Loss 0.0116\n",
      "Epoch 49 Batch 50 train Loss 0.0050 test Loss 0.0116\n",
      "Epoch 49 Batch 100 train Loss 0.0050 test Loss 0.0115\n",
      "Epoch 49 Batch 150 train Loss 0.0050 test Loss 0.0115\n",
      "Epoch 49 Batch 200 train Loss 0.0050 test Loss 0.0115\n",
      "Epoch 49 Batch 250 train Loss 0.0050 test Loss 0.0115\n",
      "Epoch 49 Batch 300 train Loss 0.0050 test Loss 0.0114\n",
      "Time taken for 1 epoch: 396.5265328884125 secs\n",
      "\n",
      "Epoch 50 Batch 0 train Loss 0.0050 test Loss 0.0114\n",
      "Epoch 50 Batch 50 train Loss 0.0049 test Loss 0.0114\n",
      "Epoch 50 Batch 100 train Loss 0.0049 test Loss 0.0114\n",
      "Epoch 50 Batch 150 train Loss 0.0049 test Loss 0.0113\n",
      "Epoch 50 Batch 200 train Loss 0.0049 test Loss 0.0113\n",
      "Epoch 50 Batch 250 train Loss 0.0049 test Loss 0.0113\n",
      "Epoch 50 Batch 300 train Loss 0.0049 test Loss 0.0113\n",
      "Time taken for 1 epoch: 396.5469419956207 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    EPOCHS = 50\n",
    "    batch_s  = 128\n",
    "    num_batches = int(pad_y_fren_tr.shape[0] / batch_s)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "#       train_loss.reset_states()\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            batch_pos_tr, batch_tar_tr, batch_pos_mask, _ = create_batch_gp_mim_2(pad_pos_tr, pad_y_fren_tr, pp)\n",
    "            # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "            # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "            train_step(batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                batch_pos_te, batch_tar_te, batch_pos_mask_te, _ = create_batch_gp_mim_2(pad_pos_te, pad_y_fren_te, pp_te)\n",
    "                test_step(batch_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                \n",
    "                print ('Epoch {} Batch {} train Loss {:.4f} test Loss {:.4f}'.format(\n",
    "                  epoch + 1, batch, train_loss.result(), test_loss.result()))\n",
    "\n",
    "        print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79421483, 0.82273214, 0.78303935, 0.78081123, 0.84008443,\n",
       "       0.93063795, 0.75938485, 0.78951207, 0.85226032, 0.82714972,\n",
       "       0.83747097, 0.82860228, 0.81902547, 0.83490666, 0.8305321 ,\n",
       "       0.83953733, 0.78421542, 0.82523238, 0.81893981, 0.7845124 ,\n",
       "       0.79444966, 0.82777025, 0.8737039 , 0.84331259, 0.77790632,\n",
       "       0.72681742, 0.83917158, 0.8450379 , 0.87636089, 0.89078672,\n",
       "       0.83436773, 0.81889668, 0.82095995, 0.8350107 , 0.75424744,\n",
       "       0.87638343, 0.8746333 , 0.78314555, 0.77521238, 0.83750439,\n",
       "       0.88058447, 0.81881745, 0.91279156, 0.78103321, 0.82639293,\n",
       "       0.7769952 , 0.81967033, 0.82250717, 0.84540936, 0.74881181,\n",
       "       0.84004019, 0.79126806, 0.81754296, 0.86202801, 0.80911221,\n",
       "       0.87486705, 0.71336508, 0.8125778 , 0.76446845])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pos, tar, pos_mask):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    combined_mask_tar = create_masks(tar)\n",
    "    out = decoder(pos, tar, False, pos_mask, combined_mask_tar)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(pos, tar, num_steps = 1):\n",
    "    '''\n",
    "    \n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (2D np array): (n + num_steps) positions \n",
    "    tar (2D np array): n targets \n",
    "    num_steps (int): how many inference steps are required\n",
    "    ------------------\n",
    "    Returns:\n",
    "    out (tf.tensor float64): the predictions for all timestamps up to n + num_steps  \n",
    "    \n",
    "    '''\n",
    "    n = tar.shape[1]\n",
    "    temp_pos = pos[:, :(n + 1)]\n",
    "    pos_mask = position_mask(temp_pos)\n",
    "    \n",
    "    out = evaluate(temp_pos, tar, pos_mask)\n",
    "    print(out[n - 1])\n",
    "    tar = tf.concat((tar, tf.reshape(out[n - 1], [1, 1])), axis = 1)\n",
    "    if num_steps > 1:\n",
    "        out = inference(pos, tar, num_steps - 1)\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.8241398577782234, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8241979832058564, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8242541256248841, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8243043343657172, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8243582809798264, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8243396267129595, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8243726973824703, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8234560203767337, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8234830424281215, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8235090683135358, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8235348383079335, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8237256968841214, shape=(), dtype=float64)\n",
      "tf.Tensor(0.823977861619993, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8240096644136029, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8241489526617438, shape=(), dtype=float64)\n",
      "tf.Tensor(0.824299003452675, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8243331891052343, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8243667639624348, shape=(), dtype=float64)\n",
      "tf.Tensor(0.824399612523655, shape=(), dtype=float64)\n",
      "tf.Tensor(0.8244320292727092, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
