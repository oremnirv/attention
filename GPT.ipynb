{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import classic_model, losses, dot_prod_attention\n",
    "from data import data_generation, batch_creator, gp_kernels\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from helpers import helpers, masks\n",
    "from inference import infer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/omernivron/Downloads/GPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te, _, df_te = data_generation.data_generator_for_gp_mimick_gpt(50000, gp_kernels.rbf_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = masks.position_mask(pad_pos_tr)\n",
    "pp_te = masks.position_mask(pad_pos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "m_tr = tf.keras.metrics.Mean()\n",
    "m_te = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = masks.create_masks(tar_inp)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pred, pred_sig = decoder(pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "#         print('pred: ')\n",
    "#         tf.print(pred_sig)\n",
    "\n",
    "        loss, mse, mask = losses.loss_function(tar_real, pred, pred_sig)\n",
    "\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "# Ask the optimizer to apply the processed gradients.\n",
    "    optimizer_c.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    m_tr.update_state(mse, mask)\n",
    "#     b = decoder.trainable_weights[0]\n",
    "#     tf.print(tf.reduce_mean(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(pos_te, tar_te, pos_mask_te):\n",
    "    '''\n",
    "    \n",
    "    ---------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask_te (np array): see description in position_mask function\n",
    "    ---------------\n",
    "    \n",
    "    '''\n",
    "    tar_inp_te = tar_te[:, :-1]\n",
    "    tar_real_te = tar_te[:, 1:]\n",
    "    combined_mask_tar_te = masks.create_masks(tar_inp_te)\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    pred, pred_sig = decoder(pos_te, tar_inp_te, False, pos_mask_te, combined_mask_tar_te)\n",
    "    t_loss, t_mse, t_mask = losses.loss_function(tar_real_te, pred, pred_sig)\n",
    "    test_loss(t_loss)\n",
    "    m_te.update_state(t_mse, t_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 59)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_pos_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 58, 59, 59)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "Tensor(\"decoder_1/Bsig/Relu:0\", shape=(128, 58, 16, 16), dtype=float64)\n",
      "Tensor(\"decoder_1/Bsig/Relu:0\", shape=(128, 58, 16, 16), dtype=float64)\n",
      "Tensor(\"decoder_1/Bsig/Relu:0\", shape=(128, 58, 16, 16), dtype=float64)\n",
      "Epoch 0 batch 0 train Loss 3.6670 test Loss 1.2995 with MSE metric 3.2465\n",
      "Epoch 0 batch 50 train Loss 1.1910 test Loss 0.8939 with MSE metric 1.1993\n",
      "Epoch 0 batch 100 train Loss 0.9576 test Loss 0.7031 with MSE metric 0.9265\n",
      "Epoch 0 batch 150 train Loss 0.8186 test Loss 0.5525 with MSE metric 0.7766\n",
      "Epoch 0 batch 200 train Loss 0.7091 test Loss 0.4333 with MSE metric 0.6716\n",
      "Epoch 0 batch 250 train Loss 0.6130 test Loss 0.3356 with MSE metric 0.5909\n",
      "Epoch 0 batch 300 train Loss 0.5296 test Loss 0.2486 with MSE metric 0.5286\n",
      "Time taken for 1 epoch: 451.4765079021454 secs\n",
      "\n",
      "Epoch 1 batch 0 train Loss 0.5109 test Loss 0.1895 with MSE metric 0.5156\n",
      "Epoch 1 batch 50 train Loss 0.4380 test Loss 0.1389 with MSE metric 0.4678\n",
      "Epoch 1 batch 100 train Loss 0.3708 test Loss 0.0852 with MSE metric 0.4281\n",
      "Epoch 1 batch 150 train Loss 0.3078 test Loss 0.0310 with MSE metric 0.3944\n",
      "Epoch 1 batch 200 train Loss 0.2493 test Loss -0.0035 with MSE metric 0.3656\n",
      "Epoch 1 batch 250 train Loss 0.1979 test Loss -0.0465 with MSE metric 0.3413\n",
      "Epoch 1 batch 300 train Loss 0.1448 test Loss -0.0809 with MSE metric 0.3193\n",
      "Time taken for 1 epoch: 471.6523702144623 secs\n",
      "\n",
      "Epoch 2 batch 0 train Loss 0.1333 test Loss -0.1198 with MSE metric 0.3146\n",
      "Epoch 2 batch 50 train Loss 0.0851 test Loss -0.1570 with MSE metric 0.2960\n",
      "Epoch 2 batch 100 train Loss 0.0393 test Loss -0.1919 with MSE metric 0.2794\n",
      "Epoch 2 batch 150 train Loss -0.0071 test Loss -0.2271 with MSE metric 0.2644\n",
      "Epoch 2 batch 200 train Loss -0.0503 test Loss -0.2482 with MSE metric 0.2509\n",
      "Epoch 2 batch 250 train Loss -0.0934 test Loss -0.2765 with MSE metric 0.2386\n",
      "Epoch 2 batch 300 train Loss -0.1343 test Loss -0.2884 with MSE metric 0.2275\n",
      "Time taken for 1 epoch: 483.3798859119415 secs\n",
      "\n",
      "Epoch 3 batch 0 train Loss -0.1433 test Loss -0.3028 with MSE metric 0.2250\n",
      "Epoch 3 batch 50 train Loss -0.1824 test Loss -0.3019 with MSE metric 0.2150\n",
      "Epoch 3 batch 100 train Loss -0.2195 test Loss -0.3106 with MSE metric 0.2060\n",
      "Epoch 3 batch 150 train Loss -0.2545 test Loss -0.3212 with MSE metric 0.1976\n",
      "Epoch 3 batch 200 train Loss -0.2866 test Loss -0.3239 with MSE metric 0.1901\n",
      "Epoch 3 batch 250 train Loss -0.3175 test Loss -0.3250 with MSE metric 0.1830\n",
      "Epoch 3 batch 300 train Loss -0.3462 test Loss -0.3469 with MSE metric 0.1766\n",
      "Time taken for 1 epoch: 486.6920659542084 secs\n",
      "\n",
      "Epoch 4 batch 0 train Loss -0.3535 test Loss -0.3533 with MSE metric 0.1750\n",
      "Epoch 4 batch 50 train Loss -0.3808 test Loss -0.3686 with MSE metric 0.1691\n",
      "Epoch 4 batch 100 train Loss -0.4077 test Loss -0.3699 with MSE metric 0.1636\n",
      "Epoch 4 batch 150 train Loss -0.4333 test Loss -0.3745 with MSE metric 0.1584\n",
      "Epoch 4 batch 200 train Loss -0.4581 test Loss -0.3841 with MSE metric 0.1536\n",
      "Epoch 4 batch 250 train Loss -0.4825 test Loss -0.3811 with MSE metric 0.1490\n",
      "Epoch 4 batch 300 train Loss -0.5017 test Loss -0.3967 with MSE metric 0.1449\n",
      "Time taken for 1 epoch: 488.10562467575073 secs\n",
      "\n",
      "Epoch 5 batch 0 train Loss -0.5066 test Loss -0.4154 with MSE metric 0.1439\n",
      "Epoch 5 batch 50 train Loss -0.5285 test Loss -0.4397 with MSE metric 0.1399\n",
      "Epoch 5 batch 100 train Loss -0.5504 test Loss -0.4562 with MSE metric 0.1361\n",
      "Epoch 5 batch 150 train Loss -0.5703 test Loss -0.4778 with MSE metric 0.1326\n",
      "Epoch 5 batch 200 train Loss -0.5903 test Loss -0.5001 with MSE metric 0.1292\n",
      "Epoch 5 batch 250 train Loss -0.6098 test Loss -0.5203 with MSE metric 0.1261\n",
      "Epoch 5 batch 300 train Loss -0.6290 test Loss -0.5424 with MSE metric 0.1230\n",
      "Time taken for 1 epoch: 487.92101526260376 secs\n",
      "\n",
      "Epoch 6 batch 0 train Loss -0.6337 test Loss -0.5668 with MSE metric 0.1223\n",
      "Epoch 6 batch 50 train Loss -0.6517 test Loss -0.5878 with MSE metric 0.1195\n",
      "Epoch 6 batch 100 train Loss -0.6688 test Loss -0.6094 with MSE metric 0.1167\n",
      "Epoch 6 batch 150 train Loss -0.6858 test Loss -0.6303 with MSE metric 0.1142\n",
      "Epoch 6 batch 200 train Loss -0.7013 test Loss -0.6510 with MSE metric 0.1117\n",
      "Epoch 6 batch 250 train Loss -0.7163 test Loss -0.6696 with MSE metric 0.1094\n",
      "Epoch 6 batch 300 train Loss -0.7313 test Loss -0.6899 with MSE metric 0.1071\n",
      "Time taken for 1 epoch: 481.3387448787689 secs\n",
      "\n",
      "Epoch 7 batch 0 train Loss -0.7350 test Loss -0.7078 with MSE metric 0.1066\n",
      "Epoch 7 batch 50 train Loss -0.7490 test Loss -0.7225 with MSE metric 0.1045\n",
      "Epoch 7 batch 100 train Loss -0.7626 test Loss -0.7399 with MSE metric 0.1024\n",
      "Epoch 7 batch 150 train Loss -0.7757 test Loss -0.7555 with MSE metric 0.1005\n",
      "Epoch 7 batch 200 train Loss -0.7883 test Loss -0.7710 with MSE metric 0.0986\n",
      "Epoch 7 batch 250 train Loss -0.8005 test Loss -0.7861 with MSE metric 0.0968\n",
      "Epoch 7 batch 300 train Loss -0.8119 test Loss -0.7997 with MSE metric 0.0951\n",
      "Time taken for 1 epoch: 474.1974232196808 secs\n",
      "\n",
      "Epoch 8 batch 0 train Loss -0.8146 test Loss -0.8128 with MSE metric 0.0947\n",
      "Epoch 8 batch 50 train Loss -0.8258 test Loss -0.8278 with MSE metric 0.0931\n",
      "Epoch 8 batch 100 train Loss -0.8371 test Loss -0.8416 with MSE metric 0.0915\n",
      "Epoch 8 batch 150 train Loss -0.8474 test Loss -0.8547 with MSE metric 0.0900\n",
      "Epoch 8 batch 200 train Loss -0.8574 test Loss -0.8682 with MSE metric 0.0885\n",
      "Epoch 8 batch 250 train Loss -0.8670 test Loss -0.8812 with MSE metric 0.0871\n",
      "Epoch 8 batch 300 train Loss -0.8765 test Loss -0.8934 with MSE metric 0.0858\n",
      "Time taken for 1 epoch: 482.363303899765 secs\n",
      "\n",
      "Epoch 9 batch 0 train Loss -0.8789 test Loss -0.9051 with MSE metric 0.0855\n",
      "Epoch 9 batch 50 train Loss -0.8876 test Loss -0.9152 with MSE metric 0.0842\n",
      "Epoch 9 batch 100 train Loss -0.8964 test Loss -0.9259 with MSE metric 0.0829\n",
      "Epoch 9 batch 150 train Loss -0.9051 test Loss -0.9375 with MSE metric 0.0817\n",
      "Epoch 9 batch 200 train Loss -0.9131 test Loss -0.9485 with MSE metric 0.0805\n",
      "Epoch 9 batch 250 train Loss -0.9212 test Loss -0.9587 with MSE metric 0.0794\n",
      "Epoch 9 batch 300 train Loss -0.9289 test Loss -0.9691 with MSE metric 0.0783\n",
      "Time taken for 1 epoch: 492.04646134376526 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = tf.summary.create_file_writer(save_dir + '/logs/')\n",
    "    optimizer_c = tf.keras.optimizers.Adam()\n",
    "    decoder = model.Decoder(16)\n",
    "    EPOCHS = 10\n",
    "    batch_s  = 128\n",
    "    run = 0; step = 0\n",
    "    num_batches = int(pad_y_fren_tr.shape[0] / batch_s)\n",
    "    tf.random.set_seed(1)    \n",
    "    checkpoint = tf.train.Checkpoint(optimizer = optimizer_c, model = decoder)\n",
    "    main_folder = \"/Users/omernivron/Downloads/GPT/ckpt/check_\"\n",
    "    folder = main_folder + str(run); helpers.mkdir(folder)\n",
    "\n",
    "    with writer.as_default():\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_n in range(num_batches):\n",
    "                batch_pos_tr, batch_tar_tr, batch_pos_mask, _ = batch_creator.create_batch_gp_mim_2(pad_pos_tr, pad_y_fren_tr, pp)\n",
    "                # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "                train_step(batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "                if batch_n % 50 == 0:\n",
    "                    batch_pos_te, batch_tar_te, batch_pos_mask_te, _ = batch_creator.create_batch_gp_mim_2(pad_pos_te, pad_y_fren_te, pp_te)\n",
    "                    test_step(batch_pos_te, batch_tar_te, batch_pos_mask_te)\n",
    "                    helpers.print_progress(epoch, batch_n, train_loss.result(), test_loss.result(), m_tr.result())\n",
    "                    helpers.tf_summaries(run, step, train_loss.result(), test_loss.result(), m_tr.result(), m_te.result())\n",
    "                    checkpoint.save(folder + '/')\n",
    "                step += 1\n",
    "\n",
    "            print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tar = pad_y_fren_te[:, 1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.0165 / sum((tar[:, 5] - np.mean(tar[:, 5]))**2) / len(tar[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar - np.mean(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tar[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((tar[:, 0] - np.mean(tar[:, 0]))**2 )/ 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum((tar - np.mean(tar))**2)) / (tar.shape[0] * tar.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[560, :].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[561, :39].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te[561, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inference(pos, tar, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with matplotlib.rc_context({'figure.figsize': [10,2.5]}):\n",
    "    plt.scatter(pos[:, :39], tar[:, :39], c='black')\n",
    "    plt.scatter(pos[:, 39:58], a[39:])\n",
    "    plt.scatter(pos[:, 39:58], df_te[561, 39:58], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
