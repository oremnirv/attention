{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn.gaussian_process as gp\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import time\n",
    "import keras\n",
    "import os\n",
    "from data_generation import *\n",
    "from batch_creator import *\n",
    "from gp_kernels import *\n",
    "from gp_priors import *\n",
    "from gp_plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp_prior(4, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_gp_mim_2(pos, tar, pos_mask, batch_s=128):\n",
    "    '''\n",
    "    Get a batch of positions, targets and position mask from data generated \n",
    "    by data_generator_for_gp_mimick_gpt function and from position_mask function \n",
    "    -------------------------\n",
    "    Parameters:\n",
    "    pos (2D np array): 1st/2nd output from data_generator_for_gp_mimick_gpt function \n",
    "    tar (2D np array): 3rd/4th output from data_generator_for_gp_mimick_gpt function  \n",
    "    pos_mask (4D np.array): output from position_mask function \n",
    "    batch_s (int): deafult 128\n",
    "    -------------------------\n",
    "    Returns:\n",
    "    batch_tar_tr (2D np array)\n",
    "    batch_pos_tr (2D np array)\n",
    "    batch_pos_mask (4D np array)\n",
    "    batch_idx_tr (1D np array): indices (=row numbers) chosen for current batch\n",
    "    \n",
    "    '''\n",
    "    shape = tar.shape[0]\n",
    "    batch_idx_tr = np.random.choice(list(range(shape)), batch_s)\n",
    "    batch_tar_tr = tar[batch_idx_tr, :]\n",
    "    batch_pos_tr = pos[batch_idx_tr, :]\n",
    "    batch_pos_mask = pos_mask[batch_idx_tr, :, :, :]\n",
    "    return batch_tar_tr, batch_pos_tr, batch_pos_mask, batch_idx_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_for_gp_mimick_gpt(num_obs, kernel, tr_percent=0.8):\n",
    "    '''\n",
    "    Generator for training a GPT inspired netowrk. Make sure x is drawn in a range that \n",
    "    Doesn't include 0 --> 0 is used for padding.\n",
    "    -----------------------\n",
    "    Parameters:\n",
    "    num_obs (int): how many observation to generate\n",
    "    kernel (function of am SKlearn kernel object): e.g. rbf_kernel which comes from gp_kernels file\n",
    "    tr_percent (float): daefult 0.8\n",
    "    -----------------------\n",
    "    Returns:\n",
    "    pad_pos_tr (np array): the first rows * tr_percent from the x generated values padded by zeros according to obs_per_sample  \n",
    "    pad_pos_te (np array): all rows of x not chosen for training \n",
    "    pad_y_fren_tr (np array): the first rows * tr_percent from the f_prior generated values padded by zeros according to obs_per_sample  \n",
    "    pad_y_fren_te (np array): all rows of f_prior not chosen for training \n",
    "    '''\n",
    "    # 59 is max_seq_len (to change, change 60 to a different number)\n",
    "    obs_per_sample = np.random.randint(20, 60, size = num_obs)\n",
    "    df = np.zeros((num_obs * 2, np.max(obs_per_sample)))\n",
    "    for i in range(0, num_obs * 2, 2):\n",
    "        x = np.random.uniform(5, 15, size=(1, obs_per_sample[int(i / 2)]))\n",
    "        k = kernel(x)\n",
    "        f_prior = generate_priors(k, obs_per_sample[int(i / 2)], 1)\n",
    "\n",
    "        df[i, :x.shape[1]] = x\n",
    "        df[i + 1, :x.shape[1]] = f_prior\n",
    "\n",
    "    rows = df.shape[0]\n",
    "    cols = df.shape[1]\n",
    "    tr_rows = int(tr_percent * rows)\n",
    "    tr_rows = tr_rows if tr_rows % 2 == 0 else tr_rows + 1\n",
    "    df_tr = df[:tr_rows, :]\n",
    "    df_te = df[tr_rows:, :]\n",
    "    \n",
    "    # get all even rows\n",
    "    pad_pos_tr = df_tr[::2, :]\n",
    "    pad_pos_te = df_te[::2, :]\n",
    "    # get all odd rows\n",
    "    pad_y_fren_tr = df_tr[1::2, :]\n",
    "    pad_y_fren_te = df_te[1::2, :]\n",
    "\n",
    "    return pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te, df_tr, df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_mask(arr):\n",
    "    '''\n",
    "    This tries to emulate the kernel matrix. \n",
    "    In the first stage we have a 2X2 matrix of ones, next\n",
    "    3X3 matrix of ones, etc.\n",
    "    -------------------------\n",
    "    Parameters:\n",
    "    arr (np array): the 1st/2nd output from data_generator_for_gp_mimick_gpt function\n",
    "    -------------------------\n",
    "    Returns:\n",
    "    mask (4D np array): if there are 100 rows and 50 cols in arr then this will \n",
    "    return [100, 50, 50, 50] array -- where the first dim is observation number \n",
    "    second dim is timestamp and third+fourth dim are the mask matrix.\n",
    "    '''\n",
    "    rows = arr.shape[0]\n",
    "    cols = arr.shape[1]\n",
    "    mask = np.zeros((rows, cols - 2, cols, cols))\n",
    "    specific = np.sum(np.equal(batch, 0), 1)\n",
    "    for i in range(2, cols + 1):\n",
    "        mask[:, i - 2, :i, :i] = np.ones((i, i))\n",
    "    for j in range(rows):\n",
    "        k  = specific[j]\n",
    "        mask[j, k:, :, :] = 0\n",
    "            \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    '''\n",
    "    Used to pad sequences that have zeros where there was no event.\n",
    "    Typically this will be combined with create_look_ahead_mask function.\n",
    "    This function is used inside an open session of tensorflow. \n",
    "    To try it out create a tf.constant tensor.\n",
    "    -------------------\n",
    "    Parameters:\n",
    "    seq (tensor): shape is (batch_size, seq_len)\n",
    "    \n",
    "    -------------------\n",
    "    Returns:\n",
    "    A binary tensor  (batch_size, 1, seq_len): 1 where there was no event and 0 otherwise.\n",
    "    \n",
    "    '''\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention. Extra dimension is used in create_masks function\n",
    "    return seq[:, tf.newaxis, :]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    '''\n",
    "    Hide future outputs from a decoder style network.\n",
    "    Used typically together with create_padding_mask function\n",
    "    -----------------------\n",
    "    Parameters:\n",
    "    size (int): max sequnce length \n",
    "    \n",
    "    -----------------------\n",
    "    Returns:\n",
    "    mask (tensor): shape is (seq_len X seq_len). Example: if size is 4, returns\n",
    "    0 1 1 1\n",
    "    0 0 1 1\n",
    "    0 0 0 1\n",
    "    0 0 0 0 \n",
    "    where 1 signifies what to hide.\n",
    "    '''\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(tar):\n",
    "    '''\n",
    "    Create unified masking hiding future from current timestamps and hiding paddings. \n",
    "    -------------------\n",
    "    Parameters: \n",
    "    tar (tensor): batch of padded target sequences \n",
    "    -------------------\n",
    "    Returns: \n",
    "    combined_mask_tar  (tensor): shape is batch_size X max_seq_len X max_seq_len\n",
    "    '''\n",
    "    \n",
    "    tar_padding_mask = create_padding_mask(tar)\n",
    "    ## this will be batch_size X 1 X 40\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    # if max seq length is 40 -- > this will be 40X40 \n",
    "    \n",
    "    \n",
    "    ## This will also be (64, 40, 40)\n",
    "    combined_mask_tar = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    \n",
    "    return combined_mask_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_pos_tr, pad_pos_te, pad_y_fren_tr, pad_y_fren_te, _, df_te = data_generator_for_gp_mimick_gpt(10000, rbf_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = position_mask(pad_pos_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    '''\n",
    "    Masked MSE. Since the target sequences are padded, \n",
    "    it is important to apply a padding mask when calculating the loss.\n",
    "    ----------------\n",
    "    Parameters:\n",
    "    real (tf.tensor float64): shape batch_size X max_seq_len. True values of sequences.\n",
    "    pred (tf.tensor float64): shape batch_size X max_seq_len. Predictions from GPT network. \n",
    "    \n",
    "    ----------------\n",
    "    Returns: \n",
    "    loss value (tf.float64)\n",
    "    '''\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.square(real - pred)\n",
    "    \n",
    "#     print('loss_ :', loss_)\n",
    "#     shape= (128X58)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_prod_position(q, k, mask):\n",
    "    '''\n",
    "    Used to create a pseudo XX^T covariance matrix for each \n",
    "    positional sequence in the batch.\n",
    "    ------------------\n",
    "    Parameters: \n",
    "    q : shape (batch_size X max_seq_len X 1). Position outptut from create_batch_gp_mim_2 function (or after another Dense layer) \n",
    "    k : shape (batch_size X max_seq_len X 1). Position outptut from create_batch_gp_mim_2 function (or after another Dense layer) \n",
    "    mask: shape (batch_size X max_seq_len X max_seq_len X max_seq_len). The positional mask created by position_mask function and selected in batch indices \n",
    "    \n",
    "    ------------------\n",
    "    Returns:\n",
    "    nl_qk (tf.tensor float64): shape (batch_size X max_seq_len X max_seq_len X max_seq_len).\n",
    "    Each observation (1st dim) has seq_len timestamps (2nd dim) and each timestamp has an associated\n",
    "    seq_len X seq_len pseudo covariance matrix (3rd & 4th dims) masked according to the timestamp.\n",
    "    \n",
    "    '''\n",
    "    matmul_qk = tf.matmul(q, k, transpose_a = True)\n",
    "    nl_qk = tf.nn.relu(matmul_qk) \n",
    "    nl_qk = nl_qk[:, tf.newaxis, :, :]\n",
    "#     print('nl_qk: ', nl_qk)\n",
    "#     shape=(128, 1, 59, 59)\n",
    "\n",
    "#     print('pos_mask:', mask)\n",
    "#     shape=(128, 59, 59, 59)\n",
    "    if mask is not None:\n",
    "        nl_qk *= (tf.cast(mask, tf.float32))\n",
    "    \n",
    "    return nl_qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q, k, v, mask):\n",
    "    '''\n",
    "    Attention inspired by Transformer (but not the same). The Transformer embeds the \n",
    "    target words to q (query), k (key), v (value). So if we have a batch of 128 sequences \n",
    "    with max length 40 and embedding layer is 20, we will get shape q = shape k = shape v\n",
    "    = (128 X  max sequence length X 20). The Transformer then transposes k (here we transpose q)\n",
    "    to get after matmul (128 X max seq X max seq) matrix. We then apply relu layer (unlike in Transformer)\n",
    "    ---------------------\n",
    "    Parameters:\n",
    "    q (tf.tensor float64): shape (batch_size, max_seq_len, 1)\n",
    "    k (tf.tensor float64): shape (batch_size, max_seq_len, 1)\n",
    "    v (tf.tensor float64): shape (batch_size, max_seq_len, 1)\n",
    "    mask (tf.tensor float64): shape (batch_size, max_seq_len, max_seq_len)\n",
    "    ---------------------\n",
    "    Returns:\n",
    "    out_tar: shape (batch_size, max_seq_len, max_seq_len). The sequences after embedding (or Dense layer) weighted by attention_weights. \n",
    "    attention_weights : shape (batch_size, max_seq_len, max_seq_len). Weights to assign for each sequence member at each timestamp (2nd dim).\n",
    "    matmul_qk: shape (batch_size, max_seq_len, max_seq_len)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # similarity\n",
    "    # q = k = v  shape := (batch_size, max_seq_len - 1, max_seq_len -1)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_a = True)\n",
    "#     print('matmul_qk: ', matmul_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "    \n",
    "    nl_qk = tf.nn.relu(matmul_qk) \n",
    "#     print('nl_qk: ', nl_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "#     nl_qk shape := (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "\n",
    "    # -1e9 will turn the softmax output in this locations to zero\n",
    "    # this is a good mask as an input for softmax -- we need also masking when \n",
    "    # want to use matmul as is \n",
    "    \n",
    "    if mask is not None:\n",
    "        nl_qk +=  (mask * -1e9)\n",
    "    \n",
    "        \n",
    "#     print('nl_qk after mask: ', nl_qk)\n",
    "#     shape=(128, 58, 58)\n",
    "        \n",
    "     # turn simialrity to scores\n",
    "    attention_weights = tf.nn.softmax(nl_qk, axis = -1)\n",
    "    \n",
    "#     print('attention_weights: ', attention_weights)\n",
    "#     shape=(128, 58, 58)\n",
    "   \n",
    "    # weight values \n",
    "    # attention_weights shape := (batch_size, max_seq_len - 1, max_seq_len - 1), \n",
    "    # v_transpose shape := max_seq_len X batch_size\n",
    "    out_tar = tf.matmul(attention_weights, v, transpose_b = True)\n",
    "    \n",
    "#   print('out_tar: ', out_tar)\n",
    "#   shape=(128, 58, 58)\n",
    "    \n",
    "    return out_tar, attention_weights, matmul_qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(dff):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(1)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, pos_d_model, tar_d_model, rate, d_att, dff):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.tar_d_model = tar_d_model\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(pos_d_model)\n",
    "        self.wk = tf.keras.layers.Dense(pos_d_model)\n",
    "        self.wv = tf.keras.layers.Dense(pos_d_model)\n",
    "                    \n",
    "        \n",
    "        self.hq = tf.keras.layers.Dense(tar_d_model)\n",
    "        self.hk = tf.keras.layers.Dense(tar_d_model)\n",
    "        self.hv = tf.keras.layers.Dense(tar_d_model)\n",
    "        \n",
    "        self.h_att = tf.keras.layers.Dense(d_att)\n",
    "        \n",
    "#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(dff)\n",
    "\n",
    "    #a call method, the layer's forward pass\n",
    "    def call(self, tar_position, tar_inp, training, pos_mask, tar_mask):\n",
    "        \n",
    "        # Adding extra dimension to allow multiplication of \n",
    "        # a sequnce with itself. \n",
    "        tar_position = tar_position[:, :, tf.newaxis]\n",
    "        \n",
    "        q_p = self.wq(tar_position) \n",
    "        k_p = self.wk(tar_position)\n",
    "#       print('q_p: ', q_p)\n",
    "#       shape=(128, 59, 59) = (batch_size X max_seq_len X max_seq_len) \n",
    "        \n",
    "        pos_attn1 = dot_prod_position(q_p, k_p, mask = pos_mask)\n",
    "#       print('pos_attn1 :', pos_attn1)\n",
    "#       shape=(128, 59, 59, 59)\n",
    "        pos_attn1 = tf.reshape(pos_attn1, shape = [tf.shape(pos_attn1)[0] ,-1])\n",
    "        # pos_attn1 is (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "    \n",
    "        tar_inp = tar_inp[:, :, tf.newaxis]\n",
    "\n",
    "        \n",
    "        q = self.hq(tar_inp) \n",
    "        k = self.hk(tar_inp)\n",
    "        v = self.hv(tar_inp)\n",
    "        \n",
    "#       print('q :', q)\n",
    "#       shape=(128, 58, 58)\n",
    "\n",
    "        tar_attn1, _, _ = dot_product_attention(q, k, v, tar_mask)\n",
    "        # tar_attn1 is (batch_size, max_seq_len - 1, max_seq_len - 1)\n",
    "\n",
    "#         print('tar_attn1 :', tar_attn1)\n",
    "#         shape=(128, 58, 58)\n",
    "        \n",
    "        position = tf.reshape(self.h_att(pos_attn1), shape = [tf.shape(pos_attn1)[0], self.tar_d_model, self.tar_d_model])\n",
    "                \n",
    "        connector = tf.matmul(position, tar_attn1) \n",
    "        \n",
    "#         print('connector :', connector)\n",
    "#         shape=(128, 58, 58)\n",
    "\n",
    "        out = tf.reshape(self.ffn(connector), shape = [tf.shape(connector)[0], tf.shape(connector)[1]])\n",
    "        \n",
    "#         print('out :', out)\n",
    "#         shape=(128, 58)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(59, 58, 0.3, 58**2, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(pos, tar, pos_mask):\n",
    "    '''\n",
    "    A typical train step function for TF2. Elements which we wish to track their gradient\n",
    "    has to be inside the GradientTape() clause. see (1) https://www.tensorflow.org/guide/migrate \n",
    "    (2) https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (np array): array of positions (x values) - the 1st/2nd output from data_generator_for_gp_mimick_gpt\n",
    "    tar (np array): array of targets. Notice that if dealing with sequnces, we typically want to have the targets go from 0 to n-1. The 3rd/4th output from data_generator_for_gp_mimick_gpt  \n",
    "    pos_mask (np array): see description in position_mask function\n",
    "    ------------------    \n",
    "    '''\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    combined_mask_tar = create_masks(tar_inp)\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = decoder(pos, tar_inp, True, pos_mask, combined_mask_tar)\n",
    "\n",
    "        loss = loss_function(tar_real, pred)\n",
    "\n",
    "    gradients = tape.gradient(loss, decoder.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 6.6077\n",
      "Epoch 1 Batch 50 Loss 3.8566\n",
      "Time taken for 1 epoch: 1088.7277348041534 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    EPOCHS = 1\n",
    "    batch_s  = 128\n",
    "    num_batches = int(pad_y_fren_tr.shape[0] / batch_s)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        train_loss.reset_states()\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            batch_tar_tr, batch_pos_tr, batch_pos_mask, _ = create_batch_gp_mim_2(pad_pos_tr, pad_y_fren_tr, pp)\n",
    "            # batch_tar_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "            # batch_pos_tr shape := 128 X 59 = (batch_size, max_seq_len)\n",
    "            train_step(batch_pos_tr, batch_tar_tr, batch_pos_mask)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, batch, train_loss.result()))\n",
    "\n",
    "        print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_te[0, :57].reshape(1, 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = df_te[1, :56].reshape(1, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(pos, tar, max_seq_len, num_steps = 1):\n",
    "    '''\n",
    "    \n",
    "    ------------------\n",
    "    Parameters:\n",
    "    pos (2D np array): (n + num_steps) positions \n",
    "    tar (2D np array): n targets \n",
    "    max_seq_len (int): this has to be the same max seq length as the trained model\n",
    "    num_steps (int): how many inference steps are required\n",
    "    ------------------\n",
    "    Returns:\n",
    "    tar \n",
    "    \n",
    "    '''\n",
    "    current_pos = pos.shape[1] \n",
    "    tar = np.concatenate((tar, np.zeros((1, max_seq_len - current_pos))), axis = 1)\n",
    "    pos = np.concatenate((pos, np.zeros((1, max_seq_len - current_pos))), axis = 1)\n",
    "    pos_mask = np.zeros((pos.shape[0], max_seq_len, max_seq_len, max_seq_len))\n",
    "    pos_mask[:, current_pos  - 2, :current_pos, :current_pos] = np.ones((current_pos, current_pos))\n",
    "    combined_mask_tar = np.ones((1, 1, max_seq_len - 1))\n",
    "    combined_mask_tar[:, :, :(current_pos - 1)] = 0\n",
    "    infer = decoder(pos, tar, False, pos_mask, combined_mask_tar)\n",
    "    print(infer)\n",
    "    tar = tf.concat((tar, tf.reshape(infer[:, current_pos - 1], [1, 1])), axis = 1)\n",
    "    if num_steps > 1:\n",
    "        inference(pos, tar, max_seq_len, num_steps - 1)\n",
    "    \n",
    "    return tar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl_qk:  tf.Tensor(\n",
      "[[[[  0.          4.427062    0.        ...   0.          0.\n",
      "    491.79285  ]\n",
      "   [  0.          1.5256025   0.        ...   0.          0.\n",
      "    169.47     ]\n",
      "   [  0.          1.503004    0.        ...   0.          0.\n",
      "    166.96419  ]\n",
      "   ...\n",
      "   [  0.          1.7961506   0.        ...   0.          0.\n",
      "    199.53403  ]\n",
      "   [ 51.10858     0.         25.943754  ...  80.389496   79.43578\n",
      "      0.       ]\n",
      "   [  0.          5.0367317   0.        ...   0.          0.\n",
      "    559.52985  ]]]], shape=(1, 1, 59, 59), dtype=float32)\n",
      "pos_mask: [[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]]\n",
      "tf.Tensor(\n",
      "[[ 0.203661    0.36735415 -0.12996577  0.30376995 -0.12448065  0.14866854\n",
      "   0.0490016  -0.04196744  0.07792129 -0.03665838  0.20971507 -0.04054343\n",
      "  -0.03573837 -0.0197856  -0.04419455  0.19836752 -0.05380202 -0.00825505\n",
      "   0.13018073  0.11194156 -0.03068871 -0.09101295 -0.04396571  0.08905517\n",
      "   0.03302398  0.16717751  0.1147763   0.3150261  -0.05466558  0.24862431\n",
      "  -0.09192622  0.11917676 -0.04906734 -0.00854765  0.1894164   0.13339756\n",
      "   0.03192396 -0.03087104  0.1665482   0.01572568 -0.03993639 -0.09662272\n",
      "  -0.13157175  0.11965099 -0.05120224 -0.08280067  0.22122614 -0.04492188\n",
      "  -0.00090495  0.21235035 -0.07534997 -0.04584189 -0.10301109  0.12550302\n",
      "   0.02076196 -0.04268137 -0.10621975 -0.08134313]], shape=(1, 58), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 59), dtype=float32, numpy=\n",
       "array([[ 0.12204104,  0.24948609,  0.26937833,  0.19292668,  0.21781027,\n",
       "         0.19172172,  0.14533263,  0.1048215 ,  0.14756879,  0.16895957,\n",
       "         0.18061471,  0.15310484,  0.13210458,  0.14132631,  0.20115127,\n",
       "         0.14773661,  0.13751109,  0.1872037 ,  0.10607509,  0.10771479,\n",
       "         0.295172  ,  0.20683539,  0.17284246,  0.14170395,  0.12530956,\n",
       "         0.02869998,  0.06135559,  0.19116488,  0.13800356,  0.09323501,\n",
       "         0.15418777,  0.16483717,  0.08807146,  0.17767818,  0.16655348,\n",
       "         0.13555975,  0.1485559 ,  0.05811815,  0.14970753,  0.10471064,\n",
       "         0.19369957,  0.17498744,  0.1516356 ,  0.13861147,  0.1436919 ,\n",
       "         0.14202945,  0.16344526,  0.23809153,  0.04674565,  0.15145893,\n",
       "         0.07398523,  0.20075752,  0.13433775,  0.06844398,  0.11045797,\n",
       "         0.15666513,  0.        ,  0.        , -0.10621975]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(pos, tar, 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset(tf.Tensor(pad_pos_tr, value_index = 0 , dtype = tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
